Base forward_time =  1.1920928955078125e-06
Base fps =  838860.8
SwinTransformer(
  (patch_embed): PatchEmbed(
    (adap_padding): AdaptivePadding()
    (projection): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (drop_after_pos): Dropout(p=0.0, inplace=False)
  (stages): ModuleList(
    (0): SwinBlockSequence(
      (blocks): ModuleList(
        (0-1): 2 x SwinBlock(
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): ShiftWindowMSA(
            (w_msa): WindowMSA(
              (qkv): Linear(in_features=96, out_features=288, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=96, out_features=96, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop): DropPath()
          )
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (ffn): FFN(
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=96, out_features=384, bias=True)
                (1): GELU(approximate='none')
                (2): Dropout(p=0.0, inplace=False)
              )
              (1): Linear(in_features=384, out_features=96, bias=True)
              (2): Dropout(p=0.0, inplace=False)
            )
            (dropout_layer): DropPath()
            (gamma2): Identity()
          )
        )
      )
      (downsample): PatchMerging(
        (adap_padding): AdaptivePadding()
        (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (reduction): Linear(in_features=384, out_features=192, bias=False)
      )
    )
    (1): SwinBlockSequence(
      (blocks): ModuleList(
        (0-1): 2 x SwinBlock(
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): ShiftWindowMSA(
            (w_msa): WindowMSA(
              (qkv): Linear(in_features=192, out_features=576, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=192, out_features=192, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop): DropPath()
          )
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (ffn): FFN(
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=192, out_features=768, bias=True)
                (1): GELU(approximate='none')
                (2): Dropout(p=0.0, inplace=False)
              )
              (1): Linear(in_features=768, out_features=192, bias=True)
              (2): Dropout(p=0.0, inplace=False)
            )
            (dropout_layer): DropPath()
            (gamma2): Identity()
          )
        )
      )
      (downsample): PatchMerging(
        (adap_padding): AdaptivePadding()
        (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (reduction): Linear(in_features=768, out_features=384, bias=False)
      )
    )
    (2): SwinBlockSequence(
      (blocks): ModuleList(
        (0-5): 6 x SwinBlock(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): ShiftWindowMSA(
            (w_msa): WindowMSA(
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop): DropPath()
          )
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (ffn): FFN(
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=384, out_features=1536, bias=True)
                (1): GELU(approximate='none')
                (2): Dropout(p=0.0, inplace=False)
              )
              (1): Linear(in_features=1536, out_features=384, bias=True)
              (2): Dropout(p=0.0, inplace=False)
            )
            (dropout_layer): DropPath()
            (gamma2): Identity()
          )
        )
      )
      (downsample): PatchMerging(
        (adap_padding): AdaptivePadding()
        (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        (reduction): Linear(in_features=1536, out_features=768, bias=False)
      )
    )
    (3): SwinBlockSequence(
      (blocks): ModuleList(
        (0-1): 2 x SwinBlock(
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): ShiftWindowMSA(
            (w_msa): WindowMSA(
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop): DropPath()
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (ffn): FFN(
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=768, out_features=3072, bias=True)
                (1): GELU(approximate='none')
                (2): Dropout(p=0.0, inplace=False)
              )
              (1): Linear(in_features=3072, out_features=768, bias=True)
              (2): Dropout(p=0.0, inplace=False)
            )
            (dropout_layer): DropPath()
            (gamma2): Identity()
          )
        )
      )
    )
  )
  (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
>>>>>>>>>>>>>>>>_freeze_stages =  -1
-----NEW SwinTransformer FORWARD PASS 



		SwinTransformer input shape:  torch.Size([1, 3, 800, 1333])


------------SwinBlockSequence -  torch.Size([1, 66800, 96])
with_cp =  False
x.requires_grad =  False
ShiftWindowMSA = query view: B, L, C=  1 66800 96
	 hw_shape =  (200, 334)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([1392, 49, 96])
B=1392, N=49, C=96
the qkv mod: 96 288 Linear(in_features=96, out_features=288, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([1392, 49, 288])
NEW C= 96
qkv.shape =  torch.Size([1392, 49, 288])
qkv.flatten.shape =  torch.Size([19643904])
self.embed_dims =  96
+++++ reshaping ....  1392 49 3 self.num_heads= 3 32
1392 49 3 3 32  =  19643904
self.num_heads =  3
C =  96
qkv after reshape  torch.Size([1392, 49, 3, 3, 32])
qkv after permute  torch.Size([3, 1392, 3, 49, 32])
		qkv.grad_fn >>>>>>  None
--proj input shape =  torch.Size([1392, 49, 96])
--proj output shape =  torch.Size([1392, 49, 96])
------------WindowMSA - x_shape Output torch.Size([1392, 49, 96]) None
attn_windows.shape before merge =  torch.Size([1392, 49, 96])
x.shape before view =  torch.Size([1, 200, 334, 96])
ShiftWindowMSA = before view B, H * W, C =  1 66800 96
------------ShiftWindowMSA -  None
------------SwinBlock -  None
		block output:  torch.Size([1, 66800, 96])
------------SwinBlockSequence -  torch.Size([1, 66800, 96])
with_cp =  False
x.requires_grad =  False
ShiftWindowMSA = query view: B, L, C=  1 66800 96
	 hw_shape =  (200, 334)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([1392, 49, 96])
B=1392, N=49, C=96
the qkv mod: 96 288 Linear(in_features=96, out_features=288, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([1392, 49, 288])
NEW C= 96
qkv.shape =  torch.Size([1392, 49, 288])
qkv.flatten.shape =  torch.Size([19643904])
self.embed_dims =  96
+++++ reshaping ....  1392 49 3 self.num_heads= 3 32
1392 49 3 3 32  =  19643904
self.num_heads =  3
C =  96
qkv after reshape  torch.Size([1392, 49, 3, 3, 32])
qkv after permute  torch.Size([3, 1392, 3, 49, 32])
		qkv.grad_fn >>>>>>  None
--proj input shape =  torch.Size([1392, 49, 96])
--proj output shape =  torch.Size([1392, 49, 96])
------------WindowMSA - x_shape Output torch.Size([1392, 49, 96]) None
attn_windows.shape before merge =  torch.Size([1392, 49, 96])
x.shape before view =  torch.Size([1, 200, 334, 96])
ShiftWindowMSA = before view B, H * W, C =  1 66800 96
------------ShiftWindowMSA -  None
------------SwinBlock -  None
		block output:  torch.Size([1, 66800, 96])
------------SwinBlockSequence/downsample -  torch.Size([1, 16700, 192]) None
down_hw_shape =  (100, 167)
x.shape =  torch.Size([1, 16700, 192])
hw_shape =  (100, 167)
stage out.shape =  torch.Size([1, 66800, 96])
out_hw_shape =  (200, 334)
stage =  0  / self.out_indices =  (1, 2, 3)
self.num_features =  [96, 192, 384, 768]
------------SwinBlockSequence -  torch.Size([1, 16700, 192])
with_cp =  False
x.requires_grad =  False
ShiftWindowMSA = query view: B, L, C=  1 16700 192
	 hw_shape =  (100, 167)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([360, 49, 192])
B=360, N=49, C=192
the qkv mod: 192 576 Linear(in_features=192, out_features=576, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([360, 49, 576])
NEW C= 192
qkv.shape =  torch.Size([360, 49, 576])
qkv.flatten.shape =  torch.Size([10160640])
self.embed_dims =  192
+++++ reshaping ....  360 49 3 self.num_heads= 6 32
360 49 3 6 32  =  10160640
self.num_heads =  6
C =  192
qkv after reshape  torch.Size([360, 49, 3, 6, 32])
qkv after permute  torch.Size([3, 360, 6, 49, 32])
		qkv.grad_fn >>>>>>  None
--proj input shape =  torch.Size([360, 49, 192])
--proj output shape =  torch.Size([360, 49, 192])
------------WindowMSA - x_shape Output torch.Size([360, 49, 192]) None
attn_windows.shape before merge =  torch.Size([360, 49, 192])
x.shape before view =  torch.Size([1, 100, 167, 192])
ShiftWindowMSA = before view B, H * W, C =  1 16700 192
------------ShiftWindowMSA -  None
------------SwinBlock -  None
		block output:  torch.Size([1, 16700, 192])
------------SwinBlockSequence -  torch.Size([1, 16700, 192])
with_cp =  False
x.requires_grad =  False
ShiftWindowMSA = query view: B, L, C=  1 16700 192
	 hw_shape =  (100, 167)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([360, 49, 192])
B=360, N=49, C=192
the qkv mod: 192 576 Linear(in_features=192, out_features=576, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([360, 49, 576])
NEW C= 192
qkv.shape =  torch.Size([360, 49, 576])
qkv.flatten.shape =  torch.Size([10160640])
self.embed_dims =  192
+++++ reshaping ....  360 49 3 self.num_heads= 6 32
360 49 3 6 32  =  10160640
self.num_heads =  6
C =  192
qkv after reshape  torch.Size([360, 49, 3, 6, 32])
qkv after permute  torch.Size([3, 360, 6, 49, 32])
		qkv.grad_fn >>>>>>  None
--proj input shape =  torch.Size([360, 49, 192])
--proj output shape =  torch.Size([360, 49, 192])
------------WindowMSA - x_shape Output torch.Size([360, 49, 192]) None
attn_windows.shape before merge =  torch.Size([360, 49, 192])
x.shape before view =  torch.Size([1, 100, 167, 192])
ShiftWindowMSA = before view B, H * W, C =  1 16700 192
------------ShiftWindowMSA -  None
------------SwinBlock -  None
		block output:  torch.Size([1, 16700, 192])
------------SwinBlockSequence/downsample -  torch.Size([1, 4200, 384]) None
down_hw_shape =  (50, 84)
x.shape =  torch.Size([1, 4200, 384])
hw_shape =  (50, 84)
stage out.shape =  torch.Size([1, 16700, 192])
out_hw_shape =  (100, 167)
stage =  1  / self.out_indices =  (1, 2, 3)
self.num_features =  [96, 192, 384, 768]
stage:  1  --- norm_layer out.shape =  torch.Size([1, 16700, 192])
torch.Size([1, 192, 100, 167])
------------SwinBlockSequence -  torch.Size([1, 4200, 384])
with_cp =  False
x.requires_grad =  False
ShiftWindowMSA = query view: B, L, C=  1 4200 384
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 384])
B=96, N=49, C=384
the qkv mod: 384 1152 Linear(in_features=384, out_features=1152, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 1152])
NEW C= 384
qkv.shape =  torch.Size([96, 49, 1152])
qkv.flatten.shape =  torch.Size([5419008])
self.embed_dims =  384
+++++ reshaping ....  96 49 3 self.num_heads= 12 32
96 49 3 12 32  =  5419008
self.num_heads =  12
C =  384
qkv after reshape  torch.Size([96, 49, 3, 12, 32])
qkv after permute  torch.Size([3, 96, 12, 49, 32])
		qkv.grad_fn >>>>>>  None
--proj input shape =  torch.Size([96, 49, 384])
--proj output shape =  torch.Size([96, 49, 384])
------------WindowMSA - x_shape Output torch.Size([96, 49, 384]) None
attn_windows.shape before merge =  torch.Size([96, 49, 384])
x.shape before view =  torch.Size([1, 50, 84, 384])
ShiftWindowMSA = before view B, H * W, C =  1 4200 384
------------ShiftWindowMSA -  None
------------SwinBlock -  None
		block output:  torch.Size([1, 4200, 384])
------------SwinBlockSequence -  torch.Size([1, 4200, 384])
with_cp =  False
x.requires_grad =  False
ShiftWindowMSA = query view: B, L, C=  1 4200 384
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 384])
B=96, N=49, C=384
the qkv mod: 384 1152 Linear(in_features=384, out_features=1152, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 1152])
NEW C= 384
qkv.shape =  torch.Size([96, 49, 1152])
qkv.flatten.shape =  torch.Size([5419008])
self.embed_dims =  384
+++++ reshaping ....  96 49 3 self.num_heads= 12 32
96 49 3 12 32  =  5419008
self.num_heads =  12
C =  384
qkv after reshape  torch.Size([96, 49, 3, 12, 32])
qkv after permute  torch.Size([3, 96, 12, 49, 32])
		qkv.grad_fn >>>>>>  None
--proj input shape =  torch.Size([96, 49, 384])
--proj output shape =  torch.Size([96, 49, 384])
------------WindowMSA - x_shape Output torch.Size([96, 49, 384]) None
attn_windows.shape before merge =  torch.Size([96, 49, 384])
x.shape before view =  torch.Size([1, 50, 84, 384])
ShiftWindowMSA = before view B, H * W, C =  1 4200 384
------------ShiftWindowMSA -  None
------------SwinBlock -  None
		block output:  torch.Size([1, 4200, 384])
------------SwinBlockSequence -  torch.Size([1, 4200, 384])
with_cp =  False
x.requires_grad =  False
ShiftWindowMSA = query view: B, L, C=  1 4200 384
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 384])
B=96, N=49, C=384
the qkv mod: 384 1152 Linear(in_features=384, out_features=1152, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 1152])
NEW C= 384
qkv.shape =  torch.Size([96, 49, 1152])
qkv.flatten.shape =  torch.Size([5419008])
self.embed_dims =  384
+++++ reshaping ....  96 49 3 self.num_heads= 12 32
96 49 3 12 32  =  5419008
self.num_heads =  12
C =  384
qkv after reshape  torch.Size([96, 49, 3, 12, 32])
qkv after permute  torch.Size([3, 96, 12, 49, 32])
		qkv.grad_fn >>>>>>  None
--proj input shape =  torch.Size([96, 49, 384])
--proj output shape =  torch.Size([96, 49, 384])
------------WindowMSA - x_shape Output torch.Size([96, 49, 384]) None
attn_windows.shape before merge =  torch.Size([96, 49, 384])
x.shape before view =  torch.Size([1, 50, 84, 384])
ShiftWindowMSA = before view B, H * W, C =  1 4200 384
------------ShiftWindowMSA -  None
------------SwinBlock -  None
		block output:  torch.Size([1, 4200, 384])
------------SwinBlockSequence -  torch.Size([1, 4200, 384])
with_cp =  False
x.requires_grad =  False
ShiftWindowMSA = query view: B, L, C=  1 4200 384
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 384])
B=96, N=49, C=384
the qkv mod: 384 1152 Linear(in_features=384, out_features=1152, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 1152])
NEW C= 384
qkv.shape =  torch.Size([96, 49, 1152])
qkv.flatten.shape =  torch.Size([5419008])
self.embed_dims =  384
+++++ reshaping ....  96 49 3 self.num_heads= 12 32
96 49 3 12 32  =  5419008
self.num_heads =  12
C =  384
qkv after reshape  torch.Size([96, 49, 3, 12, 32])
qkv after permute  torch.Size([3, 96, 12, 49, 32])
		qkv.grad_fn >>>>>>  None
--proj input shape =  torch.Size([96, 49, 384])
--proj output shape =  torch.Size([96, 49, 384])
------------WindowMSA - x_shape Output torch.Size([96, 49, 384]) None
attn_windows.shape before merge =  torch.Size([96, 49, 384])
x.shape before view =  torch.Size([1, 50, 84, 384])
ShiftWindowMSA = before view B, H * W, C =  1 4200 384
------------ShiftWindowMSA -  None
------------SwinBlock -  None
		block output:  torch.Size([1, 4200, 384])
------------SwinBlockSequence -  torch.Size([1, 4200, 384])
with_cp =  False
x.requires_grad =  False
ShiftWindowMSA = query view: B, L, C=  1 4200 384
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 384])
B=96, N=49, C=384
the qkv mod: 384 1152 Linear(in_features=384, out_features=1152, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 1152])
NEW C= 384
qkv.shape =  torch.Size([96, 49, 1152])
qkv.flatten.shape =  torch.Size([5419008])
self.embed_dims =  384
+++++ reshaping ....  96 49 3 self.num_heads= 12 32
96 49 3 12 32  =  5419008
self.num_heads =  12
C =  384
qkv after reshape  torch.Size([96, 49, 3, 12, 32])
qkv after permute  torch.Size([3, 96, 12, 49, 32])
		qkv.grad_fn >>>>>>  None
--proj input shape =  torch.Size([96, 49, 384])
--proj output shape =  torch.Size([96, 49, 384])
------------WindowMSA - x_shape Output torch.Size([96, 49, 384]) None
attn_windows.shape before merge =  torch.Size([96, 49, 384])
x.shape before view =  torch.Size([1, 50, 84, 384])
ShiftWindowMSA = before view B, H * W, C =  1 4200 384
------------ShiftWindowMSA -  None
------------SwinBlock -  None
		block output:  torch.Size([1, 4200, 384])
------------SwinBlockSequence -  torch.Size([1, 4200, 384])
with_cp =  False
x.requires_grad =  False
ShiftWindowMSA = query view: B, L, C=  1 4200 384
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 384])
B=96, N=49, C=384
the qkv mod: 384 1152 Linear(in_features=384, out_features=1152, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 1152])
NEW C= 384
qkv.shape =  torch.Size([96, 49, 1152])
qkv.flatten.shape =  torch.Size([5419008])
self.embed_dims =  384
+++++ reshaping ....  96 49 3 self.num_heads= 12 32
96 49 3 12 32  =  5419008
self.num_heads =  12
C =  384
qkv after reshape  torch.Size([96, 49, 3, 12, 32])
qkv after permute  torch.Size([3, 96, 12, 49, 32])
		qkv.grad_fn >>>>>>  None
--proj input shape =  torch.Size([96, 49, 384])
--proj output shape =  torch.Size([96, 49, 384])
------------WindowMSA - x_shape Output torch.Size([96, 49, 384]) None
attn_windows.shape before merge =  torch.Size([96, 49, 384])
x.shape before view =  torch.Size([1, 50, 84, 384])
ShiftWindowMSA = before view B, H * W, C =  1 4200 384
------------ShiftWindowMSA -  None
------------SwinBlock -  None
		block output:  torch.Size([1, 4200, 384])
------------SwinBlockSequence/downsample -  torch.Size([1, 1050, 768]) None
down_hw_shape =  (25, 42)
x.shape =  torch.Size([1, 1050, 768])
hw_shape =  (25, 42)
stage out.shape =  torch.Size([1, 4200, 384])
out_hw_shape =  (50, 84)
stage =  2  / self.out_indices =  (1, 2, 3)
self.num_features =  [96, 192, 384, 768]
stage:  2  --- norm_layer out.shape =  torch.Size([1, 4200, 384])
torch.Size([1, 384, 50, 84])
------------SwinBlockSequence -  torch.Size([1, 1050, 768])
with_cp =  False
x.requires_grad =  False
ShiftWindowMSA = query view: B, L, C=  1 1050 768
	 hw_shape =  (25, 42)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([24, 49, 768])
B=24, N=49, C=768
the qkv mod: 768 2304 Linear(in_features=768, out_features=2304, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([24, 49, 2304])
NEW C= 768
qkv.shape =  torch.Size([24, 49, 2304])
qkv.flatten.shape =  torch.Size([2709504])
self.embed_dims =  768
+++++ reshaping ....  24 49 3 self.num_heads= 24 32
24 49 3 24 32  =  2709504
self.num_heads =  24
C =  768
qkv after reshape  torch.Size([24, 49, 3, 24, 32])
qkv after permute  torch.Size([3, 24, 24, 49, 32])
		qkv.grad_fn >>>>>>  None
--proj input shape =  torch.Size([24, 49, 768])
--proj output shape =  torch.Size([24, 49, 768])
------------WindowMSA - x_shape Output torch.Size([24, 49, 768]) None
attn_windows.shape before merge =  torch.Size([24, 49, 768])
x.shape before view =  torch.Size([1, 25, 42, 768])
ShiftWindowMSA = before view B, H * W, C =  1 1050 768
------------ShiftWindowMSA -  None
------------SwinBlock -  None
		block output:  torch.Size([1, 1050, 768])
------------SwinBlockSequence -  torch.Size([1, 1050, 768])
with_cp =  False
x.requires_grad =  False
ShiftWindowMSA = query view: B, L, C=  1 1050 768
	 hw_shape =  (25, 42)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([24, 49, 768])
B=24, N=49, C=768
the qkv mod: 768 2304 Linear(in_features=768, out_features=2304, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([24, 49, 2304])
NEW C= 768
qkv.shape =  torch.Size([24, 49, 2304])
qkv.flatten.shape =  torch.Size([2709504])
self.embed_dims =  768
+++++ reshaping ....  24 49 3 self.num_heads= 24 32
24 49 3 24 32  =  2709504
self.num_heads =  24
C =  768
qkv after reshape  torch.Size([24, 49, 3, 24, 32])
qkv after permute  torch.Size([3, 24, 24, 49, 32])
		qkv.grad_fn >>>>>>  None
--proj input shape =  torch.Size([24, 49, 768])
--proj output shape =  torch.Size([24, 49, 768])
------------WindowMSA - x_shape Output torch.Size([24, 49, 768]) None
attn_windows.shape before merge =  torch.Size([24, 49, 768])
x.shape before view =  torch.Size([1, 25, 42, 768])
ShiftWindowMSA = before view B, H * W, C =  1 1050 768
------------ShiftWindowMSA -  None
------------SwinBlock -  None
		block output:  torch.Size([1, 1050, 768])
x.shape =  torch.Size([1, 1050, 768])
hw_shape =  (25, 42)
stage out.shape =  torch.Size([1, 1050, 768])
out_hw_shape =  (25, 42)
stage =  3  / self.out_indices =  (1, 2, 3)
self.num_features =  [96, 192, 384, 768]
stage:  3  --- norm_layer out.shape =  torch.Size([1, 1050, 768])
torch.Size([1, 768, 25, 42])
------------SwinTransformer -  [None, None, None]



Base Macs: 52.954591 M, Base Params: 27.520506 M
m.num_attention_heads ===  3
m.num_attention_heads ===  3
m.num_attention_heads ===  6
m.num_attention_heads ===  6
m.num_attention_heads ===  12
m.num_attention_heads ===  12
m.num_attention_heads ===  12
m.num_attention_heads ===  12
m.num_attention_heads ===  12
m.num_attention_heads ===  12
m.num_attention_heads ===  24
m.num_attention_heads ===  24
			IGNORING:  AdaptivePadding()
			IGNORING:  Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
			IGNORING:  LayerNorm((384,), eps=1e-05, elementwise_affine=True)
			IGNORING:  Linear(in_features=384, out_features=192, bias=False)
			IGNORING:  AdaptivePadding()
			IGNORING:  Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
			IGNORING:  LayerNorm((768,), eps=1e-05, elementwise_affine=True)
			IGNORING:  Linear(in_features=768, out_features=384, bias=False)
			IGNORING:  AdaptivePadding()
			IGNORING:  Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
			IGNORING:  LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
			IGNORING:  Linear(in_features=1536, out_features=768, bias=False)
			IGNORING:  Linear(in_features=96, out_features=288, bias=True)
			IGNORING:  Dropout(p=0.0, inplace=False)
			IGNORING:  Linear(in_features=96, out_features=96, bias=True)
			IGNORING:  Dropout(p=0.0, inplace=False)
			IGNORING:  Softmax(dim=-1)
			IGNORING:  Linear(in_features=96, out_features=288, bias=True)
			IGNORING:  Dropout(p=0.0, inplace=False)
			IGNORING:  Linear(in_features=96, out_features=96, bias=True)
			IGNORING:  Dropout(p=0.0, inplace=False)
			IGNORING:  Softmax(dim=-1)
			IGNORING:  Linear(in_features=192, out_features=576, bias=True)
			IGNORING:  Dropout(p=0.0, inplace=False)
			IGNORING:  Linear(in_features=192, out_features=192, bias=True)
			IGNORING:  Dropout(p=0.0, inplace=False)
			IGNORING:  Softmax(dim=-1)
			IGNORING:  Linear(in_features=192, out_features=576, bias=True)
			IGNORING:  Dropout(p=0.0, inplace=False)
			IGNORING:  Linear(in_features=192, out_features=192, bias=True)
			IGNORING:  Dropout(p=0.0, inplace=False)
			IGNORING:  Softmax(dim=-1)
			IGNORING:  Linear(in_features=384, out_features=1152, bias=True)
			IGNORING:  Dropout(p=0.0, inplace=False)
			IGNORING:  Linear(in_features=384, out_features=384, bias=True)
			IGNORING:  Dropout(p=0.0, inplace=False)
			IGNORING:  Softmax(dim=-1)
			IGNORING:  Linear(in_features=384, out_features=1152, bias=True)
			IGNORING:  Dropout(p=0.0, inplace=False)
			IGNORING:  Linear(in_features=384, out_features=384, bias=True)
			IGNORING:  Dropout(p=0.0, inplace=False)
			IGNORING:  Softmax(dim=-1)
			IGNORING:  Linear(in_features=384, out_features=1152, bias=True)
			IGNORING:  Dropout(p=0.0, inplace=False)
			IGNORING:  Linear(in_features=384, out_features=384, bias=True)
			IGNORING:  Dropout(p=0.0, inplace=False)
			IGNORING:  Softmax(dim=-1)
			IGNORING:  Linear(in_features=384, out_features=1152, bias=True)
			IGNORING:  Dropout(p=0.0, inplace=False)
			IGNORING:  Linear(in_features=384, out_features=384, bias=True)
			IGNORING:  Dropout(p=0.0, inplace=False)
			IGNORING:  Softmax(dim=-1)
			IGNORING:  Linear(in_features=384, out_features=1152, bias=True)
			IGNORING:  Dropout(p=0.0, inplace=False)
			IGNORING:  Linear(in_features=384, out_features=384, bias=True)
			IGNORING:  Dropout(p=0.0, inplace=False)
			IGNORING:  Softmax(dim=-1)
			IGNORING:  Linear(in_features=384, out_features=1152, bias=True)
			IGNORING:  Dropout(p=0.0, inplace=False)
			IGNORING:  Linear(in_features=384, out_features=384, bias=True)
			IGNORING:  Dropout(p=0.0, inplace=False)
			IGNORING:  Softmax(dim=-1)
			IGNORING:  Linear(in_features=768, out_features=2304, bias=True)
			IGNORING:  Dropout(p=0.0, inplace=False)
			IGNORING:  Linear(in_features=768, out_features=768, bias=True)
			IGNORING:  Dropout(p=0.0, inplace=False)
			IGNORING:  Softmax(dim=-1)
			IGNORING:  Linear(in_features=768, out_features=2304, bias=True)
			IGNORING:  Dropout(p=0.0, inplace=False)
			IGNORING:  Linear(in_features=768, out_features=768, bias=True)
			IGNORING:  Dropout(p=0.0, inplace=False)
			IGNORING:  Softmax(dim=-1)
self.IGNORED_LAYERS_IN_TRACING =  72 [AdaptivePadding(), Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2)), LayerNorm((384,), eps=1e-05, elementwise_affine=True), Linear(in_features=384, out_features=192, bias=False), AdaptivePadding(), Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2)), LayerNorm((768,), eps=1e-05, elementwise_affine=True), Linear(in_features=768, out_features=384, bias=False), AdaptivePadding(), Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2)), LayerNorm((1536,), eps=1e-05, elementwise_affine=True), Linear(in_features=1536, out_features=768, bias=False), Linear(in_features=96, out_features=288, bias=True), Dropout(p=0.0, inplace=False), Linear(in_features=96, out_features=96, bias=True), Dropout(p=0.0, inplace=False), Softmax(dim=-1), Linear(in_features=96, out_features=288, bias=True), Dropout(p=0.0, inplace=False), Linear(in_features=96, out_features=96, bias=True), Dropout(p=0.0, inplace=False), Softmax(dim=-1), Linear(in_features=192, out_features=576, bias=True), Dropout(p=0.0, inplace=False), Linear(in_features=192, out_features=192, bias=True), Dropout(p=0.0, inplace=False), Softmax(dim=-1), Linear(in_features=192, out_features=576, bias=True), Dropout(p=0.0, inplace=False), Linear(in_features=192, out_features=192, bias=True), Dropout(p=0.0, inplace=False), Softmax(dim=-1), Linear(in_features=384, out_features=1152, bias=True), Dropout(p=0.0, inplace=False), Linear(in_features=384, out_features=384, bias=True), Dropout(p=0.0, inplace=False), Softmax(dim=-1), Linear(in_features=384, out_features=1152, bias=True), Dropout(p=0.0, inplace=False), Linear(in_features=384, out_features=384, bias=True), Dropout(p=0.0, inplace=False), Softmax(dim=-1), Linear(in_features=384, out_features=1152, bias=True), Dropout(p=0.0, inplace=False), Linear(in_features=384, out_features=384, bias=True), Dropout(p=0.0, inplace=False), Softmax(dim=-1), Linear(in_features=384, out_features=1152, bias=True), Dropout(p=0.0, inplace=False), Linear(in_features=384, out_features=384, bias=True), Dropout(p=0.0, inplace=False), Softmax(dim=-1), Linear(in_features=384, out_features=1152, bias=True), Dropout(p=0.0, inplace=False), Linear(in_features=384, out_features=384, bias=True), Dropout(p=0.0, inplace=False), Softmax(dim=-1), Linear(in_features=384, out_features=1152, bias=True), Dropout(p=0.0, inplace=False), Linear(in_features=384, out_features=384, bias=True), Dropout(p=0.0, inplace=False), Softmax(dim=-1), Linear(in_features=768, out_features=2304, bias=True), Dropout(p=0.0, inplace=False), Linear(in_features=768, out_features=768, bias=True), Dropout(p=0.0, inplace=False), Softmax(dim=-1), Linear(in_features=768, out_features=2304, bias=True), Dropout(p=0.0, inplace=False), Linear(in_features=768, out_features=768, bias=True), Dropout(p=0.0, inplace=False), Softmax(dim=-1)]
patch_embed.projection.weight True
patch_embed.projection.bias True
patch_embed.norm.weight True
patch_embed.norm.bias True
stages.0.blocks.0.norm1.weight True
stages.0.blocks.0.norm1.bias True
stages.0.blocks.0.attn.w_msa.relative_position_bias_table True
stages.0.blocks.0.attn.w_msa.qkv.weight True
stages.0.blocks.0.attn.w_msa.qkv.bias True
stages.0.blocks.0.attn.w_msa.proj.weight True
stages.0.blocks.0.attn.w_msa.proj.bias True
stages.0.blocks.0.norm2.weight True
stages.0.blocks.0.norm2.bias True
stages.0.blocks.0.ffn.layers.0.0.weight True
stages.0.blocks.0.ffn.layers.0.0.bias True
stages.0.blocks.0.ffn.layers.1.weight True
stages.0.blocks.0.ffn.layers.1.bias True
stages.0.blocks.1.norm1.weight True
stages.0.blocks.1.norm1.bias True
stages.0.blocks.1.attn.w_msa.relative_position_bias_table True
stages.0.blocks.1.attn.w_msa.qkv.weight True
stages.0.blocks.1.attn.w_msa.qkv.bias True
stages.0.blocks.1.attn.w_msa.proj.weight True
stages.0.blocks.1.attn.w_msa.proj.bias True
stages.0.blocks.1.norm2.weight True
stages.0.blocks.1.norm2.bias True
stages.0.blocks.1.ffn.layers.0.0.weight True
stages.0.blocks.1.ffn.layers.0.0.bias True
stages.0.blocks.1.ffn.layers.1.weight True
stages.0.blocks.1.ffn.layers.1.bias True
stages.0.downsample.norm.weight True
stages.0.downsample.norm.bias True
stages.0.downsample.reduction.weight True
stages.1.blocks.0.norm1.weight True
stages.1.blocks.0.norm1.bias True
stages.1.blocks.0.attn.w_msa.relative_position_bias_table True
stages.1.blocks.0.attn.w_msa.qkv.weight True
stages.1.blocks.0.attn.w_msa.qkv.bias True
stages.1.blocks.0.attn.w_msa.proj.weight True
stages.1.blocks.0.attn.w_msa.proj.bias True
stages.1.blocks.0.norm2.weight True
stages.1.blocks.0.norm2.bias True
stages.1.blocks.0.ffn.layers.0.0.weight True
stages.1.blocks.0.ffn.layers.0.0.bias True
stages.1.blocks.0.ffn.layers.1.weight True
stages.1.blocks.0.ffn.layers.1.bias True
stages.1.blocks.1.norm1.weight True
stages.1.blocks.1.norm1.bias True
stages.1.blocks.1.attn.w_msa.relative_position_bias_table True
stages.1.blocks.1.attn.w_msa.qkv.weight True
stages.1.blocks.1.attn.w_msa.qkv.bias True
stages.1.blocks.1.attn.w_msa.proj.weight True
stages.1.blocks.1.attn.w_msa.proj.bias True
stages.1.blocks.1.norm2.weight True
stages.1.blocks.1.norm2.bias True
stages.1.blocks.1.ffn.layers.0.0.weight True
stages.1.blocks.1.ffn.layers.0.0.bias True
stages.1.blocks.1.ffn.layers.1.weight True
stages.1.blocks.1.ffn.layers.1.bias True
stages.1.downsample.norm.weight True
stages.1.downsample.norm.bias True
stages.1.downsample.reduction.weight True
stages.2.blocks.0.norm1.weight True
stages.2.blocks.0.norm1.bias True
stages.2.blocks.0.attn.w_msa.relative_position_bias_table True
stages.2.blocks.0.attn.w_msa.qkv.weight True
stages.2.blocks.0.attn.w_msa.qkv.bias True
stages.2.blocks.0.attn.w_msa.proj.weight True
stages.2.blocks.0.attn.w_msa.proj.bias True
stages.2.blocks.0.norm2.weight True
stages.2.blocks.0.norm2.bias True
stages.2.blocks.0.ffn.layers.0.0.weight True
stages.2.blocks.0.ffn.layers.0.0.bias True
stages.2.blocks.0.ffn.layers.1.weight True
stages.2.blocks.0.ffn.layers.1.bias True
stages.2.blocks.1.norm1.weight True
stages.2.blocks.1.norm1.bias True
stages.2.blocks.1.attn.w_msa.relative_position_bias_table True
stages.2.blocks.1.attn.w_msa.qkv.weight True
stages.2.blocks.1.attn.w_msa.qkv.bias True
stages.2.blocks.1.attn.w_msa.proj.weight True
stages.2.blocks.1.attn.w_msa.proj.bias True
stages.2.blocks.1.norm2.weight True
stages.2.blocks.1.norm2.bias True
stages.2.blocks.1.ffn.layers.0.0.weight True
stages.2.blocks.1.ffn.layers.0.0.bias True
stages.2.blocks.1.ffn.layers.1.weight True
stages.2.blocks.1.ffn.layers.1.bias True
stages.2.blocks.2.norm1.weight True
stages.2.blocks.2.norm1.bias True
stages.2.blocks.2.attn.w_msa.relative_position_bias_table True
stages.2.blocks.2.attn.w_msa.qkv.weight True
stages.2.blocks.2.attn.w_msa.qkv.bias True
stages.2.blocks.2.attn.w_msa.proj.weight True
stages.2.blocks.2.attn.w_msa.proj.bias True
stages.2.blocks.2.norm2.weight True
stages.2.blocks.2.norm2.bias True
stages.2.blocks.2.ffn.layers.0.0.weight True
stages.2.blocks.2.ffn.layers.0.0.bias True
stages.2.blocks.2.ffn.layers.1.weight True
stages.2.blocks.2.ffn.layers.1.bias True
stages.2.blocks.3.norm1.weight True
stages.2.blocks.3.norm1.bias True
stages.2.blocks.3.attn.w_msa.relative_position_bias_table True
stages.2.blocks.3.attn.w_msa.qkv.weight True
stages.2.blocks.3.attn.w_msa.qkv.bias True
stages.2.blocks.3.attn.w_msa.proj.weight True
stages.2.blocks.3.attn.w_msa.proj.bias True
stages.2.blocks.3.norm2.weight True
stages.2.blocks.3.norm2.bias True
stages.2.blocks.3.ffn.layers.0.0.weight True
stages.2.blocks.3.ffn.layers.0.0.bias True
stages.2.blocks.3.ffn.layers.1.weight True
stages.2.blocks.3.ffn.layers.1.bias True
stages.2.blocks.4.norm1.weight True
stages.2.blocks.4.norm1.bias True
stages.2.blocks.4.attn.w_msa.relative_position_bias_table True
stages.2.blocks.4.attn.w_msa.qkv.weight True
stages.2.blocks.4.attn.w_msa.qkv.bias True
stages.2.blocks.4.attn.w_msa.proj.weight True
stages.2.blocks.4.attn.w_msa.proj.bias True
stages.2.blocks.4.norm2.weight True
stages.2.blocks.4.norm2.bias True
stages.2.blocks.4.ffn.layers.0.0.weight True
stages.2.blocks.4.ffn.layers.0.0.bias True
stages.2.blocks.4.ffn.layers.1.weight True
stages.2.blocks.4.ffn.layers.1.bias True
stages.2.blocks.5.norm1.weight True
stages.2.blocks.5.norm1.bias True
stages.2.blocks.5.attn.w_msa.relative_position_bias_table True
stages.2.blocks.5.attn.w_msa.qkv.weight True
stages.2.blocks.5.attn.w_msa.qkv.bias True
stages.2.blocks.5.attn.w_msa.proj.weight True
stages.2.blocks.5.attn.w_msa.proj.bias True
stages.2.blocks.5.norm2.weight True
stages.2.blocks.5.norm2.bias True
stages.2.blocks.5.ffn.layers.0.0.weight True
stages.2.blocks.5.ffn.layers.0.0.bias True
stages.2.blocks.5.ffn.layers.1.weight True
stages.2.blocks.5.ffn.layers.1.bias True
stages.2.downsample.norm.weight True
stages.2.downsample.norm.bias True
stages.2.downsample.reduction.weight True
stages.3.blocks.0.norm1.weight True
stages.3.blocks.0.norm1.bias True
stages.3.blocks.0.attn.w_msa.relative_position_bias_table True
stages.3.blocks.0.attn.w_msa.qkv.weight True
stages.3.blocks.0.attn.w_msa.qkv.bias True
stages.3.blocks.0.attn.w_msa.proj.weight True
stages.3.blocks.0.attn.w_msa.proj.bias True
stages.3.blocks.0.norm2.weight True
stages.3.blocks.0.norm2.bias True
stages.3.blocks.0.ffn.layers.0.0.weight True
stages.3.blocks.0.ffn.layers.0.0.bias True
stages.3.blocks.0.ffn.layers.1.weight True
stages.3.blocks.0.ffn.layers.1.bias True
stages.3.blocks.1.norm1.weight True
stages.3.blocks.1.norm1.bias True
stages.3.blocks.1.attn.w_msa.relative_position_bias_table True
stages.3.blocks.1.attn.w_msa.qkv.weight True
stages.3.blocks.1.attn.w_msa.qkv.bias True
stages.3.blocks.1.attn.w_msa.proj.weight True
stages.3.blocks.1.attn.w_msa.proj.bias True
stages.3.blocks.1.norm2.weight True
stages.3.blocks.1.norm2.bias True
stages.3.blocks.1.ffn.layers.0.0.weight True
stages.3.blocks.1.ffn.layers.0.0.bias True
stages.3.blocks.1.ffn.layers.1.weight True
stages.3.blocks.1.ffn.layers.1.bias True
norm1.weight True
norm1.bias True
norm2.weight True
norm2.bias True
norm3.weight True
norm3.bias True
self._param_to_name =  dict_values([])
>>>>>>>>>>>>>>>>_freeze_stages =  -1
[Passed] {<class 'torch.nn.modules.conv.Conv2d'>, <class 'mmcv.cnn.bricks.wrappers.Linear'>, <class 'mmdet.models.backbones.swin.WindowMSA'>, <class 'torch.nn.modules.normalization.LayerNorm'>, <class 'mmdet.models.layers.transformer.utils.PatchMerging'>}
[Failed] {<class 'mmengine.model.base_module.ModuleList'>, <class 'torch.nn.modules.activation.Softmax'>, <class 'torch.nn.modules.fold.Unfold'>, <class 'mmdet.models.layers.transformer.utils.AdaptivePadding'>, <class 'mmengine.model.base_module.Sequential'>, <class 'torch.nn.modules.normalization.LayerNorm'>, <class 'mmdet.models.backbones.swin.SwinTransformer'>, <class 'torch.nn.modules.activation.GELU'>, <class 'torch.nn.modules.dropout.Dropout'>, <class 'mmdet.models.backbones.swin.SwinBlock'>, <class 'mmdet.models.backbones.swin.ShiftWindowMSA'>, <class 'mmdet.models.layers.transformer.utils.PatchEmbed'>, <class 'mmcv.cnn.bricks.drop.DropPath'>, <class 'mmcv.cnn.bricks.transformer.FFN'>, <class 'mmdet.models.backbones.swin.SwinBlockSequence'>, <class 'torch.nn.modules.linear.Linear'>, <class 'torch.nn.modules.linear.Identity'>}
self.IGNORED_LAYERS_IN_TRACING =  [AdaptivePadding(), Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2)), LayerNorm((384,), eps=1e-05, elementwise_affine=True), Linear(in_features=384, out_features=192, bias=False), AdaptivePadding(), Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2)), LayerNorm((768,), eps=1e-05, elementwise_affine=True), Linear(in_features=768, out_features=384, bias=False), AdaptivePadding(), Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2)), LayerNorm((1536,), eps=1e-05, elementwise_affine=True), Linear(in_features=1536, out_features=768, bias=False), Linear(in_features=96, out_features=288, bias=True), Dropout(p=0.0, inplace=False), Linear(in_features=96, out_features=96, bias=True), Dropout(p=0.0, inplace=False), Softmax(dim=-1), Linear(in_features=96, out_features=288, bias=True), Dropout(p=0.0, inplace=False), Linear(in_features=96, out_features=96, bias=True), Dropout(p=0.0, inplace=False), Softmax(dim=-1), Linear(in_features=192, out_features=576, bias=True), Dropout(p=0.0, inplace=False), Linear(in_features=192, out_features=192, bias=True), Dropout(p=0.0, inplace=False), Softmax(dim=-1), Linear(in_features=192, out_features=576, bias=True), Dropout(p=0.0, inplace=False), Linear(in_features=192, out_features=192, bias=True), Dropout(p=0.0, inplace=False), Softmax(dim=-1), Linear(in_features=384, out_features=1152, bias=True), Dropout(p=0.0, inplace=False), Linear(in_features=384, out_features=384, bias=True), Dropout(p=0.0, inplace=False), Softmax(dim=-1), Linear(in_features=384, out_features=1152, bias=True), Dropout(p=0.0, inplace=False), Linear(in_features=384, out_features=384, bias=True), Dropout(p=0.0, inplace=False), Softmax(dim=-1), Linear(in_features=384, out_features=1152, bias=True), Dropout(p=0.0, inplace=False), Linear(in_features=384, out_features=384, bias=True), Dropout(p=0.0, inplace=False), Softmax(dim=-1), Linear(in_features=384, out_features=1152, bias=True), Dropout(p=0.0, inplace=False), Linear(in_features=384, out_features=384, bias=True), Dropout(p=0.0, inplace=False), Softmax(dim=-1), Linear(in_features=384, out_features=1152, bias=True), Dropout(p=0.0, inplace=False), Linear(in_features=384, out_features=384, bias=True), Dropout(p=0.0, inplace=False), Softmax(dim=-1), Linear(in_features=384, out_features=1152, bias=True), Dropout(p=0.0, inplace=False), Linear(in_features=384, out_features=384, bias=True), Dropout(p=0.0, inplace=False), Softmax(dim=-1), Linear(in_features=768, out_features=2304, bias=True), Dropout(p=0.0, inplace=False), Linear(in_features=768, out_features=768, bias=True), Dropout(p=0.0, inplace=False), Softmax(dim=-1), Linear(in_features=768, out_features=2304, bias=True), Dropout(p=0.0, inplace=False), Linear(in_features=768, out_features=768, bias=True), Dropout(p=0.0, inplace=False), Softmax(dim=-1)]
registered_types =  (<class 'torch.nn.modules.conv._ConvNd'>, <class 'torch.nn.modules.linear.Linear'>, <class 'torch.nn.modules.batchnorm._BatchNorm'>, <class 'torch.nn.modules.conv._ConvNd'>, <class 'torch.nn.modules.activation.PReLU'>, <class 'torch.nn.modules.normalization.LayerNorm'>, <class 'torch.nn.modules.sparse.Embedding'>, <class 'torch.nn.parameter.Parameter'>, <class 'torch.nn.modules.activation.MultiheadAttention'>, <class 'torch.nn.modules.rnn.LSTM'>, <class 'torch.nn.modules.normalization.GroupNorm'>, <class 'torch.nn.modules.instancenorm._InstanceNorm'>, <class 'torch_pruning.ops._ConcatOp'>, <class 'torch_pruning.ops._SplitOp'>, <class 'torch_pruning.ops._ElementWiseOp'>, <class 'torch_pruning.ops._ElementWiseOp'>, <class 'torch_pruning.ops._ElementWiseOp'>, <class 'torch_pruning.ops._CustomizedOp'>, <class 'mmdet.models.layers.transformer.utils.PatchMerging'>, <class 'mmdet.models.backbones.swin.WindowMSA'>)
len hooks =  68
pre forward - gradfn2module =  0 dict_keys([])
-----NEW SwinTransformer FORWARD PASS 



		SwinTransformer input shape:  torch.Size([3, 800, 1333])


_record_grad_fn <class 'torch.nn.modules.conv.Conv2d'>, True, grad_fn: <SqueezeBackward1 object at 0x7f6604b8fee0>
-----NEW SwinTransformer FORWARD PASS 



		SwinTransformer input shape:  torch.Size([1, 3, 800, 1333])


_record_grad_fn <class 'torch.nn.modules.conv.Conv2d'>, True, grad_fn: <ConvolutionBackward0 object at 0x7f6604b8ff10>
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f6604b8ff40>
------------SwinBlockSequence -  torch.Size([1, 66800, 96])
with_cp =  False
x.requires_grad =  True
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f6604b8ffd0>
ShiftWindowMSA = query view: B, L, C=  1 66800 96
	 hw_shape =  (200, 334)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([1392, 49, 96])
B=1392, N=49, C=96
the qkv mod: 96 288 Linear(in_features=96, out_features=288, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([1392, 49, 288])
NEW C= 96
qkv.shape =  torch.Size([1392, 49, 288])
qkv.flatten.shape =  torch.Size([19643904])
self.embed_dims =  96
+++++ reshaping ....  1392 49 3 self.num_heads= 3 32
1392 49 3 3 32  =  19643904
self.num_heads =  3
C =  96
qkv after reshape  torch.Size([1392, 49, 3, 3, 32])
qkv after permute  torch.Size([3, 1392, 3, 49, 32])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f6604b8ffa0>
--proj input shape =  torch.Size([1392, 49, 96])
--proj output shape =  torch.Size([1392, 49, 96])
------------WindowMSA - x_shape Output torch.Size([1392, 49, 96]) <UnsafeViewBackward0 object at 0x7f6604b8ffa0>
_record_grad_fn <class 'mmdet.models.backbones.swin.WindowMSA'>, True, grad_fn: <UnsafeViewBackward0 object at 0x7f6604b8b040>
attn_windows.shape before merge =  torch.Size([1392, 49, 96])
x.shape before view =  torch.Size([1, 200, 334, 96])
ShiftWindowMSA = before view B, H * W, C =  1 66800 96
------------ShiftWindowMSA -  <ViewBackward0 object at 0x7f6604b8ffa0>
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f6604b8b070>
_record_grad_fn <class 'mmcv.cnn.bricks.wrappers.Linear'>, True, grad_fn: <ViewBackward0 object at 0x7f6604b8b0a0>
_record_grad_fn <class 'mmcv.cnn.bricks.wrappers.Linear'>, True, grad_fn: <ViewBackward0 object at 0x7f6604b8b0d0>
------------SwinBlock -  <AddBackward0 object at 0x7f6604b8ffa0>
		block output:  torch.Size([1, 66800, 96])
------------SwinBlockSequence -  torch.Size([1, 66800, 96])
with_cp =  False
x.requires_grad =  True
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f6604b8b100>
ShiftWindowMSA = query view: B, L, C=  1 66800 96
	 hw_shape =  (200, 334)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([1392, 49, 96])
B=1392, N=49, C=96
the qkv mod: 96 288 Linear(in_features=96, out_features=288, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([1392, 49, 288])
NEW C= 96
qkv.shape =  torch.Size([1392, 49, 288])
qkv.flatten.shape =  torch.Size([19643904])
self.embed_dims =  96
+++++ reshaping ....  1392 49 3 self.num_heads= 3 32
1392 49 3 3 32  =  19643904
self.num_heads =  3
C =  96
qkv after reshape  torch.Size([1392, 49, 3, 3, 32])
qkv after permute  torch.Size([3, 1392, 3, 49, 32])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f6604b8ffa0>
--proj input shape =  torch.Size([1392, 49, 96])
--proj output shape =  torch.Size([1392, 49, 96])
------------WindowMSA - x_shape Output torch.Size([1392, 49, 96]) <UnsafeViewBackward0 object at 0x7f6604b8ffa0>
_record_grad_fn <class 'mmdet.models.backbones.swin.WindowMSA'>, True, grad_fn: <UnsafeViewBackward0 object at 0x7f6604b8b130>
attn_windows.shape before merge =  torch.Size([1392, 49, 96])
x.shape before view =  torch.Size([1, 200, 334, 96])
ShiftWindowMSA = before view B, H * W, C =  1 66800 96
------------ShiftWindowMSA -  <ViewBackward0 object at 0x7f6604b8ffa0>
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f6604b8b160>
_record_grad_fn <class 'mmcv.cnn.bricks.wrappers.Linear'>, True, grad_fn: <ViewBackward0 object at 0x7f6604b8b190>
_record_grad_fn <class 'mmcv.cnn.bricks.wrappers.Linear'>, True, grad_fn: <ViewBackward0 object at 0x7f6604b8b1c0>
------------SwinBlock -  <AddBackward0 object at 0x7f6604b8ffa0>
		block output:  torch.Size([1, 66800, 96])
_record_grad_fn <class 'mmdet.models.layers.transformer.utils.PatchMerging'>, True, grad_fn: <UnsafeViewBackward0 object at 0x7f6604b8ff70>
------------SwinBlockSequence/downsample -  torch.Size([1, 16700, 192]) <UnsafeViewBackward0 object at 0x7f6604b8ff70>
down_hw_shape =  (100, 167)
x.shape =  torch.Size([1, 16700, 192])
hw_shape =  (100, 167)
stage out.shape =  torch.Size([1, 66800, 96])
out_hw_shape =  (200, 334)
stage =  0  / self.out_indices =  (1, 2, 3)
self.num_features =  [96, 192, 384, 768]
------------SwinBlockSequence -  torch.Size([1, 16700, 192])
with_cp =  False
x.requires_grad =  True
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f6604b8b220>
ShiftWindowMSA = query view: B, L, C=  1 16700 192
	 hw_shape =  (100, 167)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([360, 49, 192])
B=360, N=49, C=192
the qkv mod: 192 576 Linear(in_features=192, out_features=576, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([360, 49, 576])
NEW C= 192
qkv.shape =  torch.Size([360, 49, 576])
qkv.flatten.shape =  torch.Size([10160640])
self.embed_dims =  192
+++++ reshaping ....  360 49 3 self.num_heads= 6 32
360 49 3 6 32  =  10160640
self.num_heads =  6
C =  192
qkv after reshape  torch.Size([360, 49, 3, 6, 32])
qkv after permute  torch.Size([3, 360, 6, 49, 32])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f6604b8b1c0>
--proj input shape =  torch.Size([360, 49, 192])
--proj output shape =  torch.Size([360, 49, 192])
------------WindowMSA - x_shape Output torch.Size([360, 49, 192]) <UnsafeViewBackward0 object at 0x7f6604b8b1c0>
_record_grad_fn <class 'mmdet.models.backbones.swin.WindowMSA'>, True, grad_fn: <UnsafeViewBackward0 object at 0x7f6604b8b250>
attn_windows.shape before merge =  torch.Size([360, 49, 192])
x.shape before view =  torch.Size([1, 100, 167, 192])
ShiftWindowMSA = before view B, H * W, C =  1 16700 192
------------ShiftWindowMSA -  <ViewBackward0 object at 0x7f6604b8b1c0>
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f6604b8b280>
_record_grad_fn <class 'mmcv.cnn.bricks.wrappers.Linear'>, True, grad_fn: <ViewBackward0 object at 0x7f6604b8b2b0>
_record_grad_fn <class 'mmcv.cnn.bricks.wrappers.Linear'>, True, grad_fn: <ViewBackward0 object at 0x7f6604b8b2e0>
------------SwinBlock -  <AddBackward0 object at 0x7f6604b8b1c0>
		block output:  torch.Size([1, 16700, 192])
------------SwinBlockSequence -  torch.Size([1, 16700, 192])
with_cp =  False
x.requires_grad =  True
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f6604b8b310>
ShiftWindowMSA = query view: B, L, C=  1 16700 192
	 hw_shape =  (100, 167)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([360, 49, 192])
B=360, N=49, C=192
the qkv mod: 192 576 Linear(in_features=192, out_features=576, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([360, 49, 576])
NEW C= 192
qkv.shape =  torch.Size([360, 49, 576])
qkv.flatten.shape =  torch.Size([10160640])
self.embed_dims =  192
+++++ reshaping ....  360 49 3 self.num_heads= 6 32
360 49 3 6 32  =  10160640
self.num_heads =  6
C =  192
qkv after reshape  torch.Size([360, 49, 3, 6, 32])
qkv after permute  torch.Size([3, 360, 6, 49, 32])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f6604b8b1c0>
--proj input shape =  torch.Size([360, 49, 192])
--proj output shape =  torch.Size([360, 49, 192])
------------WindowMSA - x_shape Output torch.Size([360, 49, 192]) <UnsafeViewBackward0 object at 0x7f6604b8b1c0>
_record_grad_fn <class 'mmdet.models.backbones.swin.WindowMSA'>, True, grad_fn: <UnsafeViewBackward0 object at 0x7f6604b8b340>
attn_windows.shape before merge =  torch.Size([360, 49, 192])
x.shape before view =  torch.Size([1, 100, 167, 192])
ShiftWindowMSA = before view B, H * W, C =  1 16700 192
------------ShiftWindowMSA -  <ViewBackward0 object at 0x7f6604b8b1c0>
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f6604b8b370>
_record_grad_fn <class 'mmcv.cnn.bricks.wrappers.Linear'>, True, grad_fn: <ViewBackward0 object at 0x7f6604b8b3a0>
_record_grad_fn <class 'mmcv.cnn.bricks.wrappers.Linear'>, True, grad_fn: <ViewBackward0 object at 0x7f6604b8b3d0>
------------SwinBlock -  <AddBackward0 object at 0x7f6604b8b1c0>
		block output:  torch.Size([1, 16700, 192])
_record_grad_fn <class 'mmdet.models.layers.transformer.utils.PatchMerging'>, True, grad_fn: <UnsafeViewBackward0 object at 0x7f6604b8ffa0>
------------SwinBlockSequence/downsample -  torch.Size([1, 4200, 384]) <UnsafeViewBackward0 object at 0x7f6604b8ffa0>
down_hw_shape =  (50, 84)
x.shape =  torch.Size([1, 4200, 384])
hw_shape =  (50, 84)
stage out.shape =  torch.Size([1, 16700, 192])
out_hw_shape =  (100, 167)
stage =  1  / self.out_indices =  (1, 2, 3)
self.num_features =  [96, 192, 384, 768]
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f6604b8b1c0>
stage:  1  --- norm_layer out.shape =  torch.Size([1, 16700, 192])
torch.Size([1, 192, 100, 167])
------------SwinBlockSequence -  torch.Size([1, 4200, 384])
with_cp =  False
x.requires_grad =  True
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f6604b8b460>
ShiftWindowMSA = query view: B, L, C=  1 4200 384
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 384])
B=96, N=49, C=384
the qkv mod: 384 1152 Linear(in_features=384, out_features=1152, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 1152])
NEW C= 384
qkv.shape =  torch.Size([96, 49, 1152])
qkv.flatten.shape =  torch.Size([5419008])
self.embed_dims =  384
+++++ reshaping ....  96 49 3 self.num_heads= 12 32
96 49 3 12 32  =  5419008
self.num_heads =  12
C =  384
qkv after reshape  torch.Size([96, 49, 3, 12, 32])
qkv after permute  torch.Size([3, 96, 12, 49, 32])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f6604b8b430>
--proj input shape =  torch.Size([96, 49, 384])
--proj output shape =  torch.Size([96, 49, 384])
------------WindowMSA - x_shape Output torch.Size([96, 49, 384]) <UnsafeViewBackward0 object at 0x7f6604b8b430>
_record_grad_fn <class 'mmdet.models.backbones.swin.WindowMSA'>, True, grad_fn: <UnsafeViewBackward0 object at 0x7f6604b8b490>
attn_windows.shape before merge =  torch.Size([96, 49, 384])
x.shape before view =  torch.Size([1, 50, 84, 384])
ShiftWindowMSA = before view B, H * W, C =  1 4200 384
------------ShiftWindowMSA -  <ViewBackward0 object at 0x7f6604b8b430>
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f6604b8b4c0>
_record_grad_fn <class 'mmcv.cnn.bricks.wrappers.Linear'>, True, grad_fn: <ViewBackward0 object at 0x7f6604b8b4f0>
_record_grad_fn <class 'mmcv.cnn.bricks.wrappers.Linear'>, True, grad_fn: <ViewBackward0 object at 0x7f6604b8b520>
------------SwinBlock -  <AddBackward0 object at 0x7f6604b8b430>
		block output:  torch.Size([1, 4200, 384])
------------SwinBlockSequence -  torch.Size([1, 4200, 384])
with_cp =  False
x.requires_grad =  True
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f6604b8b550>
ShiftWindowMSA = query view: B, L, C=  1 4200 384
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 384])
B=96, N=49, C=384
the qkv mod: 384 1152 Linear(in_features=384, out_features=1152, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 1152])
NEW C= 384
qkv.shape =  torch.Size([96, 49, 1152])
qkv.flatten.shape =  torch.Size([5419008])
self.embed_dims =  384
+++++ reshaping ....  96 49 3 self.num_heads= 12 32
96 49 3 12 32  =  5419008
self.num_heads =  12
C =  384
qkv after reshape  torch.Size([96, 49, 3, 12, 32])
qkv after permute  torch.Size([3, 96, 12, 49, 32])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f6604b8b430>
--proj input shape =  torch.Size([96, 49, 384])
--proj output shape =  torch.Size([96, 49, 384])
------------WindowMSA - x_shape Output torch.Size([96, 49, 384]) <UnsafeViewBackward0 object at 0x7f6604b8b430>
_record_grad_fn <class 'mmdet.models.backbones.swin.WindowMSA'>, True, grad_fn: <UnsafeViewBackward0 object at 0x7f6604b8b580>
attn_windows.shape before merge =  torch.Size([96, 49, 384])
x.shape before view =  torch.Size([1, 50, 84, 384])
ShiftWindowMSA = before view B, H * W, C =  1 4200 384
------------ShiftWindowMSA -  <ViewBackward0 object at 0x7f6604b8b430>
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f6604b8b5b0>
_record_grad_fn <class 'mmcv.cnn.bricks.wrappers.Linear'>, True, grad_fn: <ViewBackward0 object at 0x7f6604b8b5e0>
_record_grad_fn <class 'mmcv.cnn.bricks.wrappers.Linear'>, True, grad_fn: <ViewBackward0 object at 0x7f6604b8b610>
------------SwinBlock -  <AddBackward0 object at 0x7f6604b8b430>
		block output:  torch.Size([1, 4200, 384])
------------SwinBlockSequence -  torch.Size([1, 4200, 384])
with_cp =  False
x.requires_grad =  True
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f6604b8b640>
ShiftWindowMSA = query view: B, L, C=  1 4200 384
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 384])
B=96, N=49, C=384
the qkv mod: 384 1152 Linear(in_features=384, out_features=1152, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 1152])
NEW C= 384
qkv.shape =  torch.Size([96, 49, 1152])
qkv.flatten.shape =  torch.Size([5419008])
self.embed_dims =  384
+++++ reshaping ....  96 49 3 self.num_heads= 12 32
96 49 3 12 32  =  5419008
self.num_heads =  12
C =  384
qkv after reshape  torch.Size([96, 49, 3, 12, 32])
qkv after permute  torch.Size([3, 96, 12, 49, 32])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f6604b8b430>
--proj input shape =  torch.Size([96, 49, 384])
--proj output shape =  torch.Size([96, 49, 384])
------------WindowMSA - x_shape Output torch.Size([96, 49, 384]) <UnsafeViewBackward0 object at 0x7f6604b8b430>
_record_grad_fn <class 'mmdet.models.backbones.swin.WindowMSA'>, True, grad_fn: <UnsafeViewBackward0 object at 0x7f6604b8b670>
attn_windows.shape before merge =  torch.Size([96, 49, 384])
x.shape before view =  torch.Size([1, 50, 84, 384])
ShiftWindowMSA = before view B, H * W, C =  1 4200 384
------------ShiftWindowMSA -  <ViewBackward0 object at 0x7f6604b8b430>
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f6604b8b6a0>
_record_grad_fn <class 'mmcv.cnn.bricks.wrappers.Linear'>, True, grad_fn: <ViewBackward0 object at 0x7f6604b8b6d0>
_record_grad_fn <class 'mmcv.cnn.bricks.wrappers.Linear'>, True, grad_fn: <ViewBackward0 object at 0x7f6604b8b700>
------------SwinBlock -  <AddBackward0 object at 0x7f6604b8b430>
		block output:  torch.Size([1, 4200, 384])
------------SwinBlockSequence -  torch.Size([1, 4200, 384])
with_cp =  False
x.requires_grad =  True
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f6604b8b730>
ShiftWindowMSA = query view: B, L, C=  1 4200 384
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 384])
B=96, N=49, C=384
the qkv mod: 384 1152 Linear(in_features=384, out_features=1152, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 1152])
NEW C= 384
qkv.shape =  torch.Size([96, 49, 1152])
qkv.flatten.shape =  torch.Size([5419008])
self.embed_dims =  384
+++++ reshaping ....  96 49 3 self.num_heads= 12 32
96 49 3 12 32  =  5419008
self.num_heads =  12
C =  384
qkv after reshape  torch.Size([96, 49, 3, 12, 32])
qkv after permute  torch.Size([3, 96, 12, 49, 32])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f6604b8b430>
--proj input shape =  torch.Size([96, 49, 384])
--proj output shape =  torch.Size([96, 49, 384])
------------WindowMSA - x_shape Output torch.Size([96, 49, 384]) <UnsafeViewBackward0 object at 0x7f6604b8b430>
_record_grad_fn <class 'mmdet.models.backbones.swin.WindowMSA'>, True, grad_fn: <UnsafeViewBackward0 object at 0x7f6604b8b760>
attn_windows.shape before merge =  torch.Size([96, 49, 384])
x.shape before view =  torch.Size([1, 50, 84, 384])
ShiftWindowMSA = before view B, H * W, C =  1 4200 384
------------ShiftWindowMSA -  <ViewBackward0 object at 0x7f6604b8b430>
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f6604b8b790>
_record_grad_fn <class 'mmcv.cnn.bricks.wrappers.Linear'>, True, grad_fn: <ViewBackward0 object at 0x7f6604b8b7c0>
_record_grad_fn <class 'mmcv.cnn.bricks.wrappers.Linear'>, True, grad_fn: <ViewBackward0 object at 0x7f6604b8b7f0>
------------SwinBlock -  <AddBackward0 object at 0x7f6604b8b430>
		block output:  torch.Size([1, 4200, 384])
------------SwinBlockSequence -  torch.Size([1, 4200, 384])
with_cp =  False
x.requires_grad =  True
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f6604b8b820>
ShiftWindowMSA = query view: B, L, C=  1 4200 384
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 384])
B=96, N=49, C=384
the qkv mod: 384 1152 Linear(in_features=384, out_features=1152, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 1152])
NEW C= 384
qkv.shape =  torch.Size([96, 49, 1152])
qkv.flatten.shape =  torch.Size([5419008])
self.embed_dims =  384
+++++ reshaping ....  96 49 3 self.num_heads= 12 32
96 49 3 12 32  =  5419008
self.num_heads =  12
C =  384
qkv after reshape  torch.Size([96, 49, 3, 12, 32])
qkv after permute  torch.Size([3, 96, 12, 49, 32])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f6604b8b430>
--proj input shape =  torch.Size([96, 49, 384])
--proj output shape =  torch.Size([96, 49, 384])
------------WindowMSA - x_shape Output torch.Size([96, 49, 384]) <UnsafeViewBackward0 object at 0x7f6604b8b430>
_record_grad_fn <class 'mmdet.models.backbones.swin.WindowMSA'>, True, grad_fn: <UnsafeViewBackward0 object at 0x7f6604b8b850>
attn_windows.shape before merge =  torch.Size([96, 49, 384])
x.shape before view =  torch.Size([1, 50, 84, 384])
ShiftWindowMSA = before view B, H * W, C =  1 4200 384
------------ShiftWindowMSA -  <ViewBackward0 object at 0x7f6604b8b430>
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f6604b8b880>
_record_grad_fn <class 'mmcv.cnn.bricks.wrappers.Linear'>, True, grad_fn: <ViewBackward0 object at 0x7f6604b8b8b0>
_record_grad_fn <class 'mmcv.cnn.bricks.wrappers.Linear'>, True, grad_fn: <ViewBackward0 object at 0x7f6604b8b8e0>
------------SwinBlock -  <AddBackward0 object at 0x7f6604b8b430>
		block output:  torch.Size([1, 4200, 384])
------------SwinBlockSequence -  torch.Size([1, 4200, 384])
with_cp =  False
x.requires_grad =  True
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f6604b8b910>
ShiftWindowMSA = query view: B, L, C=  1 4200 384
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 384])
B=96, N=49, C=384
the qkv mod: 384 1152 Linear(in_features=384, out_features=1152, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 1152])
NEW C= 384
qkv.shape =  torch.Size([96, 49, 1152])
qkv.flatten.shape =  torch.Size([5419008])
self.embed_dims =  384
+++++ reshaping ....  96 49 3 self.num_heads= 12 32
96 49 3 12 32  =  5419008
self.num_heads =  12
C =  384
qkv after reshape  torch.Size([96, 49, 3, 12, 32])
qkv after permute  torch.Size([3, 96, 12, 49, 32])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f6604b8b430>
--proj input shape =  torch.Size([96, 49, 384])
--proj output shape =  torch.Size([96, 49, 384])
------------WindowMSA - x_shape Output torch.Size([96, 49, 384]) <UnsafeViewBackward0 object at 0x7f6604b8b430>
_record_grad_fn <class 'mmdet.models.backbones.swin.WindowMSA'>, True, grad_fn: <UnsafeViewBackward0 object at 0x7f6604b8b940>
attn_windows.shape before merge =  torch.Size([96, 49, 384])
x.shape before view =  torch.Size([1, 50, 84, 384])
ShiftWindowMSA = before view B, H * W, C =  1 4200 384
------------ShiftWindowMSA -  <ViewBackward0 object at 0x7f6604b8b430>
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f6604b8b970>
_record_grad_fn <class 'mmcv.cnn.bricks.wrappers.Linear'>, True, grad_fn: <ViewBackward0 object at 0x7f6604b8b9a0>
_record_grad_fn <class 'mmcv.cnn.bricks.wrappers.Linear'>, True, grad_fn: <ViewBackward0 object at 0x7f6604b8b9d0>
------------SwinBlock -  <AddBackward0 object at 0x7f6604b8b430>
		block output:  torch.Size([1, 4200, 384])
_record_grad_fn <class 'mmdet.models.layers.transformer.utils.PatchMerging'>, True, grad_fn: <UnsafeViewBackward0 object at 0x7f6604b8b400>
------------SwinBlockSequence/downsample -  torch.Size([1, 1050, 768]) <UnsafeViewBackward0 object at 0x7f6604b8b400>
down_hw_shape =  (25, 42)
x.shape =  torch.Size([1, 1050, 768])
hw_shape =  (25, 42)
stage out.shape =  torch.Size([1, 4200, 384])
out_hw_shape =  (50, 84)
stage =  2  / self.out_indices =  (1, 2, 3)
self.num_features =  [96, 192, 384, 768]
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f6604b8b430>
stage:  2  --- norm_layer out.shape =  torch.Size([1, 4200, 384])
torch.Size([1, 384, 50, 84])
------------SwinBlockSequence -  torch.Size([1, 1050, 768])
with_cp =  False
x.requires_grad =  True
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f6604b8ba60>
ShiftWindowMSA = query view: B, L, C=  1 1050 768
	 hw_shape =  (25, 42)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([24, 49, 768])
B=24, N=49, C=768
the qkv mod: 768 2304 Linear(in_features=768, out_features=2304, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([24, 49, 2304])
NEW C= 768
qkv.shape =  torch.Size([24, 49, 2304])
qkv.flatten.shape =  torch.Size([2709504])
self.embed_dims =  768
+++++ reshaping ....  24 49 3 self.num_heads= 24 32
24 49 3 24 32  =  2709504
self.num_heads =  24
C =  768
qkv after reshape  torch.Size([24, 49, 3, 24, 32])
qkv after permute  torch.Size([3, 24, 24, 49, 32])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f6604b8ba30>
--proj input shape =  torch.Size([24, 49, 768])
--proj output shape =  torch.Size([24, 49, 768])
------------WindowMSA - x_shape Output torch.Size([24, 49, 768]) <UnsafeViewBackward0 object at 0x7f6604b8ba30>
_record_grad_fn <class 'mmdet.models.backbones.swin.WindowMSA'>, True, grad_fn: <UnsafeViewBackward0 object at 0x7f6604b8ba90>
attn_windows.shape before merge =  torch.Size([24, 49, 768])
x.shape before view =  torch.Size([1, 25, 42, 768])
ShiftWindowMSA = before view B, H * W, C =  1 1050 768
------------ShiftWindowMSA -  <ViewBackward0 object at 0x7f6604b8ba30>
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f6604b8bac0>
_record_grad_fn <class 'mmcv.cnn.bricks.wrappers.Linear'>, True, grad_fn: <ViewBackward0 object at 0x7f6604b8baf0>
_record_grad_fn <class 'mmcv.cnn.bricks.wrappers.Linear'>, True, grad_fn: <ViewBackward0 object at 0x7f6604b8bb20>
------------SwinBlock -  <AddBackward0 object at 0x7f6604b8ba30>
		block output:  torch.Size([1, 1050, 768])
------------SwinBlockSequence -  torch.Size([1, 1050, 768])
with_cp =  False
x.requires_grad =  True
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f6604b8bb50>
ShiftWindowMSA = query view: B, L, C=  1 1050 768
	 hw_shape =  (25, 42)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([24, 49, 768])
B=24, N=49, C=768
the qkv mod: 768 2304 Linear(in_features=768, out_features=2304, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([24, 49, 2304])
NEW C= 768
qkv.shape =  torch.Size([24, 49, 2304])
qkv.flatten.shape =  torch.Size([2709504])
self.embed_dims =  768
+++++ reshaping ....  24 49 3 self.num_heads= 24 32
24 49 3 24 32  =  2709504
self.num_heads =  24
C =  768
qkv after reshape  torch.Size([24, 49, 3, 24, 32])
qkv after permute  torch.Size([3, 24, 24, 49, 32])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f6604b8ba30>
--proj input shape =  torch.Size([24, 49, 768])
--proj output shape =  torch.Size([24, 49, 768])
------------WindowMSA - x_shape Output torch.Size([24, 49, 768]) <UnsafeViewBackward0 object at 0x7f6604b8ba30>
_record_grad_fn <class 'mmdet.models.backbones.swin.WindowMSA'>, True, grad_fn: <UnsafeViewBackward0 object at 0x7f6604b8bb80>
attn_windows.shape before merge =  torch.Size([24, 49, 768])
x.shape before view =  torch.Size([1, 25, 42, 768])
ShiftWindowMSA = before view B, H * W, C =  1 1050 768
------------ShiftWindowMSA -  <ViewBackward0 object at 0x7f6604b8ba30>
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f6604b8bbb0>
_record_grad_fn <class 'mmcv.cnn.bricks.wrappers.Linear'>, True, grad_fn: <ViewBackward0 object at 0x7f6604b8bbe0>
_record_grad_fn <class 'mmcv.cnn.bricks.wrappers.Linear'>, True, grad_fn: <ViewBackward0 object at 0x7f6604b8bc10>
------------SwinBlock -  <AddBackward0 object at 0x7f6604b8ba30>
		block output:  torch.Size([1, 1050, 768])
x.shape =  torch.Size([1, 1050, 768])
hw_shape =  (25, 42)
stage out.shape =  torch.Size([1, 1050, 768])
out_hw_shape =  (25, 42)
stage =  3  / self.out_indices =  (1, 2, 3)
self.num_features =  [96, 192, 384, 768]
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f6604b8ba00>
stage:  3  --- norm_layer out.shape =  torch.Size([1, 1050, 768])
torch.Size([1, 768, 25, 42])
------------SwinTransformer -  [<CloneBackward0 object at 0x7f6604b8ba30>, <CloneBackward0 object at 0x7f6604b8bc40>, <CloneBackward0 object at 0x7f6604b8bc70>]




visited =  {Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4)): 2, LayerNorm((96,), eps=1e-05, elementwise_affine=True): 1, LayerNorm((96,), eps=1e-05, elementwise_affine=True): 1, WindowMSA(
  (qkv): Linear(in_features=96, out_features=288, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=96, out_features=96, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
): 1, LayerNorm((96,), eps=1e-05, elementwise_affine=True): 1, Linear(in_features=96, out_features=384, bias=True): 1, Linear(in_features=384, out_features=96, bias=True): 1, LayerNorm((96,), eps=1e-05, elementwise_affine=True): 1, WindowMSA(
  (qkv): Linear(in_features=96, out_features=288, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=96, out_features=96, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
): 1, LayerNorm((96,), eps=1e-05, elementwise_affine=True): 1, Linear(in_features=96, out_features=384, bias=True): 1, Linear(in_features=384, out_features=96, bias=True): 1, PatchMerging(
  (adap_padding): AdaptivePadding()
  (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
  (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (reduction): Linear(in_features=384, out_features=192, bias=False)
): 1, LayerNorm((192,), eps=1e-05, elementwise_affine=True): 1, WindowMSA(
  (qkv): Linear(in_features=192, out_features=576, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=192, out_features=192, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
): 1, LayerNorm((192,), eps=1e-05, elementwise_affine=True): 1, Linear(in_features=192, out_features=768, bias=True): 1, Linear(in_features=768, out_features=192, bias=True): 1, LayerNorm((192,), eps=1e-05, elementwise_affine=True): 1, WindowMSA(
  (qkv): Linear(in_features=192, out_features=576, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=192, out_features=192, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
): 1, LayerNorm((192,), eps=1e-05, elementwise_affine=True): 1, Linear(in_features=192, out_features=768, bias=True): 1, Linear(in_features=768, out_features=192, bias=True): 1, PatchMerging(
  (adap_padding): AdaptivePadding()
  (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (reduction): Linear(in_features=768, out_features=384, bias=False)
): 1, LayerNorm((192,), eps=1e-05, elementwise_affine=True): 1, LayerNorm((384,), eps=1e-05, elementwise_affine=True): 1, WindowMSA(
  (qkv): Linear(in_features=384, out_features=1152, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=384, out_features=384, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
): 1, LayerNorm((384,), eps=1e-05, elementwise_affine=True): 1, Linear(in_features=384, out_features=1536, bias=True): 1, Linear(in_features=1536, out_features=384, bias=True): 1, LayerNorm((384,), eps=1e-05, elementwise_affine=True): 1, WindowMSA(
  (qkv): Linear(in_features=384, out_features=1152, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=384, out_features=384, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
): 1, LayerNorm((384,), eps=1e-05, elementwise_affine=True): 1, Linear(in_features=384, out_features=1536, bias=True): 1, Linear(in_features=1536, out_features=384, bias=True): 1, LayerNorm((384,), eps=1e-05, elementwise_affine=True): 1, WindowMSA(
  (qkv): Linear(in_features=384, out_features=1152, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=384, out_features=384, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
): 1, LayerNorm((384,), eps=1e-05, elementwise_affine=True): 1, Linear(in_features=384, out_features=1536, bias=True): 1, Linear(in_features=1536, out_features=384, bias=True): 1, LayerNorm((384,), eps=1e-05, elementwise_affine=True): 1, WindowMSA(
  (qkv): Linear(in_features=384, out_features=1152, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=384, out_features=384, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
): 1, LayerNorm((384,), eps=1e-05, elementwise_affine=True): 1, Linear(in_features=384, out_features=1536, bias=True): 1, Linear(in_features=1536, out_features=384, bias=True): 1, LayerNorm((384,), eps=1e-05, elementwise_affine=True): 1, WindowMSA(
  (qkv): Linear(in_features=384, out_features=1152, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=384, out_features=384, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
): 1, LayerNorm((384,), eps=1e-05, elementwise_affine=True): 1, Linear(in_features=384, out_features=1536, bias=True): 1, Linear(in_features=1536, out_features=384, bias=True): 1, LayerNorm((384,), eps=1e-05, elementwise_affine=True): 1, WindowMSA(
  (qkv): Linear(in_features=384, out_features=1152, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=384, out_features=384, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
): 1, LayerNorm((384,), eps=1e-05, elementwise_affine=True): 1, Linear(in_features=384, out_features=1536, bias=True): 1, Linear(in_features=1536, out_features=384, bias=True): 1, PatchMerging(
  (adap_padding): AdaptivePadding()
  (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
  (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (reduction): Linear(in_features=1536, out_features=768, bias=False)
): 1, LayerNorm((384,), eps=1e-05, elementwise_affine=True): 1, LayerNorm((768,), eps=1e-05, elementwise_affine=True): 1, WindowMSA(
  (qkv): Linear(in_features=768, out_features=2304, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=768, out_features=768, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
): 1, LayerNorm((768,), eps=1e-05, elementwise_affine=True): 1, Linear(in_features=768, out_features=3072, bias=True): 1, Linear(in_features=3072, out_features=768, bias=True): 1, LayerNorm((768,), eps=1e-05, elementwise_affine=True): 1, WindowMSA(
  (qkv): Linear(in_features=768, out_features=2304, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=768, out_features=768, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
): 1, LayerNorm((768,), eps=1e-05, elementwise_affine=True): 1, Linear(in_features=768, out_features=3072, bias=True): 1, Linear(in_features=3072, out_features=768, bias=True): 1, LayerNorm((768,), eps=1e-05, elementwise_affine=True): 1}


reused =  [Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))]

flattened_output =  <class 'torch.Tensor'>
flattened_output len =  3
flattened_output shapes =  [torch.Size([1, 192, 100, 167]), torch.Size([1, 384, 50, 84]), torch.Size([1, 768, 25, 42])]
grad_fn =  [<CloneBackward0 object at 0x7f6604b8ba30>, <CloneBackward0 object at 0x7f6604b8bc40>, <CloneBackward0 object at 0x7f6604b8bc70>]
post forward - gradfn2module =  69 dict_keys([<SqueezeBackward1 object at 0x7f6604b8fee0>, <ConvolutionBackward0 object at 0x7f6604b8ff10>, <NativeLayerNormBackward0 object at 0x7f6604b8ff40>, <NativeLayerNormBackward0 object at 0x7f6604b8ffd0>, <UnsafeViewBackward0 object at 0x7f6604b8b040>, <NativeLayerNormBackward0 object at 0x7f6604b8b070>, <ViewBackward0 object at 0x7f6604b8b0a0>, <ViewBackward0 object at 0x7f6604b8b0d0>, <NativeLayerNormBackward0 object at 0x7f6604b8b100>, <UnsafeViewBackward0 object at 0x7f6604b8b130>, <NativeLayerNormBackward0 object at 0x7f6604b8b160>, <ViewBackward0 object at 0x7f6604b8b190>, <ViewBackward0 object at 0x7f6604b8b1f0>, <UnsafeViewBackward0 object at 0x7f6604b8ff70>, <NativeLayerNormBackward0 object at 0x7f6604b8b220>, <UnsafeViewBackward0 object at 0x7f6604b8b250>, <NativeLayerNormBackward0 object at 0x7f6604b8b280>, <ViewBackward0 object at 0x7f6604b8b2b0>, <ViewBackward0 object at 0x7f6604b8b2e0>, <NativeLayerNormBackward0 object at 0x7f6604b8b310>, <UnsafeViewBackward0 object at 0x7f6604b8b340>, <NativeLayerNormBackward0 object at 0x7f6604b8b370>, <ViewBackward0 object at 0x7f6604b8b3a0>, <ViewBackward0 object at 0x7f6604b8b3d0>, <UnsafeViewBackward0 object at 0x7f6604b8ffa0>, <NativeLayerNormBackward0 object at 0x7f6604b8b1c0>, <NativeLayerNormBackward0 object at 0x7f6604b8b460>, <UnsafeViewBackward0 object at 0x7f6604b8b490>, <NativeLayerNormBackward0 object at 0x7f6604b8b4c0>, <ViewBackward0 object at 0x7f6604b8b4f0>, <ViewBackward0 object at 0x7f6604b8b520>, <NativeLayerNormBackward0 object at 0x7f6604b8b550>, <UnsafeViewBackward0 object at 0x7f6604b8b580>, <NativeLayerNormBackward0 object at 0x7f6604b8b5b0>, <ViewBackward0 object at 0x7f6604b8b5e0>, <ViewBackward0 object at 0x7f6604b8b610>, <NativeLayerNormBackward0 object at 0x7f6604b8b640>, <UnsafeViewBackward0 object at 0x7f6604b8b670>, <NativeLayerNormBackward0 object at 0x7f6604b8b6a0>, <ViewBackward0 object at 0x7f6604b8b6d0>, <ViewBackward0 object at 0x7f6604b8b700>, <NativeLayerNormBackward0 object at 0x7f6604b8b730>, <UnsafeViewBackward0 object at 0x7f6604b8b760>, <NativeLayerNormBackward0 object at 0x7f6604b8b790>, <ViewBackward0 object at 0x7f6604b8b7c0>, <ViewBackward0 object at 0x7f6604b8b7f0>, <NativeLayerNormBackward0 object at 0x7f6604b8b820>, <UnsafeViewBackward0 object at 0x7f6604b8b850>, <NativeLayerNormBackward0 object at 0x7f6604b8b880>, <ViewBackward0 object at 0x7f6604b8b8b0>, <ViewBackward0 object at 0x7f6604b8b8e0>, <NativeLayerNormBackward0 object at 0x7f6604b8b910>, <UnsafeViewBackward0 object at 0x7f6604b8b940>, <NativeLayerNormBackward0 object at 0x7f6604b8b970>, <ViewBackward0 object at 0x7f6604b8b9a0>, <ViewBackward0 object at 0x7f6604b8b9d0>, <UnsafeViewBackward0 object at 0x7f6604b8b400>, <NativeLayerNormBackward0 object at 0x7f6604b8b430>, <NativeLayerNormBackward0 object at 0x7f6604b8ba60>, <UnsafeViewBackward0 object at 0x7f6604b8ba90>, <NativeLayerNormBackward0 object at 0x7f6604b8bac0>, <ViewBackward0 object at 0x7f6604b8baf0>, <ViewBackward0 object at 0x7f6604b8bb20>, <NativeLayerNormBackward0 object at 0x7f6604b8bb50>, <UnsafeViewBackward0 object at 0x7f6604b8bb80>, <NativeLayerNormBackward0 object at 0x7f6604b8bbb0>, <ViewBackward0 object at 0x7f6604b8bbe0>, <ViewBackward0 object at 0x7f6604b8bc10>, <NativeLayerNormBackward0 object at 0x7f6604b8ba00>])

 module2node ==  dict_values([<Node: (_ElementWiseOp_0(CloneBackward0))>, <Node: (_ElementWiseOp_1(PermuteBackward0))>, <Node: (_Reshape_2())>, <Node: (norm1 (LayerNorm((192,), eps=1e-05, elementwise_affine=True)))>, <Node: (_ElementWiseOp_3(AddBackward0))>, <Node: (_ElementWiseOp_4(AddBackward0))>, <Node: (stages.1.blocks.1.ffn.layers.1 (Linear(in_features=768, out_features=192, bias=True)))>, <Node: (_ElementWiseOp_5(AddmmBackward0))>, <Node: (_Reshape_6())>, <Node: (_ElementWiseOp_7(TBackward0))>, <Node: (_ElementWiseOp_8(GeluBackward0))>, <Node: (stages.1.blocks.1.ffn.layers.0.0 (Linear(in_features=192, out_features=768, bias=True)))>, <Node: (_ElementWiseOp_9(AddmmBackward0))>, <Node: (_Reshape_10())>, <Node: (_ElementWiseOp_11(TBackward0))>, <Node: (stages.1.blocks.1.norm2 (LayerNorm((192,), eps=1e-05, elementwise_affine=True)))>, <Node: (_Reshape_12())>, <Node: (_ElementWiseOp_13(AddBackward0))>, <Node: (_ElementWiseOp_14(AddBackward0))>, <Node: (stages.1.blocks.0.ffn.layers.1 (Linear(in_features=768, out_features=192, bias=True)))>, <Node: (_ElementWiseOp_15(AddmmBackward0))>, <Node: (_Reshape_16())>, <Node: (_ElementWiseOp_17(TBackward0))>, <Node: (_ElementWiseOp_18(GeluBackward0))>, <Node: (stages.1.blocks.0.ffn.layers.0.0 (Linear(in_features=192, out_features=768, bias=True)))>, <Node: (_ElementWiseOp_19(AddmmBackward0))>, <Node: (_Reshape_20())>, <Node: (_ElementWiseOp_21(TBackward0))>, <Node: (stages.1.blocks.0.norm2 (LayerNorm((192,), eps=1e-05, elementwise_affine=True)))>, <Node: (_Reshape_22())>, <Node: (stages.0.downsample (PatchMerging(
  (adap_padding): AdaptivePadding()
  (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
  (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (reduction): Linear(in_features=384, out_features=192, bias=False)
)))>, <Node: (_ElementWiseOp_23(MmBackward0))>, <Node: (_Reshape_24())>, <Node: (_ElementWiseOp_25(TBackward0))>, <Node: (_ElementWiseOp_26(NativeLayerNormBackward0))>, <Node: (_ElementWiseOp_27(TransposeBackward0))>, <Node: (_ElementWiseOp_28(Im2ColBackward0))>, <Node: (_ElementWiseOp_29(PermuteBackward0))>, <Node: (_Reshape_30())>, <Node: (_ElementWiseOp_31(AddBackward0))>, <Node: (_ElementWiseOp_32(AddBackward0))>, <Node: (stages.0.blocks.1.ffn.layers.1 (Linear(in_features=384, out_features=96, bias=True)))>, <Node: (_ElementWiseOp_33(AddmmBackward0))>, <Node: (_Reshape_34())>, <Node: (_ElementWiseOp_35(TBackward0))>, <Node: (_ElementWiseOp_36(GeluBackward0))>, <Node: (stages.0.blocks.1.ffn.layers.0.0 (Linear(in_features=96, out_features=384, bias=True)))>, <Node: (_ElementWiseOp_37(AddmmBackward0))>, <Node: (_Reshape_38())>, <Node: (_ElementWiseOp_39(TBackward0))>, <Node: (stages.0.blocks.1.norm2 (LayerNorm((96,), eps=1e-05, elementwise_affine=True)))>, <Node: (_Reshape_40())>, <Node: (_ElementWiseOp_41(AddBackward0))>, <Node: (_ElementWiseOp_42(AddBackward0))>, <Node: (stages.0.blocks.0.ffn.layers.1 (Linear(in_features=384, out_features=96, bias=True)))>, <Node: (_ElementWiseOp_43(AddmmBackward0))>, <Node: (_Reshape_44())>, <Node: (_ElementWiseOp_45(TBackward0))>, <Node: (_ElementWiseOp_46(GeluBackward0))>, <Node: (stages.0.blocks.0.ffn.layers.0.0 (Linear(in_features=96, out_features=384, bias=True)))>, <Node: (_ElementWiseOp_47(AddmmBackward0))>, <Node: (_Reshape_48())>, <Node: (_ElementWiseOp_49(TBackward0))>, <Node: (stages.0.blocks.0.norm2 (LayerNorm((96,), eps=1e-05, elementwise_affine=True)))>, <Node: (_Reshape_50())>, <Node: (patch_embed.norm (LayerNorm((96,), eps=1e-05, elementwise_affine=True)))>, <Node: (_ElementWiseOp_51(TransposeBackward0))>, <Node: (_Reshape_52())>, <Node: (patch_embed.projection (Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))))>, <Node: (_ElementWiseOp_53(CloneBackward0))>, <Node: (_ElementWiseOp_54(SliceBackward0))>, <Node: (_ElementWiseOp_55(SliceBackward0))>, <Node: (_ElementWiseOp_56(SliceBackward0))>, <Node: (_ElementWiseOp_57(SliceBackward0))>, <Node: (_Reshape_58())>, <Node: (_ElementWiseOp_59(CloneBackward0))>, <Node: (_ElementWiseOp_60(PermuteBackward0))>, <Node: (_Reshape_61())>, <Node: (_Reshape_62())>, <Node: (stages.0.blocks.0.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=96, out_features=288, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=96, out_features=96, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)))>, <Node: (_ElementWiseOp_63(CloneBackward0))>, <Node: (_ElementWiseOp_64(TransposeBackward0))>, <Node: (_Reshape_65())>, <Node: (_ElementWiseOp_66(BmmBackward0))>, <Node: (_Reshape_67())>, <Node: (_Reshape_68())>, <Node: (_ElementWiseOp_69(CloneBackward0))>, <Node: (_ElementWiseOp_70(ExpandBackward0))>, <Node: (_ElementWiseOp_71(SelectBackward0))>, <Node: (_ElementWiseOp_72(PermuteBackward0))>, <Node: (_Reshape_73())>, <Node: (_Reshape_74())>, <Node: (_ElementWiseOp_75(AddmmBackward0))>, <Node: (_Reshape_76())>, <Node: (_ElementWiseOp_77(TBackward0))>, <Node: (_Reshape_78())>, <Node: (_Reshape_79())>, <Node: (_ElementWiseOp_80(CloneBackward0))>, <Node: (_ElementWiseOp_81(PermuteBackward0))>, <Node: (_Reshape_82())>, <Node: (_ElementWiseOp_83(ConstantPadNdBackward0))>, <Node: (_Reshape_84())>, <Node: (stages.0.blocks.0.norm1 (LayerNorm((96,), eps=1e-05, elementwise_affine=True)))>, <Node: (_ElementWiseOp_85(ExpandBackward0))>, <Node: (_ElementWiseOp_86(SoftmaxBackward0))>, <Node: (_ElementWiseOp_87(AddBackward0))>, <Node: (_Reshape_88())>, <Node: (_ElementWiseOp_89(UnsqueezeBackward0))>, <Node: (_ElementWiseOp_90(CloneBackward0))>, <Node: (_ElementWiseOp_91(PermuteBackward0))>, <Node: (_Reshape_92())>, <Node: (_ElementWiseOp_93(IndexBackward0))>, <Node: (_ElementWiseOp_94(BmmBackward0))>, <Node: (_Reshape_95())>, <Node: (_Reshape_96())>, <Node: (_ElementWiseOp_97(CloneBackward0))>, <Node: (_ElementWiseOp_98(ExpandBackward0))>, <Node: (_ElementWiseOp_99(TransposeBackward0))>, <Node: (_ElementWiseOp_100(SelectBackward0))>, <Node: (_ElementWiseOp_101(CloneBackward0))>, <Node: (_ElementWiseOp_102(ExpandBackward0))>, <Node: (_ElementWiseOp_103(MulBackward0))>, <Node: (_ElementWiseOp_104(SelectBackward0))>, <Node: (_ElementWiseOp_105(CloneBackward0))>, <Node: (_ElementWiseOp_106(SliceBackward0))>, <Node: (_ElementWiseOp_107(SliceBackward0))>, <Node: (_ElementWiseOp_108(SliceBackward0))>, <Node: (_ElementWiseOp_109(SliceBackward0))>, <Node: (_ElementWiseOp_110(RollBackward0))>, <Node: (_Reshape_111())>, <Node: (_ElementWiseOp_112(CloneBackward0))>, <Node: (_ElementWiseOp_113(PermuteBackward0))>, <Node: (_Reshape_114())>, <Node: (_Reshape_115())>, <Node: (stages.0.blocks.1.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=96, out_features=288, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=96, out_features=96, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)))>, <Node: (_ElementWiseOp_116(CloneBackward0))>, <Node: (_ElementWiseOp_117(TransposeBackward0))>, <Node: (_Reshape_118())>, <Node: (_ElementWiseOp_119(BmmBackward0))>, <Node: (_Reshape_120())>, <Node: (_Reshape_121())>, <Node: (_ElementWiseOp_122(CloneBackward0))>, <Node: (_ElementWiseOp_123(ExpandBackward0))>, <Node: (_ElementWiseOp_124(SelectBackward0))>, <Node: (_ElementWiseOp_125(PermuteBackward0))>, <Node: (_Reshape_126())>, <Node: (_Reshape_127())>, <Node: (_ElementWiseOp_128(AddmmBackward0))>, <Node: (_Reshape_129())>, <Node: (_ElementWiseOp_130(TBackward0))>, <Node: (_Reshape_131())>, <Node: (_Reshape_132())>, <Node: (_ElementWiseOp_133(CloneBackward0))>, <Node: (_ElementWiseOp_134(PermuteBackward0))>, <Node: (_Reshape_135())>, <Node: (_ElementWiseOp_136(RollBackward0))>, <Node: (_ElementWiseOp_137(ConstantPadNdBackward0))>, <Node: (_Reshape_138())>, <Node: (stages.0.blocks.1.norm1 (LayerNorm((96,), eps=1e-05, elementwise_affine=True)))>, <Node: (_ElementWiseOp_139(ExpandBackward0))>, <Node: (_ElementWiseOp_140(SoftmaxBackward0))>, <Node: (_Reshape_141())>, <Node: (_ElementWiseOp_142(AddBackward0))>, <Node: (_Reshape_143())>, <Node: (_ElementWiseOp_144(AddBackward0))>, <Node: (_Reshape_145())>, <Node: (_ElementWiseOp_146(UnsqueezeBackward0))>, <Node: (_ElementWiseOp_147(CloneBackward0))>, <Node: (_ElementWiseOp_148(PermuteBackward0))>, <Node: (_Reshape_149())>, <Node: (_ElementWiseOp_150(IndexBackward0))>, <Node: (_ElementWiseOp_151(BmmBackward0))>, <Node: (_Reshape_152())>, <Node: (_Reshape_153())>, <Node: (_ElementWiseOp_154(CloneBackward0))>, <Node: (_ElementWiseOp_155(ExpandBackward0))>, <Node: (_ElementWiseOp_156(TransposeBackward0))>, <Node: (_ElementWiseOp_157(SelectBackward0))>, <Node: (_ElementWiseOp_158(CloneBackward0))>, <Node: (_ElementWiseOp_159(ExpandBackward0))>, <Node: (_ElementWiseOp_160(MulBackward0))>, <Node: (_ElementWiseOp_161(SelectBackward0))>, <Node: (_ElementWiseOp_162(CloneBackward0))>, <Node: (_ElementWiseOp_163(SliceBackward0))>, <Node: (_ElementWiseOp_164(SliceBackward0))>, <Node: (_ElementWiseOp_165(SliceBackward0))>, <Node: (_ElementWiseOp_166(SliceBackward0))>, <Node: (_Reshape_167())>, <Node: (_ElementWiseOp_168(CloneBackward0))>, <Node: (_ElementWiseOp_169(PermuteBackward0))>, <Node: (_Reshape_170())>, <Node: (_Reshape_171())>, <Node: (stages.1.blocks.0.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=192, out_features=576, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=192, out_features=192, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)))>, <Node: (_ElementWiseOp_172(CloneBackward0))>, <Node: (_ElementWiseOp_173(TransposeBackward0))>, <Node: (_Reshape_174())>, <Node: (_ElementWiseOp_175(BmmBackward0))>, <Node: (_Reshape_176())>, <Node: (_Reshape_177())>, <Node: (_ElementWiseOp_178(CloneBackward0))>, <Node: (_ElementWiseOp_179(ExpandBackward0))>, <Node: (_ElementWiseOp_180(SelectBackward0))>, <Node: (_ElementWiseOp_181(PermuteBackward0))>, <Node: (_Reshape_182())>, <Node: (_Reshape_183())>, <Node: (_ElementWiseOp_184(AddmmBackward0))>, <Node: (_Reshape_185())>, <Node: (_ElementWiseOp_186(TBackward0))>, <Node: (_Reshape_187())>, <Node: (_Reshape_188())>, <Node: (_ElementWiseOp_189(CloneBackward0))>, <Node: (_ElementWiseOp_190(PermuteBackward0))>, <Node: (_Reshape_191())>, <Node: (_ElementWiseOp_192(ConstantPadNdBackward0))>, <Node: (_Reshape_193())>, <Node: (stages.1.blocks.0.norm1 (LayerNorm((192,), eps=1e-05, elementwise_affine=True)))>, <Node: (_ElementWiseOp_194(ExpandBackward0))>, <Node: (_ElementWiseOp_195(SoftmaxBackward0))>, <Node: (_ElementWiseOp_196(AddBackward0))>, <Node: (_Reshape_197())>, <Node: (_ElementWiseOp_198(UnsqueezeBackward0))>, <Node: (_ElementWiseOp_199(CloneBackward0))>, <Node: (_ElementWiseOp_200(PermuteBackward0))>, <Node: (_Reshape_201())>, <Node: (_ElementWiseOp_202(IndexBackward0))>, <Node: (_ElementWiseOp_203(BmmBackward0))>, <Node: (_Reshape_204())>, <Node: (_Reshape_205())>, <Node: (_ElementWiseOp_206(CloneBackward0))>, <Node: (_ElementWiseOp_207(ExpandBackward0))>, <Node: (_ElementWiseOp_208(TransposeBackward0))>, <Node: (_ElementWiseOp_209(SelectBackward0))>, <Node: (_ElementWiseOp_210(CloneBackward0))>, <Node: (_ElementWiseOp_211(ExpandBackward0))>, <Node: (_ElementWiseOp_212(MulBackward0))>, <Node: (_ElementWiseOp_213(SelectBackward0))>, <Node: (_ElementWiseOp_214(CloneBackward0))>, <Node: (_ElementWiseOp_215(SliceBackward0))>, <Node: (_ElementWiseOp_216(SliceBackward0))>, <Node: (_ElementWiseOp_217(SliceBackward0))>, <Node: (_ElementWiseOp_218(SliceBackward0))>, <Node: (_ElementWiseOp_219(RollBackward0))>, <Node: (_Reshape_220())>, <Node: (_ElementWiseOp_221(CloneBackward0))>, <Node: (_ElementWiseOp_222(PermuteBackward0))>, <Node: (_Reshape_223())>, <Node: (_Reshape_224())>, <Node: (stages.1.blocks.1.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=192, out_features=576, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=192, out_features=192, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)))>, <Node: (_ElementWiseOp_225(CloneBackward0))>, <Node: (_ElementWiseOp_226(TransposeBackward0))>, <Node: (_Reshape_227())>, <Node: (_ElementWiseOp_228(BmmBackward0))>, <Node: (_Reshape_229())>, <Node: (_Reshape_230())>, <Node: (_ElementWiseOp_231(CloneBackward0))>, <Node: (_ElementWiseOp_232(ExpandBackward0))>, <Node: (_ElementWiseOp_233(SelectBackward0))>, <Node: (_ElementWiseOp_234(PermuteBackward0))>, <Node: (_Reshape_235())>, <Node: (_Reshape_236())>, <Node: (_ElementWiseOp_237(AddmmBackward0))>, <Node: (_Reshape_238())>, <Node: (_ElementWiseOp_239(TBackward0))>, <Node: (_Reshape_240())>, <Node: (_Reshape_241())>, <Node: (_ElementWiseOp_242(CloneBackward0))>, <Node: (_ElementWiseOp_243(PermuteBackward0))>, <Node: (_Reshape_244())>, <Node: (_ElementWiseOp_245(RollBackward0))>, <Node: (_ElementWiseOp_246(ConstantPadNdBackward0))>, <Node: (_Reshape_247())>, <Node: (stages.1.blocks.1.norm1 (LayerNorm((192,), eps=1e-05, elementwise_affine=True)))>, <Node: (_ElementWiseOp_248(ExpandBackward0))>, <Node: (_ElementWiseOp_249(SoftmaxBackward0))>, <Node: (_Reshape_250())>, <Node: (_ElementWiseOp_251(AddBackward0))>, <Node: (_Reshape_252())>, <Node: (_ElementWiseOp_253(AddBackward0))>, <Node: (_Reshape_254())>, <Node: (_ElementWiseOp_255(UnsqueezeBackward0))>, <Node: (_ElementWiseOp_256(CloneBackward0))>, <Node: (_ElementWiseOp_257(PermuteBackward0))>, <Node: (_Reshape_258())>, <Node: (_ElementWiseOp_259(IndexBackward0))>, <Node: (_ElementWiseOp_260(BmmBackward0))>, <Node: (_Reshape_261())>, <Node: (_Reshape_262())>, <Node: (_ElementWiseOp_263(CloneBackward0))>, <Node: (_ElementWiseOp_264(ExpandBackward0))>, <Node: (_ElementWiseOp_265(TransposeBackward0))>, <Node: (_ElementWiseOp_266(SelectBackward0))>, <Node: (_ElementWiseOp_267(CloneBackward0))>, <Node: (_ElementWiseOp_268(ExpandBackward0))>, <Node: (_ElementWiseOp_269(MulBackward0))>, <Node: (_ElementWiseOp_270(SelectBackward0))>, <Node: (_ElementWiseOp_271(CloneBackward0))>, <Node: (_ElementWiseOp_272(PermuteBackward0))>, <Node: (_Reshape_273())>, <Node: (norm2 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)))>, <Node: (_ElementWiseOp_274(AddBackward0))>, <Node: (_ElementWiseOp_275(AddBackward0))>, <Node: (stages.2.blocks.5.ffn.layers.1 (Linear(in_features=1536, out_features=384, bias=True)))>, <Node: (_ElementWiseOp_276(AddmmBackward0))>, <Node: (_Reshape_277())>, <Node: (_ElementWiseOp_278(TBackward0))>, <Node: (_ElementWiseOp_279(GeluBackward0))>, <Node: (stages.2.blocks.5.ffn.layers.0.0 (Linear(in_features=384, out_features=1536, bias=True)))>, <Node: (_ElementWiseOp_280(AddmmBackward0))>, <Node: (_Reshape_281())>, <Node: (_ElementWiseOp_282(TBackward0))>, <Node: (stages.2.blocks.5.norm2 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)))>, <Node: (_Reshape_283())>, <Node: (_ElementWiseOp_284(AddBackward0))>, <Node: (_ElementWiseOp_285(AddBackward0))>, <Node: (stages.2.blocks.4.ffn.layers.1 (Linear(in_features=1536, out_features=384, bias=True)))>, <Node: (_ElementWiseOp_286(AddmmBackward0))>, <Node: (_Reshape_287())>, <Node: (_ElementWiseOp_288(TBackward0))>, <Node: (_ElementWiseOp_289(GeluBackward0))>, <Node: (stages.2.blocks.4.ffn.layers.0.0 (Linear(in_features=384, out_features=1536, bias=True)))>, <Node: (_ElementWiseOp_290(AddmmBackward0))>, <Node: (_Reshape_291())>, <Node: (_ElementWiseOp_292(TBackward0))>, <Node: (stages.2.blocks.4.norm2 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)))>, <Node: (_Reshape_293())>, <Node: (_ElementWiseOp_294(AddBackward0))>, <Node: (_ElementWiseOp_295(AddBackward0))>, <Node: (stages.2.blocks.3.ffn.layers.1 (Linear(in_features=1536, out_features=384, bias=True)))>, <Node: (_ElementWiseOp_296(AddmmBackward0))>, <Node: (_Reshape_297())>, <Node: (_ElementWiseOp_298(TBackward0))>, <Node: (_ElementWiseOp_299(GeluBackward0))>, <Node: (stages.2.blocks.3.ffn.layers.0.0 (Linear(in_features=384, out_features=1536, bias=True)))>, <Node: (_ElementWiseOp_300(AddmmBackward0))>, <Node: (_Reshape_301())>, <Node: (_ElementWiseOp_302(TBackward0))>, <Node: (stages.2.blocks.3.norm2 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)))>, <Node: (_Reshape_303())>, <Node: (_ElementWiseOp_304(AddBackward0))>, <Node: (_ElementWiseOp_305(AddBackward0))>, <Node: (stages.2.blocks.2.ffn.layers.1 (Linear(in_features=1536, out_features=384, bias=True)))>, <Node: (_ElementWiseOp_306(AddmmBackward0))>, <Node: (_Reshape_307())>, <Node: (_ElementWiseOp_308(TBackward0))>, <Node: (_ElementWiseOp_309(GeluBackward0))>, <Node: (stages.2.blocks.2.ffn.layers.0.0 (Linear(in_features=384, out_features=1536, bias=True)))>, <Node: (_ElementWiseOp_310(AddmmBackward0))>, <Node: (_Reshape_311())>, <Node: (_ElementWiseOp_312(TBackward0))>, <Node: (stages.2.blocks.2.norm2 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)))>, <Node: (_Reshape_313())>, <Node: (_ElementWiseOp_314(AddBackward0))>, <Node: (_ElementWiseOp_315(AddBackward0))>, <Node: (stages.2.blocks.1.ffn.layers.1 (Linear(in_features=1536, out_features=384, bias=True)))>, <Node: (_ElementWiseOp_316(AddmmBackward0))>, <Node: (_Reshape_317())>, <Node: (_ElementWiseOp_318(TBackward0))>, <Node: (_ElementWiseOp_319(GeluBackward0))>, <Node: (stages.2.blocks.1.ffn.layers.0.0 (Linear(in_features=384, out_features=1536, bias=True)))>, <Node: (_ElementWiseOp_320(AddmmBackward0))>, <Node: (_Reshape_321())>, <Node: (_ElementWiseOp_322(TBackward0))>, <Node: (stages.2.blocks.1.norm2 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)))>, <Node: (_Reshape_323())>, <Node: (_ElementWiseOp_324(AddBackward0))>, <Node: (_ElementWiseOp_325(AddBackward0))>, <Node: (stages.2.blocks.0.ffn.layers.1 (Linear(in_features=1536, out_features=384, bias=True)))>, <Node: (_ElementWiseOp_326(AddmmBackward0))>, <Node: (_Reshape_327())>, <Node: (_ElementWiseOp_328(TBackward0))>, <Node: (_ElementWiseOp_329(GeluBackward0))>, <Node: (stages.2.blocks.0.ffn.layers.0.0 (Linear(in_features=384, out_features=1536, bias=True)))>, <Node: (_ElementWiseOp_330(AddmmBackward0))>, <Node: (_Reshape_331())>, <Node: (_ElementWiseOp_332(TBackward0))>, <Node: (stages.2.blocks.0.norm2 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)))>, <Node: (_Reshape_333())>, <Node: (stages.1.downsample (PatchMerging(
  (adap_padding): AdaptivePadding()
  (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (reduction): Linear(in_features=768, out_features=384, bias=False)
)))>, <Node: (_ElementWiseOp_334(MmBackward0))>, <Node: (_Reshape_335())>, <Node: (_ElementWiseOp_336(TBackward0))>, <Node: (_ElementWiseOp_337(NativeLayerNormBackward0))>, <Node: (_ElementWiseOp_338(TransposeBackward0))>, <Node: (_ElementWiseOp_339(Im2ColBackward0))>, <Node: (_ElementWiseOp_340(ConstantPadNdBackward0))>, <Node: (_ElementWiseOp_341(PermuteBackward0))>, <Node: (_Reshape_342())>, <Node: (_ElementWiseOp_343(SliceBackward0))>, <Node: (_ElementWiseOp_344(SliceBackward0))>, <Node: (_ElementWiseOp_345(SliceBackward0))>, <Node: (_Reshape_346())>, <Node: (_ElementWiseOp_347(CloneBackward0))>, <Node: (_ElementWiseOp_348(PermuteBackward0))>, <Node: (_Reshape_349())>, <Node: (_Reshape_350())>, <Node: (stages.2.blocks.0.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=384, out_features=1152, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=384, out_features=384, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)))>, <Node: (_ElementWiseOp_351(CloneBackward0))>, <Node: (_ElementWiseOp_352(TransposeBackward0))>, <Node: (_Reshape_353())>, <Node: (_ElementWiseOp_354(BmmBackward0))>, <Node: (_Reshape_355())>, <Node: (_Reshape_356())>, <Node: (_ElementWiseOp_357(CloneBackward0))>, <Node: (_ElementWiseOp_358(ExpandBackward0))>, <Node: (_ElementWiseOp_359(SelectBackward0))>, <Node: (_ElementWiseOp_360(PermuteBackward0))>, <Node: (_Reshape_361())>, <Node: (_Reshape_362())>, <Node: (_ElementWiseOp_363(AddmmBackward0))>, <Node: (_Reshape_364())>, <Node: (_ElementWiseOp_365(TBackward0))>, <Node: (_Reshape_366())>, <Node: (_Reshape_367())>, <Node: (_ElementWiseOp_368(CloneBackward0))>, <Node: (_ElementWiseOp_369(PermuteBackward0))>, <Node: (_Reshape_370())>, <Node: (_ElementWiseOp_371(ConstantPadNdBackward0))>, <Node: (_Reshape_372())>, <Node: (stages.2.blocks.0.norm1 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)))>, <Node: (_ElementWiseOp_373(ExpandBackward0))>, <Node: (_ElementWiseOp_374(SoftmaxBackward0))>, <Node: (_ElementWiseOp_375(AddBackward0))>, <Node: (_Reshape_376())>, <Node: (_ElementWiseOp_377(UnsqueezeBackward0))>, <Node: (_ElementWiseOp_378(CloneBackward0))>, <Node: (_ElementWiseOp_379(PermuteBackward0))>, <Node: (_Reshape_380())>, <Node: (_ElementWiseOp_381(IndexBackward0))>, <Node: (_ElementWiseOp_382(BmmBackward0))>, <Node: (_Reshape_383())>, <Node: (_Reshape_384())>, <Node: (_ElementWiseOp_385(CloneBackward0))>, <Node: (_ElementWiseOp_386(ExpandBackward0))>, <Node: (_ElementWiseOp_387(TransposeBackward0))>, <Node: (_ElementWiseOp_388(SelectBackward0))>, <Node: (_ElementWiseOp_389(CloneBackward0))>, <Node: (_ElementWiseOp_390(ExpandBackward0))>, <Node: (_ElementWiseOp_391(MulBackward0))>, <Node: (_ElementWiseOp_392(SelectBackward0))>, <Node: (_ElementWiseOp_393(SliceBackward0))>, <Node: (_ElementWiseOp_394(SliceBackward0))>, <Node: (_ElementWiseOp_395(SliceBackward0))>, <Node: (_ElementWiseOp_396(RollBackward0))>, <Node: (_Reshape_397())>, <Node: (_ElementWiseOp_398(CloneBackward0))>, <Node: (_ElementWiseOp_399(PermuteBackward0))>, <Node: (_Reshape_400())>, <Node: (_Reshape_401())>, <Node: (stages.2.blocks.1.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=384, out_features=1152, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=384, out_features=384, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)))>, <Node: (_ElementWiseOp_402(CloneBackward0))>, <Node: (_ElementWiseOp_403(TransposeBackward0))>, <Node: (_Reshape_404())>, <Node: (_ElementWiseOp_405(BmmBackward0))>, <Node: (_Reshape_406())>, <Node: (_Reshape_407())>, <Node: (_ElementWiseOp_408(CloneBackward0))>, <Node: (_ElementWiseOp_409(ExpandBackward0))>, <Node: (_ElementWiseOp_410(SelectBackward0))>, <Node: (_ElementWiseOp_411(PermuteBackward0))>, <Node: (_Reshape_412())>, <Node: (_Reshape_413())>, <Node: (_ElementWiseOp_414(AddmmBackward0))>, <Node: (_Reshape_415())>, <Node: (_ElementWiseOp_416(TBackward0))>, <Node: (_Reshape_417())>, <Node: (_Reshape_418())>, <Node: (_ElementWiseOp_419(CloneBackward0))>, <Node: (_ElementWiseOp_420(PermuteBackward0))>, <Node: (_Reshape_421())>, <Node: (_ElementWiseOp_422(RollBackward0))>, <Node: (_ElementWiseOp_423(ConstantPadNdBackward0))>, <Node: (_Reshape_424())>, <Node: (stages.2.blocks.1.norm1 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)))>, <Node: (_ElementWiseOp_425(ExpandBackward0))>, <Node: (_ElementWiseOp_426(SoftmaxBackward0))>, <Node: (_Reshape_427())>, <Node: (_ElementWiseOp_428(AddBackward0))>, <Node: (_Reshape_429())>, <Node: (_ElementWiseOp_430(AddBackward0))>, <Node: (_Reshape_431())>, <Node: (_ElementWiseOp_432(UnsqueezeBackward0))>, <Node: (_ElementWiseOp_433(CloneBackward0))>, <Node: (_ElementWiseOp_434(PermuteBackward0))>, <Node: (_Reshape_435())>, <Node: (_ElementWiseOp_436(IndexBackward0))>, <Node: (_ElementWiseOp_437(BmmBackward0))>, <Node: (_Reshape_438())>, <Node: (_Reshape_439())>, <Node: (_ElementWiseOp_440(CloneBackward0))>, <Node: (_ElementWiseOp_441(ExpandBackward0))>, <Node: (_ElementWiseOp_442(TransposeBackward0))>, <Node: (_ElementWiseOp_443(SelectBackward0))>, <Node: (_ElementWiseOp_444(CloneBackward0))>, <Node: (_ElementWiseOp_445(ExpandBackward0))>, <Node: (_ElementWiseOp_446(MulBackward0))>, <Node: (_ElementWiseOp_447(SelectBackward0))>, <Node: (_ElementWiseOp_448(SliceBackward0))>, <Node: (_ElementWiseOp_449(SliceBackward0))>, <Node: (_ElementWiseOp_450(SliceBackward0))>, <Node: (_Reshape_451())>, <Node: (_ElementWiseOp_452(CloneBackward0))>, <Node: (_ElementWiseOp_453(PermuteBackward0))>, <Node: (_Reshape_454())>, <Node: (_Reshape_455())>, <Node: (stages.2.blocks.2.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=384, out_features=1152, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=384, out_features=384, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)))>, <Node: (_ElementWiseOp_456(CloneBackward0))>, <Node: (_ElementWiseOp_457(TransposeBackward0))>, <Node: (_Reshape_458())>, <Node: (_ElementWiseOp_459(BmmBackward0))>, <Node: (_Reshape_460())>, <Node: (_Reshape_461())>, <Node: (_ElementWiseOp_462(CloneBackward0))>, <Node: (_ElementWiseOp_463(ExpandBackward0))>, <Node: (_ElementWiseOp_464(SelectBackward0))>, <Node: (_ElementWiseOp_465(PermuteBackward0))>, <Node: (_Reshape_466())>, <Node: (_Reshape_467())>, <Node: (_ElementWiseOp_468(AddmmBackward0))>, <Node: (_Reshape_469())>, <Node: (_ElementWiseOp_470(TBackward0))>, <Node: (_Reshape_471())>, <Node: (_Reshape_472())>, <Node: (_ElementWiseOp_473(CloneBackward0))>, <Node: (_ElementWiseOp_474(PermuteBackward0))>, <Node: (_Reshape_475())>, <Node: (_ElementWiseOp_476(ConstantPadNdBackward0))>, <Node: (_Reshape_477())>, <Node: (stages.2.blocks.2.norm1 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)))>, <Node: (_ElementWiseOp_478(ExpandBackward0))>, <Node: (_ElementWiseOp_479(SoftmaxBackward0))>, <Node: (_ElementWiseOp_480(AddBackward0))>, <Node: (_Reshape_481())>, <Node: (_ElementWiseOp_482(UnsqueezeBackward0))>, <Node: (_ElementWiseOp_483(CloneBackward0))>, <Node: (_ElementWiseOp_484(PermuteBackward0))>, <Node: (_Reshape_485())>, <Node: (_ElementWiseOp_486(IndexBackward0))>, <Node: (_ElementWiseOp_487(BmmBackward0))>, <Node: (_Reshape_488())>, <Node: (_Reshape_489())>, <Node: (_ElementWiseOp_490(CloneBackward0))>, <Node: (_ElementWiseOp_491(ExpandBackward0))>, <Node: (_ElementWiseOp_492(TransposeBackward0))>, <Node: (_ElementWiseOp_493(SelectBackward0))>, <Node: (_ElementWiseOp_494(CloneBackward0))>, <Node: (_ElementWiseOp_495(ExpandBackward0))>, <Node: (_ElementWiseOp_496(MulBackward0))>, <Node: (_ElementWiseOp_497(SelectBackward0))>, <Node: (_ElementWiseOp_498(SliceBackward0))>, <Node: (_ElementWiseOp_499(SliceBackward0))>, <Node: (_ElementWiseOp_500(SliceBackward0))>, <Node: (_ElementWiseOp_501(RollBackward0))>, <Node: (_Reshape_502())>, <Node: (_ElementWiseOp_503(CloneBackward0))>, <Node: (_ElementWiseOp_504(PermuteBackward0))>, <Node: (_Reshape_505())>, <Node: (_Reshape_506())>, <Node: (stages.2.blocks.3.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=384, out_features=1152, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=384, out_features=384, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)))>, <Node: (_ElementWiseOp_507(CloneBackward0))>, <Node: (_ElementWiseOp_508(TransposeBackward0))>, <Node: (_Reshape_509())>, <Node: (_ElementWiseOp_510(BmmBackward0))>, <Node: (_Reshape_511())>, <Node: (_Reshape_512())>, <Node: (_ElementWiseOp_513(CloneBackward0))>, <Node: (_ElementWiseOp_514(ExpandBackward0))>, <Node: (_ElementWiseOp_515(SelectBackward0))>, <Node: (_ElementWiseOp_516(PermuteBackward0))>, <Node: (_Reshape_517())>, <Node: (_Reshape_518())>, <Node: (_ElementWiseOp_519(AddmmBackward0))>, <Node: (_Reshape_520())>, <Node: (_ElementWiseOp_521(TBackward0))>, <Node: (_Reshape_522())>, <Node: (_Reshape_523())>, <Node: (_ElementWiseOp_524(CloneBackward0))>, <Node: (_ElementWiseOp_525(PermuteBackward0))>, <Node: (_Reshape_526())>, <Node: (_ElementWiseOp_527(RollBackward0))>, <Node: (_ElementWiseOp_528(ConstantPadNdBackward0))>, <Node: (_Reshape_529())>, <Node: (stages.2.blocks.3.norm1 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)))>, <Node: (_ElementWiseOp_530(ExpandBackward0))>, <Node: (_ElementWiseOp_531(SoftmaxBackward0))>, <Node: (_Reshape_532())>, <Node: (_ElementWiseOp_533(AddBackward0))>, <Node: (_Reshape_534())>, <Node: (_ElementWiseOp_535(AddBackward0))>, <Node: (_Reshape_536())>, <Node: (_ElementWiseOp_537(UnsqueezeBackward0))>, <Node: (_ElementWiseOp_538(CloneBackward0))>, <Node: (_ElementWiseOp_539(PermuteBackward0))>, <Node: (_Reshape_540())>, <Node: (_ElementWiseOp_541(IndexBackward0))>, <Node: (_ElementWiseOp_542(BmmBackward0))>, <Node: (_Reshape_543())>, <Node: (_Reshape_544())>, <Node: (_ElementWiseOp_545(CloneBackward0))>, <Node: (_ElementWiseOp_546(ExpandBackward0))>, <Node: (_ElementWiseOp_547(TransposeBackward0))>, <Node: (_ElementWiseOp_548(SelectBackward0))>, <Node: (_ElementWiseOp_549(CloneBackward0))>, <Node: (_ElementWiseOp_550(ExpandBackward0))>, <Node: (_ElementWiseOp_551(MulBackward0))>, <Node: (_ElementWiseOp_552(SelectBackward0))>, <Node: (_ElementWiseOp_553(SliceBackward0))>, <Node: (_ElementWiseOp_554(SliceBackward0))>, <Node: (_ElementWiseOp_555(SliceBackward0))>, <Node: (_Reshape_556())>, <Node: (_ElementWiseOp_557(CloneBackward0))>, <Node: (_ElementWiseOp_558(PermuteBackward0))>, <Node: (_Reshape_559())>, <Node: (_Reshape_560())>, <Node: (stages.2.blocks.4.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=384, out_features=1152, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=384, out_features=384, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)))>, <Node: (_ElementWiseOp_561(CloneBackward0))>, <Node: (_ElementWiseOp_562(TransposeBackward0))>, <Node: (_Reshape_563())>, <Node: (_ElementWiseOp_564(BmmBackward0))>, <Node: (_Reshape_565())>, <Node: (_Reshape_566())>, <Node: (_ElementWiseOp_567(CloneBackward0))>, <Node: (_ElementWiseOp_568(ExpandBackward0))>, <Node: (_ElementWiseOp_569(SelectBackward0))>, <Node: (_ElementWiseOp_570(PermuteBackward0))>, <Node: (_Reshape_571())>, <Node: (_Reshape_572())>, <Node: (_ElementWiseOp_573(AddmmBackward0))>, <Node: (_Reshape_574())>, <Node: (_ElementWiseOp_575(TBackward0))>, <Node: (_Reshape_576())>, <Node: (_Reshape_577())>, <Node: (_ElementWiseOp_578(CloneBackward0))>, <Node: (_ElementWiseOp_579(PermuteBackward0))>, <Node: (_Reshape_580())>, <Node: (_ElementWiseOp_581(ConstantPadNdBackward0))>, <Node: (_Reshape_582())>, <Node: (stages.2.blocks.4.norm1 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)))>, <Node: (_ElementWiseOp_583(ExpandBackward0))>, <Node: (_ElementWiseOp_584(SoftmaxBackward0))>, <Node: (_ElementWiseOp_585(AddBackward0))>, <Node: (_Reshape_586())>, <Node: (_ElementWiseOp_587(UnsqueezeBackward0))>, <Node: (_ElementWiseOp_588(CloneBackward0))>, <Node: (_ElementWiseOp_589(PermuteBackward0))>, <Node: (_Reshape_590())>, <Node: (_ElementWiseOp_591(IndexBackward0))>, <Node: (_ElementWiseOp_592(BmmBackward0))>, <Node: (_Reshape_593())>, <Node: (_Reshape_594())>, <Node: (_ElementWiseOp_595(CloneBackward0))>, <Node: (_ElementWiseOp_596(ExpandBackward0))>, <Node: (_ElementWiseOp_597(TransposeBackward0))>, <Node: (_ElementWiseOp_598(SelectBackward0))>, <Node: (_ElementWiseOp_599(CloneBackward0))>, <Node: (_ElementWiseOp_600(ExpandBackward0))>, <Node: (_ElementWiseOp_601(MulBackward0))>, <Node: (_ElementWiseOp_602(SelectBackward0))>, <Node: (_ElementWiseOp_603(SliceBackward0))>, <Node: (_ElementWiseOp_604(SliceBackward0))>, <Node: (_ElementWiseOp_605(SliceBackward0))>, <Node: (_ElementWiseOp_606(RollBackward0))>, <Node: (_Reshape_607())>, <Node: (_ElementWiseOp_608(CloneBackward0))>, <Node: (_ElementWiseOp_609(PermuteBackward0))>, <Node: (_Reshape_610())>, <Node: (_Reshape_611())>, <Node: (stages.2.blocks.5.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=384, out_features=1152, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=384, out_features=384, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)))>, <Node: (_ElementWiseOp_612(CloneBackward0))>, <Node: (_ElementWiseOp_613(TransposeBackward0))>, <Node: (_Reshape_614())>, <Node: (_ElementWiseOp_615(BmmBackward0))>, <Node: (_Reshape_616())>, <Node: (_Reshape_617())>, <Node: (_ElementWiseOp_618(CloneBackward0))>, <Node: (_ElementWiseOp_619(ExpandBackward0))>, <Node: (_ElementWiseOp_620(SelectBackward0))>, <Node: (_ElementWiseOp_621(PermuteBackward0))>, <Node: (_Reshape_622())>, <Node: (_Reshape_623())>, <Node: (_ElementWiseOp_624(AddmmBackward0))>, <Node: (_Reshape_625())>, <Node: (_ElementWiseOp_626(TBackward0))>, <Node: (_Reshape_627())>, <Node: (_Reshape_628())>, <Node: (_ElementWiseOp_629(CloneBackward0))>, <Node: (_ElementWiseOp_630(PermuteBackward0))>, <Node: (_Reshape_631())>, <Node: (_ElementWiseOp_632(RollBackward0))>, <Node: (_ElementWiseOp_633(ConstantPadNdBackward0))>, <Node: (_Reshape_634())>, <Node: (stages.2.blocks.5.norm1 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)))>, <Node: (_ElementWiseOp_635(ExpandBackward0))>, <Node: (_ElementWiseOp_636(SoftmaxBackward0))>, <Node: (_Reshape_637())>, <Node: (_ElementWiseOp_638(AddBackward0))>, <Node: (_Reshape_639())>, <Node: (_ElementWiseOp_640(AddBackward0))>, <Node: (_Reshape_641())>, <Node: (_ElementWiseOp_642(UnsqueezeBackward0))>, <Node: (_ElementWiseOp_643(CloneBackward0))>, <Node: (_ElementWiseOp_644(PermuteBackward0))>, <Node: (_Reshape_645())>, <Node: (_ElementWiseOp_646(IndexBackward0))>, <Node: (_ElementWiseOp_647(BmmBackward0))>, <Node: (_Reshape_648())>, <Node: (_Reshape_649())>, <Node: (_ElementWiseOp_650(CloneBackward0))>, <Node: (_ElementWiseOp_651(ExpandBackward0))>, <Node: (_ElementWiseOp_652(TransposeBackward0))>, <Node: (_ElementWiseOp_653(SelectBackward0))>, <Node: (_ElementWiseOp_654(CloneBackward0))>, <Node: (_ElementWiseOp_655(ExpandBackward0))>, <Node: (_ElementWiseOp_656(MulBackward0))>, <Node: (_ElementWiseOp_657(SelectBackward0))>, <Node: (_ElementWiseOp_658(CloneBackward0))>, <Node: (_ElementWiseOp_659(PermuteBackward0))>, <Node: (_Reshape_660())>, <Node: (norm3 (LayerNorm((768,), eps=1e-05, elementwise_affine=True)))>, <Node: (_ElementWiseOp_661(AddBackward0))>, <Node: (_ElementWiseOp_662(AddBackward0))>, <Node: (stages.3.blocks.1.ffn.layers.1 (Linear(in_features=3072, out_features=768, bias=True)))>, <Node: (_ElementWiseOp_663(AddmmBackward0))>, <Node: (_Reshape_664())>, <Node: (_ElementWiseOp_665(TBackward0))>, <Node: (_ElementWiseOp_666(GeluBackward0))>, <Node: (stages.3.blocks.1.ffn.layers.0.0 (Linear(in_features=768, out_features=3072, bias=True)))>, <Node: (_ElementWiseOp_667(AddmmBackward0))>, <Node: (_Reshape_668())>, <Node: (_ElementWiseOp_669(TBackward0))>, <Node: (stages.3.blocks.1.norm2 (LayerNorm((768,), eps=1e-05, elementwise_affine=True)))>, <Node: (_Reshape_670())>, <Node: (_ElementWiseOp_671(AddBackward0))>, <Node: (_ElementWiseOp_672(AddBackward0))>, <Node: (stages.3.blocks.0.ffn.layers.1 (Linear(in_features=3072, out_features=768, bias=True)))>, <Node: (_ElementWiseOp_673(AddmmBackward0))>, <Node: (_Reshape_674())>, <Node: (_ElementWiseOp_675(TBackward0))>, <Node: (_ElementWiseOp_676(GeluBackward0))>, <Node: (stages.3.blocks.0.ffn.layers.0.0 (Linear(in_features=768, out_features=3072, bias=True)))>, <Node: (_ElementWiseOp_677(AddmmBackward0))>, <Node: (_Reshape_678())>, <Node: (_ElementWiseOp_679(TBackward0))>, <Node: (stages.3.blocks.0.norm2 (LayerNorm((768,), eps=1e-05, elementwise_affine=True)))>, <Node: (_Reshape_680())>, <Node: (stages.2.downsample (PatchMerging(
  (adap_padding): AdaptivePadding()
  (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
  (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (reduction): Linear(in_features=1536, out_features=768, bias=False)
)))>, <Node: (_ElementWiseOp_681(MmBackward0))>, <Node: (_Reshape_682())>, <Node: (_ElementWiseOp_683(TBackward0))>, <Node: (_ElementWiseOp_684(NativeLayerNormBackward0))>, <Node: (_ElementWiseOp_685(TransposeBackward0))>, <Node: (_ElementWiseOp_686(Im2ColBackward0))>, <Node: (_ElementWiseOp_687(PermuteBackward0))>, <Node: (_Reshape_688())>, <Node: (_ElementWiseOp_689(SliceBackward0))>, <Node: (_ElementWiseOp_690(SliceBackward0))>, <Node: (_ElementWiseOp_691(SliceBackward0))>, <Node: (_Reshape_692())>, <Node: (_ElementWiseOp_693(CloneBackward0))>, <Node: (_ElementWiseOp_694(PermuteBackward0))>, <Node: (_Reshape_695())>, <Node: (_Reshape_696())>, <Node: (stages.3.blocks.0.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=768, out_features=2304, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=768, out_features=768, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)))>, <Node: (_ElementWiseOp_697(CloneBackward0))>, <Node: (_ElementWiseOp_698(TransposeBackward0))>, <Node: (_Reshape_699())>, <Node: (_ElementWiseOp_700(BmmBackward0))>, <Node: (_Reshape_701())>, <Node: (_Reshape_702())>, <Node: (_ElementWiseOp_703(CloneBackward0))>, <Node: (_ElementWiseOp_704(ExpandBackward0))>, <Node: (_ElementWiseOp_705(SelectBackward0))>, <Node: (_ElementWiseOp_706(PermuteBackward0))>, <Node: (_Reshape_707())>, <Node: (_Reshape_708())>, <Node: (_ElementWiseOp_709(AddmmBackward0))>, <Node: (_Reshape_710())>, <Node: (_ElementWiseOp_711(TBackward0))>, <Node: (_Reshape_712())>, <Node: (_Reshape_713())>, <Node: (_ElementWiseOp_714(CloneBackward0))>, <Node: (_ElementWiseOp_715(PermuteBackward0))>, <Node: (_Reshape_716())>, <Node: (_ElementWiseOp_717(ConstantPadNdBackward0))>, <Node: (_Reshape_718())>, <Node: (stages.3.blocks.0.norm1 (LayerNorm((768,), eps=1e-05, elementwise_affine=True)))>, <Node: (_ElementWiseOp_719(ExpandBackward0))>, <Node: (_ElementWiseOp_720(SoftmaxBackward0))>, <Node: (_ElementWiseOp_721(AddBackward0))>, <Node: (_Reshape_722())>, <Node: (_ElementWiseOp_723(UnsqueezeBackward0))>, <Node: (_ElementWiseOp_724(CloneBackward0))>, <Node: (_ElementWiseOp_725(PermuteBackward0))>, <Node: (_Reshape_726())>, <Node: (_ElementWiseOp_727(IndexBackward0))>, <Node: (_ElementWiseOp_728(BmmBackward0))>, <Node: (_Reshape_729())>, <Node: (_Reshape_730())>, <Node: (_ElementWiseOp_731(CloneBackward0))>, <Node: (_ElementWiseOp_732(ExpandBackward0))>, <Node: (_ElementWiseOp_733(TransposeBackward0))>, <Node: (_ElementWiseOp_734(SelectBackward0))>, <Node: (_ElementWiseOp_735(CloneBackward0))>, <Node: (_ElementWiseOp_736(ExpandBackward0))>, <Node: (_ElementWiseOp_737(MulBackward0))>, <Node: (_ElementWiseOp_738(SelectBackward0))>, <Node: (_ElementWiseOp_739(SliceBackward0))>, <Node: (_ElementWiseOp_740(SliceBackward0))>, <Node: (_ElementWiseOp_741(SliceBackward0))>, <Node: (_ElementWiseOp_742(RollBackward0))>, <Node: (_Reshape_743())>, <Node: (_ElementWiseOp_744(CloneBackward0))>, <Node: (_ElementWiseOp_745(PermuteBackward0))>, <Node: (_Reshape_746())>, <Node: (_Reshape_747())>, <Node: (stages.3.blocks.1.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=768, out_features=2304, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=768, out_features=768, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)))>, <Node: (_ElementWiseOp_748(CloneBackward0))>, <Node: (_ElementWiseOp_749(TransposeBackward0))>, <Node: (_Reshape_750())>, <Node: (_ElementWiseOp_751(BmmBackward0))>, <Node: (_Reshape_752())>, <Node: (_Reshape_753())>, <Node: (_ElementWiseOp_754(CloneBackward0))>, <Node: (_ElementWiseOp_755(ExpandBackward0))>, <Node: (_ElementWiseOp_756(SelectBackward0))>, <Node: (_ElementWiseOp_757(PermuteBackward0))>, <Node: (_Reshape_758())>, <Node: (_Reshape_759())>, <Node: (_ElementWiseOp_760(AddmmBackward0))>, <Node: (_Reshape_761())>, <Node: (_ElementWiseOp_762(TBackward0))>, <Node: (_Reshape_763())>, <Node: (_Reshape_764())>, <Node: (_ElementWiseOp_765(CloneBackward0))>, <Node: (_ElementWiseOp_766(PermuteBackward0))>, <Node: (_Reshape_767())>, <Node: (_ElementWiseOp_768(RollBackward0))>, <Node: (_ElementWiseOp_769(ConstantPadNdBackward0))>, <Node: (_Reshape_770())>, <Node: (stages.3.blocks.1.norm1 (LayerNorm((768,), eps=1e-05, elementwise_affine=True)))>, <Node: (_ElementWiseOp_771(ExpandBackward0))>, <Node: (_ElementWiseOp_772(SoftmaxBackward0))>, <Node: (_Reshape_773())>, <Node: (_ElementWiseOp_774(AddBackward0))>, <Node: (_Reshape_775())>, <Node: (_ElementWiseOp_776(AddBackward0))>, <Node: (_Reshape_777())>, <Node: (_ElementWiseOp_778(UnsqueezeBackward0))>, <Node: (_ElementWiseOp_779(CloneBackward0))>, <Node: (_ElementWiseOp_780(PermuteBackward0))>, <Node: (_Reshape_781())>, <Node: (_ElementWiseOp_782(IndexBackward0))>, <Node: (_ElementWiseOp_783(BmmBackward0))>, <Node: (_Reshape_784())>, <Node: (_Reshape_785())>, <Node: (_ElementWiseOp_786(CloneBackward0))>, <Node: (_ElementWiseOp_787(ExpandBackward0))>, <Node: (_ElementWiseOp_788(TransposeBackward0))>, <Node: (_ElementWiseOp_789(SelectBackward0))>, <Node: (_ElementWiseOp_790(CloneBackward0))>, <Node: (_ElementWiseOp_791(ExpandBackward0))>, <Node: (_ElementWiseOp_792(MulBackward0))>, <Node: (_ElementWiseOp_793(SelectBackward0))>]) 

PatchMergingPruner () get_out_channels/  192 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
PatchMergingPruner () get_in_channels/  96 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
WindowMSAPruner layer.embed_dims =  96
WindowMSAPruner layer.qkv.in_features =  96
WindowMSAPruner layer.embed_dims =  96
WindowMSAPruner layer.qkv.in_features =  96
WindowMSAPruner layer.embed_dims =  192
WindowMSAPruner layer.qkv.in_features =  192
WindowMSAPruner layer.embed_dims =  192
WindowMSAPruner layer.qkv.in_features =  192
PatchMergingPruner () get_out_channels/  384 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
PatchMergingPruner () get_in_channels/  192 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
WindowMSAPruner layer.embed_dims =  384
WindowMSAPruner layer.qkv.in_features =  384
WindowMSAPruner layer.embed_dims =  384
WindowMSAPruner layer.qkv.in_features =  384
WindowMSAPruner layer.embed_dims =  384
WindowMSAPruner layer.qkv.in_features =  384
WindowMSAPruner layer.embed_dims =  384
WindowMSAPruner layer.qkv.in_features =  384
WindowMSAPruner layer.embed_dims =  384
WindowMSAPruner layer.qkv.in_features =  384
WindowMSAPruner layer.embed_dims =  384
WindowMSAPruner layer.qkv.in_features =  384
PatchMergingPruner () get_out_channels/  768 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
PatchMergingPruner () get_in_channels/  384 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
WindowMSAPruner layer.embed_dims =  768
WindowMSAPruner layer.qkv.in_features =  768
WindowMSAPruner layer.embed_dims =  768
WindowMSAPruner layer.qkv.in_features =  768
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
	get_all_groups -- Adding layer:  <class 'torch.nn.modules.normalization.LayerNorm'>
PatchMergingPruner () get_in_channels/  192 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
WindowMSAPruner layer.qkv.in_features =  192
PatchMergingPruner () get_out_channels/  192 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
WindowMSAPruner layer.qkv.in_features =  192
WindowMSAPruner layer.embed_dims =  192
WindowMSAPruner layer.embed_dims =  192
		prune_local()/  _check_pruning_ratio OK
idxs =  96
prunable_chs =  192
idxs =  96
prunable_chs =  192
PatchMergingPruner () get_in_channels/  192 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
idxs =  96
prunable_chs =  192
idxs =  96
prunable_chs =  192
idxs =  96
prunable_chs =  192
WindowMSAPruner layer.qkv.in_features =  192
PatchMergingPruner () get_out_channels/  192 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
idxs =  96
prunable_chs =  192
idxs =  96
prunable_chs =  192
idxs =  96
prunable_chs =  192
WindowMSAPruner layer.qkv.in_features =  192
WindowMSAPruner layer.embed_dims =  192
idxs =  96
prunable_chs =  192
WindowMSAPruner layer.embed_dims =  192
idxs =  96
prunable_chs =  192
pruner.step 
--------------------------------
          Pruning Group
--------------------------------
[0] prune_out_channels on norm1 (LayerNorm((192,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on norm1 (LayerNorm((192,), eps=1e-05, elementwise_affine=True)), #idxs=96
[1] prune_out_channels on norm1 (LayerNorm((192,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _ElementWiseOp_3(AddBackward0), #idxs=96
[2] prune_out_channels on norm1 (LayerNorm((192,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_2(), #idxs=96
[3] prune_out_channels on _Reshape_2() => prune_out_channels on _ElementWiseOp_1(PermuteBackward0), #idxs=96
[4] prune_out_channels on _ElementWiseOp_1(PermuteBackward0) => prune_out_channels on _ElementWiseOp_0(CloneBackward0), #idxs=96
[5] prune_out_channels on _ElementWiseOp_3(AddBackward0) => prune_out_channels on _ElementWiseOp_4(AddBackward0), #idxs=96
[6] prune_out_channels on _ElementWiseOp_3(AddBackward0) => prune_out_channels on stages.1.blocks.1.ffn.layers.1 (Linear(in_features=768, out_features=192, bias=True)), #idxs=96
[7] prune_out_channels on _ElementWiseOp_3(AddBackward0) => prune_out_channels on _Reshape_342(), #idxs=96
[8] prune_out_channels on _Reshape_342() => prune_out_channels on _ElementWiseOp_341(PermuteBackward0), #idxs=96
[9] prune_out_channels on _ElementWiseOp_341(PermuteBackward0) => prune_out_channels on _ElementWiseOp_340(ConstantPadNdBackward0), #idxs=96
[10] prune_out_channels on _ElementWiseOp_340(ConstantPadNdBackward0) => prune_out_channels on _ElementWiseOp_339(Im2ColBackward0), #idxs=96
[11] prune_out_channels on _ElementWiseOp_339(Im2ColBackward0) => prune_out_channels on _ElementWiseOp_338(TransposeBackward0), #idxs=96
[12] prune_out_channels on _ElementWiseOp_338(TransposeBackward0) => prune_out_channels on _ElementWiseOp_337(NativeLayerNormBackward0), #idxs=96
[13] prune_out_channels on _ElementWiseOp_337(NativeLayerNormBackward0) => prune_out_channels on _Reshape_335(), #idxs=96
[14] prune_out_channels on _Reshape_335() => prune_out_channels on _ElementWiseOp_334(MmBackward0), #idxs=96
[15] prune_out_channels on _ElementWiseOp_334(MmBackward0) => prune_out_channels on _ElementWiseOp_336(TBackward0), #idxs=96
[16] prune_out_channels on _ElementWiseOp_334(MmBackward0) => prune_in_channels on stages.1.downsample (PatchMerging(
  (adap_padding): AdaptivePadding()
  (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (reduction): Linear(in_features=768, out_features=384, bias=False)
)), #idxs=96
[17] prune_out_channels on _ElementWiseOp_4(AddBackward0) => prune_out_channels on _Reshape_12(), #idxs=96
[18] prune_out_channels on _ElementWiseOp_4(AddBackward0) => prune_out_channels on _ElementWiseOp_13(AddBackward0), #idxs=96
[19] prune_out_channels on _ElementWiseOp_4(AddBackward0) => prune_out_channels on stages.1.blocks.1.norm2 (LayerNorm((192,), eps=1e-05, elementwise_affine=True)), #idxs=96
[20] prune_out_channels on stages.1.blocks.1.norm2 (LayerNorm((192,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_10(), #idxs=96
[21] prune_out_channels on _Reshape_10() => prune_out_channels on _ElementWiseOp_9(AddmmBackward0), #idxs=96
[22] prune_out_channels on _ElementWiseOp_9(AddmmBackward0) => prune_out_channels on _ElementWiseOp_11(TBackward0), #idxs=96
[23] prune_out_channels on _ElementWiseOp_9(AddmmBackward0) => prune_in_channels on stages.1.blocks.1.ffn.layers.0.0 (Linear(in_features=192, out_features=768, bias=True)), #idxs=96
[24] prune_out_channels on _ElementWiseOp_13(AddBackward0) => prune_out_channels on _ElementWiseOp_14(AddBackward0), #idxs=96
[25] prune_out_channels on _ElementWiseOp_13(AddBackward0) => prune_out_channels on stages.1.blocks.0.ffn.layers.1 (Linear(in_features=768, out_features=192, bias=True)), #idxs=96
[26] prune_out_channels on _ElementWiseOp_13(AddBackward0) => prune_out_channels on stages.1.blocks.1.norm1 (LayerNorm((192,), eps=1e-05, elementwise_affine=True)), #idxs=96
[27] prune_out_channels on stages.1.blocks.1.norm1 (LayerNorm((192,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_247(), #idxs=96
[28] prune_out_channels on _Reshape_247() => prune_out_channels on _ElementWiseOp_246(ConstantPadNdBackward0), #idxs=96
[29] prune_out_channels on _ElementWiseOp_246(ConstantPadNdBackward0) => prune_out_channels on _ElementWiseOp_245(RollBackward0), #idxs=96
[30] prune_out_channels on _ElementWiseOp_245(RollBackward0) => prune_out_channels on _Reshape_244(), #idxs=96
[31] prune_out_channels on _Reshape_244() => prune_out_channels on _ElementWiseOp_243(PermuteBackward0), #idxs=96
[32] prune_out_channels on _ElementWiseOp_243(PermuteBackward0) => prune_out_channels on _ElementWiseOp_242(CloneBackward0), #idxs=96
[33] prune_out_channels on _ElementWiseOp_242(CloneBackward0) => prune_out_channels on _Reshape_241(), #idxs=96
[34] prune_out_channels on _Reshape_241() => prune_out_channels on _Reshape_240(), #idxs=96
[35] prune_out_channels on _Reshape_240() => prune_out_channels on _Reshape_238(), #idxs=96
[36] prune_out_channels on _Reshape_238() => prune_out_channels on _ElementWiseOp_237(AddmmBackward0), #idxs=96
[37] prune_out_channels on _ElementWiseOp_237(AddmmBackward0) => prune_out_channels on _ElementWiseOp_239(TBackward0), #idxs=96
[38] prune_out_channels on _ElementWiseOp_237(AddmmBackward0) => prune_out_channels on _Reshape_236(), #idxs=96
[39] prune_out_channels on _Reshape_236() => prune_out_channels on _Reshape_235(), #idxs=96
[40] prune_out_channels on _Reshape_235() => prune_out_channels on _ElementWiseOp_234(PermuteBackward0), #idxs=96
[41] prune_out_channels on _ElementWiseOp_234(PermuteBackward0) => prune_out_channels on _ElementWiseOp_233(SelectBackward0), #idxs=96
[42] prune_out_channels on _ElementWiseOp_234(PermuteBackward0) => prune_out_channels on _ElementWiseOp_266(SelectBackward0), #idxs=96
[43] prune_out_channels on _ElementWiseOp_234(PermuteBackward0) => prune_out_channels on _ElementWiseOp_270(SelectBackward0), #idxs=96
[44] prune_out_channels on _ElementWiseOp_270(SelectBackward0) => prune_out_channels on _ElementWiseOp_269(MulBackward0), #idxs=96
[45] prune_out_channels on _ElementWiseOp_269(MulBackward0) => prune_out_channels on _ElementWiseOp_268(ExpandBackward0), #idxs=96
[46] prune_out_channels on _ElementWiseOp_268(ExpandBackward0) => prune_out_channels on _ElementWiseOp_267(CloneBackward0), #idxs=96
[47] prune_out_channels on _ElementWiseOp_267(CloneBackward0) => prune_out_channels on _Reshape_261(), #idxs=96
[48] prune_out_channels on _Reshape_261() => prune_out_channels on _ElementWiseOp_260(BmmBackward0), #idxs=96
[49] prune_out_channels on _ElementWiseOp_260(BmmBackward0) => prune_out_channels on _Reshape_262(), #idxs=96
[50] prune_out_channels on _ElementWiseOp_260(BmmBackward0) => prune_out_channels on _Reshape_254(), #idxs=96
[51] prune_out_channels on _Reshape_254() => prune_out_channels on _ElementWiseOp_253(AddBackward0), #idxs=96
[52] prune_out_channels on _ElementWiseOp_253(AddBackward0) => prune_out_channels on _ElementWiseOp_255(UnsqueezeBackward0), #idxs=96
[53] prune_out_channels on _ElementWiseOp_253(AddBackward0) => prune_out_channels on _Reshape_252(), #idxs=96
[54] prune_out_channels on _Reshape_252() => prune_out_channels on _ElementWiseOp_251(AddBackward0), #idxs=96
[55] prune_out_channels on _ElementWiseOp_251(AddBackward0) => prune_out_channels on _Reshape_250(), #idxs=96
[56] prune_out_channels on _Reshape_250() => prune_out_channels on _ElementWiseOp_249(SoftmaxBackward0), #idxs=96
[57] prune_out_channels on _ElementWiseOp_249(SoftmaxBackward0) => prune_out_channels on _ElementWiseOp_248(ExpandBackward0), #idxs=96
[58] prune_out_channels on _ElementWiseOp_248(ExpandBackward0) => prune_out_channels on _Reshape_229(), #idxs=96
[59] prune_out_channels on _Reshape_229() => prune_out_channels on _ElementWiseOp_228(BmmBackward0), #idxs=96
[60] prune_out_channels on _ElementWiseOp_228(BmmBackward0) => prune_out_channels on _Reshape_230(), #idxs=96
[61] prune_out_channels on _ElementWiseOp_228(BmmBackward0) => prune_out_channels on _Reshape_227(), #idxs=96
[62] prune_out_channels on _Reshape_227() => prune_out_channels on _ElementWiseOp_226(TransposeBackward0), #idxs=96
[63] prune_out_channels on _ElementWiseOp_226(TransposeBackward0) => prune_out_channels on _ElementWiseOp_225(CloneBackward0), #idxs=96
[64] prune_out_channels on _ElementWiseOp_225(CloneBackward0) => prune_in_channels on stages.1.blocks.1.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=192, out_features=576, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=192, out_features=192, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)), #idxs=96
[65] prune_out_channels on _Reshape_230() => prune_out_channels on _ElementWiseOp_231(CloneBackward0), #idxs=96
[66] prune_out_channels on _ElementWiseOp_231(CloneBackward0) => prune_out_channels on _ElementWiseOp_232(ExpandBackward0), #idxs=96
[67] prune_out_channels on _ElementWiseOp_255(UnsqueezeBackward0) => prune_out_channels on _ElementWiseOp_256(CloneBackward0), #idxs=96
[68] prune_out_channels on _ElementWiseOp_256(CloneBackward0) => prune_out_channels on _ElementWiseOp_257(PermuteBackward0), #idxs=96
[69] prune_out_channels on _ElementWiseOp_257(PermuteBackward0) => prune_out_channels on _Reshape_258(), #idxs=96
[70] prune_out_channels on _Reshape_258() => prune_out_channels on _ElementWiseOp_259(IndexBackward0), #idxs=96
[71] prune_out_channels on _Reshape_262() => prune_out_channels on _ElementWiseOp_263(CloneBackward0), #idxs=96
[72] prune_out_channels on _ElementWiseOp_263(CloneBackward0) => prune_out_channels on _ElementWiseOp_264(ExpandBackward0), #idxs=96
[73] prune_out_channels on _ElementWiseOp_264(ExpandBackward0) => prune_out_channels on _ElementWiseOp_265(TransposeBackward0), #idxs=96
[74] prune_out_channels on _ElementWiseOp_14(AddBackward0) => prune_out_channels on _Reshape_22(), #idxs=96
[75] prune_out_channels on _ElementWiseOp_14(AddBackward0) => prune_out_channels on stages.0.downsample (PatchMerging(
  (adap_padding): AdaptivePadding()
  (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
  (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (reduction): Linear(in_features=384, out_features=192, bias=False)
)), #idxs=96
[76] prune_out_channels on _ElementWiseOp_14(AddBackward0) => prune_out_channels on stages.1.blocks.0.norm2 (LayerNorm((192,), eps=1e-05, elementwise_affine=True)), #idxs=96
[77] prune_out_channels on stages.1.blocks.0.norm2 (LayerNorm((192,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_20(), #idxs=96
[78] prune_out_channels on _Reshape_20() => prune_out_channels on _ElementWiseOp_19(AddmmBackward0), #idxs=96
[79] prune_out_channels on _ElementWiseOp_19(AddmmBackward0) => prune_out_channels on _ElementWiseOp_21(TBackward0), #idxs=96
[80] prune_out_channels on _ElementWiseOp_19(AddmmBackward0) => prune_in_channels on stages.1.blocks.0.ffn.layers.0.0 (Linear(in_features=192, out_features=768, bias=True)), #idxs=96
[81] prune_out_channels on stages.0.downsample (PatchMerging(
  (adap_padding): AdaptivePadding()
  (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
  (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (reduction): Linear(in_features=384, out_features=192, bias=False)
)) => prune_out_channels on stages.1.blocks.0.norm1 (LayerNorm((192,), eps=1e-05, elementwise_affine=True)), #idxs=96
[82] prune_out_channels on stages.1.blocks.0.norm1 (LayerNorm((192,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_193(), #idxs=96
[83] prune_out_channels on _Reshape_193() => prune_out_channels on _ElementWiseOp_192(ConstantPadNdBackward0), #idxs=96
[84] prune_out_channels on _ElementWiseOp_192(ConstantPadNdBackward0) => prune_out_channels on _Reshape_191(), #idxs=96
[85] prune_out_channels on _Reshape_191() => prune_out_channels on _ElementWiseOp_190(PermuteBackward0), #idxs=96
[86] prune_out_channels on _ElementWiseOp_190(PermuteBackward0) => prune_out_channels on _ElementWiseOp_189(CloneBackward0), #idxs=96
[87] prune_out_channels on _ElementWiseOp_189(CloneBackward0) => prune_out_channels on _Reshape_188(), #idxs=96
[88] prune_out_channels on _Reshape_188() => prune_out_channels on _Reshape_187(), #idxs=96
[89] prune_out_channels on _Reshape_187() => prune_out_channels on _Reshape_185(), #idxs=96
[90] prune_out_channels on _Reshape_185() => prune_out_channels on _ElementWiseOp_184(AddmmBackward0), #idxs=96
[91] prune_out_channels on _ElementWiseOp_184(AddmmBackward0) => prune_out_channels on _ElementWiseOp_186(TBackward0), #idxs=96
[92] prune_out_channels on _ElementWiseOp_184(AddmmBackward0) => prune_out_channels on _Reshape_183(), #idxs=96
[93] prune_out_channels on _Reshape_183() => prune_out_channels on _Reshape_182(), #idxs=96
[94] prune_out_channels on _Reshape_182() => prune_out_channels on _ElementWiseOp_181(PermuteBackward0), #idxs=96
[95] prune_out_channels on _ElementWiseOp_181(PermuteBackward0) => prune_out_channels on _ElementWiseOp_180(SelectBackward0), #idxs=96
[96] prune_out_channels on _ElementWiseOp_181(PermuteBackward0) => prune_out_channels on _ElementWiseOp_209(SelectBackward0), #idxs=96
[97] prune_out_channels on _ElementWiseOp_181(PermuteBackward0) => prune_out_channels on _ElementWiseOp_213(SelectBackward0), #idxs=96
[98] prune_out_channels on _ElementWiseOp_213(SelectBackward0) => prune_out_channels on _ElementWiseOp_212(MulBackward0), #idxs=96
[99] prune_out_channels on _ElementWiseOp_212(MulBackward0) => prune_out_channels on _ElementWiseOp_211(ExpandBackward0), #idxs=96
[100] prune_out_channels on _ElementWiseOp_211(ExpandBackward0) => prune_out_channels on _ElementWiseOp_210(CloneBackward0), #idxs=96
[101] prune_out_channels on _ElementWiseOp_210(CloneBackward0) => prune_out_channels on _Reshape_204(), #idxs=96
[102] prune_out_channels on _Reshape_204() => prune_out_channels on _ElementWiseOp_203(BmmBackward0), #idxs=96
[103] prune_out_channels on _ElementWiseOp_203(BmmBackward0) => prune_out_channels on _Reshape_205(), #idxs=96
[104] prune_out_channels on _ElementWiseOp_203(BmmBackward0) => prune_out_channels on _Reshape_197(), #idxs=96
[105] prune_out_channels on _Reshape_197() => prune_out_channels on _ElementWiseOp_196(AddBackward0), #idxs=96
[106] prune_out_channels on _ElementWiseOp_196(AddBackward0) => prune_out_channels on _ElementWiseOp_198(UnsqueezeBackward0), #idxs=96
[107] prune_out_channels on _ElementWiseOp_196(AddBackward0) => prune_out_channels on _ElementWiseOp_195(SoftmaxBackward0), #idxs=96
[108] prune_out_channels on _ElementWiseOp_195(SoftmaxBackward0) => prune_out_channels on _ElementWiseOp_194(ExpandBackward0), #idxs=96
[109] prune_out_channels on _ElementWiseOp_194(ExpandBackward0) => prune_out_channels on _Reshape_176(), #idxs=96
[110] prune_out_channels on _Reshape_176() => prune_out_channels on _ElementWiseOp_175(BmmBackward0), #idxs=96
[111] prune_out_channels on _ElementWiseOp_175(BmmBackward0) => prune_out_channels on _Reshape_177(), #idxs=96
[112] prune_out_channels on _ElementWiseOp_175(BmmBackward0) => prune_out_channels on _Reshape_174(), #idxs=96
[113] prune_out_channels on _Reshape_174() => prune_out_channels on _ElementWiseOp_173(TransposeBackward0), #idxs=96
[114] prune_out_channels on _ElementWiseOp_173(TransposeBackward0) => prune_out_channels on _ElementWiseOp_172(CloneBackward0), #idxs=96
[115] prune_out_channels on _ElementWiseOp_172(CloneBackward0) => prune_in_channels on stages.1.blocks.0.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=192, out_features=576, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=192, out_features=192, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)), #idxs=96
[116] prune_out_channels on _Reshape_177() => prune_out_channels on _ElementWiseOp_178(CloneBackward0), #idxs=96
[117] prune_out_channels on _ElementWiseOp_178(CloneBackward0) => prune_out_channels on _ElementWiseOp_179(ExpandBackward0), #idxs=96
[118] prune_out_channels on _ElementWiseOp_198(UnsqueezeBackward0) => prune_out_channels on _ElementWiseOp_199(CloneBackward0), #idxs=96
[119] prune_out_channels on _ElementWiseOp_199(CloneBackward0) => prune_out_channels on _ElementWiseOp_200(PermuteBackward0), #idxs=96
[120] prune_out_channels on _ElementWiseOp_200(PermuteBackward0) => prune_out_channels on _Reshape_201(), #idxs=96
[121] prune_out_channels on _Reshape_201() => prune_out_channels on _ElementWiseOp_202(IndexBackward0), #idxs=96
[122] prune_out_channels on _Reshape_205() => prune_out_channels on _ElementWiseOp_206(CloneBackward0), #idxs=96
[123] prune_out_channels on _ElementWiseOp_206(CloneBackward0) => prune_out_channels on _ElementWiseOp_207(ExpandBackward0), #idxs=96
[124] prune_out_channels on _ElementWiseOp_207(ExpandBackward0) => prune_out_channels on _ElementWiseOp_208(TransposeBackward0), #idxs=96
[125] prune_out_channels on _Reshape_22() => prune_out_channels on _ElementWiseOp_162(CloneBackward0), #idxs=96
[126] prune_out_channels on _ElementWiseOp_162(CloneBackward0) => prune_out_channels on _ElementWiseOp_163(SliceBackward0), #idxs=96
[127] prune_out_channels on _ElementWiseOp_163(SliceBackward0) => prune_out_channels on _ElementWiseOp_164(SliceBackward0), #idxs=96
[128] prune_out_channels on _ElementWiseOp_164(SliceBackward0) => prune_out_channels on _ElementWiseOp_165(SliceBackward0), #idxs=96
[129] prune_out_channels on _ElementWiseOp_165(SliceBackward0) => prune_out_channels on _ElementWiseOp_166(SliceBackward0), #idxs=96
[130] prune_out_channels on _ElementWiseOp_166(SliceBackward0) => prune_out_channels on _Reshape_167(), #idxs=96
[131] prune_out_channels on _Reshape_167() => prune_out_channels on _ElementWiseOp_168(CloneBackward0), #idxs=96
[132] prune_out_channels on _ElementWiseOp_168(CloneBackward0) => prune_out_channels on _ElementWiseOp_169(PermuteBackward0), #idxs=96
[133] prune_out_channels on _ElementWiseOp_169(PermuteBackward0) => prune_out_channels on _Reshape_170(), #idxs=96
[134] prune_out_channels on _Reshape_170() => prune_out_channels on _Reshape_171(), #idxs=96
[135] prune_out_channels on _Reshape_171() => prune_out_channels on stages.1.blocks.0.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=192, out_features=576, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=192, out_features=192, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)), #idxs=96
[136] prune_out_channels on _Reshape_12() => prune_out_channels on _ElementWiseOp_214(CloneBackward0), #idxs=96
[137] prune_out_channels on _ElementWiseOp_214(CloneBackward0) => prune_out_channels on _ElementWiseOp_215(SliceBackward0), #idxs=96
[138] prune_out_channels on _ElementWiseOp_215(SliceBackward0) => prune_out_channels on _ElementWiseOp_216(SliceBackward0), #idxs=96
[139] prune_out_channels on _ElementWiseOp_216(SliceBackward0) => prune_out_channels on _ElementWiseOp_217(SliceBackward0), #idxs=96
[140] prune_out_channels on _ElementWiseOp_217(SliceBackward0) => prune_out_channels on _ElementWiseOp_218(SliceBackward0), #idxs=96
[141] prune_out_channels on _ElementWiseOp_218(SliceBackward0) => prune_out_channels on _ElementWiseOp_219(RollBackward0), #idxs=96
[142] prune_out_channels on _ElementWiseOp_219(RollBackward0) => prune_out_channels on _Reshape_220(), #idxs=96
[143] prune_out_channels on _Reshape_220() => prune_out_channels on _ElementWiseOp_221(CloneBackward0), #idxs=96
[144] prune_out_channels on _ElementWiseOp_221(CloneBackward0) => prune_out_channels on _ElementWiseOp_222(PermuteBackward0), #idxs=96
[145] prune_out_channels on _ElementWiseOp_222(PermuteBackward0) => prune_out_channels on _Reshape_223(), #idxs=96
[146] prune_out_channels on _Reshape_223() => prune_out_channels on _Reshape_224(), #idxs=96
[147] prune_out_channels on _Reshape_224() => prune_out_channels on stages.1.blocks.1.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=192, out_features=576, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=192, out_features=192, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)), #idxs=96
--------------------------------

PatchMergingPruner () prune_in_channels/  768 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
PatchMergingPruner () prune_in_channels/ dim =  192
len indx =  96
idxs_repeated =  384
WindowMSAPruner prune_in_channels() /  96
PatchMergingPruner () prune_out_channels/  192 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
WindowMSAPruner prune_in_channels() /  96
WindowMSAPruner prune_out_channels() /  96
WindowMSAPruner prune_out_channels/ dim =  192
	len indx =  96
WindowMSAPruner - idxs =  [2, 3, 4, 8, 9, 10, 11, 12, 13, 18, 19, 20, 23, 24, 30, 31, 34, 35, 39, 41, 42, 43, 46, 48, 51, 55, 56, 61, 67, 68, 72, 74, 76, 77, 78, 79, 80, 81, 85, 86, 89, 91, 94, 95, 96, 97, 99, 102, 103, 104, 107, 109, 111, 112, 113, 114, 116, 117, 120, 121, 122, 123, 124, 128, 129, 131, 132, 133, 134, 135, 142, 143, 145, 147, 148, 153, 154, 157, 158, 160, 164, 165, 168, 172, 174, 176, 177, 178, 180, 183, 185, 186, 187, 189, 190, 191]
	idxs_repeated =  384
WindowMSAPruner - idxs_repeated =  [2, 3, 4, 8, 9, 10, 11, 12, 13, 18, 19, 20, 23, 24, 30, 31, 34, 35, 39, 41, 42, 43, 46, 48, 51, 55, 56, 61, 67, 68, 72, 74, 76, 77, 78, 79, 80, 81, 85, 86, 89, 91, 94, 95, 96, 97, 99, 102, 103, 104, 107, 109, 111, 112, 113, 114, 116, 117, 120, 121, 122, 123, 124, 128, 129, 131, 132, 133, 134, 135, 142, 143, 145, 147, 148, 153, 154, 157, 158, 160, 164, 165, 168, 172, 174, 176, 177, 178, 180, 183, 185, 186, 187, 189, 190, 191, 194, 195, 196, 200, 201, 202, 203, 204, 205, 210, 211, 212, 215, 216, 222, 223, 226, 227, 231, 233, 234, 235, 238, 240, 243, 247, 248, 253, 259, 260, 264, 266, 268, 269, 270, 271, 272, 273, 277, 278, 281, 283, 286, 287, 288, 289, 291, 294, 295, 296, 299, 301, 303, 304, 305, 306, 308, 309, 312, 313, 314, 315, 316, 320, 321, 323, 324, 325, 326, 327, 334, 335, 337, 339, 340, 345, 346, 349, 350, 352, 356, 357, 360, 364, 366, 368, 369, 370, 372, 375, 377, 378, 379, 381, 382, 383, 386, 387, 388, 392, 393, 394, 395, 396, 397, 402, 403, 404, 407, 408, 414, 415, 418, 419, 423, 425, 426, 427, 430, 432, 435, 439, 440, 445, 451, 452, 456, 458, 460, 461, 462, 463, 464, 465, 469, 470, 473, 475, 478, 479, 480, 481, 483, 486, 487, 488, 491, 493, 495, 496, 497, 498, 500, 501, 504, 505, 506, 507, 508, 512, 513, 515, 516, 517, 518, 519, 526, 527, 529, 531, 532, 537, 538, 541, 542, 544, 548, 549, 552, 556, 558, 560, 561, 562, 564, 567, 569, 570, 571, 573, 574, 575, 578, 579, 580, 584, 585, 586, 587, 588, 589, 594, 595, 596, 599, 600, 606, 607, 610, 611, 615, 617, 618, 619, 622, 624, 627, 631, 632, 637, 643, 644, 648, 650, 652, 653, 654, 655, 656, 657, 661, 662, 665, 667, 670, 671, 672, 673, 675, 678, 679, 680, 683, 685, 687, 688, 689, 690, 692, 693, 696, 697, 698, 699, 700, 704, 705, 707, 708, 709, 710, 711, 718, 719, 721, 723, 724, 729, 730, 733, 734, 736, 740, 741, 744, 748, 750, 752, 753, 754, 756, 759, 761, 762, 763, 765, 766, 767]
WindowMSAPruner prune_out_channels idxs_repeated =  384
WindowMSAPruner prune_out_channels() /  96
WindowMSAPruner prune_out_channels/ dim =  192
	len indx =  96
WindowMSAPruner - idxs =  [2, 3, 4, 8, 9, 10, 11, 12, 13, 18, 19, 20, 23, 24, 30, 31, 34, 35, 39, 41, 42, 43, 46, 48, 51, 55, 56, 61, 67, 68, 72, 74, 76, 77, 78, 79, 80, 81, 85, 86, 89, 91, 94, 95, 96, 97, 99, 102, 103, 104, 107, 109, 111, 112, 113, 114, 116, 117, 120, 121, 122, 123, 124, 128, 129, 131, 132, 133, 134, 135, 142, 143, 145, 147, 148, 153, 154, 157, 158, 160, 164, 165, 168, 172, 174, 176, 177, 178, 180, 183, 185, 186, 187, 189, 190, 191]
	idxs_repeated =  384
WindowMSAPruner - idxs_repeated =  [2, 3, 4, 8, 9, 10, 11, 12, 13, 18, 19, 20, 23, 24, 30, 31, 34, 35, 39, 41, 42, 43, 46, 48, 51, 55, 56, 61, 67, 68, 72, 74, 76, 77, 78, 79, 80, 81, 85, 86, 89, 91, 94, 95, 96, 97, 99, 102, 103, 104, 107, 109, 111, 112, 113, 114, 116, 117, 120, 121, 122, 123, 124, 128, 129, 131, 132, 133, 134, 135, 142, 143, 145, 147, 148, 153, 154, 157, 158, 160, 164, 165, 168, 172, 174, 176, 177, 178, 180, 183, 185, 186, 187, 189, 190, 191, 194, 195, 196, 200, 201, 202, 203, 204, 205, 210, 211, 212, 215, 216, 222, 223, 226, 227, 231, 233, 234, 235, 238, 240, 243, 247, 248, 253, 259, 260, 264, 266, 268, 269, 270, 271, 272, 273, 277, 278, 281, 283, 286, 287, 288, 289, 291, 294, 295, 296, 299, 301, 303, 304, 305, 306, 308, 309, 312, 313, 314, 315, 316, 320, 321, 323, 324, 325, 326, 327, 334, 335, 337, 339, 340, 345, 346, 349, 350, 352, 356, 357, 360, 364, 366, 368, 369, 370, 372, 375, 377, 378, 379, 381, 382, 383, 386, 387, 388, 392, 393, 394, 395, 396, 397, 402, 403, 404, 407, 408, 414, 415, 418, 419, 423, 425, 426, 427, 430, 432, 435, 439, 440, 445, 451, 452, 456, 458, 460, 461, 462, 463, 464, 465, 469, 470, 473, 475, 478, 479, 480, 481, 483, 486, 487, 488, 491, 493, 495, 496, 497, 498, 500, 501, 504, 505, 506, 507, 508, 512, 513, 515, 516, 517, 518, 519, 526, 527, 529, 531, 532, 537, 538, 541, 542, 544, 548, 549, 552, 556, 558, 560, 561, 562, 564, 567, 569, 570, 571, 573, 574, 575, 578, 579, 580, 584, 585, 586, 587, 588, 589, 594, 595, 596, 599, 600, 606, 607, 610, 611, 615, 617, 618, 619, 622, 624, 627, 631, 632, 637, 643, 644, 648, 650, 652, 653, 654, 655, 656, 657, 661, 662, 665, 667, 670, 671, 672, 673, 675, 678, 679, 680, 683, 685, 687, 688, 689, 690, 692, 693, 696, 697, 698, 699, 700, 704, 705, 707, 708, 709, 710, 711, 718, 719, 721, 723, 724, 729, 730, 733, 734, 736, 740, 741, 744, 748, 750, 752, 753, 754, 756, 759, 761, 762, 763, 765, 766, 767]
WindowMSAPruner prune_out_channels idxs_repeated =  384
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
	get_all_groups -- Adding layer:  <class 'mmcv.cnn.bricks.wrappers.Linear'>
		prune_local()/  _check_pruning_ratio OK
idxs =  384
prunable_chs =  768
pruner.step 
--------------------------------
          Pruning Group
--------------------------------
[0] prune_out_channels on stages.1.blocks.1.ffn.layers.0.0 (Linear(in_features=96, out_features=768, bias=True)) => prune_out_channels on stages.1.blocks.1.ffn.layers.0.0 (Linear(in_features=96, out_features=768, bias=True)), #idxs=384
[1] prune_out_channels on stages.1.blocks.1.ffn.layers.0.0 (Linear(in_features=96, out_features=768, bias=True)) => prune_out_channels on _ElementWiseOp_8(GeluBackward0), #idxs=384
[2] prune_out_channels on _ElementWiseOp_8(GeluBackward0) => prune_out_channels on _Reshape_6(), #idxs=384
[3] prune_out_channels on _Reshape_6() => prune_out_channels on _ElementWiseOp_5(AddmmBackward0), #idxs=384
[4] prune_out_channels on _ElementWiseOp_5(AddmmBackward0) => prune_out_channels on _ElementWiseOp_7(TBackward0), #idxs=384
[5] prune_out_channels on _ElementWiseOp_5(AddmmBackward0) => prune_in_channels on stages.1.blocks.1.ffn.layers.1 (Linear(in_features=768, out_features=96, bias=True)), #idxs=384
--------------------------------

<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
continue 4
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
	get_all_groups -- Adding layer:  <class 'mmcv.cnn.bricks.wrappers.Linear'>
		prune_local()/  _check_pruning_ratio OK
idxs =  384
prunable_chs =  768
pruner.step 
--------------------------------
          Pruning Group
--------------------------------
[0] prune_out_channels on stages.1.blocks.0.ffn.layers.0.0 (Linear(in_features=96, out_features=768, bias=True)) => prune_out_channels on stages.1.blocks.0.ffn.layers.0.0 (Linear(in_features=96, out_features=768, bias=True)), #idxs=384
[1] prune_out_channels on stages.1.blocks.0.ffn.layers.0.0 (Linear(in_features=96, out_features=768, bias=True)) => prune_out_channels on _ElementWiseOp_18(GeluBackward0), #idxs=384
[2] prune_out_channels on _ElementWiseOp_18(GeluBackward0) => prune_out_channels on _Reshape_16(), #idxs=384
[3] prune_out_channels on _Reshape_16() => prune_out_channels on _ElementWiseOp_15(AddmmBackward0), #idxs=384
[4] prune_out_channels on _ElementWiseOp_15(AddmmBackward0) => prune_out_channels on _ElementWiseOp_17(TBackward0), #idxs=384
[5] prune_out_channels on _ElementWiseOp_15(AddmmBackward0) => prune_in_channels on stages.1.blocks.0.ffn.layers.1 (Linear(in_features=768, out_features=96, bias=True)), #idxs=384
--------------------------------

<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
continue 4
<class 'torch_pruning.ops._ReshapeOp'> continue 2
PatchMergingPruner () get_out_channels/  96 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
	get_all_groups -- Adding layer:  <class 'mmcv.cnn.bricks.wrappers.Linear'>
PatchMergingPruner () get_in_channels/  96 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
WindowMSAPruner layer.qkv.in_features =  96
WindowMSAPruner layer.qkv.in_features =  96
WindowMSAPruner layer.embed_dims =  96
WindowMSAPruner layer.embed_dims =  96
		prune_local()/  _check_pruning_ratio OK
idxs =  48
prunable_chs =  96
PatchMergingPruner () get_in_channels/  96 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
idxs =  48
prunable_chs =  96
idxs =  48
prunable_chs =  96
idxs =  48
prunable_chs =  96
WindowMSAPruner layer.qkv.in_features =  96
idxs =  48
prunable_chs =  96
idxs =  48
prunable_chs =  96
idxs =  48
prunable_chs =  96
WindowMSAPruner layer.qkv.in_features =  96
idxs =  48
prunable_chs =  96
WindowMSAPruner layer.embed_dims =  96
idxs =  48
prunable_chs =  96
WindowMSAPruner layer.embed_dims =  96
idxs =  48
prunable_chs =  96
pruner.step 
--------------------------------
          Pruning Group
--------------------------------
[0] prune_out_channels on stages.0.blocks.1.ffn.layers.1 (Linear(in_features=384, out_features=96, bias=True)) => prune_out_channels on stages.0.blocks.1.ffn.layers.1 (Linear(in_features=384, out_features=96, bias=True)), #idxs=48
[1] prune_out_channels on stages.0.blocks.1.ffn.layers.1 (Linear(in_features=384, out_features=96, bias=True)) => prune_out_channels on _ElementWiseOp_31(AddBackward0), #idxs=48
[2] prune_out_channels on _ElementWiseOp_31(AddBackward0) => prune_out_channels on _ElementWiseOp_32(AddBackward0), #idxs=48
[3] prune_out_channels on _ElementWiseOp_31(AddBackward0) => prune_out_channels on _Reshape_30(), #idxs=48
[4] prune_out_channels on _Reshape_30() => prune_out_channels on _ElementWiseOp_29(PermuteBackward0), #idxs=48
[5] prune_out_channels on _ElementWiseOp_29(PermuteBackward0) => prune_out_channels on _ElementWiseOp_28(Im2ColBackward0), #idxs=48
[6] prune_out_channels on _ElementWiseOp_28(Im2ColBackward0) => prune_out_channels on _ElementWiseOp_27(TransposeBackward0), #idxs=48
[7] prune_out_channels on _ElementWiseOp_27(TransposeBackward0) => prune_out_channels on _ElementWiseOp_26(NativeLayerNormBackward0), #idxs=48
[8] prune_out_channels on _ElementWiseOp_26(NativeLayerNormBackward0) => prune_out_channels on _Reshape_24(), #idxs=48
[9] prune_out_channels on _Reshape_24() => prune_out_channels on _ElementWiseOp_23(MmBackward0), #idxs=48
[10] prune_out_channels on _ElementWiseOp_23(MmBackward0) => prune_out_channels on _ElementWiseOp_25(TBackward0), #idxs=48
[11] prune_out_channels on _ElementWiseOp_23(MmBackward0) => prune_in_channels on stages.0.downsample (PatchMerging(
  (adap_padding): AdaptivePadding()
  (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
  (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (reduction): Linear(in_features=384, out_features=96, bias=False)
)), #idxs=48
[12] prune_out_channels on _ElementWiseOp_32(AddBackward0) => prune_out_channels on _Reshape_40(), #idxs=48
[13] prune_out_channels on _ElementWiseOp_32(AddBackward0) => prune_out_channels on _ElementWiseOp_41(AddBackward0), #idxs=48
[14] prune_out_channels on _ElementWiseOp_32(AddBackward0) => prune_out_channels on stages.0.blocks.1.norm2 (LayerNorm((96,), eps=1e-05, elementwise_affine=True)), #idxs=48
[15] prune_out_channels on stages.0.blocks.1.norm2 (LayerNorm((96,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_38(), #idxs=48
[16] prune_out_channels on _Reshape_38() => prune_out_channels on _ElementWiseOp_37(AddmmBackward0), #idxs=48
[17] prune_out_channels on _ElementWiseOp_37(AddmmBackward0) => prune_out_channels on _ElementWiseOp_39(TBackward0), #idxs=48
[18] prune_out_channels on _ElementWiseOp_37(AddmmBackward0) => prune_in_channels on stages.0.blocks.1.ffn.layers.0.0 (Linear(in_features=96, out_features=384, bias=True)), #idxs=48
[19] prune_out_channels on _ElementWiseOp_41(AddBackward0) => prune_out_channels on _ElementWiseOp_42(AddBackward0), #idxs=48
[20] prune_out_channels on _ElementWiseOp_41(AddBackward0) => prune_out_channels on stages.0.blocks.0.ffn.layers.1 (Linear(in_features=384, out_features=96, bias=True)), #idxs=48
[21] prune_out_channels on _ElementWiseOp_41(AddBackward0) => prune_out_channels on stages.0.blocks.1.norm1 (LayerNorm((96,), eps=1e-05, elementwise_affine=True)), #idxs=48
[22] prune_out_channels on stages.0.blocks.1.norm1 (LayerNorm((96,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_138(), #idxs=48
[23] prune_out_channels on _Reshape_138() => prune_out_channels on _ElementWiseOp_137(ConstantPadNdBackward0), #idxs=48
[24] prune_out_channels on _ElementWiseOp_137(ConstantPadNdBackward0) => prune_out_channels on _ElementWiseOp_136(RollBackward0), #idxs=48
[25] prune_out_channels on _ElementWiseOp_136(RollBackward0) => prune_out_channels on _Reshape_135(), #idxs=48
[26] prune_out_channels on _Reshape_135() => prune_out_channels on _ElementWiseOp_134(PermuteBackward0), #idxs=48
[27] prune_out_channels on _ElementWiseOp_134(PermuteBackward0) => prune_out_channels on _ElementWiseOp_133(CloneBackward0), #idxs=48
[28] prune_out_channels on _ElementWiseOp_133(CloneBackward0) => prune_out_channels on _Reshape_132(), #idxs=48
[29] prune_out_channels on _Reshape_132() => prune_out_channels on _Reshape_131(), #idxs=48
[30] prune_out_channels on _Reshape_131() => prune_out_channels on _Reshape_129(), #idxs=48
[31] prune_out_channels on _Reshape_129() => prune_out_channels on _ElementWiseOp_128(AddmmBackward0), #idxs=48
[32] prune_out_channels on _ElementWiseOp_128(AddmmBackward0) => prune_out_channels on _ElementWiseOp_130(TBackward0), #idxs=48
[33] prune_out_channels on _ElementWiseOp_128(AddmmBackward0) => prune_out_channels on _Reshape_127(), #idxs=48
[34] prune_out_channels on _Reshape_127() => prune_out_channels on _Reshape_126(), #idxs=48
[35] prune_out_channels on _Reshape_126() => prune_out_channels on _ElementWiseOp_125(PermuteBackward0), #idxs=48
[36] prune_out_channels on _ElementWiseOp_125(PermuteBackward0) => prune_out_channels on _ElementWiseOp_124(SelectBackward0), #idxs=48
[37] prune_out_channels on _ElementWiseOp_125(PermuteBackward0) => prune_out_channels on _ElementWiseOp_157(SelectBackward0), #idxs=48
[38] prune_out_channels on _ElementWiseOp_125(PermuteBackward0) => prune_out_channels on _ElementWiseOp_161(SelectBackward0), #idxs=48
[39] prune_out_channels on _ElementWiseOp_161(SelectBackward0) => prune_out_channels on _ElementWiseOp_160(MulBackward0), #idxs=48
[40] prune_out_channels on _ElementWiseOp_160(MulBackward0) => prune_out_channels on _ElementWiseOp_159(ExpandBackward0), #idxs=48
[41] prune_out_channels on _ElementWiseOp_159(ExpandBackward0) => prune_out_channels on _ElementWiseOp_158(CloneBackward0), #idxs=48
[42] prune_out_channels on _ElementWiseOp_158(CloneBackward0) => prune_out_channels on _Reshape_152(), #idxs=48
[43] prune_out_channels on _Reshape_152() => prune_out_channels on _ElementWiseOp_151(BmmBackward0), #idxs=48
[44] prune_out_channels on _ElementWiseOp_151(BmmBackward0) => prune_out_channels on _Reshape_153(), #idxs=48
[45] prune_out_channels on _ElementWiseOp_151(BmmBackward0) => prune_out_channels on _Reshape_145(), #idxs=48
[46] prune_out_channels on _Reshape_145() => prune_out_channels on _ElementWiseOp_144(AddBackward0), #idxs=48
[47] prune_out_channels on _ElementWiseOp_144(AddBackward0) => prune_out_channels on _ElementWiseOp_146(UnsqueezeBackward0), #idxs=48
[48] prune_out_channels on _ElementWiseOp_144(AddBackward0) => prune_out_channels on _Reshape_143(), #idxs=48
[49] prune_out_channels on _Reshape_143() => prune_out_channels on _ElementWiseOp_142(AddBackward0), #idxs=48
[50] prune_out_channels on _ElementWiseOp_142(AddBackward0) => prune_out_channels on _Reshape_141(), #idxs=48
[51] prune_out_channels on _Reshape_141() => prune_out_channels on _ElementWiseOp_140(SoftmaxBackward0), #idxs=48
[52] prune_out_channels on _ElementWiseOp_140(SoftmaxBackward0) => prune_out_channels on _ElementWiseOp_139(ExpandBackward0), #idxs=48
[53] prune_out_channels on _ElementWiseOp_139(ExpandBackward0) => prune_out_channels on _Reshape_120(), #idxs=48
[54] prune_out_channels on _Reshape_120() => prune_out_channels on _ElementWiseOp_119(BmmBackward0), #idxs=48
[55] prune_out_channels on _ElementWiseOp_119(BmmBackward0) => prune_out_channels on _Reshape_121(), #idxs=48
[56] prune_out_channels on _ElementWiseOp_119(BmmBackward0) => prune_out_channels on _Reshape_118(), #idxs=48
[57] prune_out_channels on _Reshape_118() => prune_out_channels on _ElementWiseOp_117(TransposeBackward0), #idxs=48
[58] prune_out_channels on _ElementWiseOp_117(TransposeBackward0) => prune_out_channels on _ElementWiseOp_116(CloneBackward0), #idxs=48
[59] prune_out_channels on _ElementWiseOp_116(CloneBackward0) => prune_in_channels on stages.0.blocks.1.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=96, out_features=288, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=96, out_features=96, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)), #idxs=48
[60] prune_out_channels on _Reshape_121() => prune_out_channels on _ElementWiseOp_122(CloneBackward0), #idxs=48
[61] prune_out_channels on _ElementWiseOp_122(CloneBackward0) => prune_out_channels on _ElementWiseOp_123(ExpandBackward0), #idxs=48
[62] prune_out_channels on _ElementWiseOp_146(UnsqueezeBackward0) => prune_out_channels on _ElementWiseOp_147(CloneBackward0), #idxs=48
[63] prune_out_channels on _ElementWiseOp_147(CloneBackward0) => prune_out_channels on _ElementWiseOp_148(PermuteBackward0), #idxs=48
[64] prune_out_channels on _ElementWiseOp_148(PermuteBackward0) => prune_out_channels on _Reshape_149(), #idxs=48
[65] prune_out_channels on _Reshape_149() => prune_out_channels on _ElementWiseOp_150(IndexBackward0), #idxs=48
[66] prune_out_channels on _Reshape_153() => prune_out_channels on _ElementWiseOp_154(CloneBackward0), #idxs=48
[67] prune_out_channels on _ElementWiseOp_154(CloneBackward0) => prune_out_channels on _ElementWiseOp_155(ExpandBackward0), #idxs=48
[68] prune_out_channels on _ElementWiseOp_155(ExpandBackward0) => prune_out_channels on _ElementWiseOp_156(TransposeBackward0), #idxs=48
[69] prune_out_channels on _ElementWiseOp_42(AddBackward0) => prune_out_channels on _Reshape_50(), #idxs=48
[70] prune_out_channels on _ElementWiseOp_42(AddBackward0) => prune_out_channels on patch_embed.norm (LayerNorm((96,), eps=1e-05, elementwise_affine=True)), #idxs=48
[71] prune_out_channels on _ElementWiseOp_42(AddBackward0) => prune_out_channels on stages.0.blocks.0.norm2 (LayerNorm((96,), eps=1e-05, elementwise_affine=True)), #idxs=48
[72] prune_out_channels on stages.0.blocks.0.norm2 (LayerNorm((96,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_48(), #idxs=48
[73] prune_out_channels on _Reshape_48() => prune_out_channels on _ElementWiseOp_47(AddmmBackward0), #idxs=48
[74] prune_out_channels on _ElementWiseOp_47(AddmmBackward0) => prune_out_channels on _ElementWiseOp_49(TBackward0), #idxs=48
[75] prune_out_channels on _ElementWiseOp_47(AddmmBackward0) => prune_in_channels on stages.0.blocks.0.ffn.layers.0.0 (Linear(in_features=96, out_features=384, bias=True)), #idxs=48
[76] prune_out_channels on patch_embed.norm (LayerNorm((96,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _ElementWiseOp_51(TransposeBackward0), #idxs=48
[77] prune_out_channels on patch_embed.norm (LayerNorm((96,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on stages.0.blocks.0.norm1 (LayerNorm((96,), eps=1e-05, elementwise_affine=True)), #idxs=48
[78] prune_out_channels on stages.0.blocks.0.norm1 (LayerNorm((96,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_84(), #idxs=48
[79] prune_out_channels on _Reshape_84() => prune_out_channels on _ElementWiseOp_83(ConstantPadNdBackward0), #idxs=48
[80] prune_out_channels on _ElementWiseOp_83(ConstantPadNdBackward0) => prune_out_channels on _Reshape_82(), #idxs=48
[81] prune_out_channels on _Reshape_82() => prune_out_channels on _ElementWiseOp_81(PermuteBackward0), #idxs=48
[82] prune_out_channels on _ElementWiseOp_81(PermuteBackward0) => prune_out_channels on _ElementWiseOp_80(CloneBackward0), #idxs=48
[83] prune_out_channels on _ElementWiseOp_80(CloneBackward0) => prune_out_channels on _Reshape_79(), #idxs=48
[84] prune_out_channels on _Reshape_79() => prune_out_channels on _Reshape_78(), #idxs=48
[85] prune_out_channels on _Reshape_78() => prune_out_channels on _Reshape_76(), #idxs=48
[86] prune_out_channels on _Reshape_76() => prune_out_channels on _ElementWiseOp_75(AddmmBackward0), #idxs=48
[87] prune_out_channels on _ElementWiseOp_75(AddmmBackward0) => prune_out_channels on _ElementWiseOp_77(TBackward0), #idxs=48
[88] prune_out_channels on _ElementWiseOp_75(AddmmBackward0) => prune_out_channels on _Reshape_74(), #idxs=48
[89] prune_out_channels on _Reshape_74() => prune_out_channels on _Reshape_73(), #idxs=48
[90] prune_out_channels on _Reshape_73() => prune_out_channels on _ElementWiseOp_72(PermuteBackward0), #idxs=48
[91] prune_out_channels on _ElementWiseOp_72(PermuteBackward0) => prune_out_channels on _ElementWiseOp_71(SelectBackward0), #idxs=48
[92] prune_out_channels on _ElementWiseOp_72(PermuteBackward0) => prune_out_channels on _ElementWiseOp_100(SelectBackward0), #idxs=48
[93] prune_out_channels on _ElementWiseOp_72(PermuteBackward0) => prune_out_channels on _ElementWiseOp_104(SelectBackward0), #idxs=48
[94] prune_out_channels on _ElementWiseOp_104(SelectBackward0) => prune_out_channels on _ElementWiseOp_103(MulBackward0), #idxs=48
[95] prune_out_channels on _ElementWiseOp_103(MulBackward0) => prune_out_channels on _ElementWiseOp_102(ExpandBackward0), #idxs=48
[96] prune_out_channels on _ElementWiseOp_102(ExpandBackward0) => prune_out_channels on _ElementWiseOp_101(CloneBackward0), #idxs=48
[97] prune_out_channels on _ElementWiseOp_101(CloneBackward0) => prune_out_channels on _Reshape_95(), #idxs=48
[98] prune_out_channels on _Reshape_95() => prune_out_channels on _ElementWiseOp_94(BmmBackward0), #idxs=48
[99] prune_out_channels on _ElementWiseOp_94(BmmBackward0) => prune_out_channels on _Reshape_96(), #idxs=48
[100] prune_out_channels on _ElementWiseOp_94(BmmBackward0) => prune_out_channels on _Reshape_88(), #idxs=48
[101] prune_out_channels on _Reshape_88() => prune_out_channels on _ElementWiseOp_87(AddBackward0), #idxs=48
[102] prune_out_channels on _ElementWiseOp_87(AddBackward0) => prune_out_channels on _ElementWiseOp_89(UnsqueezeBackward0), #idxs=48
[103] prune_out_channels on _ElementWiseOp_87(AddBackward0) => prune_out_channels on _ElementWiseOp_86(SoftmaxBackward0), #idxs=48
[104] prune_out_channels on _ElementWiseOp_86(SoftmaxBackward0) => prune_out_channels on _ElementWiseOp_85(ExpandBackward0), #idxs=48
[105] prune_out_channels on _ElementWiseOp_85(ExpandBackward0) => prune_out_channels on _Reshape_67(), #idxs=48
[106] prune_out_channels on _Reshape_67() => prune_out_channels on _ElementWiseOp_66(BmmBackward0), #idxs=48
[107] prune_out_channels on _ElementWiseOp_66(BmmBackward0) => prune_out_channels on _Reshape_68(), #idxs=48
[108] prune_out_channels on _ElementWiseOp_66(BmmBackward0) => prune_out_channels on _Reshape_65(), #idxs=48
[109] prune_out_channels on _Reshape_65() => prune_out_channels on _ElementWiseOp_64(TransposeBackward0), #idxs=48
[110] prune_out_channels on _ElementWiseOp_64(TransposeBackward0) => prune_out_channels on _ElementWiseOp_63(CloneBackward0), #idxs=48
[111] prune_out_channels on _ElementWiseOp_63(CloneBackward0) => prune_in_channels on stages.0.blocks.0.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=96, out_features=288, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=96, out_features=96, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)), #idxs=48
[112] prune_out_channels on _Reshape_68() => prune_out_channels on _ElementWiseOp_69(CloneBackward0), #idxs=48
[113] prune_out_channels on _ElementWiseOp_69(CloneBackward0) => prune_out_channels on _ElementWiseOp_70(ExpandBackward0), #idxs=48
[114] prune_out_channels on _ElementWiseOp_89(UnsqueezeBackward0) => prune_out_channels on _ElementWiseOp_90(CloneBackward0), #idxs=48
[115] prune_out_channels on _ElementWiseOp_90(CloneBackward0) => prune_out_channels on _ElementWiseOp_91(PermuteBackward0), #idxs=48
[116] prune_out_channels on _ElementWiseOp_91(PermuteBackward0) => prune_out_channels on _Reshape_92(), #idxs=48
[117] prune_out_channels on _Reshape_92() => prune_out_channels on _ElementWiseOp_93(IndexBackward0), #idxs=48
[118] prune_out_channels on _Reshape_96() => prune_out_channels on _ElementWiseOp_97(CloneBackward0), #idxs=48
[119] prune_out_channels on _ElementWiseOp_97(CloneBackward0) => prune_out_channels on _ElementWiseOp_98(ExpandBackward0), #idxs=48
[120] prune_out_channels on _ElementWiseOp_98(ExpandBackward0) => prune_out_channels on _ElementWiseOp_99(TransposeBackward0), #idxs=48
[121] prune_out_channels on _ElementWiseOp_51(TransposeBackward0) => prune_out_channels on _Reshape_52(), #idxs=48
[122] prune_out_channels on _Reshape_52() => prune_out_channels on patch_embed.projection (Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))), #idxs=48
[123] prune_out_channels on _Reshape_50() => prune_out_channels on _ElementWiseOp_53(CloneBackward0), #idxs=48
[124] prune_out_channels on _ElementWiseOp_53(CloneBackward0) => prune_out_channels on _ElementWiseOp_54(SliceBackward0), #idxs=48
[125] prune_out_channels on _ElementWiseOp_54(SliceBackward0) => prune_out_channels on _ElementWiseOp_55(SliceBackward0), #idxs=48
[126] prune_out_channels on _ElementWiseOp_55(SliceBackward0) => prune_out_channels on _ElementWiseOp_56(SliceBackward0), #idxs=48
[127] prune_out_channels on _ElementWiseOp_56(SliceBackward0) => prune_out_channels on _ElementWiseOp_57(SliceBackward0), #idxs=48
[128] prune_out_channels on _ElementWiseOp_57(SliceBackward0) => prune_out_channels on _Reshape_58(), #idxs=48
[129] prune_out_channels on _Reshape_58() => prune_out_channels on _ElementWiseOp_59(CloneBackward0), #idxs=48
[130] prune_out_channels on _ElementWiseOp_59(CloneBackward0) => prune_out_channels on _ElementWiseOp_60(PermuteBackward0), #idxs=48
[131] prune_out_channels on _ElementWiseOp_60(PermuteBackward0) => prune_out_channels on _Reshape_61(), #idxs=48
[132] prune_out_channels on _Reshape_61() => prune_out_channels on _Reshape_62(), #idxs=48
[133] prune_out_channels on _Reshape_62() => prune_out_channels on stages.0.blocks.0.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=96, out_features=288, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=96, out_features=96, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)), #idxs=48
[134] prune_out_channels on _Reshape_40() => prune_out_channels on _ElementWiseOp_105(CloneBackward0), #idxs=48
[135] prune_out_channels on _ElementWiseOp_105(CloneBackward0) => prune_out_channels on _ElementWiseOp_106(SliceBackward0), #idxs=48
[136] prune_out_channels on _ElementWiseOp_106(SliceBackward0) => prune_out_channels on _ElementWiseOp_107(SliceBackward0), #idxs=48
[137] prune_out_channels on _ElementWiseOp_107(SliceBackward0) => prune_out_channels on _ElementWiseOp_108(SliceBackward0), #idxs=48
[138] prune_out_channels on _ElementWiseOp_108(SliceBackward0) => prune_out_channels on _ElementWiseOp_109(SliceBackward0), #idxs=48
[139] prune_out_channels on _ElementWiseOp_109(SliceBackward0) => prune_out_channels on _ElementWiseOp_110(RollBackward0), #idxs=48
[140] prune_out_channels on _ElementWiseOp_110(RollBackward0) => prune_out_channels on _Reshape_111(), #idxs=48
[141] prune_out_channels on _Reshape_111() => prune_out_channels on _ElementWiseOp_112(CloneBackward0), #idxs=48
[142] prune_out_channels on _ElementWiseOp_112(CloneBackward0) => prune_out_channels on _ElementWiseOp_113(PermuteBackward0), #idxs=48
[143] prune_out_channels on _ElementWiseOp_113(PermuteBackward0) => prune_out_channels on _Reshape_114(), #idxs=48
[144] prune_out_channels on _Reshape_114() => prune_out_channels on _Reshape_115(), #idxs=48
[145] prune_out_channels on _Reshape_115() => prune_out_channels on stages.0.blocks.1.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=96, out_features=288, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=96, out_features=96, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)), #idxs=48
--------------------------------

PatchMergingPruner () prune_in_channels/  384 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
PatchMergingPruner () prune_in_channels/ dim =  96
len indx =  48
idxs_repeated =  192
WindowMSAPruner prune_in_channels() /  48
WindowMSAPruner prune_in_channels() /  48
WindowMSAPruner prune_out_channels() /  48
WindowMSAPruner prune_out_channels/ dim =  96
	len indx =  48
WindowMSAPruner - idxs =  [8, 14, 15, 21, 22, 25, 26, 27, 28, 29, 30, 31, 32, 34, 35, 36, 37, 42, 44, 45, 47, 48, 50, 51, 52, 53, 55, 56, 58, 61, 63, 64, 65, 66, 67, 68, 69, 71, 76, 77, 79, 80, 86, 87, 90, 91, 93, 94]
	idxs_repeated =  192
WindowMSAPruner - idxs_repeated =  [8, 14, 15, 21, 22, 25, 26, 27, 28, 29, 30, 31, 32, 34, 35, 36, 37, 42, 44, 45, 47, 48, 50, 51, 52, 53, 55, 56, 58, 61, 63, 64, 65, 66, 67, 68, 69, 71, 76, 77, 79, 80, 86, 87, 90, 91, 93, 94, 104, 110, 111, 117, 118, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 138, 140, 141, 143, 144, 146, 147, 148, 149, 151, 152, 154, 157, 159, 160, 161, 162, 163, 164, 165, 167, 172, 173, 175, 176, 182, 183, 186, 187, 189, 190, 200, 206, 207, 213, 214, 217, 218, 219, 220, 221, 222, 223, 224, 226, 227, 228, 229, 234, 236, 237, 239, 240, 242, 243, 244, 245, 247, 248, 250, 253, 255, 256, 257, 258, 259, 260, 261, 263, 268, 269, 271, 272, 278, 279, 282, 283, 285, 286, 296, 302, 303, 309, 310, 313, 314, 315, 316, 317, 318, 319, 320, 322, 323, 324, 325, 330, 332, 333, 335, 336, 338, 339, 340, 341, 343, 344, 346, 349, 351, 352, 353, 354, 355, 356, 357, 359, 364, 365, 367, 368, 374, 375, 378, 379, 381, 382]
WindowMSAPruner prune_out_channels idxs_repeated =  192
WindowMSAPruner prune_out_channels() /  48
WindowMSAPruner prune_out_channels/ dim =  96
	len indx =  48
WindowMSAPruner - idxs =  [8, 14, 15, 21, 22, 25, 26, 27, 28, 29, 30, 31, 32, 34, 35, 36, 37, 42, 44, 45, 47, 48, 50, 51, 52, 53, 55, 56, 58, 61, 63, 64, 65, 66, 67, 68, 69, 71, 76, 77, 79, 80, 86, 87, 90, 91, 93, 94]
	idxs_repeated =  192
WindowMSAPruner - idxs_repeated =  [8, 14, 15, 21, 22, 25, 26, 27, 28, 29, 30, 31, 32, 34, 35, 36, 37, 42, 44, 45, 47, 48, 50, 51, 52, 53, 55, 56, 58, 61, 63, 64, 65, 66, 67, 68, 69, 71, 76, 77, 79, 80, 86, 87, 90, 91, 93, 94, 104, 110, 111, 117, 118, 121, 122, 123, 124, 125, 126, 127, 128, 130, 131, 132, 133, 138, 140, 141, 143, 144, 146, 147, 148, 149, 151, 152, 154, 157, 159, 160, 161, 162, 163, 164, 165, 167, 172, 173, 175, 176, 182, 183, 186, 187, 189, 190, 200, 206, 207, 213, 214, 217, 218, 219, 220, 221, 222, 223, 224, 226, 227, 228, 229, 234, 236, 237, 239, 240, 242, 243, 244, 245, 247, 248, 250, 253, 255, 256, 257, 258, 259, 260, 261, 263, 268, 269, 271, 272, 278, 279, 282, 283, 285, 286, 296, 302, 303, 309, 310, 313, 314, 315, 316, 317, 318, 319, 320, 322, 323, 324, 325, 330, 332, 333, 335, 336, 338, 339, 340, 341, 343, 344, 346, 349, 351, 352, 353, 354, 355, 356, 357, 359, 364, 365, 367, 368, 374, 375, 378, 379, 381, 382]
WindowMSAPruner prune_out_channels idxs_repeated =  192
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
	get_all_groups -- Adding layer:  <class 'mmcv.cnn.bricks.wrappers.Linear'>
		prune_local()/  _check_pruning_ratio OK
idxs =  192
prunable_chs =  384
pruner.step 
--------------------------------
          Pruning Group
--------------------------------
[0] prune_out_channels on stages.0.blocks.1.ffn.layers.0.0 (Linear(in_features=48, out_features=384, bias=True)) => prune_out_channels on stages.0.blocks.1.ffn.layers.0.0 (Linear(in_features=48, out_features=384, bias=True)), #idxs=192
[1] prune_out_channels on stages.0.blocks.1.ffn.layers.0.0 (Linear(in_features=48, out_features=384, bias=True)) => prune_out_channels on _ElementWiseOp_36(GeluBackward0), #idxs=192
[2] prune_out_channels on _ElementWiseOp_36(GeluBackward0) => prune_out_channels on _Reshape_34(), #idxs=192
[3] prune_out_channels on _Reshape_34() => prune_out_channels on _ElementWiseOp_33(AddmmBackward0), #idxs=192
[4] prune_out_channels on _ElementWiseOp_33(AddmmBackward0) => prune_out_channels on _ElementWiseOp_35(TBackward0), #idxs=192
[5] prune_out_channels on _ElementWiseOp_33(AddmmBackward0) => prune_in_channels on stages.0.blocks.1.ffn.layers.1 (Linear(in_features=384, out_features=48, bias=True)), #idxs=192
--------------------------------

<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
continue 4
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
	get_all_groups -- Adding layer:  <class 'mmcv.cnn.bricks.wrappers.Linear'>
		prune_local()/  _check_pruning_ratio OK
idxs =  192
prunable_chs =  384
pruner.step 
--------------------------------
          Pruning Group
--------------------------------
[0] prune_out_channels on stages.0.blocks.0.ffn.layers.0.0 (Linear(in_features=48, out_features=384, bias=True)) => prune_out_channels on stages.0.blocks.0.ffn.layers.0.0 (Linear(in_features=48, out_features=384, bias=True)), #idxs=192
[1] prune_out_channels on stages.0.blocks.0.ffn.layers.0.0 (Linear(in_features=48, out_features=384, bias=True)) => prune_out_channels on _ElementWiseOp_46(GeluBackward0), #idxs=192
[2] prune_out_channels on _ElementWiseOp_46(GeluBackward0) => prune_out_channels on _Reshape_44(), #idxs=192
[3] prune_out_channels on _Reshape_44() => prune_out_channels on _ElementWiseOp_43(AddmmBackward0), #idxs=192
[4] prune_out_channels on _ElementWiseOp_43(AddmmBackward0) => prune_out_channels on _ElementWiseOp_45(TBackward0), #idxs=192
[5] prune_out_channels on _ElementWiseOp_43(AddmmBackward0) => prune_in_channels on stages.0.blocks.0.ffn.layers.1 (Linear(in_features=384, out_features=48, bias=True)), #idxs=192
--------------------------------

<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
continue 4
<class 'torch_pruning.ops._ReshapeOp'> continue 2
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch.nn.modules.conv.Conv2d'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
WindowMSAPruner layer.embed_dims =  48
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
WindowMSAPruner layer.embed_dims =  48
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
WindowMSAPruner layer.embed_dims =  96
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
WindowMSAPruner layer.embed_dims =  96
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
	get_all_groups -- Adding layer:  <class 'torch.nn.modules.normalization.LayerNorm'>
PatchMergingPruner () get_in_channels/  384 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
WindowMSAPruner layer.qkv.in_features =  384
WindowMSAPruner layer.qkv.in_features =  384
WindowMSAPruner layer.qkv.in_features =  384
WindowMSAPruner layer.qkv.in_features =  384
WindowMSAPruner layer.qkv.in_features =  384
PatchMergingPruner () get_out_channels/  384 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
WindowMSAPruner layer.qkv.in_features =  384
WindowMSAPruner layer.embed_dims =  384
WindowMSAPruner layer.embed_dims =  384
WindowMSAPruner layer.embed_dims =  384
WindowMSAPruner layer.embed_dims =  384
WindowMSAPruner layer.embed_dims =  384
WindowMSAPruner layer.embed_dims =  384
		prune_local()/  _check_pruning_ratio OK
idxs =  192
prunable_chs =  384
idxs =  192
prunable_chs =  384
PatchMergingPruner () get_in_channels/  384 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
idxs =  192
prunable_chs =  384
idxs =  192
prunable_chs =  384
idxs =  192
prunable_chs =  384
WindowMSAPruner layer.qkv.in_features =  384
idxs =  192
prunable_chs =  384
idxs =  192
prunable_chs =  384
idxs =  192
prunable_chs =  384
WindowMSAPruner layer.qkv.in_features =  384
idxs =  192
prunable_chs =  384
idxs =  192
prunable_chs =  384
idxs =  192
prunable_chs =  384
WindowMSAPruner layer.qkv.in_features =  384
idxs =  192
prunable_chs =  384
idxs =  192
prunable_chs =  384
idxs =  192
prunable_chs =  384
WindowMSAPruner layer.qkv.in_features =  384
idxs =  192
prunable_chs =  384
idxs =  192
prunable_chs =  384
idxs =  192
prunable_chs =  384
WindowMSAPruner layer.qkv.in_features =  384
PatchMergingPruner () get_out_channels/  384 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
idxs =  192
prunable_chs =  384
idxs =  192
prunable_chs =  384
idxs =  192
prunable_chs =  384
WindowMSAPruner layer.qkv.in_features =  384
WindowMSAPruner layer.embed_dims =  384
idxs =  192
prunable_chs =  384
WindowMSAPruner layer.embed_dims =  384
idxs =  192
prunable_chs =  384
WindowMSAPruner layer.embed_dims =  384
idxs =  192
prunable_chs =  384
WindowMSAPruner layer.embed_dims =  384
idxs =  192
prunable_chs =  384
WindowMSAPruner layer.embed_dims =  384
idxs =  192
prunable_chs =  384
WindowMSAPruner layer.embed_dims =  384
idxs =  192
prunable_chs =  384
pruner.step 
--------------------------------
          Pruning Group
--------------------------------
[0] prune_out_channels on norm2 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on norm2 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)), #idxs=192
[1] prune_out_channels on norm2 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _ElementWiseOp_274(AddBackward0), #idxs=192
[2] prune_out_channels on norm2 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_273(), #idxs=192
[3] prune_out_channels on _Reshape_273() => prune_out_channels on _ElementWiseOp_272(PermuteBackward0), #idxs=192
[4] prune_out_channels on _ElementWiseOp_272(PermuteBackward0) => prune_out_channels on _ElementWiseOp_271(CloneBackward0), #idxs=192
[5] prune_out_channels on _ElementWiseOp_274(AddBackward0) => prune_out_channels on _ElementWiseOp_275(AddBackward0), #idxs=192
[6] prune_out_channels on _ElementWiseOp_274(AddBackward0) => prune_out_channels on stages.2.blocks.5.ffn.layers.1 (Linear(in_features=1536, out_features=384, bias=True)), #idxs=192
[7] prune_out_channels on _ElementWiseOp_274(AddBackward0) => prune_out_channels on _Reshape_688(), #idxs=192
[8] prune_out_channels on _Reshape_688() => prune_out_channels on _ElementWiseOp_687(PermuteBackward0), #idxs=192
[9] prune_out_channels on _ElementWiseOp_687(PermuteBackward0) => prune_out_channels on _ElementWiseOp_686(Im2ColBackward0), #idxs=192
[10] prune_out_channels on _ElementWiseOp_686(Im2ColBackward0) => prune_out_channels on _ElementWiseOp_685(TransposeBackward0), #idxs=192
[11] prune_out_channels on _ElementWiseOp_685(TransposeBackward0) => prune_out_channels on _ElementWiseOp_684(NativeLayerNormBackward0), #idxs=192
[12] prune_out_channels on _ElementWiseOp_684(NativeLayerNormBackward0) => prune_out_channels on _Reshape_682(), #idxs=192
[13] prune_out_channels on _Reshape_682() => prune_out_channels on _ElementWiseOp_681(MmBackward0), #idxs=192
[14] prune_out_channels on _ElementWiseOp_681(MmBackward0) => prune_out_channels on _ElementWiseOp_683(TBackward0), #idxs=192
[15] prune_out_channels on _ElementWiseOp_681(MmBackward0) => prune_in_channels on stages.2.downsample (PatchMerging(
  (adap_padding): AdaptivePadding()
  (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
  (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (reduction): Linear(in_features=1536, out_features=768, bias=False)
)), #idxs=192
[16] prune_out_channels on _ElementWiseOp_275(AddBackward0) => prune_out_channels on _Reshape_283(), #idxs=192
[17] prune_out_channels on _ElementWiseOp_275(AddBackward0) => prune_out_channels on _ElementWiseOp_284(AddBackward0), #idxs=192
[18] prune_out_channels on _ElementWiseOp_275(AddBackward0) => prune_out_channels on stages.2.blocks.5.norm2 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)), #idxs=192
[19] prune_out_channels on stages.2.blocks.5.norm2 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_281(), #idxs=192
[20] prune_out_channels on _Reshape_281() => prune_out_channels on _ElementWiseOp_280(AddmmBackward0), #idxs=192
[21] prune_out_channels on _ElementWiseOp_280(AddmmBackward0) => prune_out_channels on _ElementWiseOp_282(TBackward0), #idxs=192
[22] prune_out_channels on _ElementWiseOp_280(AddmmBackward0) => prune_in_channels on stages.2.blocks.5.ffn.layers.0.0 (Linear(in_features=384, out_features=1536, bias=True)), #idxs=192
[23] prune_out_channels on _ElementWiseOp_284(AddBackward0) => prune_out_channels on _ElementWiseOp_285(AddBackward0), #idxs=192
[24] prune_out_channels on _ElementWiseOp_284(AddBackward0) => prune_out_channels on stages.2.blocks.4.ffn.layers.1 (Linear(in_features=1536, out_features=384, bias=True)), #idxs=192
[25] prune_out_channels on _ElementWiseOp_284(AddBackward0) => prune_out_channels on stages.2.blocks.5.norm1 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)), #idxs=192
[26] prune_out_channels on stages.2.blocks.5.norm1 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_634(), #idxs=192
[27] prune_out_channels on _Reshape_634() => prune_out_channels on _ElementWiseOp_633(ConstantPadNdBackward0), #idxs=192
[28] prune_out_channels on _ElementWiseOp_633(ConstantPadNdBackward0) => prune_out_channels on _ElementWiseOp_632(RollBackward0), #idxs=192
[29] prune_out_channels on _ElementWiseOp_632(RollBackward0) => prune_out_channels on _Reshape_631(), #idxs=192
[30] prune_out_channels on _Reshape_631() => prune_out_channels on _ElementWiseOp_630(PermuteBackward0), #idxs=192
[31] prune_out_channels on _ElementWiseOp_630(PermuteBackward0) => prune_out_channels on _ElementWiseOp_629(CloneBackward0), #idxs=192
[32] prune_out_channels on _ElementWiseOp_629(CloneBackward0) => prune_out_channels on _Reshape_628(), #idxs=192
[33] prune_out_channels on _Reshape_628() => prune_out_channels on _Reshape_627(), #idxs=192
[34] prune_out_channels on _Reshape_627() => prune_out_channels on _Reshape_625(), #idxs=192
[35] prune_out_channels on _Reshape_625() => prune_out_channels on _ElementWiseOp_624(AddmmBackward0), #idxs=192
[36] prune_out_channels on _ElementWiseOp_624(AddmmBackward0) => prune_out_channels on _ElementWiseOp_626(TBackward0), #idxs=192
[37] prune_out_channels on _ElementWiseOp_624(AddmmBackward0) => prune_out_channels on _Reshape_623(), #idxs=192
[38] prune_out_channels on _Reshape_623() => prune_out_channels on _Reshape_622(), #idxs=192
[39] prune_out_channels on _Reshape_622() => prune_out_channels on _ElementWiseOp_621(PermuteBackward0), #idxs=192
[40] prune_out_channels on _ElementWiseOp_621(PermuteBackward0) => prune_out_channels on _ElementWiseOp_620(SelectBackward0), #idxs=192
[41] prune_out_channels on _ElementWiseOp_621(PermuteBackward0) => prune_out_channels on _ElementWiseOp_653(SelectBackward0), #idxs=192
[42] prune_out_channels on _ElementWiseOp_621(PermuteBackward0) => prune_out_channels on _ElementWiseOp_657(SelectBackward0), #idxs=192
[43] prune_out_channels on _ElementWiseOp_657(SelectBackward0) => prune_out_channels on _ElementWiseOp_656(MulBackward0), #idxs=192
[44] prune_out_channels on _ElementWiseOp_656(MulBackward0) => prune_out_channels on _ElementWiseOp_655(ExpandBackward0), #idxs=192
[45] prune_out_channels on _ElementWiseOp_655(ExpandBackward0) => prune_out_channels on _ElementWiseOp_654(CloneBackward0), #idxs=192
[46] prune_out_channels on _ElementWiseOp_654(CloneBackward0) => prune_out_channels on _Reshape_648(), #idxs=192
[47] prune_out_channels on _Reshape_648() => prune_out_channels on _ElementWiseOp_647(BmmBackward0), #idxs=192
[48] prune_out_channels on _ElementWiseOp_647(BmmBackward0) => prune_out_channels on _Reshape_649(), #idxs=192
[49] prune_out_channels on _ElementWiseOp_647(BmmBackward0) => prune_out_channels on _Reshape_641(), #idxs=192
[50] prune_out_channels on _Reshape_641() => prune_out_channels on _ElementWiseOp_640(AddBackward0), #idxs=192
[51] prune_out_channels on _ElementWiseOp_640(AddBackward0) => prune_out_channels on _ElementWiseOp_642(UnsqueezeBackward0), #idxs=192
[52] prune_out_channels on _ElementWiseOp_640(AddBackward0) => prune_out_channels on _Reshape_639(), #idxs=192
[53] prune_out_channels on _Reshape_639() => prune_out_channels on _ElementWiseOp_638(AddBackward0), #idxs=192
[54] prune_out_channels on _ElementWiseOp_638(AddBackward0) => prune_out_channels on _Reshape_637(), #idxs=192
[55] prune_out_channels on _Reshape_637() => prune_out_channels on _ElementWiseOp_636(SoftmaxBackward0), #idxs=192
[56] prune_out_channels on _ElementWiseOp_636(SoftmaxBackward0) => prune_out_channels on _ElementWiseOp_635(ExpandBackward0), #idxs=192
[57] prune_out_channels on _ElementWiseOp_635(ExpandBackward0) => prune_out_channels on _Reshape_616(), #idxs=192
[58] prune_out_channels on _Reshape_616() => prune_out_channels on _ElementWiseOp_615(BmmBackward0), #idxs=192
[59] prune_out_channels on _ElementWiseOp_615(BmmBackward0) => prune_out_channels on _Reshape_617(), #idxs=192
[60] prune_out_channels on _ElementWiseOp_615(BmmBackward0) => prune_out_channels on _Reshape_614(), #idxs=192
[61] prune_out_channels on _Reshape_614() => prune_out_channels on _ElementWiseOp_613(TransposeBackward0), #idxs=192
[62] prune_out_channels on _ElementWiseOp_613(TransposeBackward0) => prune_out_channels on _ElementWiseOp_612(CloneBackward0), #idxs=192
[63] prune_out_channels on _ElementWiseOp_612(CloneBackward0) => prune_in_channels on stages.2.blocks.5.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=384, out_features=1152, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=384, out_features=384, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)), #idxs=192
[64] prune_out_channels on _Reshape_617() => prune_out_channels on _ElementWiseOp_618(CloneBackward0), #idxs=192
[65] prune_out_channels on _ElementWiseOp_618(CloneBackward0) => prune_out_channels on _ElementWiseOp_619(ExpandBackward0), #idxs=192
[66] prune_out_channels on _ElementWiseOp_642(UnsqueezeBackward0) => prune_out_channels on _ElementWiseOp_643(CloneBackward0), #idxs=192
[67] prune_out_channels on _ElementWiseOp_643(CloneBackward0) => prune_out_channels on _ElementWiseOp_644(PermuteBackward0), #idxs=192
[68] prune_out_channels on _ElementWiseOp_644(PermuteBackward0) => prune_out_channels on _Reshape_645(), #idxs=192
[69] prune_out_channels on _Reshape_645() => prune_out_channels on _ElementWiseOp_646(IndexBackward0), #idxs=192
[70] prune_out_channels on _Reshape_649() => prune_out_channels on _ElementWiseOp_650(CloneBackward0), #idxs=192
[71] prune_out_channels on _ElementWiseOp_650(CloneBackward0) => prune_out_channels on _ElementWiseOp_651(ExpandBackward0), #idxs=192
[72] prune_out_channels on _ElementWiseOp_651(ExpandBackward0) => prune_out_channels on _ElementWiseOp_652(TransposeBackward0), #idxs=192
[73] prune_out_channels on _ElementWiseOp_285(AddBackward0) => prune_out_channels on _Reshape_293(), #idxs=192
[74] prune_out_channels on _ElementWiseOp_285(AddBackward0) => prune_out_channels on _ElementWiseOp_294(AddBackward0), #idxs=192
[75] prune_out_channels on _ElementWiseOp_285(AddBackward0) => prune_out_channels on stages.2.blocks.4.norm2 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)), #idxs=192
[76] prune_out_channels on stages.2.blocks.4.norm2 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_291(), #idxs=192
[77] prune_out_channels on _Reshape_291() => prune_out_channels on _ElementWiseOp_290(AddmmBackward0), #idxs=192
[78] prune_out_channels on _ElementWiseOp_290(AddmmBackward0) => prune_out_channels on _ElementWiseOp_292(TBackward0), #idxs=192
[79] prune_out_channels on _ElementWiseOp_290(AddmmBackward0) => prune_in_channels on stages.2.blocks.4.ffn.layers.0.0 (Linear(in_features=384, out_features=1536, bias=True)), #idxs=192
[80] prune_out_channels on _ElementWiseOp_294(AddBackward0) => prune_out_channels on _ElementWiseOp_295(AddBackward0), #idxs=192
[81] prune_out_channels on _ElementWiseOp_294(AddBackward0) => prune_out_channels on stages.2.blocks.3.ffn.layers.1 (Linear(in_features=1536, out_features=384, bias=True)), #idxs=192
[82] prune_out_channels on _ElementWiseOp_294(AddBackward0) => prune_out_channels on stages.2.blocks.4.norm1 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)), #idxs=192
[83] prune_out_channels on stages.2.blocks.4.norm1 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_582(), #idxs=192
[84] prune_out_channels on _Reshape_582() => prune_out_channels on _ElementWiseOp_581(ConstantPadNdBackward0), #idxs=192
[85] prune_out_channels on _ElementWiseOp_581(ConstantPadNdBackward0) => prune_out_channels on _Reshape_580(), #idxs=192
[86] prune_out_channels on _Reshape_580() => prune_out_channels on _ElementWiseOp_579(PermuteBackward0), #idxs=192
[87] prune_out_channels on _ElementWiseOp_579(PermuteBackward0) => prune_out_channels on _ElementWiseOp_578(CloneBackward0), #idxs=192
[88] prune_out_channels on _ElementWiseOp_578(CloneBackward0) => prune_out_channels on _Reshape_577(), #idxs=192
[89] prune_out_channels on _Reshape_577() => prune_out_channels on _Reshape_576(), #idxs=192
[90] prune_out_channels on _Reshape_576() => prune_out_channels on _Reshape_574(), #idxs=192
[91] prune_out_channels on _Reshape_574() => prune_out_channels on _ElementWiseOp_573(AddmmBackward0), #idxs=192
[92] prune_out_channels on _ElementWiseOp_573(AddmmBackward0) => prune_out_channels on _ElementWiseOp_575(TBackward0), #idxs=192
[93] prune_out_channels on _ElementWiseOp_573(AddmmBackward0) => prune_out_channels on _Reshape_572(), #idxs=192
[94] prune_out_channels on _Reshape_572() => prune_out_channels on _Reshape_571(), #idxs=192
[95] prune_out_channels on _Reshape_571() => prune_out_channels on _ElementWiseOp_570(PermuteBackward0), #idxs=192
[96] prune_out_channels on _ElementWiseOp_570(PermuteBackward0) => prune_out_channels on _ElementWiseOp_569(SelectBackward0), #idxs=192
[97] prune_out_channels on _ElementWiseOp_570(PermuteBackward0) => prune_out_channels on _ElementWiseOp_598(SelectBackward0), #idxs=192
[98] prune_out_channels on _ElementWiseOp_570(PermuteBackward0) => prune_out_channels on _ElementWiseOp_602(SelectBackward0), #idxs=192
[99] prune_out_channels on _ElementWiseOp_602(SelectBackward0) => prune_out_channels on _ElementWiseOp_601(MulBackward0), #idxs=192
[100] prune_out_channels on _ElementWiseOp_601(MulBackward0) => prune_out_channels on _ElementWiseOp_600(ExpandBackward0), #idxs=192
[101] prune_out_channels on _ElementWiseOp_600(ExpandBackward0) => prune_out_channels on _ElementWiseOp_599(CloneBackward0), #idxs=192
[102] prune_out_channels on _ElementWiseOp_599(CloneBackward0) => prune_out_channels on _Reshape_593(), #idxs=192
[103] prune_out_channels on _Reshape_593() => prune_out_channels on _ElementWiseOp_592(BmmBackward0), #idxs=192
[104] prune_out_channels on _ElementWiseOp_592(BmmBackward0) => prune_out_channels on _Reshape_594(), #idxs=192
[105] prune_out_channels on _ElementWiseOp_592(BmmBackward0) => prune_out_channels on _Reshape_586(), #idxs=192
[106] prune_out_channels on _Reshape_586() => prune_out_channels on _ElementWiseOp_585(AddBackward0), #idxs=192
[107] prune_out_channels on _ElementWiseOp_585(AddBackward0) => prune_out_channels on _ElementWiseOp_587(UnsqueezeBackward0), #idxs=192
[108] prune_out_channels on _ElementWiseOp_585(AddBackward0) => prune_out_channels on _ElementWiseOp_584(SoftmaxBackward0), #idxs=192
[109] prune_out_channels on _ElementWiseOp_584(SoftmaxBackward0) => prune_out_channels on _ElementWiseOp_583(ExpandBackward0), #idxs=192
[110] prune_out_channels on _ElementWiseOp_583(ExpandBackward0) => prune_out_channels on _Reshape_565(), #idxs=192
[111] prune_out_channels on _Reshape_565() => prune_out_channels on _ElementWiseOp_564(BmmBackward0), #idxs=192
[112] prune_out_channels on _ElementWiseOp_564(BmmBackward0) => prune_out_channels on _Reshape_566(), #idxs=192
[113] prune_out_channels on _ElementWiseOp_564(BmmBackward0) => prune_out_channels on _Reshape_563(), #idxs=192
[114] prune_out_channels on _Reshape_563() => prune_out_channels on _ElementWiseOp_562(TransposeBackward0), #idxs=192
[115] prune_out_channels on _ElementWiseOp_562(TransposeBackward0) => prune_out_channels on _ElementWiseOp_561(CloneBackward0), #idxs=192
[116] prune_out_channels on _ElementWiseOp_561(CloneBackward0) => prune_in_channels on stages.2.blocks.4.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=384, out_features=1152, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=384, out_features=384, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)), #idxs=192
[117] prune_out_channels on _Reshape_566() => prune_out_channels on _ElementWiseOp_567(CloneBackward0), #idxs=192
[118] prune_out_channels on _ElementWiseOp_567(CloneBackward0) => prune_out_channels on _ElementWiseOp_568(ExpandBackward0), #idxs=192
[119] prune_out_channels on _ElementWiseOp_587(UnsqueezeBackward0) => prune_out_channels on _ElementWiseOp_588(CloneBackward0), #idxs=192
[120] prune_out_channels on _ElementWiseOp_588(CloneBackward0) => prune_out_channels on _ElementWiseOp_589(PermuteBackward0), #idxs=192
[121] prune_out_channels on _ElementWiseOp_589(PermuteBackward0) => prune_out_channels on _Reshape_590(), #idxs=192
[122] prune_out_channels on _Reshape_590() => prune_out_channels on _ElementWiseOp_591(IndexBackward0), #idxs=192
[123] prune_out_channels on _Reshape_594() => prune_out_channels on _ElementWiseOp_595(CloneBackward0), #idxs=192
[124] prune_out_channels on _ElementWiseOp_595(CloneBackward0) => prune_out_channels on _ElementWiseOp_596(ExpandBackward0), #idxs=192
[125] prune_out_channels on _ElementWiseOp_596(ExpandBackward0) => prune_out_channels on _ElementWiseOp_597(TransposeBackward0), #idxs=192
[126] prune_out_channels on _ElementWiseOp_295(AddBackward0) => prune_out_channels on _Reshape_303(), #idxs=192
[127] prune_out_channels on _ElementWiseOp_295(AddBackward0) => prune_out_channels on _ElementWiseOp_304(AddBackward0), #idxs=192
[128] prune_out_channels on _ElementWiseOp_295(AddBackward0) => prune_out_channels on stages.2.blocks.3.norm2 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)), #idxs=192
[129] prune_out_channels on stages.2.blocks.3.norm2 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_301(), #idxs=192
[130] prune_out_channels on _Reshape_301() => prune_out_channels on _ElementWiseOp_300(AddmmBackward0), #idxs=192
[131] prune_out_channels on _ElementWiseOp_300(AddmmBackward0) => prune_out_channels on _ElementWiseOp_302(TBackward0), #idxs=192
[132] prune_out_channels on _ElementWiseOp_300(AddmmBackward0) => prune_in_channels on stages.2.blocks.3.ffn.layers.0.0 (Linear(in_features=384, out_features=1536, bias=True)), #idxs=192
[133] prune_out_channels on _ElementWiseOp_304(AddBackward0) => prune_out_channels on _ElementWiseOp_305(AddBackward0), #idxs=192
[134] prune_out_channels on _ElementWiseOp_304(AddBackward0) => prune_out_channels on stages.2.blocks.2.ffn.layers.1 (Linear(in_features=1536, out_features=384, bias=True)), #idxs=192
[135] prune_out_channels on _ElementWiseOp_304(AddBackward0) => prune_out_channels on stages.2.blocks.3.norm1 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)), #idxs=192
[136] prune_out_channels on stages.2.blocks.3.norm1 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_529(), #idxs=192
[137] prune_out_channels on _Reshape_529() => prune_out_channels on _ElementWiseOp_528(ConstantPadNdBackward0), #idxs=192
[138] prune_out_channels on _ElementWiseOp_528(ConstantPadNdBackward0) => prune_out_channels on _ElementWiseOp_527(RollBackward0), #idxs=192
[139] prune_out_channels on _ElementWiseOp_527(RollBackward0) => prune_out_channels on _Reshape_526(), #idxs=192
[140] prune_out_channels on _Reshape_526() => prune_out_channels on _ElementWiseOp_525(PermuteBackward0), #idxs=192
[141] prune_out_channels on _ElementWiseOp_525(PermuteBackward0) => prune_out_channels on _ElementWiseOp_524(CloneBackward0), #idxs=192
[142] prune_out_channels on _ElementWiseOp_524(CloneBackward0) => prune_out_channels on _Reshape_523(), #idxs=192
[143] prune_out_channels on _Reshape_523() => prune_out_channels on _Reshape_522(), #idxs=192
[144] prune_out_channels on _Reshape_522() => prune_out_channels on _Reshape_520(), #idxs=192
[145] prune_out_channels on _Reshape_520() => prune_out_channels on _ElementWiseOp_519(AddmmBackward0), #idxs=192
[146] prune_out_channels on _ElementWiseOp_519(AddmmBackward0) => prune_out_channels on _ElementWiseOp_521(TBackward0), #idxs=192
[147] prune_out_channels on _ElementWiseOp_519(AddmmBackward0) => prune_out_channels on _Reshape_518(), #idxs=192
[148] prune_out_channels on _Reshape_518() => prune_out_channels on _Reshape_517(), #idxs=192
[149] prune_out_channels on _Reshape_517() => prune_out_channels on _ElementWiseOp_516(PermuteBackward0), #idxs=192
[150] prune_out_channels on _ElementWiseOp_516(PermuteBackward0) => prune_out_channels on _ElementWiseOp_515(SelectBackward0), #idxs=192
[151] prune_out_channels on _ElementWiseOp_516(PermuteBackward0) => prune_out_channels on _ElementWiseOp_548(SelectBackward0), #idxs=192
[152] prune_out_channels on _ElementWiseOp_516(PermuteBackward0) => prune_out_channels on _ElementWiseOp_552(SelectBackward0), #idxs=192
[153] prune_out_channels on _ElementWiseOp_552(SelectBackward0) => prune_out_channels on _ElementWiseOp_551(MulBackward0), #idxs=192
[154] prune_out_channels on _ElementWiseOp_551(MulBackward0) => prune_out_channels on _ElementWiseOp_550(ExpandBackward0), #idxs=192
[155] prune_out_channels on _ElementWiseOp_550(ExpandBackward0) => prune_out_channels on _ElementWiseOp_549(CloneBackward0), #idxs=192
[156] prune_out_channels on _ElementWiseOp_549(CloneBackward0) => prune_out_channels on _Reshape_543(), #idxs=192
[157] prune_out_channels on _Reshape_543() => prune_out_channels on _ElementWiseOp_542(BmmBackward0), #idxs=192
[158] prune_out_channels on _ElementWiseOp_542(BmmBackward0) => prune_out_channels on _Reshape_544(), #idxs=192
[159] prune_out_channels on _ElementWiseOp_542(BmmBackward0) => prune_out_channels on _Reshape_536(), #idxs=192
[160] prune_out_channels on _Reshape_536() => prune_out_channels on _ElementWiseOp_535(AddBackward0), #idxs=192
[161] prune_out_channels on _ElementWiseOp_535(AddBackward0) => prune_out_channels on _ElementWiseOp_537(UnsqueezeBackward0), #idxs=192
[162] prune_out_channels on _ElementWiseOp_535(AddBackward0) => prune_out_channels on _Reshape_534(), #idxs=192
[163] prune_out_channels on _Reshape_534() => prune_out_channels on _ElementWiseOp_533(AddBackward0), #idxs=192
[164] prune_out_channels on _ElementWiseOp_533(AddBackward0) => prune_out_channels on _Reshape_532(), #idxs=192
[165] prune_out_channels on _Reshape_532() => prune_out_channels on _ElementWiseOp_531(SoftmaxBackward0), #idxs=192
[166] prune_out_channels on _ElementWiseOp_531(SoftmaxBackward0) => prune_out_channels on _ElementWiseOp_530(ExpandBackward0), #idxs=192
[167] prune_out_channels on _ElementWiseOp_530(ExpandBackward0) => prune_out_channels on _Reshape_511(), #idxs=192
[168] prune_out_channels on _Reshape_511() => prune_out_channels on _ElementWiseOp_510(BmmBackward0), #idxs=192
[169] prune_out_channels on _ElementWiseOp_510(BmmBackward0) => prune_out_channels on _Reshape_512(), #idxs=192
[170] prune_out_channels on _ElementWiseOp_510(BmmBackward0) => prune_out_channels on _Reshape_509(), #idxs=192
[171] prune_out_channels on _Reshape_509() => prune_out_channels on _ElementWiseOp_508(TransposeBackward0), #idxs=192
[172] prune_out_channels on _ElementWiseOp_508(TransposeBackward0) => prune_out_channels on _ElementWiseOp_507(CloneBackward0), #idxs=192
[173] prune_out_channels on _ElementWiseOp_507(CloneBackward0) => prune_in_channels on stages.2.blocks.3.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=384, out_features=1152, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=384, out_features=384, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)), #idxs=192
[174] prune_out_channels on _Reshape_512() => prune_out_channels on _ElementWiseOp_513(CloneBackward0), #idxs=192
[175] prune_out_channels on _ElementWiseOp_513(CloneBackward0) => prune_out_channels on _ElementWiseOp_514(ExpandBackward0), #idxs=192
[176] prune_out_channels on _ElementWiseOp_537(UnsqueezeBackward0) => prune_out_channels on _ElementWiseOp_538(CloneBackward0), #idxs=192
[177] prune_out_channels on _ElementWiseOp_538(CloneBackward0) => prune_out_channels on _ElementWiseOp_539(PermuteBackward0), #idxs=192
[178] prune_out_channels on _ElementWiseOp_539(PermuteBackward0) => prune_out_channels on _Reshape_540(), #idxs=192
[179] prune_out_channels on _Reshape_540() => prune_out_channels on _ElementWiseOp_541(IndexBackward0), #idxs=192
[180] prune_out_channels on _Reshape_544() => prune_out_channels on _ElementWiseOp_545(CloneBackward0), #idxs=192
[181] prune_out_channels on _ElementWiseOp_545(CloneBackward0) => prune_out_channels on _ElementWiseOp_546(ExpandBackward0), #idxs=192
[182] prune_out_channels on _ElementWiseOp_546(ExpandBackward0) => prune_out_channels on _ElementWiseOp_547(TransposeBackward0), #idxs=192
[183] prune_out_channels on _ElementWiseOp_305(AddBackward0) => prune_out_channels on _Reshape_313(), #idxs=192
[184] prune_out_channels on _ElementWiseOp_305(AddBackward0) => prune_out_channels on _ElementWiseOp_314(AddBackward0), #idxs=192
[185] prune_out_channels on _ElementWiseOp_305(AddBackward0) => prune_out_channels on stages.2.blocks.2.norm2 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)), #idxs=192
[186] prune_out_channels on stages.2.blocks.2.norm2 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_311(), #idxs=192
[187] prune_out_channels on _Reshape_311() => prune_out_channels on _ElementWiseOp_310(AddmmBackward0), #idxs=192
[188] prune_out_channels on _ElementWiseOp_310(AddmmBackward0) => prune_out_channels on _ElementWiseOp_312(TBackward0), #idxs=192
[189] prune_out_channels on _ElementWiseOp_310(AddmmBackward0) => prune_in_channels on stages.2.blocks.2.ffn.layers.0.0 (Linear(in_features=384, out_features=1536, bias=True)), #idxs=192
[190] prune_out_channels on _ElementWiseOp_314(AddBackward0) => prune_out_channels on _ElementWiseOp_315(AddBackward0), #idxs=192
[191] prune_out_channels on _ElementWiseOp_314(AddBackward0) => prune_out_channels on stages.2.blocks.1.ffn.layers.1 (Linear(in_features=1536, out_features=384, bias=True)), #idxs=192
[192] prune_out_channels on _ElementWiseOp_314(AddBackward0) => prune_out_channels on stages.2.blocks.2.norm1 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)), #idxs=192
[193] prune_out_channels on stages.2.blocks.2.norm1 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_477(), #idxs=192
[194] prune_out_channels on _Reshape_477() => prune_out_channels on _ElementWiseOp_476(ConstantPadNdBackward0), #idxs=192
[195] prune_out_channels on _ElementWiseOp_476(ConstantPadNdBackward0) => prune_out_channels on _Reshape_475(), #idxs=192
[196] prune_out_channels on _Reshape_475() => prune_out_channels on _ElementWiseOp_474(PermuteBackward0), #idxs=192
[197] prune_out_channels on _ElementWiseOp_474(PermuteBackward0) => prune_out_channels on _ElementWiseOp_473(CloneBackward0), #idxs=192
[198] prune_out_channels on _ElementWiseOp_473(CloneBackward0) => prune_out_channels on _Reshape_472(), #idxs=192
[199] prune_out_channels on _Reshape_472() => prune_out_channels on _Reshape_471(), #idxs=192
[200] prune_out_channels on _Reshape_471() => prune_out_channels on _Reshape_469(), #idxs=192
[201] prune_out_channels on _Reshape_469() => prune_out_channels on _ElementWiseOp_468(AddmmBackward0), #idxs=192
[202] prune_out_channels on _ElementWiseOp_468(AddmmBackward0) => prune_out_channels on _ElementWiseOp_470(TBackward0), #idxs=192
[203] prune_out_channels on _ElementWiseOp_468(AddmmBackward0) => prune_out_channels on _Reshape_467(), #idxs=192
[204] prune_out_channels on _Reshape_467() => prune_out_channels on _Reshape_466(), #idxs=192
[205] prune_out_channels on _Reshape_466() => prune_out_channels on _ElementWiseOp_465(PermuteBackward0), #idxs=192
[206] prune_out_channels on _ElementWiseOp_465(PermuteBackward0) => prune_out_channels on _ElementWiseOp_464(SelectBackward0), #idxs=192
[207] prune_out_channels on _ElementWiseOp_465(PermuteBackward0) => prune_out_channels on _ElementWiseOp_493(SelectBackward0), #idxs=192
[208] prune_out_channels on _ElementWiseOp_465(PermuteBackward0) => prune_out_channels on _ElementWiseOp_497(SelectBackward0), #idxs=192
[209] prune_out_channels on _ElementWiseOp_497(SelectBackward0) => prune_out_channels on _ElementWiseOp_496(MulBackward0), #idxs=192
[210] prune_out_channels on _ElementWiseOp_496(MulBackward0) => prune_out_channels on _ElementWiseOp_495(ExpandBackward0), #idxs=192
[211] prune_out_channels on _ElementWiseOp_495(ExpandBackward0) => prune_out_channels on _ElementWiseOp_494(CloneBackward0), #idxs=192
[212] prune_out_channels on _ElementWiseOp_494(CloneBackward0) => prune_out_channels on _Reshape_488(), #idxs=192
[213] prune_out_channels on _Reshape_488() => prune_out_channels on _ElementWiseOp_487(BmmBackward0), #idxs=192
[214] prune_out_channels on _ElementWiseOp_487(BmmBackward0) => prune_out_channels on _Reshape_489(), #idxs=192
[215] prune_out_channels on _ElementWiseOp_487(BmmBackward0) => prune_out_channels on _Reshape_481(), #idxs=192
[216] prune_out_channels on _Reshape_481() => prune_out_channels on _ElementWiseOp_480(AddBackward0), #idxs=192
[217] prune_out_channels on _ElementWiseOp_480(AddBackward0) => prune_out_channels on _ElementWiseOp_482(UnsqueezeBackward0), #idxs=192
[218] prune_out_channels on _ElementWiseOp_480(AddBackward0) => prune_out_channels on _ElementWiseOp_479(SoftmaxBackward0), #idxs=192
[219] prune_out_channels on _ElementWiseOp_479(SoftmaxBackward0) => prune_out_channels on _ElementWiseOp_478(ExpandBackward0), #idxs=192
[220] prune_out_channels on _ElementWiseOp_478(ExpandBackward0) => prune_out_channels on _Reshape_460(), #idxs=192
[221] prune_out_channels on _Reshape_460() => prune_out_channels on _ElementWiseOp_459(BmmBackward0), #idxs=192
[222] prune_out_channels on _ElementWiseOp_459(BmmBackward0) => prune_out_channels on _Reshape_461(), #idxs=192
[223] prune_out_channels on _ElementWiseOp_459(BmmBackward0) => prune_out_channels on _Reshape_458(), #idxs=192
[224] prune_out_channels on _Reshape_458() => prune_out_channels on _ElementWiseOp_457(TransposeBackward0), #idxs=192
[225] prune_out_channels on _ElementWiseOp_457(TransposeBackward0) => prune_out_channels on _ElementWiseOp_456(CloneBackward0), #idxs=192
[226] prune_out_channels on _ElementWiseOp_456(CloneBackward0) => prune_in_channels on stages.2.blocks.2.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=384, out_features=1152, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=384, out_features=384, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)), #idxs=192
[227] prune_out_channels on _Reshape_461() => prune_out_channels on _ElementWiseOp_462(CloneBackward0), #idxs=192
[228] prune_out_channels on _ElementWiseOp_462(CloneBackward0) => prune_out_channels on _ElementWiseOp_463(ExpandBackward0), #idxs=192
[229] prune_out_channels on _ElementWiseOp_482(UnsqueezeBackward0) => prune_out_channels on _ElementWiseOp_483(CloneBackward0), #idxs=192
[230] prune_out_channels on _ElementWiseOp_483(CloneBackward0) => prune_out_channels on _ElementWiseOp_484(PermuteBackward0), #idxs=192
[231] prune_out_channels on _ElementWiseOp_484(PermuteBackward0) => prune_out_channels on _Reshape_485(), #idxs=192
[232] prune_out_channels on _Reshape_485() => prune_out_channels on _ElementWiseOp_486(IndexBackward0), #idxs=192
[233] prune_out_channels on _Reshape_489() => prune_out_channels on _ElementWiseOp_490(CloneBackward0), #idxs=192
[234] prune_out_channels on _ElementWiseOp_490(CloneBackward0) => prune_out_channels on _ElementWiseOp_491(ExpandBackward0), #idxs=192
[235] prune_out_channels on _ElementWiseOp_491(ExpandBackward0) => prune_out_channels on _ElementWiseOp_492(TransposeBackward0), #idxs=192
[236] prune_out_channels on _ElementWiseOp_315(AddBackward0) => prune_out_channels on _Reshape_323(), #idxs=192
[237] prune_out_channels on _ElementWiseOp_315(AddBackward0) => prune_out_channels on _ElementWiseOp_324(AddBackward0), #idxs=192
[238] prune_out_channels on _ElementWiseOp_315(AddBackward0) => prune_out_channels on stages.2.blocks.1.norm2 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)), #idxs=192
[239] prune_out_channels on stages.2.blocks.1.norm2 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_321(), #idxs=192
[240] prune_out_channels on _Reshape_321() => prune_out_channels on _ElementWiseOp_320(AddmmBackward0), #idxs=192
[241] prune_out_channels on _ElementWiseOp_320(AddmmBackward0) => prune_out_channels on _ElementWiseOp_322(TBackward0), #idxs=192
[242] prune_out_channels on _ElementWiseOp_320(AddmmBackward0) => prune_in_channels on stages.2.blocks.1.ffn.layers.0.0 (Linear(in_features=384, out_features=1536, bias=True)), #idxs=192
[243] prune_out_channels on _ElementWiseOp_324(AddBackward0) => prune_out_channels on _ElementWiseOp_325(AddBackward0), #idxs=192
[244] prune_out_channels on _ElementWiseOp_324(AddBackward0) => prune_out_channels on stages.2.blocks.0.ffn.layers.1 (Linear(in_features=1536, out_features=384, bias=True)), #idxs=192
[245] prune_out_channels on _ElementWiseOp_324(AddBackward0) => prune_out_channels on stages.2.blocks.1.norm1 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)), #idxs=192
[246] prune_out_channels on stages.2.blocks.1.norm1 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_424(), #idxs=192
[247] prune_out_channels on _Reshape_424() => prune_out_channels on _ElementWiseOp_423(ConstantPadNdBackward0), #idxs=192
[248] prune_out_channels on _ElementWiseOp_423(ConstantPadNdBackward0) => prune_out_channels on _ElementWiseOp_422(RollBackward0), #idxs=192
[249] prune_out_channels on _ElementWiseOp_422(RollBackward0) => prune_out_channels on _Reshape_421(), #idxs=192
[250] prune_out_channels on _Reshape_421() => prune_out_channels on _ElementWiseOp_420(PermuteBackward0), #idxs=192
[251] prune_out_channels on _ElementWiseOp_420(PermuteBackward0) => prune_out_channels on _ElementWiseOp_419(CloneBackward0), #idxs=192
[252] prune_out_channels on _ElementWiseOp_419(CloneBackward0) => prune_out_channels on _Reshape_418(), #idxs=192
[253] prune_out_channels on _Reshape_418() => prune_out_channels on _Reshape_417(), #idxs=192
[254] prune_out_channels on _Reshape_417() => prune_out_channels on _Reshape_415(), #idxs=192
[255] prune_out_channels on _Reshape_415() => prune_out_channels on _ElementWiseOp_414(AddmmBackward0), #idxs=192
[256] prune_out_channels on _ElementWiseOp_414(AddmmBackward0) => prune_out_channels on _ElementWiseOp_416(TBackward0), #idxs=192
[257] prune_out_channels on _ElementWiseOp_414(AddmmBackward0) => prune_out_channels on _Reshape_413(), #idxs=192
[258] prune_out_channels on _Reshape_413() => prune_out_channels on _Reshape_412(), #idxs=192
[259] prune_out_channels on _Reshape_412() => prune_out_channels on _ElementWiseOp_411(PermuteBackward0), #idxs=192
[260] prune_out_channels on _ElementWiseOp_411(PermuteBackward0) => prune_out_channels on _ElementWiseOp_410(SelectBackward0), #idxs=192
[261] prune_out_channels on _ElementWiseOp_411(PermuteBackward0) => prune_out_channels on _ElementWiseOp_443(SelectBackward0), #idxs=192
[262] prune_out_channels on _ElementWiseOp_411(PermuteBackward0) => prune_out_channels on _ElementWiseOp_447(SelectBackward0), #idxs=192
[263] prune_out_channels on _ElementWiseOp_447(SelectBackward0) => prune_out_channels on _ElementWiseOp_446(MulBackward0), #idxs=192
[264] prune_out_channels on _ElementWiseOp_446(MulBackward0) => prune_out_channels on _ElementWiseOp_445(ExpandBackward0), #idxs=192
[265] prune_out_channels on _ElementWiseOp_445(ExpandBackward0) => prune_out_channels on _ElementWiseOp_444(CloneBackward0), #idxs=192
[266] prune_out_channels on _ElementWiseOp_444(CloneBackward0) => prune_out_channels on _Reshape_438(), #idxs=192
[267] prune_out_channels on _Reshape_438() => prune_out_channels on _ElementWiseOp_437(BmmBackward0), #idxs=192
[268] prune_out_channels on _ElementWiseOp_437(BmmBackward0) => prune_out_channels on _Reshape_439(), #idxs=192
[269] prune_out_channels on _ElementWiseOp_437(BmmBackward0) => prune_out_channels on _Reshape_431(), #idxs=192
[270] prune_out_channels on _Reshape_431() => prune_out_channels on _ElementWiseOp_430(AddBackward0), #idxs=192
[271] prune_out_channels on _ElementWiseOp_430(AddBackward0) => prune_out_channels on _ElementWiseOp_432(UnsqueezeBackward0), #idxs=192
[272] prune_out_channels on _ElementWiseOp_430(AddBackward0) => prune_out_channels on _Reshape_429(), #idxs=192
[273] prune_out_channels on _Reshape_429() => prune_out_channels on _ElementWiseOp_428(AddBackward0), #idxs=192
[274] prune_out_channels on _ElementWiseOp_428(AddBackward0) => prune_out_channels on _Reshape_427(), #idxs=192
[275] prune_out_channels on _Reshape_427() => prune_out_channels on _ElementWiseOp_426(SoftmaxBackward0), #idxs=192
[276] prune_out_channels on _ElementWiseOp_426(SoftmaxBackward0) => prune_out_channels on _ElementWiseOp_425(ExpandBackward0), #idxs=192
[277] prune_out_channels on _ElementWiseOp_425(ExpandBackward0) => prune_out_channels on _Reshape_406(), #idxs=192
[278] prune_out_channels on _Reshape_406() => prune_out_channels on _ElementWiseOp_405(BmmBackward0), #idxs=192
[279] prune_out_channels on _ElementWiseOp_405(BmmBackward0) => prune_out_channels on _Reshape_407(), #idxs=192
[280] prune_out_channels on _ElementWiseOp_405(BmmBackward0) => prune_out_channels on _Reshape_404(), #idxs=192
[281] prune_out_channels on _Reshape_404() => prune_out_channels on _ElementWiseOp_403(TransposeBackward0), #idxs=192
[282] prune_out_channels on _ElementWiseOp_403(TransposeBackward0) => prune_out_channels on _ElementWiseOp_402(CloneBackward0), #idxs=192
[283] prune_out_channels on _ElementWiseOp_402(CloneBackward0) => prune_in_channels on stages.2.blocks.1.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=384, out_features=1152, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=384, out_features=384, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)), #idxs=192
[284] prune_out_channels on _Reshape_407() => prune_out_channels on _ElementWiseOp_408(CloneBackward0), #idxs=192
[285] prune_out_channels on _ElementWiseOp_408(CloneBackward0) => prune_out_channels on _ElementWiseOp_409(ExpandBackward0), #idxs=192
[286] prune_out_channels on _ElementWiseOp_432(UnsqueezeBackward0) => prune_out_channels on _ElementWiseOp_433(CloneBackward0), #idxs=192
[287] prune_out_channels on _ElementWiseOp_433(CloneBackward0) => prune_out_channels on _ElementWiseOp_434(PermuteBackward0), #idxs=192
[288] prune_out_channels on _ElementWiseOp_434(PermuteBackward0) => prune_out_channels on _Reshape_435(), #idxs=192
[289] prune_out_channels on _Reshape_435() => prune_out_channels on _ElementWiseOp_436(IndexBackward0), #idxs=192
[290] prune_out_channels on _Reshape_439() => prune_out_channels on _ElementWiseOp_440(CloneBackward0), #idxs=192
[291] prune_out_channels on _ElementWiseOp_440(CloneBackward0) => prune_out_channels on _ElementWiseOp_441(ExpandBackward0), #idxs=192
[292] prune_out_channels on _ElementWiseOp_441(ExpandBackward0) => prune_out_channels on _ElementWiseOp_442(TransposeBackward0), #idxs=192
[293] prune_out_channels on _ElementWiseOp_325(AddBackward0) => prune_out_channels on _Reshape_333(), #idxs=192
[294] prune_out_channels on _ElementWiseOp_325(AddBackward0) => prune_out_channels on stages.1.downsample (PatchMerging(
  (adap_padding): AdaptivePadding()
  (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
  (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (reduction): Linear(in_features=384, out_features=384, bias=False)
)), #idxs=192
[295] prune_out_channels on _ElementWiseOp_325(AddBackward0) => prune_out_channels on stages.2.blocks.0.norm2 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)), #idxs=192
[296] prune_out_channels on stages.2.blocks.0.norm2 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_331(), #idxs=192
[297] prune_out_channels on _Reshape_331() => prune_out_channels on _ElementWiseOp_330(AddmmBackward0), #idxs=192
[298] prune_out_channels on _ElementWiseOp_330(AddmmBackward0) => prune_out_channels on _ElementWiseOp_332(TBackward0), #idxs=192
[299] prune_out_channels on _ElementWiseOp_330(AddmmBackward0) => prune_in_channels on stages.2.blocks.0.ffn.layers.0.0 (Linear(in_features=384, out_features=1536, bias=True)), #idxs=192
[300] prune_out_channels on stages.1.downsample (PatchMerging(
  (adap_padding): AdaptivePadding()
  (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
  (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (reduction): Linear(in_features=384, out_features=384, bias=False)
)) => prune_out_channels on stages.2.blocks.0.norm1 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)), #idxs=192
[301] prune_out_channels on stages.2.blocks.0.norm1 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_372(), #idxs=192
[302] prune_out_channels on _Reshape_372() => prune_out_channels on _ElementWiseOp_371(ConstantPadNdBackward0), #idxs=192
[303] prune_out_channels on _ElementWiseOp_371(ConstantPadNdBackward0) => prune_out_channels on _Reshape_370(), #idxs=192
[304] prune_out_channels on _Reshape_370() => prune_out_channels on _ElementWiseOp_369(PermuteBackward0), #idxs=192
[305] prune_out_channels on _ElementWiseOp_369(PermuteBackward0) => prune_out_channels on _ElementWiseOp_368(CloneBackward0), #idxs=192
[306] prune_out_channels on _ElementWiseOp_368(CloneBackward0) => prune_out_channels on _Reshape_367(), #idxs=192
[307] prune_out_channels on _Reshape_367() => prune_out_channels on _Reshape_366(), #idxs=192
[308] prune_out_channels on _Reshape_366() => prune_out_channels on _Reshape_364(), #idxs=192
[309] prune_out_channels on _Reshape_364() => prune_out_channels on _ElementWiseOp_363(AddmmBackward0), #idxs=192
[310] prune_out_channels on _ElementWiseOp_363(AddmmBackward0) => prune_out_channels on _ElementWiseOp_365(TBackward0), #idxs=192
[311] prune_out_channels on _ElementWiseOp_363(AddmmBackward0) => prune_out_channels on _Reshape_362(), #idxs=192
[312] prune_out_channels on _Reshape_362() => prune_out_channels on _Reshape_361(), #idxs=192
[313] prune_out_channels on _Reshape_361() => prune_out_channels on _ElementWiseOp_360(PermuteBackward0), #idxs=192
[314] prune_out_channels on _ElementWiseOp_360(PermuteBackward0) => prune_out_channels on _ElementWiseOp_359(SelectBackward0), #idxs=192
[315] prune_out_channels on _ElementWiseOp_360(PermuteBackward0) => prune_out_channels on _ElementWiseOp_388(SelectBackward0), #idxs=192
[316] prune_out_channels on _ElementWiseOp_360(PermuteBackward0) => prune_out_channels on _ElementWiseOp_392(SelectBackward0), #idxs=192
[317] prune_out_channels on _ElementWiseOp_392(SelectBackward0) => prune_out_channels on _ElementWiseOp_391(MulBackward0), #idxs=192
[318] prune_out_channels on _ElementWiseOp_391(MulBackward0) => prune_out_channels on _ElementWiseOp_390(ExpandBackward0), #idxs=192
[319] prune_out_channels on _ElementWiseOp_390(ExpandBackward0) => prune_out_channels on _ElementWiseOp_389(CloneBackward0), #idxs=192
[320] prune_out_channels on _ElementWiseOp_389(CloneBackward0) => prune_out_channels on _Reshape_383(), #idxs=192
[321] prune_out_channels on _Reshape_383() => prune_out_channels on _ElementWiseOp_382(BmmBackward0), #idxs=192
[322] prune_out_channels on _ElementWiseOp_382(BmmBackward0) => prune_out_channels on _Reshape_384(), #idxs=192
[323] prune_out_channels on _ElementWiseOp_382(BmmBackward0) => prune_out_channels on _Reshape_376(), #idxs=192
[324] prune_out_channels on _Reshape_376() => prune_out_channels on _ElementWiseOp_375(AddBackward0), #idxs=192
[325] prune_out_channels on _ElementWiseOp_375(AddBackward0) => prune_out_channels on _ElementWiseOp_377(UnsqueezeBackward0), #idxs=192
[326] prune_out_channels on _ElementWiseOp_375(AddBackward0) => prune_out_channels on _ElementWiseOp_374(SoftmaxBackward0), #idxs=192
[327] prune_out_channels on _ElementWiseOp_374(SoftmaxBackward0) => prune_out_channels on _ElementWiseOp_373(ExpandBackward0), #idxs=192
[328] prune_out_channels on _ElementWiseOp_373(ExpandBackward0) => prune_out_channels on _Reshape_355(), #idxs=192
[329] prune_out_channels on _Reshape_355() => prune_out_channels on _ElementWiseOp_354(BmmBackward0), #idxs=192
[330] prune_out_channels on _ElementWiseOp_354(BmmBackward0) => prune_out_channels on _Reshape_356(), #idxs=192
[331] prune_out_channels on _ElementWiseOp_354(BmmBackward0) => prune_out_channels on _Reshape_353(), #idxs=192
[332] prune_out_channels on _Reshape_353() => prune_out_channels on _ElementWiseOp_352(TransposeBackward0), #idxs=192
[333] prune_out_channels on _ElementWiseOp_352(TransposeBackward0) => prune_out_channels on _ElementWiseOp_351(CloneBackward0), #idxs=192
[334] prune_out_channels on _ElementWiseOp_351(CloneBackward0) => prune_in_channels on stages.2.blocks.0.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=384, out_features=1152, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=384, out_features=384, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)), #idxs=192
[335] prune_out_channels on _Reshape_356() => prune_out_channels on _ElementWiseOp_357(CloneBackward0), #idxs=192
[336] prune_out_channels on _ElementWiseOp_357(CloneBackward0) => prune_out_channels on _ElementWiseOp_358(ExpandBackward0), #idxs=192
[337] prune_out_channels on _ElementWiseOp_377(UnsqueezeBackward0) => prune_out_channels on _ElementWiseOp_378(CloneBackward0), #idxs=192
[338] prune_out_channels on _ElementWiseOp_378(CloneBackward0) => prune_out_channels on _ElementWiseOp_379(PermuteBackward0), #idxs=192
[339] prune_out_channels on _ElementWiseOp_379(PermuteBackward0) => prune_out_channels on _Reshape_380(), #idxs=192
[340] prune_out_channels on _Reshape_380() => prune_out_channels on _ElementWiseOp_381(IndexBackward0), #idxs=192
[341] prune_out_channels on _Reshape_384() => prune_out_channels on _ElementWiseOp_385(CloneBackward0), #idxs=192
[342] prune_out_channels on _ElementWiseOp_385(CloneBackward0) => prune_out_channels on _ElementWiseOp_386(ExpandBackward0), #idxs=192
[343] prune_out_channels on _ElementWiseOp_386(ExpandBackward0) => prune_out_channels on _ElementWiseOp_387(TransposeBackward0), #idxs=192
[344] prune_out_channels on _Reshape_333() => prune_out_channels on _ElementWiseOp_343(SliceBackward0), #idxs=192
[345] prune_out_channels on _ElementWiseOp_343(SliceBackward0) => prune_out_channels on _ElementWiseOp_344(SliceBackward0), #idxs=192
[346] prune_out_channels on _ElementWiseOp_344(SliceBackward0) => prune_out_channels on _ElementWiseOp_345(SliceBackward0), #idxs=192
[347] prune_out_channels on _ElementWiseOp_345(SliceBackward0) => prune_out_channels on _Reshape_346(), #idxs=192
[348] prune_out_channels on _Reshape_346() => prune_out_channels on _ElementWiseOp_347(CloneBackward0), #idxs=192
[349] prune_out_channels on _ElementWiseOp_347(CloneBackward0) => prune_out_channels on _ElementWiseOp_348(PermuteBackward0), #idxs=192
[350] prune_out_channels on _ElementWiseOp_348(PermuteBackward0) => prune_out_channels on _Reshape_349(), #idxs=192
[351] prune_out_channels on _Reshape_349() => prune_out_channels on _Reshape_350(), #idxs=192
[352] prune_out_channels on _Reshape_350() => prune_out_channels on stages.2.blocks.0.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=384, out_features=1152, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=384, out_features=384, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)), #idxs=192
[353] prune_out_channels on _Reshape_323() => prune_out_channels on _ElementWiseOp_393(SliceBackward0), #idxs=192
[354] prune_out_channels on _ElementWiseOp_393(SliceBackward0) => prune_out_channels on _ElementWiseOp_394(SliceBackward0), #idxs=192
[355] prune_out_channels on _ElementWiseOp_394(SliceBackward0) => prune_out_channels on _ElementWiseOp_395(SliceBackward0), #idxs=192
[356] prune_out_channels on _ElementWiseOp_395(SliceBackward0) => prune_out_channels on _ElementWiseOp_396(RollBackward0), #idxs=192
[357] prune_out_channels on _ElementWiseOp_396(RollBackward0) => prune_out_channels on _Reshape_397(), #idxs=192
[358] prune_out_channels on _Reshape_397() => prune_out_channels on _ElementWiseOp_398(CloneBackward0), #idxs=192
[359] prune_out_channels on _ElementWiseOp_398(CloneBackward0) => prune_out_channels on _ElementWiseOp_399(PermuteBackward0), #idxs=192
[360] prune_out_channels on _ElementWiseOp_399(PermuteBackward0) => prune_out_channels on _Reshape_400(), #idxs=192
[361] prune_out_channels on _Reshape_400() => prune_out_channels on _Reshape_401(), #idxs=192
[362] prune_out_channels on _Reshape_401() => prune_out_channels on stages.2.blocks.1.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=384, out_features=1152, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=384, out_features=384, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)), #idxs=192
[363] prune_out_channels on _Reshape_313() => prune_out_channels on _ElementWiseOp_448(SliceBackward0), #idxs=192
[364] prune_out_channels on _ElementWiseOp_448(SliceBackward0) => prune_out_channels on _ElementWiseOp_449(SliceBackward0), #idxs=192
[365] prune_out_channels on _ElementWiseOp_449(SliceBackward0) => prune_out_channels on _ElementWiseOp_450(SliceBackward0), #idxs=192
[366] prune_out_channels on _ElementWiseOp_450(SliceBackward0) => prune_out_channels on _Reshape_451(), #idxs=192
[367] prune_out_channels on _Reshape_451() => prune_out_channels on _ElementWiseOp_452(CloneBackward0), #idxs=192
[368] prune_out_channels on _ElementWiseOp_452(CloneBackward0) => prune_out_channels on _ElementWiseOp_453(PermuteBackward0), #idxs=192
[369] prune_out_channels on _ElementWiseOp_453(PermuteBackward0) => prune_out_channels on _Reshape_454(), #idxs=192
[370] prune_out_channels on _Reshape_454() => prune_out_channels on _Reshape_455(), #idxs=192
[371] prune_out_channels on _Reshape_455() => prune_out_channels on stages.2.blocks.2.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=384, out_features=1152, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=384, out_features=384, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)), #idxs=192
[372] prune_out_channels on _Reshape_303() => prune_out_channels on _ElementWiseOp_498(SliceBackward0), #idxs=192
[373] prune_out_channels on _ElementWiseOp_498(SliceBackward0) => prune_out_channels on _ElementWiseOp_499(SliceBackward0), #idxs=192
[374] prune_out_channels on _ElementWiseOp_499(SliceBackward0) => prune_out_channels on _ElementWiseOp_500(SliceBackward0), #idxs=192
[375] prune_out_channels on _ElementWiseOp_500(SliceBackward0) => prune_out_channels on _ElementWiseOp_501(RollBackward0), #idxs=192
[376] prune_out_channels on _ElementWiseOp_501(RollBackward0) => prune_out_channels on _Reshape_502(), #idxs=192
[377] prune_out_channels on _Reshape_502() => prune_out_channels on _ElementWiseOp_503(CloneBackward0), #idxs=192
[378] prune_out_channels on _ElementWiseOp_503(CloneBackward0) => prune_out_channels on _ElementWiseOp_504(PermuteBackward0), #idxs=192
[379] prune_out_channels on _ElementWiseOp_504(PermuteBackward0) => prune_out_channels on _Reshape_505(), #idxs=192
[380] prune_out_channels on _Reshape_505() => prune_out_channels on _Reshape_506(), #idxs=192
[381] prune_out_channels on _Reshape_506() => prune_out_channels on stages.2.blocks.3.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=384, out_features=1152, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=384, out_features=384, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)), #idxs=192
[382] prune_out_channels on _Reshape_293() => prune_out_channels on _ElementWiseOp_553(SliceBackward0), #idxs=192
[383] prune_out_channels on _ElementWiseOp_553(SliceBackward0) => prune_out_channels on _ElementWiseOp_554(SliceBackward0), #idxs=192
[384] prune_out_channels on _ElementWiseOp_554(SliceBackward0) => prune_out_channels on _ElementWiseOp_555(SliceBackward0), #idxs=192
[385] prune_out_channels on _ElementWiseOp_555(SliceBackward0) => prune_out_channels on _Reshape_556(), #idxs=192
[386] prune_out_channels on _Reshape_556() => prune_out_channels on _ElementWiseOp_557(CloneBackward0), #idxs=192
[387] prune_out_channels on _ElementWiseOp_557(CloneBackward0) => prune_out_channels on _ElementWiseOp_558(PermuteBackward0), #idxs=192
[388] prune_out_channels on _ElementWiseOp_558(PermuteBackward0) => prune_out_channels on _Reshape_559(), #idxs=192
[389] prune_out_channels on _Reshape_559() => prune_out_channels on _Reshape_560(), #idxs=192
[390] prune_out_channels on _Reshape_560() => prune_out_channels on stages.2.blocks.4.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=384, out_features=1152, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=384, out_features=384, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)), #idxs=192
[391] prune_out_channels on _Reshape_283() => prune_out_channels on _ElementWiseOp_603(SliceBackward0), #idxs=192
[392] prune_out_channels on _ElementWiseOp_603(SliceBackward0) => prune_out_channels on _ElementWiseOp_604(SliceBackward0), #idxs=192
[393] prune_out_channels on _ElementWiseOp_604(SliceBackward0) => prune_out_channels on _ElementWiseOp_605(SliceBackward0), #idxs=192
[394] prune_out_channels on _ElementWiseOp_605(SliceBackward0) => prune_out_channels on _ElementWiseOp_606(RollBackward0), #idxs=192
[395] prune_out_channels on _ElementWiseOp_606(RollBackward0) => prune_out_channels on _Reshape_607(), #idxs=192
[396] prune_out_channels on _Reshape_607() => prune_out_channels on _ElementWiseOp_608(CloneBackward0), #idxs=192
[397] prune_out_channels on _ElementWiseOp_608(CloneBackward0) => prune_out_channels on _ElementWiseOp_609(PermuteBackward0), #idxs=192
[398] prune_out_channels on _ElementWiseOp_609(PermuteBackward0) => prune_out_channels on _Reshape_610(), #idxs=192
[399] prune_out_channels on _Reshape_610() => prune_out_channels on _Reshape_611(), #idxs=192
[400] prune_out_channels on _Reshape_611() => prune_out_channels on stages.2.blocks.5.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=384, out_features=1152, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=384, out_features=384, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)), #idxs=192
--------------------------------

PatchMergingPruner () prune_in_channels/  1536 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
PatchMergingPruner () prune_in_channels/ dim =  384
len indx =  192
idxs_repeated =  768
WindowMSAPruner prune_in_channels() /  192
WindowMSAPruner prune_in_channels() /  192
WindowMSAPruner prune_in_channels() /  192
WindowMSAPruner prune_in_channels() /  192
WindowMSAPruner prune_in_channels() /  192
PatchMergingPruner () prune_out_channels/  384 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
WindowMSAPruner prune_in_channels() /  192
WindowMSAPruner prune_out_channels() /  192
WindowMSAPruner prune_out_channels/ dim =  384
	len indx =  192
WindowMSAPruner - idxs =  [0, 1, 3, 6, 15, 17, 19, 20, 23, 24, 27, 28, 29, 30, 33, 36, 39, 40, 41, 43, 44, 45, 48, 49, 52, 53, 55, 56, 57, 59, 60, 62, 63, 64, 65, 67, 68, 69, 70, 71, 73, 74, 75, 77, 79, 80, 81, 83, 85, 88, 90, 91, 93, 97, 98, 101, 102, 105, 106, 108, 110, 116, 117, 121, 125, 128, 129, 130, 131, 132, 134, 135, 136, 137, 138, 139, 144, 145, 152, 157, 159, 163, 167, 168, 172, 174, 177, 180, 181, 185, 188, 191, 192, 193, 195, 198, 203, 204, 205, 209, 211, 212, 218, 219, 220, 222, 225, 227, 230, 233, 234, 238, 240, 241, 243, 244, 245, 246, 248, 249, 254, 255, 256, 257, 263, 264, 265, 268, 269, 270, 273, 274, 275, 279, 282, 283, 284, 286, 287, 288, 289, 290, 293, 296, 297, 298, 299, 300, 302, 305, 306, 307, 308, 309, 310, 314, 315, 317, 320, 321, 322, 323, 326, 328, 332, 333, 336, 337, 338, 339, 344, 345, 346, 347, 350, 353, 355, 357, 360, 361, 362, 364, 370, 373, 374, 375, 376, 377, 380, 381, 382, 383]
	idxs_repeated =  768
WindowMSAPruner - idxs_repeated =  [0, 1, 3, 6, 15, 17, 19, 20, 23, 24, 27, 28, 29, 30, 33, 36, 39, 40, 41, 43, 44, 45, 48, 49, 52, 53, 55, 56, 57, 59, 60, 62, 63, 64, 65, 67, 68, 69, 70, 71, 73, 74, 75, 77, 79, 80, 81, 83, 85, 88, 90, 91, 93, 97, 98, 101, 102, 105, 106, 108, 110, 116, 117, 121, 125, 128, 129, 130, 131, 132, 134, 135, 136, 137, 138, 139, 144, 145, 152, 157, 159, 163, 167, 168, 172, 174, 177, 180, 181, 185, 188, 191, 192, 193, 195, 198, 203, 204, 205, 209, 211, 212, 218, 219, 220, 222, 225, 227, 230, 233, 234, 238, 240, 241, 243, 244, 245, 246, 248, 249, 254, 255, 256, 257, 263, 264, 265, 268, 269, 270, 273, 274, 275, 279, 282, 283, 284, 286, 287, 288, 289, 290, 293, 296, 297, 298, 299, 300, 302, 305, 306, 307, 308, 309, 310, 314, 315, 317, 320, 321, 322, 323, 326, 328, 332, 333, 336, 337, 338, 339, 344, 345, 346, 347, 350, 353, 355, 357, 360, 361, 362, 364, 370, 373, 374, 375, 376, 377, 380, 381, 382, 383, 384, 385, 387, 390, 399, 401, 403, 404, 407, 408, 411, 412, 413, 414, 417, 420, 423, 424, 425, 427, 428, 429, 432, 433, 436, 437, 439, 440, 441, 443, 444, 446, 447, 448, 449, 451, 452, 453, 454, 455, 457, 458, 459, 461, 463, 464, 465, 467, 469, 472, 474, 475, 477, 481, 482, 485, 486, 489, 490, 492, 494, 500, 501, 505, 509, 512, 513, 514, 515, 516, 518, 519, 520, 521, 522, 523, 528, 529, 536, 541, 543, 547, 551, 552, 556, 558, 561, 564, 565, 569, 572, 575, 576, 577, 579, 582, 587, 588, 589, 593, 595, 596, 602, 603, 604, 606, 609, 611, 614, 617, 618, 622, 624, 625, 627, 628, 629, 630, 632, 633, 638, 639, 640, 641, 647, 648, 649, 652, 653, 654, 657, 658, 659, 663, 666, 667, 668, 670, 671, 672, 673, 674, 677, 680, 681, 682, 683, 684, 686, 689, 690, 691, 692, 693, 694, 698, 699, 701, 704, 705, 706, 707, 710, 712, 716, 717, 720, 721, 722, 723, 728, 729, 730, 731, 734, 737, 739, 741, 744, 745, 746, 748, 754, 757, 758, 759, 760, 761, 764, 765, 766, 767, 768, 769, 771, 774, 783, 785, 787, 788, 791, 792, 795, 796, 797, 798, 801, 804, 807, 808, 809, 811, 812, 813, 816, 817, 820, 821, 823, 824, 825, 827, 828, 830, 831, 832, 833, 835, 836, 837, 838, 839, 841, 842, 843, 845, 847, 848, 849, 851, 853, 856, 858, 859, 861, 865, 866, 869, 870, 873, 874, 876, 878, 884, 885, 889, 893, 896, 897, 898, 899, 900, 902, 903, 904, 905, 906, 907, 912, 913, 920, 925, 927, 931, 935, 936, 940, 942, 945, 948, 949, 953, 956, 959, 960, 961, 963, 966, 971, 972, 973, 977, 979, 980, 986, 987, 988, 990, 993, 995, 998, 1001, 1002, 1006, 1008, 1009, 1011, 1012, 1013, 1014, 1016, 1017, 1022, 1023, 1024, 1025, 1031, 1032, 1033, 1036, 1037, 1038, 1041, 1042, 1043, 1047, 1050, 1051, 1052, 1054, 1055, 1056, 1057, 1058, 1061, 1064, 1065, 1066, 1067, 1068, 1070, 1073, 1074, 1075, 1076, 1077, 1078, 1082, 1083, 1085, 1088, 1089, 1090, 1091, 1094, 1096, 1100, 1101, 1104, 1105, 1106, 1107, 1112, 1113, 1114, 1115, 1118, 1121, 1123, 1125, 1128, 1129, 1130, 1132, 1138, 1141, 1142, 1143, 1144, 1145, 1148, 1149, 1150, 1151, 1152, 1153, 1155, 1158, 1167, 1169, 1171, 1172, 1175, 1176, 1179, 1180, 1181, 1182, 1185, 1188, 1191, 1192, 1193, 1195, 1196, 1197, 1200, 1201, 1204, 1205, 1207, 1208, 1209, 1211, 1212, 1214, 1215, 1216, 1217, 1219, 1220, 1221, 1222, 1223, 1225, 1226, 1227, 1229, 1231, 1232, 1233, 1235, 1237, 1240, 1242, 1243, 1245, 1249, 1250, 1253, 1254, 1257, 1258, 1260, 1262, 1268, 1269, 1273, 1277, 1280, 1281, 1282, 1283, 1284, 1286, 1287, 1288, 1289, 1290, 1291, 1296, 1297, 1304, 1309, 1311, 1315, 1319, 1320, 1324, 1326, 1329, 1332, 1333, 1337, 1340, 1343, 1344, 1345, 1347, 1350, 1355, 1356, 1357, 1361, 1363, 1364, 1370, 1371, 1372, 1374, 1377, 1379, 1382, 1385, 1386, 1390, 1392, 1393, 1395, 1396, 1397, 1398, 1400, 1401, 1406, 1407, 1408, 1409, 1415, 1416, 1417, 1420, 1421, 1422, 1425, 1426, 1427, 1431, 1434, 1435, 1436, 1438, 1439, 1440, 1441, 1442, 1445, 1448, 1449, 1450, 1451, 1452, 1454, 1457, 1458, 1459, 1460, 1461, 1462, 1466, 1467, 1469, 1472, 1473, 1474, 1475, 1478, 1480, 1484, 1485, 1488, 1489, 1490, 1491, 1496, 1497, 1498, 1499, 1502, 1505, 1507, 1509, 1512, 1513, 1514, 1516, 1522, 1525, 1526, 1527, 1528, 1529, 1532, 1533, 1534, 1535]
WindowMSAPruner prune_out_channels idxs_repeated =  768
WindowMSAPruner prune_out_channels() /  192
WindowMSAPruner prune_out_channels/ dim =  384
	len indx =  192
WindowMSAPruner - idxs =  [0, 1, 3, 6, 15, 17, 19, 20, 23, 24, 27, 28, 29, 30, 33, 36, 39, 40, 41, 43, 44, 45, 48, 49, 52, 53, 55, 56, 57, 59, 60, 62, 63, 64, 65, 67, 68, 69, 70, 71, 73, 74, 75, 77, 79, 80, 81, 83, 85, 88, 90, 91, 93, 97, 98, 101, 102, 105, 106, 108, 110, 116, 117, 121, 125, 128, 129, 130, 131, 132, 134, 135, 136, 137, 138, 139, 144, 145, 152, 157, 159, 163, 167, 168, 172, 174, 177, 180, 181, 185, 188, 191, 192, 193, 195, 198, 203, 204, 205, 209, 211, 212, 218, 219, 220, 222, 225, 227, 230, 233, 234, 238, 240, 241, 243, 244, 245, 246, 248, 249, 254, 255, 256, 257, 263, 264, 265, 268, 269, 270, 273, 274, 275, 279, 282, 283, 284, 286, 287, 288, 289, 290, 293, 296, 297, 298, 299, 300, 302, 305, 306, 307, 308, 309, 310, 314, 315, 317, 320, 321, 322, 323, 326, 328, 332, 333, 336, 337, 338, 339, 344, 345, 346, 347, 350, 353, 355, 357, 360, 361, 362, 364, 370, 373, 374, 375, 376, 377, 380, 381, 382, 383]
	idxs_repeated =  768
WindowMSAPruner - idxs_repeated =  [0, 1, 3, 6, 15, 17, 19, 20, 23, 24, 27, 28, 29, 30, 33, 36, 39, 40, 41, 43, 44, 45, 48, 49, 52, 53, 55, 56, 57, 59, 60, 62, 63, 64, 65, 67, 68, 69, 70, 71, 73, 74, 75, 77, 79, 80, 81, 83, 85, 88, 90, 91, 93, 97, 98, 101, 102, 105, 106, 108, 110, 116, 117, 121, 125, 128, 129, 130, 131, 132, 134, 135, 136, 137, 138, 139, 144, 145, 152, 157, 159, 163, 167, 168, 172, 174, 177, 180, 181, 185, 188, 191, 192, 193, 195, 198, 203, 204, 205, 209, 211, 212, 218, 219, 220, 222, 225, 227, 230, 233, 234, 238, 240, 241, 243, 244, 245, 246, 248, 249, 254, 255, 256, 257, 263, 264, 265, 268, 269, 270, 273, 274, 275, 279, 282, 283, 284, 286, 287, 288, 289, 290, 293, 296, 297, 298, 299, 300, 302, 305, 306, 307, 308, 309, 310, 314, 315, 317, 320, 321, 322, 323, 326, 328, 332, 333, 336, 337, 338, 339, 344, 345, 346, 347, 350, 353, 355, 357, 360, 361, 362, 364, 370, 373, 374, 375, 376, 377, 380, 381, 382, 383, 384, 385, 387, 390, 399, 401, 403, 404, 407, 408, 411, 412, 413, 414, 417, 420, 423, 424, 425, 427, 428, 429, 432, 433, 436, 437, 439, 440, 441, 443, 444, 446, 447, 448, 449, 451, 452, 453, 454, 455, 457, 458, 459, 461, 463, 464, 465, 467, 469, 472, 474, 475, 477, 481, 482, 485, 486, 489, 490, 492, 494, 500, 501, 505, 509, 512, 513, 514, 515, 516, 518, 519, 520, 521, 522, 523, 528, 529, 536, 541, 543, 547, 551, 552, 556, 558, 561, 564, 565, 569, 572, 575, 576, 577, 579, 582, 587, 588, 589, 593, 595, 596, 602, 603, 604, 606, 609, 611, 614, 617, 618, 622, 624, 625, 627, 628, 629, 630, 632, 633, 638, 639, 640, 641, 647, 648, 649, 652, 653, 654, 657, 658, 659, 663, 666, 667, 668, 670, 671, 672, 673, 674, 677, 680, 681, 682, 683, 684, 686, 689, 690, 691, 692, 693, 694, 698, 699, 701, 704, 705, 706, 707, 710, 712, 716, 717, 720, 721, 722, 723, 728, 729, 730, 731, 734, 737, 739, 741, 744, 745, 746, 748, 754, 757, 758, 759, 760, 761, 764, 765, 766, 767, 768, 769, 771, 774, 783, 785, 787, 788, 791, 792, 795, 796, 797, 798, 801, 804, 807, 808, 809, 811, 812, 813, 816, 817, 820, 821, 823, 824, 825, 827, 828, 830, 831, 832, 833, 835, 836, 837, 838, 839, 841, 842, 843, 845, 847, 848, 849, 851, 853, 856, 858, 859, 861, 865, 866, 869, 870, 873, 874, 876, 878, 884, 885, 889, 893, 896, 897, 898, 899, 900, 902, 903, 904, 905, 906, 907, 912, 913, 920, 925, 927, 931, 935, 936, 940, 942, 945, 948, 949, 953, 956, 959, 960, 961, 963, 966, 971, 972, 973, 977, 979, 980, 986, 987, 988, 990, 993, 995, 998, 1001, 1002, 1006, 1008, 1009, 1011, 1012, 1013, 1014, 1016, 1017, 1022, 1023, 1024, 1025, 1031, 1032, 1033, 1036, 1037, 1038, 1041, 1042, 1043, 1047, 1050, 1051, 1052, 1054, 1055, 1056, 1057, 1058, 1061, 1064, 1065, 1066, 1067, 1068, 1070, 1073, 1074, 1075, 1076, 1077, 1078, 1082, 1083, 1085, 1088, 1089, 1090, 1091, 1094, 1096, 1100, 1101, 1104, 1105, 1106, 1107, 1112, 1113, 1114, 1115, 1118, 1121, 1123, 1125, 1128, 1129, 1130, 1132, 1138, 1141, 1142, 1143, 1144, 1145, 1148, 1149, 1150, 1151, 1152, 1153, 1155, 1158, 1167, 1169, 1171, 1172, 1175, 1176, 1179, 1180, 1181, 1182, 1185, 1188, 1191, 1192, 1193, 1195, 1196, 1197, 1200, 1201, 1204, 1205, 1207, 1208, 1209, 1211, 1212, 1214, 1215, 1216, 1217, 1219, 1220, 1221, 1222, 1223, 1225, 1226, 1227, 1229, 1231, 1232, 1233, 1235, 1237, 1240, 1242, 1243, 1245, 1249, 1250, 1253, 1254, 1257, 1258, 1260, 1262, 1268, 1269, 1273, 1277, 1280, 1281, 1282, 1283, 1284, 1286, 1287, 1288, 1289, 1290, 1291, 1296, 1297, 1304, 1309, 1311, 1315, 1319, 1320, 1324, 1326, 1329, 1332, 1333, 1337, 1340, 1343, 1344, 1345, 1347, 1350, 1355, 1356, 1357, 1361, 1363, 1364, 1370, 1371, 1372, 1374, 1377, 1379, 1382, 1385, 1386, 1390, 1392, 1393, 1395, 1396, 1397, 1398, 1400, 1401, 1406, 1407, 1408, 1409, 1415, 1416, 1417, 1420, 1421, 1422, 1425, 1426, 1427, 1431, 1434, 1435, 1436, 1438, 1439, 1440, 1441, 1442, 1445, 1448, 1449, 1450, 1451, 1452, 1454, 1457, 1458, 1459, 1460, 1461, 1462, 1466, 1467, 1469, 1472, 1473, 1474, 1475, 1478, 1480, 1484, 1485, 1488, 1489, 1490, 1491, 1496, 1497, 1498, 1499, 1502, 1505, 1507, 1509, 1512, 1513, 1514, 1516, 1522, 1525, 1526, 1527, 1528, 1529, 1532, 1533, 1534, 1535]
WindowMSAPruner prune_out_channels idxs_repeated =  768
WindowMSAPruner prune_out_channels() /  192
WindowMSAPruner prune_out_channels/ dim =  384
	len indx =  192
WindowMSAPruner - idxs =  [0, 1, 3, 6, 15, 17, 19, 20, 23, 24, 27, 28, 29, 30, 33, 36, 39, 40, 41, 43, 44, 45, 48, 49, 52, 53, 55, 56, 57, 59, 60, 62, 63, 64, 65, 67, 68, 69, 70, 71, 73, 74, 75, 77, 79, 80, 81, 83, 85, 88, 90, 91, 93, 97, 98, 101, 102, 105, 106, 108, 110, 116, 117, 121, 125, 128, 129, 130, 131, 132, 134, 135, 136, 137, 138, 139, 144, 145, 152, 157, 159, 163, 167, 168, 172, 174, 177, 180, 181, 185, 188, 191, 192, 193, 195, 198, 203, 204, 205, 209, 211, 212, 218, 219, 220, 222, 225, 227, 230, 233, 234, 238, 240, 241, 243, 244, 245, 246, 248, 249, 254, 255, 256, 257, 263, 264, 265, 268, 269, 270, 273, 274, 275, 279, 282, 283, 284, 286, 287, 288, 289, 290, 293, 296, 297, 298, 299, 300, 302, 305, 306, 307, 308, 309, 310, 314, 315, 317, 320, 321, 322, 323, 326, 328, 332, 333, 336, 337, 338, 339, 344, 345, 346, 347, 350, 353, 355, 357, 360, 361, 362, 364, 370, 373, 374, 375, 376, 377, 380, 381, 382, 383]
	idxs_repeated =  768
WindowMSAPruner - idxs_repeated =  [0, 1, 3, 6, 15, 17, 19, 20, 23, 24, 27, 28, 29, 30, 33, 36, 39, 40, 41, 43, 44, 45, 48, 49, 52, 53, 55, 56, 57, 59, 60, 62, 63, 64, 65, 67, 68, 69, 70, 71, 73, 74, 75, 77, 79, 80, 81, 83, 85, 88, 90, 91, 93, 97, 98, 101, 102, 105, 106, 108, 110, 116, 117, 121, 125, 128, 129, 130, 131, 132, 134, 135, 136, 137, 138, 139, 144, 145, 152, 157, 159, 163, 167, 168, 172, 174, 177, 180, 181, 185, 188, 191, 192, 193, 195, 198, 203, 204, 205, 209, 211, 212, 218, 219, 220, 222, 225, 227, 230, 233, 234, 238, 240, 241, 243, 244, 245, 246, 248, 249, 254, 255, 256, 257, 263, 264, 265, 268, 269, 270, 273, 274, 275, 279, 282, 283, 284, 286, 287, 288, 289, 290, 293, 296, 297, 298, 299, 300, 302, 305, 306, 307, 308, 309, 310, 314, 315, 317, 320, 321, 322, 323, 326, 328, 332, 333, 336, 337, 338, 339, 344, 345, 346, 347, 350, 353, 355, 357, 360, 361, 362, 364, 370, 373, 374, 375, 376, 377, 380, 381, 382, 383, 384, 385, 387, 390, 399, 401, 403, 404, 407, 408, 411, 412, 413, 414, 417, 420, 423, 424, 425, 427, 428, 429, 432, 433, 436, 437, 439, 440, 441, 443, 444, 446, 447, 448, 449, 451, 452, 453, 454, 455, 457, 458, 459, 461, 463, 464, 465, 467, 469, 472, 474, 475, 477, 481, 482, 485, 486, 489, 490, 492, 494, 500, 501, 505, 509, 512, 513, 514, 515, 516, 518, 519, 520, 521, 522, 523, 528, 529, 536, 541, 543, 547, 551, 552, 556, 558, 561, 564, 565, 569, 572, 575, 576, 577, 579, 582, 587, 588, 589, 593, 595, 596, 602, 603, 604, 606, 609, 611, 614, 617, 618, 622, 624, 625, 627, 628, 629, 630, 632, 633, 638, 639, 640, 641, 647, 648, 649, 652, 653, 654, 657, 658, 659, 663, 666, 667, 668, 670, 671, 672, 673, 674, 677, 680, 681, 682, 683, 684, 686, 689, 690, 691, 692, 693, 694, 698, 699, 701, 704, 705, 706, 707, 710, 712, 716, 717, 720, 721, 722, 723, 728, 729, 730, 731, 734, 737, 739, 741, 744, 745, 746, 748, 754, 757, 758, 759, 760, 761, 764, 765, 766, 767, 768, 769, 771, 774, 783, 785, 787, 788, 791, 792, 795, 796, 797, 798, 801, 804, 807, 808, 809, 811, 812, 813, 816, 817, 820, 821, 823, 824, 825, 827, 828, 830, 831, 832, 833, 835, 836, 837, 838, 839, 841, 842, 843, 845, 847, 848, 849, 851, 853, 856, 858, 859, 861, 865, 866, 869, 870, 873, 874, 876, 878, 884, 885, 889, 893, 896, 897, 898, 899, 900, 902, 903, 904, 905, 906, 907, 912, 913, 920, 925, 927, 931, 935, 936, 940, 942, 945, 948, 949, 953, 956, 959, 960, 961, 963, 966, 971, 972, 973, 977, 979, 980, 986, 987, 988, 990, 993, 995, 998, 1001, 1002, 1006, 1008, 1009, 1011, 1012, 1013, 1014, 1016, 1017, 1022, 1023, 1024, 1025, 1031, 1032, 1033, 1036, 1037, 1038, 1041, 1042, 1043, 1047, 1050, 1051, 1052, 1054, 1055, 1056, 1057, 1058, 1061, 1064, 1065, 1066, 1067, 1068, 1070, 1073, 1074, 1075, 1076, 1077, 1078, 1082, 1083, 1085, 1088, 1089, 1090, 1091, 1094, 1096, 1100, 1101, 1104, 1105, 1106, 1107, 1112, 1113, 1114, 1115, 1118, 1121, 1123, 1125, 1128, 1129, 1130, 1132, 1138, 1141, 1142, 1143, 1144, 1145, 1148, 1149, 1150, 1151, 1152, 1153, 1155, 1158, 1167, 1169, 1171, 1172, 1175, 1176, 1179, 1180, 1181, 1182, 1185, 1188, 1191, 1192, 1193, 1195, 1196, 1197, 1200, 1201, 1204, 1205, 1207, 1208, 1209, 1211, 1212, 1214, 1215, 1216, 1217, 1219, 1220, 1221, 1222, 1223, 1225, 1226, 1227, 1229, 1231, 1232, 1233, 1235, 1237, 1240, 1242, 1243, 1245, 1249, 1250, 1253, 1254, 1257, 1258, 1260, 1262, 1268, 1269, 1273, 1277, 1280, 1281, 1282, 1283, 1284, 1286, 1287, 1288, 1289, 1290, 1291, 1296, 1297, 1304, 1309, 1311, 1315, 1319, 1320, 1324, 1326, 1329, 1332, 1333, 1337, 1340, 1343, 1344, 1345, 1347, 1350, 1355, 1356, 1357, 1361, 1363, 1364, 1370, 1371, 1372, 1374, 1377, 1379, 1382, 1385, 1386, 1390, 1392, 1393, 1395, 1396, 1397, 1398, 1400, 1401, 1406, 1407, 1408, 1409, 1415, 1416, 1417, 1420, 1421, 1422, 1425, 1426, 1427, 1431, 1434, 1435, 1436, 1438, 1439, 1440, 1441, 1442, 1445, 1448, 1449, 1450, 1451, 1452, 1454, 1457, 1458, 1459, 1460, 1461, 1462, 1466, 1467, 1469, 1472, 1473, 1474, 1475, 1478, 1480, 1484, 1485, 1488, 1489, 1490, 1491, 1496, 1497, 1498, 1499, 1502, 1505, 1507, 1509, 1512, 1513, 1514, 1516, 1522, 1525, 1526, 1527, 1528, 1529, 1532, 1533, 1534, 1535]
WindowMSAPruner prune_out_channels idxs_repeated =  768
WindowMSAPruner prune_out_channels() /  192
WindowMSAPruner prune_out_channels/ dim =  384
	len indx =  192
WindowMSAPruner - idxs =  [0, 1, 3, 6, 15, 17, 19, 20, 23, 24, 27, 28, 29, 30, 33, 36, 39, 40, 41, 43, 44, 45, 48, 49, 52, 53, 55, 56, 57, 59, 60, 62, 63, 64, 65, 67, 68, 69, 70, 71, 73, 74, 75, 77, 79, 80, 81, 83, 85, 88, 90, 91, 93, 97, 98, 101, 102, 105, 106, 108, 110, 116, 117, 121, 125, 128, 129, 130, 131, 132, 134, 135, 136, 137, 138, 139, 144, 145, 152, 157, 159, 163, 167, 168, 172, 174, 177, 180, 181, 185, 188, 191, 192, 193, 195, 198, 203, 204, 205, 209, 211, 212, 218, 219, 220, 222, 225, 227, 230, 233, 234, 238, 240, 241, 243, 244, 245, 246, 248, 249, 254, 255, 256, 257, 263, 264, 265, 268, 269, 270, 273, 274, 275, 279, 282, 283, 284, 286, 287, 288, 289, 290, 293, 296, 297, 298, 299, 300, 302, 305, 306, 307, 308, 309, 310, 314, 315, 317, 320, 321, 322, 323, 326, 328, 332, 333, 336, 337, 338, 339, 344, 345, 346, 347, 350, 353, 355, 357, 360, 361, 362, 364, 370, 373, 374, 375, 376, 377, 380, 381, 382, 383]
	idxs_repeated =  768
WindowMSAPruner - idxs_repeated =  [0, 1, 3, 6, 15, 17, 19, 20, 23, 24, 27, 28, 29, 30, 33, 36, 39, 40, 41, 43, 44, 45, 48, 49, 52, 53, 55, 56, 57, 59, 60, 62, 63, 64, 65, 67, 68, 69, 70, 71, 73, 74, 75, 77, 79, 80, 81, 83, 85, 88, 90, 91, 93, 97, 98, 101, 102, 105, 106, 108, 110, 116, 117, 121, 125, 128, 129, 130, 131, 132, 134, 135, 136, 137, 138, 139, 144, 145, 152, 157, 159, 163, 167, 168, 172, 174, 177, 180, 181, 185, 188, 191, 192, 193, 195, 198, 203, 204, 205, 209, 211, 212, 218, 219, 220, 222, 225, 227, 230, 233, 234, 238, 240, 241, 243, 244, 245, 246, 248, 249, 254, 255, 256, 257, 263, 264, 265, 268, 269, 270, 273, 274, 275, 279, 282, 283, 284, 286, 287, 288, 289, 290, 293, 296, 297, 298, 299, 300, 302, 305, 306, 307, 308, 309, 310, 314, 315, 317, 320, 321, 322, 323, 326, 328, 332, 333, 336, 337, 338, 339, 344, 345, 346, 347, 350, 353, 355, 357, 360, 361, 362, 364, 370, 373, 374, 375, 376, 377, 380, 381, 382, 383, 384, 385, 387, 390, 399, 401, 403, 404, 407, 408, 411, 412, 413, 414, 417, 420, 423, 424, 425, 427, 428, 429, 432, 433, 436, 437, 439, 440, 441, 443, 444, 446, 447, 448, 449, 451, 452, 453, 454, 455, 457, 458, 459, 461, 463, 464, 465, 467, 469, 472, 474, 475, 477, 481, 482, 485, 486, 489, 490, 492, 494, 500, 501, 505, 509, 512, 513, 514, 515, 516, 518, 519, 520, 521, 522, 523, 528, 529, 536, 541, 543, 547, 551, 552, 556, 558, 561, 564, 565, 569, 572, 575, 576, 577, 579, 582, 587, 588, 589, 593, 595, 596, 602, 603, 604, 606, 609, 611, 614, 617, 618, 622, 624, 625, 627, 628, 629, 630, 632, 633, 638, 639, 640, 641, 647, 648, 649, 652, 653, 654, 657, 658, 659, 663, 666, 667, 668, 670, 671, 672, 673, 674, 677, 680, 681, 682, 683, 684, 686, 689, 690, 691, 692, 693, 694, 698, 699, 701, 704, 705, 706, 707, 710, 712, 716, 717, 720, 721, 722, 723, 728, 729, 730, 731, 734, 737, 739, 741, 744, 745, 746, 748, 754, 757, 758, 759, 760, 761, 764, 765, 766, 767, 768, 769, 771, 774, 783, 785, 787, 788, 791, 792, 795, 796, 797, 798, 801, 804, 807, 808, 809, 811, 812, 813, 816, 817, 820, 821, 823, 824, 825, 827, 828, 830, 831, 832, 833, 835, 836, 837, 838, 839, 841, 842, 843, 845, 847, 848, 849, 851, 853, 856, 858, 859, 861, 865, 866, 869, 870, 873, 874, 876, 878, 884, 885, 889, 893, 896, 897, 898, 899, 900, 902, 903, 904, 905, 906, 907, 912, 913, 920, 925, 927, 931, 935, 936, 940, 942, 945, 948, 949, 953, 956, 959, 960, 961, 963, 966, 971, 972, 973, 977, 979, 980, 986, 987, 988, 990, 993, 995, 998, 1001, 1002, 1006, 1008, 1009, 1011, 1012, 1013, 1014, 1016, 1017, 1022, 1023, 1024, 1025, 1031, 1032, 1033, 1036, 1037, 1038, 1041, 1042, 1043, 1047, 1050, 1051, 1052, 1054, 1055, 1056, 1057, 1058, 1061, 1064, 1065, 1066, 1067, 1068, 1070, 1073, 1074, 1075, 1076, 1077, 1078, 1082, 1083, 1085, 1088, 1089, 1090, 1091, 1094, 1096, 1100, 1101, 1104, 1105, 1106, 1107, 1112, 1113, 1114, 1115, 1118, 1121, 1123, 1125, 1128, 1129, 1130, 1132, 1138, 1141, 1142, 1143, 1144, 1145, 1148, 1149, 1150, 1151, 1152, 1153, 1155, 1158, 1167, 1169, 1171, 1172, 1175, 1176, 1179, 1180, 1181, 1182, 1185, 1188, 1191, 1192, 1193, 1195, 1196, 1197, 1200, 1201, 1204, 1205, 1207, 1208, 1209, 1211, 1212, 1214, 1215, 1216, 1217, 1219, 1220, 1221, 1222, 1223, 1225, 1226, 1227, 1229, 1231, 1232, 1233, 1235, 1237, 1240, 1242, 1243, 1245, 1249, 1250, 1253, 1254, 1257, 1258, 1260, 1262, 1268, 1269, 1273, 1277, 1280, 1281, 1282, 1283, 1284, 1286, 1287, 1288, 1289, 1290, 1291, 1296, 1297, 1304, 1309, 1311, 1315, 1319, 1320, 1324, 1326, 1329, 1332, 1333, 1337, 1340, 1343, 1344, 1345, 1347, 1350, 1355, 1356, 1357, 1361, 1363, 1364, 1370, 1371, 1372, 1374, 1377, 1379, 1382, 1385, 1386, 1390, 1392, 1393, 1395, 1396, 1397, 1398, 1400, 1401, 1406, 1407, 1408, 1409, 1415, 1416, 1417, 1420, 1421, 1422, 1425, 1426, 1427, 1431, 1434, 1435, 1436, 1438, 1439, 1440, 1441, 1442, 1445, 1448, 1449, 1450, 1451, 1452, 1454, 1457, 1458, 1459, 1460, 1461, 1462, 1466, 1467, 1469, 1472, 1473, 1474, 1475, 1478, 1480, 1484, 1485, 1488, 1489, 1490, 1491, 1496, 1497, 1498, 1499, 1502, 1505, 1507, 1509, 1512, 1513, 1514, 1516, 1522, 1525, 1526, 1527, 1528, 1529, 1532, 1533, 1534, 1535]
WindowMSAPruner prune_out_channels idxs_repeated =  768
WindowMSAPruner prune_out_channels() /  192
WindowMSAPruner prune_out_channels/ dim =  384
	len indx =  192
WindowMSAPruner - idxs =  [0, 1, 3, 6, 15, 17, 19, 20, 23, 24, 27, 28, 29, 30, 33, 36, 39, 40, 41, 43, 44, 45, 48, 49, 52, 53, 55, 56, 57, 59, 60, 62, 63, 64, 65, 67, 68, 69, 70, 71, 73, 74, 75, 77, 79, 80, 81, 83, 85, 88, 90, 91, 93, 97, 98, 101, 102, 105, 106, 108, 110, 116, 117, 121, 125, 128, 129, 130, 131, 132, 134, 135, 136, 137, 138, 139, 144, 145, 152, 157, 159, 163, 167, 168, 172, 174, 177, 180, 181, 185, 188, 191, 192, 193, 195, 198, 203, 204, 205, 209, 211, 212, 218, 219, 220, 222, 225, 227, 230, 233, 234, 238, 240, 241, 243, 244, 245, 246, 248, 249, 254, 255, 256, 257, 263, 264, 265, 268, 269, 270, 273, 274, 275, 279, 282, 283, 284, 286, 287, 288, 289, 290, 293, 296, 297, 298, 299, 300, 302, 305, 306, 307, 308, 309, 310, 314, 315, 317, 320, 321, 322, 323, 326, 328, 332, 333, 336, 337, 338, 339, 344, 345, 346, 347, 350, 353, 355, 357, 360, 361, 362, 364, 370, 373, 374, 375, 376, 377, 380, 381, 382, 383]
	idxs_repeated =  768
WindowMSAPruner - idxs_repeated =  [0, 1, 3, 6, 15, 17, 19, 20, 23, 24, 27, 28, 29, 30, 33, 36, 39, 40, 41, 43, 44, 45, 48, 49, 52, 53, 55, 56, 57, 59, 60, 62, 63, 64, 65, 67, 68, 69, 70, 71, 73, 74, 75, 77, 79, 80, 81, 83, 85, 88, 90, 91, 93, 97, 98, 101, 102, 105, 106, 108, 110, 116, 117, 121, 125, 128, 129, 130, 131, 132, 134, 135, 136, 137, 138, 139, 144, 145, 152, 157, 159, 163, 167, 168, 172, 174, 177, 180, 181, 185, 188, 191, 192, 193, 195, 198, 203, 204, 205, 209, 211, 212, 218, 219, 220, 222, 225, 227, 230, 233, 234, 238, 240, 241, 243, 244, 245, 246, 248, 249, 254, 255, 256, 257, 263, 264, 265, 268, 269, 270, 273, 274, 275, 279, 282, 283, 284, 286, 287, 288, 289, 290, 293, 296, 297, 298, 299, 300, 302, 305, 306, 307, 308, 309, 310, 314, 315, 317, 320, 321, 322, 323, 326, 328, 332, 333, 336, 337, 338, 339, 344, 345, 346, 347, 350, 353, 355, 357, 360, 361, 362, 364, 370, 373, 374, 375, 376, 377, 380, 381, 382, 383, 384, 385, 387, 390, 399, 401, 403, 404, 407, 408, 411, 412, 413, 414, 417, 420, 423, 424, 425, 427, 428, 429, 432, 433, 436, 437, 439, 440, 441, 443, 444, 446, 447, 448, 449, 451, 452, 453, 454, 455, 457, 458, 459, 461, 463, 464, 465, 467, 469, 472, 474, 475, 477, 481, 482, 485, 486, 489, 490, 492, 494, 500, 501, 505, 509, 512, 513, 514, 515, 516, 518, 519, 520, 521, 522, 523, 528, 529, 536, 541, 543, 547, 551, 552, 556, 558, 561, 564, 565, 569, 572, 575, 576, 577, 579, 582, 587, 588, 589, 593, 595, 596, 602, 603, 604, 606, 609, 611, 614, 617, 618, 622, 624, 625, 627, 628, 629, 630, 632, 633, 638, 639, 640, 641, 647, 648, 649, 652, 653, 654, 657, 658, 659, 663, 666, 667, 668, 670, 671, 672, 673, 674, 677, 680, 681, 682, 683, 684, 686, 689, 690, 691, 692, 693, 694, 698, 699, 701, 704, 705, 706, 707, 710, 712, 716, 717, 720, 721, 722, 723, 728, 729, 730, 731, 734, 737, 739, 741, 744, 745, 746, 748, 754, 757, 758, 759, 760, 761, 764, 765, 766, 767, 768, 769, 771, 774, 783, 785, 787, 788, 791, 792, 795, 796, 797, 798, 801, 804, 807, 808, 809, 811, 812, 813, 816, 817, 820, 821, 823, 824, 825, 827, 828, 830, 831, 832, 833, 835, 836, 837, 838, 839, 841, 842, 843, 845, 847, 848, 849, 851, 853, 856, 858, 859, 861, 865, 866, 869, 870, 873, 874, 876, 878, 884, 885, 889, 893, 896, 897, 898, 899, 900, 902, 903, 904, 905, 906, 907, 912, 913, 920, 925, 927, 931, 935, 936, 940, 942, 945, 948, 949, 953, 956, 959, 960, 961, 963, 966, 971, 972, 973, 977, 979, 980, 986, 987, 988, 990, 993, 995, 998, 1001, 1002, 1006, 1008, 1009, 1011, 1012, 1013, 1014, 1016, 1017, 1022, 1023, 1024, 1025, 1031, 1032, 1033, 1036, 1037, 1038, 1041, 1042, 1043, 1047, 1050, 1051, 1052, 1054, 1055, 1056, 1057, 1058, 1061, 1064, 1065, 1066, 1067, 1068, 1070, 1073, 1074, 1075, 1076, 1077, 1078, 1082, 1083, 1085, 1088, 1089, 1090, 1091, 1094, 1096, 1100, 1101, 1104, 1105, 1106, 1107, 1112, 1113, 1114, 1115, 1118, 1121, 1123, 1125, 1128, 1129, 1130, 1132, 1138, 1141, 1142, 1143, 1144, 1145, 1148, 1149, 1150, 1151, 1152, 1153, 1155, 1158, 1167, 1169, 1171, 1172, 1175, 1176, 1179, 1180, 1181, 1182, 1185, 1188, 1191, 1192, 1193, 1195, 1196, 1197, 1200, 1201, 1204, 1205, 1207, 1208, 1209, 1211, 1212, 1214, 1215, 1216, 1217, 1219, 1220, 1221, 1222, 1223, 1225, 1226, 1227, 1229, 1231, 1232, 1233, 1235, 1237, 1240, 1242, 1243, 1245, 1249, 1250, 1253, 1254, 1257, 1258, 1260, 1262, 1268, 1269, 1273, 1277, 1280, 1281, 1282, 1283, 1284, 1286, 1287, 1288, 1289, 1290, 1291, 1296, 1297, 1304, 1309, 1311, 1315, 1319, 1320, 1324, 1326, 1329, 1332, 1333, 1337, 1340, 1343, 1344, 1345, 1347, 1350, 1355, 1356, 1357, 1361, 1363, 1364, 1370, 1371, 1372, 1374, 1377, 1379, 1382, 1385, 1386, 1390, 1392, 1393, 1395, 1396, 1397, 1398, 1400, 1401, 1406, 1407, 1408, 1409, 1415, 1416, 1417, 1420, 1421, 1422, 1425, 1426, 1427, 1431, 1434, 1435, 1436, 1438, 1439, 1440, 1441, 1442, 1445, 1448, 1449, 1450, 1451, 1452, 1454, 1457, 1458, 1459, 1460, 1461, 1462, 1466, 1467, 1469, 1472, 1473, 1474, 1475, 1478, 1480, 1484, 1485, 1488, 1489, 1490, 1491, 1496, 1497, 1498, 1499, 1502, 1505, 1507, 1509, 1512, 1513, 1514, 1516, 1522, 1525, 1526, 1527, 1528, 1529, 1532, 1533, 1534, 1535]
WindowMSAPruner prune_out_channels idxs_repeated =  768
WindowMSAPruner prune_out_channels() /  192
WindowMSAPruner prune_out_channels/ dim =  384
	len indx =  192
WindowMSAPruner - idxs =  [0, 1, 3, 6, 15, 17, 19, 20, 23, 24, 27, 28, 29, 30, 33, 36, 39, 40, 41, 43, 44, 45, 48, 49, 52, 53, 55, 56, 57, 59, 60, 62, 63, 64, 65, 67, 68, 69, 70, 71, 73, 74, 75, 77, 79, 80, 81, 83, 85, 88, 90, 91, 93, 97, 98, 101, 102, 105, 106, 108, 110, 116, 117, 121, 125, 128, 129, 130, 131, 132, 134, 135, 136, 137, 138, 139, 144, 145, 152, 157, 159, 163, 167, 168, 172, 174, 177, 180, 181, 185, 188, 191, 192, 193, 195, 198, 203, 204, 205, 209, 211, 212, 218, 219, 220, 222, 225, 227, 230, 233, 234, 238, 240, 241, 243, 244, 245, 246, 248, 249, 254, 255, 256, 257, 263, 264, 265, 268, 269, 270, 273, 274, 275, 279, 282, 283, 284, 286, 287, 288, 289, 290, 293, 296, 297, 298, 299, 300, 302, 305, 306, 307, 308, 309, 310, 314, 315, 317, 320, 321, 322, 323, 326, 328, 332, 333, 336, 337, 338, 339, 344, 345, 346, 347, 350, 353, 355, 357, 360, 361, 362, 364, 370, 373, 374, 375, 376, 377, 380, 381, 382, 383]
	idxs_repeated =  768
WindowMSAPruner - idxs_repeated =  [0, 1, 3, 6, 15, 17, 19, 20, 23, 24, 27, 28, 29, 30, 33, 36, 39, 40, 41, 43, 44, 45, 48, 49, 52, 53, 55, 56, 57, 59, 60, 62, 63, 64, 65, 67, 68, 69, 70, 71, 73, 74, 75, 77, 79, 80, 81, 83, 85, 88, 90, 91, 93, 97, 98, 101, 102, 105, 106, 108, 110, 116, 117, 121, 125, 128, 129, 130, 131, 132, 134, 135, 136, 137, 138, 139, 144, 145, 152, 157, 159, 163, 167, 168, 172, 174, 177, 180, 181, 185, 188, 191, 192, 193, 195, 198, 203, 204, 205, 209, 211, 212, 218, 219, 220, 222, 225, 227, 230, 233, 234, 238, 240, 241, 243, 244, 245, 246, 248, 249, 254, 255, 256, 257, 263, 264, 265, 268, 269, 270, 273, 274, 275, 279, 282, 283, 284, 286, 287, 288, 289, 290, 293, 296, 297, 298, 299, 300, 302, 305, 306, 307, 308, 309, 310, 314, 315, 317, 320, 321, 322, 323, 326, 328, 332, 333, 336, 337, 338, 339, 344, 345, 346, 347, 350, 353, 355, 357, 360, 361, 362, 364, 370, 373, 374, 375, 376, 377, 380, 381, 382, 383, 384, 385, 387, 390, 399, 401, 403, 404, 407, 408, 411, 412, 413, 414, 417, 420, 423, 424, 425, 427, 428, 429, 432, 433, 436, 437, 439, 440, 441, 443, 444, 446, 447, 448, 449, 451, 452, 453, 454, 455, 457, 458, 459, 461, 463, 464, 465, 467, 469, 472, 474, 475, 477, 481, 482, 485, 486, 489, 490, 492, 494, 500, 501, 505, 509, 512, 513, 514, 515, 516, 518, 519, 520, 521, 522, 523, 528, 529, 536, 541, 543, 547, 551, 552, 556, 558, 561, 564, 565, 569, 572, 575, 576, 577, 579, 582, 587, 588, 589, 593, 595, 596, 602, 603, 604, 606, 609, 611, 614, 617, 618, 622, 624, 625, 627, 628, 629, 630, 632, 633, 638, 639, 640, 641, 647, 648, 649, 652, 653, 654, 657, 658, 659, 663, 666, 667, 668, 670, 671, 672, 673, 674, 677, 680, 681, 682, 683, 684, 686, 689, 690, 691, 692, 693, 694, 698, 699, 701, 704, 705, 706, 707, 710, 712, 716, 717, 720, 721, 722, 723, 728, 729, 730, 731, 734, 737, 739, 741, 744, 745, 746, 748, 754, 757, 758, 759, 760, 761, 764, 765, 766, 767, 768, 769, 771, 774, 783, 785, 787, 788, 791, 792, 795, 796, 797, 798, 801, 804, 807, 808, 809, 811, 812, 813, 816, 817, 820, 821, 823, 824, 825, 827, 828, 830, 831, 832, 833, 835, 836, 837, 838, 839, 841, 842, 843, 845, 847, 848, 849, 851, 853, 856, 858, 859, 861, 865, 866, 869, 870, 873, 874, 876, 878, 884, 885, 889, 893, 896, 897, 898, 899, 900, 902, 903, 904, 905, 906, 907, 912, 913, 920, 925, 927, 931, 935, 936, 940, 942, 945, 948, 949, 953, 956, 959, 960, 961, 963, 966, 971, 972, 973, 977, 979, 980, 986, 987, 988, 990, 993, 995, 998, 1001, 1002, 1006, 1008, 1009, 1011, 1012, 1013, 1014, 1016, 1017, 1022, 1023, 1024, 1025, 1031, 1032, 1033, 1036, 1037, 1038, 1041, 1042, 1043, 1047, 1050, 1051, 1052, 1054, 1055, 1056, 1057, 1058, 1061, 1064, 1065, 1066, 1067, 1068, 1070, 1073, 1074, 1075, 1076, 1077, 1078, 1082, 1083, 1085, 1088, 1089, 1090, 1091, 1094, 1096, 1100, 1101, 1104, 1105, 1106, 1107, 1112, 1113, 1114, 1115, 1118, 1121, 1123, 1125, 1128, 1129, 1130, 1132, 1138, 1141, 1142, 1143, 1144, 1145, 1148, 1149, 1150, 1151, 1152, 1153, 1155, 1158, 1167, 1169, 1171, 1172, 1175, 1176, 1179, 1180, 1181, 1182, 1185, 1188, 1191, 1192, 1193, 1195, 1196, 1197, 1200, 1201, 1204, 1205, 1207, 1208, 1209, 1211, 1212, 1214, 1215, 1216, 1217, 1219, 1220, 1221, 1222, 1223, 1225, 1226, 1227, 1229, 1231, 1232, 1233, 1235, 1237, 1240, 1242, 1243, 1245, 1249, 1250, 1253, 1254, 1257, 1258, 1260, 1262, 1268, 1269, 1273, 1277, 1280, 1281, 1282, 1283, 1284, 1286, 1287, 1288, 1289, 1290, 1291, 1296, 1297, 1304, 1309, 1311, 1315, 1319, 1320, 1324, 1326, 1329, 1332, 1333, 1337, 1340, 1343, 1344, 1345, 1347, 1350, 1355, 1356, 1357, 1361, 1363, 1364, 1370, 1371, 1372, 1374, 1377, 1379, 1382, 1385, 1386, 1390, 1392, 1393, 1395, 1396, 1397, 1398, 1400, 1401, 1406, 1407, 1408, 1409, 1415, 1416, 1417, 1420, 1421, 1422, 1425, 1426, 1427, 1431, 1434, 1435, 1436, 1438, 1439, 1440, 1441, 1442, 1445, 1448, 1449, 1450, 1451, 1452, 1454, 1457, 1458, 1459, 1460, 1461, 1462, 1466, 1467, 1469, 1472, 1473, 1474, 1475, 1478, 1480, 1484, 1485, 1488, 1489, 1490, 1491, 1496, 1497, 1498, 1499, 1502, 1505, 1507, 1509, 1512, 1513, 1514, 1516, 1522, 1525, 1526, 1527, 1528, 1529, 1532, 1533, 1534, 1535]
WindowMSAPruner prune_out_channels idxs_repeated =  768
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
	get_all_groups -- Adding layer:  <class 'mmcv.cnn.bricks.wrappers.Linear'>
		prune_local()/  _check_pruning_ratio OK
idxs =  768
prunable_chs =  1536
pruner.step 
--------------------------------
          Pruning Group
--------------------------------
[0] prune_out_channels on stages.2.blocks.5.ffn.layers.0.0 (Linear(in_features=192, out_features=1536, bias=True)) => prune_out_channels on stages.2.blocks.5.ffn.layers.0.0 (Linear(in_features=192, out_features=1536, bias=True)), #idxs=768
[1] prune_out_channels on stages.2.blocks.5.ffn.layers.0.0 (Linear(in_features=192, out_features=1536, bias=True)) => prune_out_channels on _ElementWiseOp_279(GeluBackward0), #idxs=768
[2] prune_out_channels on _ElementWiseOp_279(GeluBackward0) => prune_out_channels on _Reshape_277(), #idxs=768
[3] prune_out_channels on _Reshape_277() => prune_out_channels on _ElementWiseOp_276(AddmmBackward0), #idxs=768
[4] prune_out_channels on _ElementWiseOp_276(AddmmBackward0) => prune_out_channels on _ElementWiseOp_278(TBackward0), #idxs=768
[5] prune_out_channels on _ElementWiseOp_276(AddmmBackward0) => prune_in_channels on stages.2.blocks.5.ffn.layers.1 (Linear(in_features=1536, out_features=192, bias=True)), #idxs=768
--------------------------------

<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
continue 4
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
	get_all_groups -- Adding layer:  <class 'mmcv.cnn.bricks.wrappers.Linear'>
		prune_local()/  _check_pruning_ratio OK
idxs =  768
prunable_chs =  1536
pruner.step 
--------------------------------
          Pruning Group
--------------------------------
[0] prune_out_channels on stages.2.blocks.4.ffn.layers.0.0 (Linear(in_features=192, out_features=1536, bias=True)) => prune_out_channels on stages.2.blocks.4.ffn.layers.0.0 (Linear(in_features=192, out_features=1536, bias=True)), #idxs=768
[1] prune_out_channels on stages.2.blocks.4.ffn.layers.0.0 (Linear(in_features=192, out_features=1536, bias=True)) => prune_out_channels on _ElementWiseOp_289(GeluBackward0), #idxs=768
[2] prune_out_channels on _ElementWiseOp_289(GeluBackward0) => prune_out_channels on _Reshape_287(), #idxs=768
[3] prune_out_channels on _Reshape_287() => prune_out_channels on _ElementWiseOp_286(AddmmBackward0), #idxs=768
[4] prune_out_channels on _ElementWiseOp_286(AddmmBackward0) => prune_out_channels on _ElementWiseOp_288(TBackward0), #idxs=768
[5] prune_out_channels on _ElementWiseOp_286(AddmmBackward0) => prune_in_channels on stages.2.blocks.4.ffn.layers.1 (Linear(in_features=1536, out_features=192, bias=True)), #idxs=768
--------------------------------

<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
continue 4
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
	get_all_groups -- Adding layer:  <class 'mmcv.cnn.bricks.wrappers.Linear'>
		prune_local()/  _check_pruning_ratio OK
idxs =  768
prunable_chs =  1536
pruner.step 
--------------------------------
          Pruning Group
--------------------------------
[0] prune_out_channels on stages.2.blocks.3.ffn.layers.0.0 (Linear(in_features=192, out_features=1536, bias=True)) => prune_out_channels on stages.2.blocks.3.ffn.layers.0.0 (Linear(in_features=192, out_features=1536, bias=True)), #idxs=768
[1] prune_out_channels on stages.2.blocks.3.ffn.layers.0.0 (Linear(in_features=192, out_features=1536, bias=True)) => prune_out_channels on _ElementWiseOp_299(GeluBackward0), #idxs=768
[2] prune_out_channels on _ElementWiseOp_299(GeluBackward0) => prune_out_channels on _Reshape_297(), #idxs=768
[3] prune_out_channels on _Reshape_297() => prune_out_channels on _ElementWiseOp_296(AddmmBackward0), #idxs=768
[4] prune_out_channels on _ElementWiseOp_296(AddmmBackward0) => prune_out_channels on _ElementWiseOp_298(TBackward0), #idxs=768
[5] prune_out_channels on _ElementWiseOp_296(AddmmBackward0) => prune_in_channels on stages.2.blocks.3.ffn.layers.1 (Linear(in_features=1536, out_features=192, bias=True)), #idxs=768
--------------------------------

<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
continue 4
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
	get_all_groups -- Adding layer:  <class 'mmcv.cnn.bricks.wrappers.Linear'>
		prune_local()/  _check_pruning_ratio OK
idxs =  768
prunable_chs =  1536
pruner.step 
--------------------------------
          Pruning Group
--------------------------------
[0] prune_out_channels on stages.2.blocks.2.ffn.layers.0.0 (Linear(in_features=192, out_features=1536, bias=True)) => prune_out_channels on stages.2.blocks.2.ffn.layers.0.0 (Linear(in_features=192, out_features=1536, bias=True)), #idxs=768
[1] prune_out_channels on stages.2.blocks.2.ffn.layers.0.0 (Linear(in_features=192, out_features=1536, bias=True)) => prune_out_channels on _ElementWiseOp_309(GeluBackward0), #idxs=768
[2] prune_out_channels on _ElementWiseOp_309(GeluBackward0) => prune_out_channels on _Reshape_307(), #idxs=768
[3] prune_out_channels on _Reshape_307() => prune_out_channels on _ElementWiseOp_306(AddmmBackward0), #idxs=768
[4] prune_out_channels on _ElementWiseOp_306(AddmmBackward0) => prune_out_channels on _ElementWiseOp_308(TBackward0), #idxs=768
[5] prune_out_channels on _ElementWiseOp_306(AddmmBackward0) => prune_in_channels on stages.2.blocks.2.ffn.layers.1 (Linear(in_features=1536, out_features=192, bias=True)), #idxs=768
--------------------------------

<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
continue 4
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
	get_all_groups -- Adding layer:  <class 'mmcv.cnn.bricks.wrappers.Linear'>
		prune_local()/  _check_pruning_ratio OK
idxs =  768
prunable_chs =  1536
pruner.step 
--------------------------------
          Pruning Group
--------------------------------
[0] prune_out_channels on stages.2.blocks.1.ffn.layers.0.0 (Linear(in_features=192, out_features=1536, bias=True)) => prune_out_channels on stages.2.blocks.1.ffn.layers.0.0 (Linear(in_features=192, out_features=1536, bias=True)), #idxs=768
[1] prune_out_channels on stages.2.blocks.1.ffn.layers.0.0 (Linear(in_features=192, out_features=1536, bias=True)) => prune_out_channels on _ElementWiseOp_319(GeluBackward0), #idxs=768
[2] prune_out_channels on _ElementWiseOp_319(GeluBackward0) => prune_out_channels on _Reshape_317(), #idxs=768
[3] prune_out_channels on _Reshape_317() => prune_out_channels on _ElementWiseOp_316(AddmmBackward0), #idxs=768
[4] prune_out_channels on _ElementWiseOp_316(AddmmBackward0) => prune_out_channels on _ElementWiseOp_318(TBackward0), #idxs=768
[5] prune_out_channels on _ElementWiseOp_316(AddmmBackward0) => prune_in_channels on stages.2.blocks.1.ffn.layers.1 (Linear(in_features=1536, out_features=192, bias=True)), #idxs=768
--------------------------------

<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
continue 4
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
	get_all_groups -- Adding layer:  <class 'mmcv.cnn.bricks.wrappers.Linear'>
		prune_local()/  _check_pruning_ratio OK
idxs =  768
prunable_chs =  1536
pruner.step 
--------------------------------
          Pruning Group
--------------------------------
[0] prune_out_channels on stages.2.blocks.0.ffn.layers.0.0 (Linear(in_features=192, out_features=1536, bias=True)) => prune_out_channels on stages.2.blocks.0.ffn.layers.0.0 (Linear(in_features=192, out_features=1536, bias=True)), #idxs=768
[1] prune_out_channels on stages.2.blocks.0.ffn.layers.0.0 (Linear(in_features=192, out_features=1536, bias=True)) => prune_out_channels on _ElementWiseOp_329(GeluBackward0), #idxs=768
[2] prune_out_channels on _ElementWiseOp_329(GeluBackward0) => prune_out_channels on _Reshape_327(), #idxs=768
[3] prune_out_channels on _Reshape_327() => prune_out_channels on _ElementWiseOp_326(AddmmBackward0), #idxs=768
[4] prune_out_channels on _ElementWiseOp_326(AddmmBackward0) => prune_out_channels on _ElementWiseOp_328(TBackward0), #idxs=768
[5] prune_out_channels on _ElementWiseOp_326(AddmmBackward0) => prune_in_channels on stages.2.blocks.0.ffn.layers.1 (Linear(in_features=1536, out_features=192, bias=True)), #idxs=768
--------------------------------

<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
continue 4
<class 'torch_pruning.ops._ReshapeOp'> continue 2
PatchMergingPruner () get_out_channels/  192 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
WindowMSAPruner layer.embed_dims =  192
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
WindowMSAPruner layer.embed_dims =  192
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
WindowMSAPruner layer.embed_dims =  192
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
WindowMSAPruner layer.embed_dims =  192
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
WindowMSAPruner layer.embed_dims =  192
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
WindowMSAPruner layer.embed_dims =  192
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
	get_all_groups -- Adding layer:  <class 'torch.nn.modules.normalization.LayerNorm'>
WindowMSAPruner layer.qkv.in_features =  768
PatchMergingPruner () get_out_channels/  768 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
WindowMSAPruner layer.qkv.in_features =  768
WindowMSAPruner layer.embed_dims =  768
WindowMSAPruner layer.embed_dims =  768
		prune_local()/  _check_pruning_ratio OK
idxs =  384
prunable_chs =  768
idxs =  384
prunable_chs =  768
idxs =  384
prunable_chs =  768
idxs =  384
prunable_chs =  768
idxs =  384
prunable_chs =  768
WindowMSAPruner layer.qkv.in_features =  768
PatchMergingPruner () get_out_channels/  768 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
idxs =  384
prunable_chs =  768
idxs =  384
prunable_chs =  768
idxs =  384
prunable_chs =  768
WindowMSAPruner layer.qkv.in_features =  768
WindowMSAPruner layer.embed_dims =  768
idxs =  384
prunable_chs =  768
WindowMSAPruner layer.embed_dims =  768
idxs =  384
prunable_chs =  768
pruner.step 
--------------------------------
          Pruning Group
--------------------------------
[0] prune_out_channels on norm3 (LayerNorm((768,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on norm3 (LayerNorm((768,), eps=1e-05, elementwise_affine=True)), #idxs=384
[1] prune_out_channels on norm3 (LayerNorm((768,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _ElementWiseOp_661(AddBackward0), #idxs=384
[2] prune_out_channels on norm3 (LayerNorm((768,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_660(), #idxs=384
[3] prune_out_channels on _Reshape_660() => prune_out_channels on _ElementWiseOp_659(PermuteBackward0), #idxs=384
[4] prune_out_channels on _ElementWiseOp_659(PermuteBackward0) => prune_out_channels on _ElementWiseOp_658(CloneBackward0), #idxs=384
[5] prune_out_channels on _ElementWiseOp_661(AddBackward0) => prune_out_channels on _ElementWiseOp_662(AddBackward0), #idxs=384
[6] prune_out_channels on _ElementWiseOp_661(AddBackward0) => prune_out_channels on stages.3.blocks.1.ffn.layers.1 (Linear(in_features=3072, out_features=768, bias=True)), #idxs=384
[7] prune_out_channels on _ElementWiseOp_662(AddBackward0) => prune_out_channels on _Reshape_670(), #idxs=384
[8] prune_out_channels on _ElementWiseOp_662(AddBackward0) => prune_out_channels on _ElementWiseOp_671(AddBackward0), #idxs=384
[9] prune_out_channels on _ElementWiseOp_662(AddBackward0) => prune_out_channels on stages.3.blocks.1.norm2 (LayerNorm((768,), eps=1e-05, elementwise_affine=True)), #idxs=384
[10] prune_out_channels on stages.3.blocks.1.norm2 (LayerNorm((768,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_668(), #idxs=384
[11] prune_out_channels on _Reshape_668() => prune_out_channels on _ElementWiseOp_667(AddmmBackward0), #idxs=384
[12] prune_out_channels on _ElementWiseOp_667(AddmmBackward0) => prune_out_channels on _ElementWiseOp_669(TBackward0), #idxs=384
[13] prune_out_channels on _ElementWiseOp_667(AddmmBackward0) => prune_in_channels on stages.3.blocks.1.ffn.layers.0.0 (Linear(in_features=768, out_features=3072, bias=True)), #idxs=384
[14] prune_out_channels on _ElementWiseOp_671(AddBackward0) => prune_out_channels on _ElementWiseOp_672(AddBackward0), #idxs=384
[15] prune_out_channels on _ElementWiseOp_671(AddBackward0) => prune_out_channels on stages.3.blocks.0.ffn.layers.1 (Linear(in_features=3072, out_features=768, bias=True)), #idxs=384
[16] prune_out_channels on _ElementWiseOp_671(AddBackward0) => prune_out_channels on stages.3.blocks.1.norm1 (LayerNorm((768,), eps=1e-05, elementwise_affine=True)), #idxs=384
[17] prune_out_channels on stages.3.blocks.1.norm1 (LayerNorm((768,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_770(), #idxs=384
[18] prune_out_channels on _Reshape_770() => prune_out_channels on _ElementWiseOp_769(ConstantPadNdBackward0), #idxs=384
[19] prune_out_channels on _ElementWiseOp_769(ConstantPadNdBackward0) => prune_out_channels on _ElementWiseOp_768(RollBackward0), #idxs=384
[20] prune_out_channels on _ElementWiseOp_768(RollBackward0) => prune_out_channels on _Reshape_767(), #idxs=384
[21] prune_out_channels on _Reshape_767() => prune_out_channels on _ElementWiseOp_766(PermuteBackward0), #idxs=384
[22] prune_out_channels on _ElementWiseOp_766(PermuteBackward0) => prune_out_channels on _ElementWiseOp_765(CloneBackward0), #idxs=384
[23] prune_out_channels on _ElementWiseOp_765(CloneBackward0) => prune_out_channels on _Reshape_764(), #idxs=384
[24] prune_out_channels on _Reshape_764() => prune_out_channels on _Reshape_763(), #idxs=384
[25] prune_out_channels on _Reshape_763() => prune_out_channels on _Reshape_761(), #idxs=384
[26] prune_out_channels on _Reshape_761() => prune_out_channels on _ElementWiseOp_760(AddmmBackward0), #idxs=384
[27] prune_out_channels on _ElementWiseOp_760(AddmmBackward0) => prune_out_channels on _ElementWiseOp_762(TBackward0), #idxs=384
[28] prune_out_channels on _ElementWiseOp_760(AddmmBackward0) => prune_out_channels on _Reshape_759(), #idxs=384
[29] prune_out_channels on _Reshape_759() => prune_out_channels on _Reshape_758(), #idxs=384
[30] prune_out_channels on _Reshape_758() => prune_out_channels on _ElementWiseOp_757(PermuteBackward0), #idxs=384
[31] prune_out_channels on _ElementWiseOp_757(PermuteBackward0) => prune_out_channels on _ElementWiseOp_756(SelectBackward0), #idxs=384
[32] prune_out_channels on _ElementWiseOp_757(PermuteBackward0) => prune_out_channels on _ElementWiseOp_789(SelectBackward0), #idxs=384
[33] prune_out_channels on _ElementWiseOp_757(PermuteBackward0) => prune_out_channels on _ElementWiseOp_793(SelectBackward0), #idxs=384
[34] prune_out_channels on _ElementWiseOp_793(SelectBackward0) => prune_out_channels on _ElementWiseOp_792(MulBackward0), #idxs=384
[35] prune_out_channels on _ElementWiseOp_792(MulBackward0) => prune_out_channels on _ElementWiseOp_791(ExpandBackward0), #idxs=384
[36] prune_out_channels on _ElementWiseOp_791(ExpandBackward0) => prune_out_channels on _ElementWiseOp_790(CloneBackward0), #idxs=384
[37] prune_out_channels on _ElementWiseOp_790(CloneBackward0) => prune_out_channels on _Reshape_784(), #idxs=384
[38] prune_out_channels on _Reshape_784() => prune_out_channels on _ElementWiseOp_783(BmmBackward0), #idxs=384
[39] prune_out_channels on _ElementWiseOp_783(BmmBackward0) => prune_out_channels on _Reshape_785(), #idxs=384
[40] prune_out_channels on _ElementWiseOp_783(BmmBackward0) => prune_out_channels on _Reshape_777(), #idxs=384
[41] prune_out_channels on _Reshape_777() => prune_out_channels on _ElementWiseOp_776(AddBackward0), #idxs=384
[42] prune_out_channels on _ElementWiseOp_776(AddBackward0) => prune_out_channels on _ElementWiseOp_778(UnsqueezeBackward0), #idxs=384
[43] prune_out_channels on _ElementWiseOp_776(AddBackward0) => prune_out_channels on _Reshape_775(), #idxs=384
[44] prune_out_channels on _Reshape_775() => prune_out_channels on _ElementWiseOp_774(AddBackward0), #idxs=384
[45] prune_out_channels on _ElementWiseOp_774(AddBackward0) => prune_out_channels on _Reshape_773(), #idxs=384
[46] prune_out_channels on _Reshape_773() => prune_out_channels on _ElementWiseOp_772(SoftmaxBackward0), #idxs=384
[47] prune_out_channels on _ElementWiseOp_772(SoftmaxBackward0) => prune_out_channels on _ElementWiseOp_771(ExpandBackward0), #idxs=384
[48] prune_out_channels on _ElementWiseOp_771(ExpandBackward0) => prune_out_channels on _Reshape_752(), #idxs=384
[49] prune_out_channels on _Reshape_752() => prune_out_channels on _ElementWiseOp_751(BmmBackward0), #idxs=384
[50] prune_out_channels on _ElementWiseOp_751(BmmBackward0) => prune_out_channels on _Reshape_753(), #idxs=384
[51] prune_out_channels on _ElementWiseOp_751(BmmBackward0) => prune_out_channels on _Reshape_750(), #idxs=384
[52] prune_out_channels on _Reshape_750() => prune_out_channels on _ElementWiseOp_749(TransposeBackward0), #idxs=384
[53] prune_out_channels on _ElementWiseOp_749(TransposeBackward0) => prune_out_channels on _ElementWiseOp_748(CloneBackward0), #idxs=384
[54] prune_out_channels on _ElementWiseOp_748(CloneBackward0) => prune_in_channels on stages.3.blocks.1.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=768, out_features=2304, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=768, out_features=768, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)), #idxs=384
[55] prune_out_channels on _Reshape_753() => prune_out_channels on _ElementWiseOp_754(CloneBackward0), #idxs=384
[56] prune_out_channels on _ElementWiseOp_754(CloneBackward0) => prune_out_channels on _ElementWiseOp_755(ExpandBackward0), #idxs=384
[57] prune_out_channels on _ElementWiseOp_778(UnsqueezeBackward0) => prune_out_channels on _ElementWiseOp_779(CloneBackward0), #idxs=384
[58] prune_out_channels on _ElementWiseOp_779(CloneBackward0) => prune_out_channels on _ElementWiseOp_780(PermuteBackward0), #idxs=384
[59] prune_out_channels on _ElementWiseOp_780(PermuteBackward0) => prune_out_channels on _Reshape_781(), #idxs=384
[60] prune_out_channels on _Reshape_781() => prune_out_channels on _ElementWiseOp_782(IndexBackward0), #idxs=384
[61] prune_out_channels on _Reshape_785() => prune_out_channels on _ElementWiseOp_786(CloneBackward0), #idxs=384
[62] prune_out_channels on _ElementWiseOp_786(CloneBackward0) => prune_out_channels on _ElementWiseOp_787(ExpandBackward0), #idxs=384
[63] prune_out_channels on _ElementWiseOp_787(ExpandBackward0) => prune_out_channels on _ElementWiseOp_788(TransposeBackward0), #idxs=384
[64] prune_out_channels on _ElementWiseOp_672(AddBackward0) => prune_out_channels on _Reshape_680(), #idxs=384
[65] prune_out_channels on _ElementWiseOp_672(AddBackward0) => prune_out_channels on stages.2.downsample (PatchMerging(
  (adap_padding): AdaptivePadding()
  (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (reduction): Linear(in_features=768, out_features=768, bias=False)
)), #idxs=384
[66] prune_out_channels on _ElementWiseOp_672(AddBackward0) => prune_out_channels on stages.3.blocks.0.norm2 (LayerNorm((768,), eps=1e-05, elementwise_affine=True)), #idxs=384
[67] prune_out_channels on stages.3.blocks.0.norm2 (LayerNorm((768,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_678(), #idxs=384
[68] prune_out_channels on _Reshape_678() => prune_out_channels on _ElementWiseOp_677(AddmmBackward0), #idxs=384
[69] prune_out_channels on _ElementWiseOp_677(AddmmBackward0) => prune_out_channels on _ElementWiseOp_679(TBackward0), #idxs=384
[70] prune_out_channels on _ElementWiseOp_677(AddmmBackward0) => prune_in_channels on stages.3.blocks.0.ffn.layers.0.0 (Linear(in_features=768, out_features=3072, bias=True)), #idxs=384
[71] prune_out_channels on stages.2.downsample (PatchMerging(
  (adap_padding): AdaptivePadding()
  (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (reduction): Linear(in_features=768, out_features=768, bias=False)
)) => prune_out_channels on stages.3.blocks.0.norm1 (LayerNorm((768,), eps=1e-05, elementwise_affine=True)), #idxs=384
[72] prune_out_channels on stages.3.blocks.0.norm1 (LayerNorm((768,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_718(), #idxs=384
[73] prune_out_channels on _Reshape_718() => prune_out_channels on _ElementWiseOp_717(ConstantPadNdBackward0), #idxs=384
[74] prune_out_channels on _ElementWiseOp_717(ConstantPadNdBackward0) => prune_out_channels on _Reshape_716(), #idxs=384
[75] prune_out_channels on _Reshape_716() => prune_out_channels on _ElementWiseOp_715(PermuteBackward0), #idxs=384
[76] prune_out_channels on _ElementWiseOp_715(PermuteBackward0) => prune_out_channels on _ElementWiseOp_714(CloneBackward0), #idxs=384
[77] prune_out_channels on _ElementWiseOp_714(CloneBackward0) => prune_out_channels on _Reshape_713(), #idxs=384
[78] prune_out_channels on _Reshape_713() => prune_out_channels on _Reshape_712(), #idxs=384
[79] prune_out_channels on _Reshape_712() => prune_out_channels on _Reshape_710(), #idxs=384
[80] prune_out_channels on _Reshape_710() => prune_out_channels on _ElementWiseOp_709(AddmmBackward0), #idxs=384
[81] prune_out_channels on _ElementWiseOp_709(AddmmBackward0) => prune_out_channels on _ElementWiseOp_711(TBackward0), #idxs=384
[82] prune_out_channels on _ElementWiseOp_709(AddmmBackward0) => prune_out_channels on _Reshape_708(), #idxs=384
[83] prune_out_channels on _Reshape_708() => prune_out_channels on _Reshape_707(), #idxs=384
[84] prune_out_channels on _Reshape_707() => prune_out_channels on _ElementWiseOp_706(PermuteBackward0), #idxs=384
[85] prune_out_channels on _ElementWiseOp_706(PermuteBackward0) => prune_out_channels on _ElementWiseOp_705(SelectBackward0), #idxs=384
[86] prune_out_channels on _ElementWiseOp_706(PermuteBackward0) => prune_out_channels on _ElementWiseOp_734(SelectBackward0), #idxs=384
[87] prune_out_channels on _ElementWiseOp_706(PermuteBackward0) => prune_out_channels on _ElementWiseOp_738(SelectBackward0), #idxs=384
[88] prune_out_channels on _ElementWiseOp_738(SelectBackward0) => prune_out_channels on _ElementWiseOp_737(MulBackward0), #idxs=384
[89] prune_out_channels on _ElementWiseOp_737(MulBackward0) => prune_out_channels on _ElementWiseOp_736(ExpandBackward0), #idxs=384
[90] prune_out_channels on _ElementWiseOp_736(ExpandBackward0) => prune_out_channels on _ElementWiseOp_735(CloneBackward0), #idxs=384
[91] prune_out_channels on _ElementWiseOp_735(CloneBackward0) => prune_out_channels on _Reshape_729(), #idxs=384
[92] prune_out_channels on _Reshape_729() => prune_out_channels on _ElementWiseOp_728(BmmBackward0), #idxs=384
[93] prune_out_channels on _ElementWiseOp_728(BmmBackward0) => prune_out_channels on _Reshape_730(), #idxs=384
[94] prune_out_channels on _ElementWiseOp_728(BmmBackward0) => prune_out_channels on _Reshape_722(), #idxs=384
[95] prune_out_channels on _Reshape_722() => prune_out_channels on _ElementWiseOp_721(AddBackward0), #idxs=384
[96] prune_out_channels on _ElementWiseOp_721(AddBackward0) => prune_out_channels on _ElementWiseOp_723(UnsqueezeBackward0), #idxs=384
[97] prune_out_channels on _ElementWiseOp_721(AddBackward0) => prune_out_channels on _ElementWiseOp_720(SoftmaxBackward0), #idxs=384
[98] prune_out_channels on _ElementWiseOp_720(SoftmaxBackward0) => prune_out_channels on _ElementWiseOp_719(ExpandBackward0), #idxs=384
[99] prune_out_channels on _ElementWiseOp_719(ExpandBackward0) => prune_out_channels on _Reshape_701(), #idxs=384
[100] prune_out_channels on _Reshape_701() => prune_out_channels on _ElementWiseOp_700(BmmBackward0), #idxs=384
[101] prune_out_channels on _ElementWiseOp_700(BmmBackward0) => prune_out_channels on _Reshape_702(), #idxs=384
[102] prune_out_channels on _ElementWiseOp_700(BmmBackward0) => prune_out_channels on _Reshape_699(), #idxs=384
[103] prune_out_channels on _Reshape_699() => prune_out_channels on _ElementWiseOp_698(TransposeBackward0), #idxs=384
[104] prune_out_channels on _ElementWiseOp_698(TransposeBackward0) => prune_out_channels on _ElementWiseOp_697(CloneBackward0), #idxs=384
[105] prune_out_channels on _ElementWiseOp_697(CloneBackward0) => prune_in_channels on stages.3.blocks.0.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=768, out_features=2304, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=768, out_features=768, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)), #idxs=384
[106] prune_out_channels on _Reshape_702() => prune_out_channels on _ElementWiseOp_703(CloneBackward0), #idxs=384
[107] prune_out_channels on _ElementWiseOp_703(CloneBackward0) => prune_out_channels on _ElementWiseOp_704(ExpandBackward0), #idxs=384
[108] prune_out_channels on _ElementWiseOp_723(UnsqueezeBackward0) => prune_out_channels on _ElementWiseOp_724(CloneBackward0), #idxs=384
[109] prune_out_channels on _ElementWiseOp_724(CloneBackward0) => prune_out_channels on _ElementWiseOp_725(PermuteBackward0), #idxs=384
[110] prune_out_channels on _ElementWiseOp_725(PermuteBackward0) => prune_out_channels on _Reshape_726(), #idxs=384
[111] prune_out_channels on _Reshape_726() => prune_out_channels on _ElementWiseOp_727(IndexBackward0), #idxs=384
[112] prune_out_channels on _Reshape_730() => prune_out_channels on _ElementWiseOp_731(CloneBackward0), #idxs=384
[113] prune_out_channels on _ElementWiseOp_731(CloneBackward0) => prune_out_channels on _ElementWiseOp_732(ExpandBackward0), #idxs=384
[114] prune_out_channels on _ElementWiseOp_732(ExpandBackward0) => prune_out_channels on _ElementWiseOp_733(TransposeBackward0), #idxs=384
[115] prune_out_channels on _Reshape_680() => prune_out_channels on _ElementWiseOp_689(SliceBackward0), #idxs=384
[116] prune_out_channels on _ElementWiseOp_689(SliceBackward0) => prune_out_channels on _ElementWiseOp_690(SliceBackward0), #idxs=384
[117] prune_out_channels on _ElementWiseOp_690(SliceBackward0) => prune_out_channels on _ElementWiseOp_691(SliceBackward0), #idxs=384
[118] prune_out_channels on _ElementWiseOp_691(SliceBackward0) => prune_out_channels on _Reshape_692(), #idxs=384
[119] prune_out_channels on _Reshape_692() => prune_out_channels on _ElementWiseOp_693(CloneBackward0), #idxs=384
[120] prune_out_channels on _ElementWiseOp_693(CloneBackward0) => prune_out_channels on _ElementWiseOp_694(PermuteBackward0), #idxs=384
[121] prune_out_channels on _ElementWiseOp_694(PermuteBackward0) => prune_out_channels on _Reshape_695(), #idxs=384
[122] prune_out_channels on _Reshape_695() => prune_out_channels on _Reshape_696(), #idxs=384
[123] prune_out_channels on _Reshape_696() => prune_out_channels on stages.3.blocks.0.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=768, out_features=2304, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=768, out_features=768, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)), #idxs=384
[124] prune_out_channels on _Reshape_670() => prune_out_channels on _ElementWiseOp_739(SliceBackward0), #idxs=384
[125] prune_out_channels on _ElementWiseOp_739(SliceBackward0) => prune_out_channels on _ElementWiseOp_740(SliceBackward0), #idxs=384
[126] prune_out_channels on _ElementWiseOp_740(SliceBackward0) => prune_out_channels on _ElementWiseOp_741(SliceBackward0), #idxs=384
[127] prune_out_channels on _ElementWiseOp_741(SliceBackward0) => prune_out_channels on _ElementWiseOp_742(RollBackward0), #idxs=384
[128] prune_out_channels on _ElementWiseOp_742(RollBackward0) => prune_out_channels on _Reshape_743(), #idxs=384
[129] prune_out_channels on _Reshape_743() => prune_out_channels on _ElementWiseOp_744(CloneBackward0), #idxs=384
[130] prune_out_channels on _ElementWiseOp_744(CloneBackward0) => prune_out_channels on _ElementWiseOp_745(PermuteBackward0), #idxs=384
[131] prune_out_channels on _ElementWiseOp_745(PermuteBackward0) => prune_out_channels on _Reshape_746(), #idxs=384
[132] prune_out_channels on _Reshape_746() => prune_out_channels on _Reshape_747(), #idxs=384
[133] prune_out_channels on _Reshape_747() => prune_out_channels on stages.3.blocks.1.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=768, out_features=2304, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=768, out_features=768, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)), #idxs=384
--------------------------------

WindowMSAPruner prune_in_channels() /  384
PatchMergingPruner () prune_out_channels/  768 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
WindowMSAPruner prune_in_channels() /  384
WindowMSAPruner prune_out_channels() /  384
WindowMSAPruner prune_out_channels/ dim =  768
	len indx =  384
WindowMSAPruner - idxs =  [0, 1, 7, 9, 11, 14, 15, 16, 18, 19, 22, 25, 32, 39, 40, 43, 51, 52, 53, 54, 58, 61, 62, 64, 69, 72, 74, 75, 78, 81, 82, 84, 89, 91, 93, 94, 98, 100, 101, 103, 104, 105, 106, 108, 111, 112, 115, 117, 120, 121, 123, 124, 125, 127, 132, 133, 134, 136, 137, 142, 144, 145, 146, 147, 149, 150, 151, 152, 153, 154, 155, 157, 158, 161, 163, 164, 166, 167, 168, 169, 172, 173, 177, 178, 181, 187, 189, 190, 198, 199, 200, 202, 205, 206, 207, 209, 210, 212, 213, 214, 217, 218, 220, 221, 225, 226, 227, 228, 229, 230, 231, 233, 234, 236, 237, 238, 239, 240, 246, 247, 248, 251, 254, 255, 257, 259, 261, 263, 264, 266, 267, 270, 271, 273, 274, 275, 277, 279, 280, 281, 282, 286, 287, 288, 291, 294, 296, 298, 299, 302, 304, 306, 308, 310, 311, 313, 314, 317, 319, 320, 321, 322, 323, 324, 325, 326, 328, 333, 335, 338, 343, 346, 347, 348, 349, 350, 351, 352, 353, 354, 358, 360, 361, 362, 363, 365, 368, 371, 372, 373, 374, 378, 380, 385, 386, 388, 391, 392, 395, 397, 400, 403, 405, 406, 407, 408, 410, 411, 412, 415, 419, 420, 421, 424, 426, 427, 428, 430, 433, 435, 436, 440, 441, 442, 443, 444, 447, 452, 456, 458, 459, 464, 466, 468, 469, 470, 472, 477, 479, 483, 485, 486, 488, 489, 491, 492, 494, 495, 501, 503, 505, 507, 508, 510, 511, 514, 518, 519, 520, 521, 522, 524, 525, 526, 530, 532, 534, 536, 537, 539, 540, 541, 545, 547, 548, 549, 551, 554, 557, 558, 559, 560, 561, 562, 565, 567, 570, 575, 576, 577, 578, 583, 587, 588, 591, 593, 595, 596, 601, 603, 604, 608, 611, 612, 615, 618, 619, 622, 623, 625, 626, 629, 630, 632, 634, 636, 641, 643, 644, 645, 648, 649, 650, 652, 654, 655, 658, 659, 661, 667, 668, 670, 672, 675, 676, 677, 678, 680, 682, 683, 684, 685, 686, 690, 692, 693, 696, 697, 698, 699, 700, 703, 706, 707, 710, 712, 719, 722, 723, 724, 726, 727, 728, 729, 730, 733, 734, 739, 740, 741, 743, 744, 745, 749, 750, 751, 752, 756, 757, 759, 762, 763, 765, 767]
	idxs_repeated =  1536
WindowMSAPruner - idxs_repeated =  [0, 1, 7, 9, 11, 14, 15, 16, 18, 19, 22, 25, 32, 39, 40, 43, 51, 52, 53, 54, 58, 61, 62, 64, 69, 72, 74, 75, 78, 81, 82, 84, 89, 91, 93, 94, 98, 100, 101, 103, 104, 105, 106, 108, 111, 112, 115, 117, 120, 121, 123, 124, 125, 127, 132, 133, 134, 136, 137, 142, 144, 145, 146, 147, 149, 150, 151, 152, 153, 154, 155, 157, 158, 161, 163, 164, 166, 167, 168, 169, 172, 173, 177, 178, 181, 187, 189, 190, 198, 199, 200, 202, 205, 206, 207, 209, 210, 212, 213, 214, 217, 218, 220, 221, 225, 226, 227, 228, 229, 230, 231, 233, 234, 236, 237, 238, 239, 240, 246, 247, 248, 251, 254, 255, 257, 259, 261, 263, 264, 266, 267, 270, 271, 273, 274, 275, 277, 279, 280, 281, 282, 286, 287, 288, 291, 294, 296, 298, 299, 302, 304, 306, 308, 310, 311, 313, 314, 317, 319, 320, 321, 322, 323, 324, 325, 326, 328, 333, 335, 338, 343, 346, 347, 348, 349, 350, 351, 352, 353, 354, 358, 360, 361, 362, 363, 365, 368, 371, 372, 373, 374, 378, 380, 385, 386, 388, 391, 392, 395, 397, 400, 403, 405, 406, 407, 408, 410, 411, 412, 415, 419, 420, 421, 424, 426, 427, 428, 430, 433, 435, 436, 440, 441, 442, 443, 444, 447, 452, 456, 458, 459, 464, 466, 468, 469, 470, 472, 477, 479, 483, 485, 486, 488, 489, 491, 492, 494, 495, 501, 503, 505, 507, 508, 510, 511, 514, 518, 519, 520, 521, 522, 524, 525, 526, 530, 532, 534, 536, 537, 539, 540, 541, 545, 547, 548, 549, 551, 554, 557, 558, 559, 560, 561, 562, 565, 567, 570, 575, 576, 577, 578, 583, 587, 588, 591, 593, 595, 596, 601, 603, 604, 608, 611, 612, 615, 618, 619, 622, 623, 625, 626, 629, 630, 632, 634, 636, 641, 643, 644, 645, 648, 649, 650, 652, 654, 655, 658, 659, 661, 667, 668, 670, 672, 675, 676, 677, 678, 680, 682, 683, 684, 685, 686, 690, 692, 693, 696, 697, 698, 699, 700, 703, 706, 707, 710, 712, 719, 722, 723, 724, 726, 727, 728, 729, 730, 733, 734, 739, 740, 741, 743, 744, 745, 749, 750, 751, 752, 756, 757, 759, 762, 763, 765, 767, 768, 769, 775, 777, 779, 782, 783, 784, 786, 787, 790, 793, 800, 807, 808, 811, 819, 820, 821, 822, 826, 829, 830, 832, 837, 840, 842, 843, 846, 849, 850, 852, 857, 859, 861, 862, 866, 868, 869, 871, 872, 873, 874, 876, 879, 880, 883, 885, 888, 889, 891, 892, 893, 895, 900, 901, 902, 904, 905, 910, 912, 913, 914, 915, 917, 918, 919, 920, 921, 922, 923, 925, 926, 929, 931, 932, 934, 935, 936, 937, 940, 941, 945, 946, 949, 955, 957, 958, 966, 967, 968, 970, 973, 974, 975, 977, 978, 980, 981, 982, 985, 986, 988, 989, 993, 994, 995, 996, 997, 998, 999, 1001, 1002, 1004, 1005, 1006, 1007, 1008, 1014, 1015, 1016, 1019, 1022, 1023, 1025, 1027, 1029, 1031, 1032, 1034, 1035, 1038, 1039, 1041, 1042, 1043, 1045, 1047, 1048, 1049, 1050, 1054, 1055, 1056, 1059, 1062, 1064, 1066, 1067, 1070, 1072, 1074, 1076, 1078, 1079, 1081, 1082, 1085, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1096, 1101, 1103, 1106, 1111, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1126, 1128, 1129, 1130, 1131, 1133, 1136, 1139, 1140, 1141, 1142, 1146, 1148, 1153, 1154, 1156, 1159, 1160, 1163, 1165, 1168, 1171, 1173, 1174, 1175, 1176, 1178, 1179, 1180, 1183, 1187, 1188, 1189, 1192, 1194, 1195, 1196, 1198, 1201, 1203, 1204, 1208, 1209, 1210, 1211, 1212, 1215, 1220, 1224, 1226, 1227, 1232, 1234, 1236, 1237, 1238, 1240, 1245, 1247, 1251, 1253, 1254, 1256, 1257, 1259, 1260, 1262, 1263, 1269, 1271, 1273, 1275, 1276, 1278, 1279, 1282, 1286, 1287, 1288, 1289, 1290, 1292, 1293, 1294, 1298, 1300, 1302, 1304, 1305, 1307, 1308, 1309, 1313, 1315, 1316, 1317, 1319, 1322, 1325, 1326, 1327, 1328, 1329, 1330, 1333, 1335, 1338, 1343, 1344, 1345, 1346, 1351, 1355, 1356, 1359, 1361, 1363, 1364, 1369, 1371, 1372, 1376, 1379, 1380, 1383, 1386, 1387, 1390, 1391, 1393, 1394, 1397, 1398, 1400, 1402, 1404, 1409, 1411, 1412, 1413, 1416, 1417, 1418, 1420, 1422, 1423, 1426, 1427, 1429, 1435, 1436, 1438, 1440, 1443, 1444, 1445, 1446, 1448, 1450, 1451, 1452, 1453, 1454, 1458, 1460, 1461, 1464, 1465, 1466, 1467, 1468, 1471, 1474, 1475, 1478, 1480, 1487, 1490, 1491, 1492, 1494, 1495, 1496, 1497, 1498, 1501, 1502, 1507, 1508, 1509, 1511, 1512, 1513, 1517, 1518, 1519, 1520, 1524, 1525, 1527, 1530, 1531, 1533, 1535, 1536, 1537, 1543, 1545, 1547, 1550, 1551, 1552, 1554, 1555, 1558, 1561, 1568, 1575, 1576, 1579, 1587, 1588, 1589, 1590, 1594, 1597, 1598, 1600, 1605, 1608, 1610, 1611, 1614, 1617, 1618, 1620, 1625, 1627, 1629, 1630, 1634, 1636, 1637, 1639, 1640, 1641, 1642, 1644, 1647, 1648, 1651, 1653, 1656, 1657, 1659, 1660, 1661, 1663, 1668, 1669, 1670, 1672, 1673, 1678, 1680, 1681, 1682, 1683, 1685, 1686, 1687, 1688, 1689, 1690, 1691, 1693, 1694, 1697, 1699, 1700, 1702, 1703, 1704, 1705, 1708, 1709, 1713, 1714, 1717, 1723, 1725, 1726, 1734, 1735, 1736, 1738, 1741, 1742, 1743, 1745, 1746, 1748, 1749, 1750, 1753, 1754, 1756, 1757, 1761, 1762, 1763, 1764, 1765, 1766, 1767, 1769, 1770, 1772, 1773, 1774, 1775, 1776, 1782, 1783, 1784, 1787, 1790, 1791, 1793, 1795, 1797, 1799, 1800, 1802, 1803, 1806, 1807, 1809, 1810, 1811, 1813, 1815, 1816, 1817, 1818, 1822, 1823, 1824, 1827, 1830, 1832, 1834, 1835, 1838, 1840, 1842, 1844, 1846, 1847, 1849, 1850, 1853, 1855, 1856, 1857, 1858, 1859, 1860, 1861, 1862, 1864, 1869, 1871, 1874, 1879, 1882, 1883, 1884, 1885, 1886, 1887, 1888, 1889, 1890, 1894, 1896, 1897, 1898, 1899, 1901, 1904, 1907, 1908, 1909, 1910, 1914, 1916, 1921, 1922, 1924, 1927, 1928, 1931, 1933, 1936, 1939, 1941, 1942, 1943, 1944, 1946, 1947, 1948, 1951, 1955, 1956, 1957, 1960, 1962, 1963, 1964, 1966, 1969, 1971, 1972, 1976, 1977, 1978, 1979, 1980, 1983, 1988, 1992, 1994, 1995, 2000, 2002, 2004, 2005, 2006, 2008, 2013, 2015, 2019, 2021, 2022, 2024, 2025, 2027, 2028, 2030, 2031, 2037, 2039, 2041, 2043, 2044, 2046, 2047, 2050, 2054, 2055, 2056, 2057, 2058, 2060, 2061, 2062, 2066, 2068, 2070, 2072, 2073, 2075, 2076, 2077, 2081, 2083, 2084, 2085, 2087, 2090, 2093, 2094, 2095, 2096, 2097, 2098, 2101, 2103, 2106, 2111, 2112, 2113, 2114, 2119, 2123, 2124, 2127, 2129, 2131, 2132, 2137, 2139, 2140, 2144, 2147, 2148, 2151, 2154, 2155, 2158, 2159, 2161, 2162, 2165, 2166, 2168, 2170, 2172, 2177, 2179, 2180, 2181, 2184, 2185, 2186, 2188, 2190, 2191, 2194, 2195, 2197, 2203, 2204, 2206, 2208, 2211, 2212, 2213, 2214, 2216, 2218, 2219, 2220, 2221, 2222, 2226, 2228, 2229, 2232, 2233, 2234, 2235, 2236, 2239, 2242, 2243, 2246, 2248, 2255, 2258, 2259, 2260, 2262, 2263, 2264, 2265, 2266, 2269, 2270, 2275, 2276, 2277, 2279, 2280, 2281, 2285, 2286, 2287, 2288, 2292, 2293, 2295, 2298, 2299, 2301, 2303, 2304, 2305, 2311, 2313, 2315, 2318, 2319, 2320, 2322, 2323, 2326, 2329, 2336, 2343, 2344, 2347, 2355, 2356, 2357, 2358, 2362, 2365, 2366, 2368, 2373, 2376, 2378, 2379, 2382, 2385, 2386, 2388, 2393, 2395, 2397, 2398, 2402, 2404, 2405, 2407, 2408, 2409, 2410, 2412, 2415, 2416, 2419, 2421, 2424, 2425, 2427, 2428, 2429, 2431, 2436, 2437, 2438, 2440, 2441, 2446, 2448, 2449, 2450, 2451, 2453, 2454, 2455, 2456, 2457, 2458, 2459, 2461, 2462, 2465, 2467, 2468, 2470, 2471, 2472, 2473, 2476, 2477, 2481, 2482, 2485, 2491, 2493, 2494, 2502, 2503, 2504, 2506, 2509, 2510, 2511, 2513, 2514, 2516, 2517, 2518, 2521, 2522, 2524, 2525, 2529, 2530, 2531, 2532, 2533, 2534, 2535, 2537, 2538, 2540, 2541, 2542, 2543, 2544, 2550, 2551, 2552, 2555, 2558, 2559, 2561, 2563, 2565, 2567, 2568, 2570, 2571, 2574, 2575, 2577, 2578, 2579, 2581, 2583, 2584, 2585, 2586, 2590, 2591, 2592, 2595, 2598, 2600, 2602, 2603, 2606, 2608, 2610, 2612, 2614, 2615, 2617, 2618, 2621, 2623, 2624, 2625, 2626, 2627, 2628, 2629, 2630, 2632, 2637, 2639, 2642, 2647, 2650, 2651, 2652, 2653, 2654, 2655, 2656, 2657, 2658, 2662, 2664, 2665, 2666, 2667, 2669, 2672, 2675, 2676, 2677, 2678, 2682, 2684, 2689, 2690, 2692, 2695, 2696, 2699, 2701, 2704, 2707, 2709, 2710, 2711, 2712, 2714, 2715, 2716, 2719, 2723, 2724, 2725, 2728, 2730, 2731, 2732, 2734, 2737, 2739, 2740, 2744, 2745, 2746, 2747, 2748, 2751, 2756, 2760, 2762, 2763, 2768, 2770, 2772, 2773, 2774, 2776, 2781, 2783, 2787, 2789, 2790, 2792, 2793, 2795, 2796, 2798, 2799, 2805, 2807, 2809, 2811, 2812, 2814, 2815, 2818, 2822, 2823, 2824, 2825, 2826, 2828, 2829, 2830, 2834, 2836, 2838, 2840, 2841, 2843, 2844, 2845, 2849, 2851, 2852, 2853, 2855, 2858, 2861, 2862, 2863, 2864, 2865, 2866, 2869, 2871, 2874, 2879, 2880, 2881, 2882, 2887, 2891, 2892, 2895, 2897, 2899, 2900, 2905, 2907, 2908, 2912, 2915, 2916, 2919, 2922, 2923, 2926, 2927, 2929, 2930, 2933, 2934, 2936, 2938, 2940, 2945, 2947, 2948, 2949, 2952, 2953, 2954, 2956, 2958, 2959, 2962, 2963, 2965, 2971, 2972, 2974, 2976, 2979, 2980, 2981, 2982, 2984, 2986, 2987, 2988, 2989, 2990, 2994, 2996, 2997, 3000, 3001, 3002, 3003, 3004, 3007, 3010, 3011, 3014, 3016, 3023, 3026, 3027, 3028, 3030, 3031, 3032, 3033, 3034, 3037, 3038, 3043, 3044, 3045, 3047, 3048, 3049, 3053, 3054, 3055, 3056, 3060, 3061, 3063, 3066, 3067, 3069, 3071]
WindowMSAPruner prune_out_channels idxs_repeated =  1536
WindowMSAPruner prune_out_channels() /  384
WindowMSAPruner prune_out_channels/ dim =  768
	len indx =  384
WindowMSAPruner - idxs =  [0, 1, 7, 9, 11, 14, 15, 16, 18, 19, 22, 25, 32, 39, 40, 43, 51, 52, 53, 54, 58, 61, 62, 64, 69, 72, 74, 75, 78, 81, 82, 84, 89, 91, 93, 94, 98, 100, 101, 103, 104, 105, 106, 108, 111, 112, 115, 117, 120, 121, 123, 124, 125, 127, 132, 133, 134, 136, 137, 142, 144, 145, 146, 147, 149, 150, 151, 152, 153, 154, 155, 157, 158, 161, 163, 164, 166, 167, 168, 169, 172, 173, 177, 178, 181, 187, 189, 190, 198, 199, 200, 202, 205, 206, 207, 209, 210, 212, 213, 214, 217, 218, 220, 221, 225, 226, 227, 228, 229, 230, 231, 233, 234, 236, 237, 238, 239, 240, 246, 247, 248, 251, 254, 255, 257, 259, 261, 263, 264, 266, 267, 270, 271, 273, 274, 275, 277, 279, 280, 281, 282, 286, 287, 288, 291, 294, 296, 298, 299, 302, 304, 306, 308, 310, 311, 313, 314, 317, 319, 320, 321, 322, 323, 324, 325, 326, 328, 333, 335, 338, 343, 346, 347, 348, 349, 350, 351, 352, 353, 354, 358, 360, 361, 362, 363, 365, 368, 371, 372, 373, 374, 378, 380, 385, 386, 388, 391, 392, 395, 397, 400, 403, 405, 406, 407, 408, 410, 411, 412, 415, 419, 420, 421, 424, 426, 427, 428, 430, 433, 435, 436, 440, 441, 442, 443, 444, 447, 452, 456, 458, 459, 464, 466, 468, 469, 470, 472, 477, 479, 483, 485, 486, 488, 489, 491, 492, 494, 495, 501, 503, 505, 507, 508, 510, 511, 514, 518, 519, 520, 521, 522, 524, 525, 526, 530, 532, 534, 536, 537, 539, 540, 541, 545, 547, 548, 549, 551, 554, 557, 558, 559, 560, 561, 562, 565, 567, 570, 575, 576, 577, 578, 583, 587, 588, 591, 593, 595, 596, 601, 603, 604, 608, 611, 612, 615, 618, 619, 622, 623, 625, 626, 629, 630, 632, 634, 636, 641, 643, 644, 645, 648, 649, 650, 652, 654, 655, 658, 659, 661, 667, 668, 670, 672, 675, 676, 677, 678, 680, 682, 683, 684, 685, 686, 690, 692, 693, 696, 697, 698, 699, 700, 703, 706, 707, 710, 712, 719, 722, 723, 724, 726, 727, 728, 729, 730, 733, 734, 739, 740, 741, 743, 744, 745, 749, 750, 751, 752, 756, 757, 759, 762, 763, 765, 767]
	idxs_repeated =  1536
WindowMSAPruner - idxs_repeated =  [0, 1, 7, 9, 11, 14, 15, 16, 18, 19, 22, 25, 32, 39, 40, 43, 51, 52, 53, 54, 58, 61, 62, 64, 69, 72, 74, 75, 78, 81, 82, 84, 89, 91, 93, 94, 98, 100, 101, 103, 104, 105, 106, 108, 111, 112, 115, 117, 120, 121, 123, 124, 125, 127, 132, 133, 134, 136, 137, 142, 144, 145, 146, 147, 149, 150, 151, 152, 153, 154, 155, 157, 158, 161, 163, 164, 166, 167, 168, 169, 172, 173, 177, 178, 181, 187, 189, 190, 198, 199, 200, 202, 205, 206, 207, 209, 210, 212, 213, 214, 217, 218, 220, 221, 225, 226, 227, 228, 229, 230, 231, 233, 234, 236, 237, 238, 239, 240, 246, 247, 248, 251, 254, 255, 257, 259, 261, 263, 264, 266, 267, 270, 271, 273, 274, 275, 277, 279, 280, 281, 282, 286, 287, 288, 291, 294, 296, 298, 299, 302, 304, 306, 308, 310, 311, 313, 314, 317, 319, 320, 321, 322, 323, 324, 325, 326, 328, 333, 335, 338, 343, 346, 347, 348, 349, 350, 351, 352, 353, 354, 358, 360, 361, 362, 363, 365, 368, 371, 372, 373, 374, 378, 380, 385, 386, 388, 391, 392, 395, 397, 400, 403, 405, 406, 407, 408, 410, 411, 412, 415, 419, 420, 421, 424, 426, 427, 428, 430, 433, 435, 436, 440, 441, 442, 443, 444, 447, 452, 456, 458, 459, 464, 466, 468, 469, 470, 472, 477, 479, 483, 485, 486, 488, 489, 491, 492, 494, 495, 501, 503, 505, 507, 508, 510, 511, 514, 518, 519, 520, 521, 522, 524, 525, 526, 530, 532, 534, 536, 537, 539, 540, 541, 545, 547, 548, 549, 551, 554, 557, 558, 559, 560, 561, 562, 565, 567, 570, 575, 576, 577, 578, 583, 587, 588, 591, 593, 595, 596, 601, 603, 604, 608, 611, 612, 615, 618, 619, 622, 623, 625, 626, 629, 630, 632, 634, 636, 641, 643, 644, 645, 648, 649, 650, 652, 654, 655, 658, 659, 661, 667, 668, 670, 672, 675, 676, 677, 678, 680, 682, 683, 684, 685, 686, 690, 692, 693, 696, 697, 698, 699, 700, 703, 706, 707, 710, 712, 719, 722, 723, 724, 726, 727, 728, 729, 730, 733, 734, 739, 740, 741, 743, 744, 745, 749, 750, 751, 752, 756, 757, 759, 762, 763, 765, 767, 768, 769, 775, 777, 779, 782, 783, 784, 786, 787, 790, 793, 800, 807, 808, 811, 819, 820, 821, 822, 826, 829, 830, 832, 837, 840, 842, 843, 846, 849, 850, 852, 857, 859, 861, 862, 866, 868, 869, 871, 872, 873, 874, 876, 879, 880, 883, 885, 888, 889, 891, 892, 893, 895, 900, 901, 902, 904, 905, 910, 912, 913, 914, 915, 917, 918, 919, 920, 921, 922, 923, 925, 926, 929, 931, 932, 934, 935, 936, 937, 940, 941, 945, 946, 949, 955, 957, 958, 966, 967, 968, 970, 973, 974, 975, 977, 978, 980, 981, 982, 985, 986, 988, 989, 993, 994, 995, 996, 997, 998, 999, 1001, 1002, 1004, 1005, 1006, 1007, 1008, 1014, 1015, 1016, 1019, 1022, 1023, 1025, 1027, 1029, 1031, 1032, 1034, 1035, 1038, 1039, 1041, 1042, 1043, 1045, 1047, 1048, 1049, 1050, 1054, 1055, 1056, 1059, 1062, 1064, 1066, 1067, 1070, 1072, 1074, 1076, 1078, 1079, 1081, 1082, 1085, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1096, 1101, 1103, 1106, 1111, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1126, 1128, 1129, 1130, 1131, 1133, 1136, 1139, 1140, 1141, 1142, 1146, 1148, 1153, 1154, 1156, 1159, 1160, 1163, 1165, 1168, 1171, 1173, 1174, 1175, 1176, 1178, 1179, 1180, 1183, 1187, 1188, 1189, 1192, 1194, 1195, 1196, 1198, 1201, 1203, 1204, 1208, 1209, 1210, 1211, 1212, 1215, 1220, 1224, 1226, 1227, 1232, 1234, 1236, 1237, 1238, 1240, 1245, 1247, 1251, 1253, 1254, 1256, 1257, 1259, 1260, 1262, 1263, 1269, 1271, 1273, 1275, 1276, 1278, 1279, 1282, 1286, 1287, 1288, 1289, 1290, 1292, 1293, 1294, 1298, 1300, 1302, 1304, 1305, 1307, 1308, 1309, 1313, 1315, 1316, 1317, 1319, 1322, 1325, 1326, 1327, 1328, 1329, 1330, 1333, 1335, 1338, 1343, 1344, 1345, 1346, 1351, 1355, 1356, 1359, 1361, 1363, 1364, 1369, 1371, 1372, 1376, 1379, 1380, 1383, 1386, 1387, 1390, 1391, 1393, 1394, 1397, 1398, 1400, 1402, 1404, 1409, 1411, 1412, 1413, 1416, 1417, 1418, 1420, 1422, 1423, 1426, 1427, 1429, 1435, 1436, 1438, 1440, 1443, 1444, 1445, 1446, 1448, 1450, 1451, 1452, 1453, 1454, 1458, 1460, 1461, 1464, 1465, 1466, 1467, 1468, 1471, 1474, 1475, 1478, 1480, 1487, 1490, 1491, 1492, 1494, 1495, 1496, 1497, 1498, 1501, 1502, 1507, 1508, 1509, 1511, 1512, 1513, 1517, 1518, 1519, 1520, 1524, 1525, 1527, 1530, 1531, 1533, 1535, 1536, 1537, 1543, 1545, 1547, 1550, 1551, 1552, 1554, 1555, 1558, 1561, 1568, 1575, 1576, 1579, 1587, 1588, 1589, 1590, 1594, 1597, 1598, 1600, 1605, 1608, 1610, 1611, 1614, 1617, 1618, 1620, 1625, 1627, 1629, 1630, 1634, 1636, 1637, 1639, 1640, 1641, 1642, 1644, 1647, 1648, 1651, 1653, 1656, 1657, 1659, 1660, 1661, 1663, 1668, 1669, 1670, 1672, 1673, 1678, 1680, 1681, 1682, 1683, 1685, 1686, 1687, 1688, 1689, 1690, 1691, 1693, 1694, 1697, 1699, 1700, 1702, 1703, 1704, 1705, 1708, 1709, 1713, 1714, 1717, 1723, 1725, 1726, 1734, 1735, 1736, 1738, 1741, 1742, 1743, 1745, 1746, 1748, 1749, 1750, 1753, 1754, 1756, 1757, 1761, 1762, 1763, 1764, 1765, 1766, 1767, 1769, 1770, 1772, 1773, 1774, 1775, 1776, 1782, 1783, 1784, 1787, 1790, 1791, 1793, 1795, 1797, 1799, 1800, 1802, 1803, 1806, 1807, 1809, 1810, 1811, 1813, 1815, 1816, 1817, 1818, 1822, 1823, 1824, 1827, 1830, 1832, 1834, 1835, 1838, 1840, 1842, 1844, 1846, 1847, 1849, 1850, 1853, 1855, 1856, 1857, 1858, 1859, 1860, 1861, 1862, 1864, 1869, 1871, 1874, 1879, 1882, 1883, 1884, 1885, 1886, 1887, 1888, 1889, 1890, 1894, 1896, 1897, 1898, 1899, 1901, 1904, 1907, 1908, 1909, 1910, 1914, 1916, 1921, 1922, 1924, 1927, 1928, 1931, 1933, 1936, 1939, 1941, 1942, 1943, 1944, 1946, 1947, 1948, 1951, 1955, 1956, 1957, 1960, 1962, 1963, 1964, 1966, 1969, 1971, 1972, 1976, 1977, 1978, 1979, 1980, 1983, 1988, 1992, 1994, 1995, 2000, 2002, 2004, 2005, 2006, 2008, 2013, 2015, 2019, 2021, 2022, 2024, 2025, 2027, 2028, 2030, 2031, 2037, 2039, 2041, 2043, 2044, 2046, 2047, 2050, 2054, 2055, 2056, 2057, 2058, 2060, 2061, 2062, 2066, 2068, 2070, 2072, 2073, 2075, 2076, 2077, 2081, 2083, 2084, 2085, 2087, 2090, 2093, 2094, 2095, 2096, 2097, 2098, 2101, 2103, 2106, 2111, 2112, 2113, 2114, 2119, 2123, 2124, 2127, 2129, 2131, 2132, 2137, 2139, 2140, 2144, 2147, 2148, 2151, 2154, 2155, 2158, 2159, 2161, 2162, 2165, 2166, 2168, 2170, 2172, 2177, 2179, 2180, 2181, 2184, 2185, 2186, 2188, 2190, 2191, 2194, 2195, 2197, 2203, 2204, 2206, 2208, 2211, 2212, 2213, 2214, 2216, 2218, 2219, 2220, 2221, 2222, 2226, 2228, 2229, 2232, 2233, 2234, 2235, 2236, 2239, 2242, 2243, 2246, 2248, 2255, 2258, 2259, 2260, 2262, 2263, 2264, 2265, 2266, 2269, 2270, 2275, 2276, 2277, 2279, 2280, 2281, 2285, 2286, 2287, 2288, 2292, 2293, 2295, 2298, 2299, 2301, 2303, 2304, 2305, 2311, 2313, 2315, 2318, 2319, 2320, 2322, 2323, 2326, 2329, 2336, 2343, 2344, 2347, 2355, 2356, 2357, 2358, 2362, 2365, 2366, 2368, 2373, 2376, 2378, 2379, 2382, 2385, 2386, 2388, 2393, 2395, 2397, 2398, 2402, 2404, 2405, 2407, 2408, 2409, 2410, 2412, 2415, 2416, 2419, 2421, 2424, 2425, 2427, 2428, 2429, 2431, 2436, 2437, 2438, 2440, 2441, 2446, 2448, 2449, 2450, 2451, 2453, 2454, 2455, 2456, 2457, 2458, 2459, 2461, 2462, 2465, 2467, 2468, 2470, 2471, 2472, 2473, 2476, 2477, 2481, 2482, 2485, 2491, 2493, 2494, 2502, 2503, 2504, 2506, 2509, 2510, 2511, 2513, 2514, 2516, 2517, 2518, 2521, 2522, 2524, 2525, 2529, 2530, 2531, 2532, 2533, 2534, 2535, 2537, 2538, 2540, 2541, 2542, 2543, 2544, 2550, 2551, 2552, 2555, 2558, 2559, 2561, 2563, 2565, 2567, 2568, 2570, 2571, 2574, 2575, 2577, 2578, 2579, 2581, 2583, 2584, 2585, 2586, 2590, 2591, 2592, 2595, 2598, 2600, 2602, 2603, 2606, 2608, 2610, 2612, 2614, 2615, 2617, 2618, 2621, 2623, 2624, 2625, 2626, 2627, 2628, 2629, 2630, 2632, 2637, 2639, 2642, 2647, 2650, 2651, 2652, 2653, 2654, 2655, 2656, 2657, 2658, 2662, 2664, 2665, 2666, 2667, 2669, 2672, 2675, 2676, 2677, 2678, 2682, 2684, 2689, 2690, 2692, 2695, 2696, 2699, 2701, 2704, 2707, 2709, 2710, 2711, 2712, 2714, 2715, 2716, 2719, 2723, 2724, 2725, 2728, 2730, 2731, 2732, 2734, 2737, 2739, 2740, 2744, 2745, 2746, 2747, 2748, 2751, 2756, 2760, 2762, 2763, 2768, 2770, 2772, 2773, 2774, 2776, 2781, 2783, 2787, 2789, 2790, 2792, 2793, 2795, 2796, 2798, 2799, 2805, 2807, 2809, 2811, 2812, 2814, 2815, 2818, 2822, 2823, 2824, 2825, 2826, 2828, 2829, 2830, 2834, 2836, 2838, 2840, 2841, 2843, 2844, 2845, 2849, 2851, 2852, 2853, 2855, 2858, 2861, 2862, 2863, 2864, 2865, 2866, 2869, 2871, 2874, 2879, 2880, 2881, 2882, 2887, 2891, 2892, 2895, 2897, 2899, 2900, 2905, 2907, 2908, 2912, 2915, 2916, 2919, 2922, 2923, 2926, 2927, 2929, 2930, 2933, 2934, 2936, 2938, 2940, 2945, 2947, 2948, 2949, 2952, 2953, 2954, 2956, 2958, 2959, 2962, 2963, 2965, 2971, 2972, 2974, 2976, 2979, 2980, 2981, 2982, 2984, 2986, 2987, 2988, 2989, 2990, 2994, 2996, 2997, 3000, 3001, 3002, 3003, 3004, 3007, 3010, 3011, 3014, 3016, 3023, 3026, 3027, 3028, 3030, 3031, 3032, 3033, 3034, 3037, 3038, 3043, 3044, 3045, 3047, 3048, 3049, 3053, 3054, 3055, 3056, 3060, 3061, 3063, 3066, 3067, 3069, 3071]
WindowMSAPruner prune_out_channels idxs_repeated =  1536
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
	get_all_groups -- Adding layer:  <class 'mmcv.cnn.bricks.wrappers.Linear'>
		prune_local()/  _check_pruning_ratio OK
idxs =  1536
prunable_chs =  3072
pruner.step 
--------------------------------
          Pruning Group
--------------------------------
[0] prune_out_channels on stages.3.blocks.1.ffn.layers.0.0 (Linear(in_features=384, out_features=3072, bias=True)) => prune_out_channels on stages.3.blocks.1.ffn.layers.0.0 (Linear(in_features=384, out_features=3072, bias=True)), #idxs=1536
[1] prune_out_channels on stages.3.blocks.1.ffn.layers.0.0 (Linear(in_features=384, out_features=3072, bias=True)) => prune_out_channels on _ElementWiseOp_666(GeluBackward0), #idxs=1536
[2] prune_out_channels on _ElementWiseOp_666(GeluBackward0) => prune_out_channels on _Reshape_664(), #idxs=1536
[3] prune_out_channels on _Reshape_664() => prune_out_channels on _ElementWiseOp_663(AddmmBackward0), #idxs=1536
[4] prune_out_channels on _ElementWiseOp_663(AddmmBackward0) => prune_out_channels on _ElementWiseOp_665(TBackward0), #idxs=1536
[5] prune_out_channels on _ElementWiseOp_663(AddmmBackward0) => prune_in_channels on stages.3.blocks.1.ffn.layers.1 (Linear(in_features=3072, out_features=384, bias=True)), #idxs=1536
--------------------------------

<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
continue 4
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
	get_all_groups -- Adding layer:  <class 'mmcv.cnn.bricks.wrappers.Linear'>
		prune_local()/  _check_pruning_ratio OK
idxs =  1536
prunable_chs =  3072
pruner.step 
--------------------------------
          Pruning Group
--------------------------------
[0] prune_out_channels on stages.3.blocks.0.ffn.layers.0.0 (Linear(in_features=384, out_features=3072, bias=True)) => prune_out_channels on stages.3.blocks.0.ffn.layers.0.0 (Linear(in_features=384, out_features=3072, bias=True)), #idxs=1536
[1] prune_out_channels on stages.3.blocks.0.ffn.layers.0.0 (Linear(in_features=384, out_features=3072, bias=True)) => prune_out_channels on _ElementWiseOp_676(GeluBackward0), #idxs=1536
[2] prune_out_channels on _ElementWiseOp_676(GeluBackward0) => prune_out_channels on _Reshape_674(), #idxs=1536
[3] prune_out_channels on _Reshape_674() => prune_out_channels on _ElementWiseOp_673(AddmmBackward0), #idxs=1536
[4] prune_out_channels on _ElementWiseOp_673(AddmmBackward0) => prune_out_channels on _ElementWiseOp_675(TBackward0), #idxs=1536
[5] prune_out_channels on _ElementWiseOp_673(AddmmBackward0) => prune_in_channels on stages.3.blocks.0.ffn.layers.1 (Linear(in_features=3072, out_features=384, bias=True)), #idxs=1536
--------------------------------

<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
continue 4
<class 'torch_pruning.ops._ReshapeOp'> continue 2
PatchMergingPruner () get_out_channels/  384 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
WindowMSAPruner layer.embed_dims =  384
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
WindowMSAPruner layer.embed_dims =  384
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
SwinTransformer(
  (patch_embed): PatchEmbed(
    (adap_padding): AdaptivePadding()
    (projection): Conv2d(3, 48, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((48,), eps=1e-05, elementwise_affine=True)
  )
  (drop_after_pos): Dropout(p=0.0, inplace=False)
  (stages): ModuleList(
    (0): SwinBlockSequence(
      (blocks): ModuleList(
        (0-1): 2 x SwinBlock(
          (norm1): LayerNorm((48,), eps=1e-05, elementwise_affine=True)
          (attn): ShiftWindowMSA(
            (w_msa): WindowMSA(
              (qkv): Linear(in_features=48, out_features=96, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=48, out_features=48, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop): DropPath()
          )
          (norm2): LayerNorm((48,), eps=1e-05, elementwise_affine=True)
          (ffn): FFN(
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=48, out_features=192, bias=True)
                (1): GELU(approximate='none')
                (2): Dropout(p=0.0, inplace=False)
              )
              (1): Linear(in_features=192, out_features=48, bias=True)
              (2): Dropout(p=0.0, inplace=False)
            )
            (dropout_layer): DropPath()
            (gamma2): Identity()
          )
        )
      )
      (downsample): PatchMerging(
        (adap_padding): AdaptivePadding()
        (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
        (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (reduction): Linear(in_features=192, out_features=96, bias=False)
      )
    )
    (1): SwinBlockSequence(
      (blocks): ModuleList(
        (0-1): 2 x SwinBlock(
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): ShiftWindowMSA(
            (w_msa): WindowMSA(
              (qkv): Linear(in_features=96, out_features=192, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=96, out_features=96, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop): DropPath()
          )
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (ffn): FFN(
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=96, out_features=384, bias=True)
                (1): GELU(approximate='none')
                (2): Dropout(p=0.0, inplace=False)
              )
              (1): Linear(in_features=384, out_features=96, bias=True)
              (2): Dropout(p=0.0, inplace=False)
            )
            (dropout_layer): DropPath()
            (gamma2): Identity()
          )
        )
      )
      (downsample): PatchMerging(
        (adap_padding): AdaptivePadding()
        (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (reduction): Linear(in_features=384, out_features=192, bias=False)
      )
    )
    (2): SwinBlockSequence(
      (blocks): ModuleList(
        (0-5): 6 x SwinBlock(
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): ShiftWindowMSA(
            (w_msa): WindowMSA(
              (qkv): Linear(in_features=192, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=192, out_features=192, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop): DropPath()
          )
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (ffn): FFN(
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=192, out_features=768, bias=True)
                (1): GELU(approximate='none')
                (2): Dropout(p=0.0, inplace=False)
              )
              (1): Linear(in_features=768, out_features=192, bias=True)
              (2): Dropout(p=0.0, inplace=False)
            )
            (dropout_layer): DropPath()
            (gamma2): Identity()
          )
        )
      )
      (downsample): PatchMerging(
        (adap_padding): AdaptivePadding()
        (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (reduction): Linear(in_features=768, out_features=384, bias=False)
      )
    )
    (3): SwinBlockSequence(
      (blocks): ModuleList(
        (0-1): 2 x SwinBlock(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): ShiftWindowMSA(
            (w_msa): WindowMSA(
              (qkv): Linear(in_features=384, out_features=768, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop): DropPath()
          )
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (ffn): FFN(
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=384, out_features=1536, bias=True)
                (1): GELU(approximate='none')
                (2): Dropout(p=0.0, inplace=False)
              )
              (1): Linear(in_features=1536, out_features=384, bias=True)
              (2): Dropout(p=0.0, inplace=False)
            )
            (dropout_layer): DropPath()
            (gamma2): Identity()
          )
        )
      )
    )
  )
  (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  (norm3): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
)
- prev m.qkv.out_features =  96
- prev m.qkv.in_features =  48
 - prev m.num_heads =  3
 - prev m.embed_dims =  96
head_embed_dims =  32
0.1767766952966369 96
 - after m.embed_dims =  96

- prev m.qkv.out_features =  96
- prev m.qkv.in_features =  48
 - prev m.num_heads =  3
 - prev m.embed_dims =  96
head_embed_dims =  32
0.1767766952966369 96
 - after m.embed_dims =  96

- prev m.qkv.out_features =  192
- prev m.qkv.in_features =  96
 - prev m.num_heads =  6
 - prev m.embed_dims =  192
head_embed_dims =  32
0.1767766952966369 192
 - after m.embed_dims =  192

- prev m.qkv.out_features =  192
- prev m.qkv.in_features =  96
 - prev m.num_heads =  6
 - prev m.embed_dims =  192
head_embed_dims =  32
0.1767766952966369 192
 - after m.embed_dims =  192

- prev m.qkv.out_features =  384
- prev m.qkv.in_features =  192
 - prev m.num_heads =  12
 - prev m.embed_dims =  384
head_embed_dims =  32
0.1767766952966369 384
 - after m.embed_dims =  384

- prev m.qkv.out_features =  384
- prev m.qkv.in_features =  192
 - prev m.num_heads =  12
 - prev m.embed_dims =  384
head_embed_dims =  32
0.1767766952966369 384
 - after m.embed_dims =  384

- prev m.qkv.out_features =  384
- prev m.qkv.in_features =  192
 - prev m.num_heads =  12
 - prev m.embed_dims =  384
head_embed_dims =  32
0.1767766952966369 384
 - after m.embed_dims =  384

- prev m.qkv.out_features =  384
- prev m.qkv.in_features =  192
 - prev m.num_heads =  12
 - prev m.embed_dims =  384
head_embed_dims =  32
0.1767766952966369 384
 - after m.embed_dims =  384

- prev m.qkv.out_features =  384
- prev m.qkv.in_features =  192
 - prev m.num_heads =  12
 - prev m.embed_dims =  384
head_embed_dims =  32
0.1767766952966369 384
 - after m.embed_dims =  384

- prev m.qkv.out_features =  384
- prev m.qkv.in_features =  192
 - prev m.num_heads =  12
 - prev m.embed_dims =  384
head_embed_dims =  32
0.1767766952966369 384
 - after m.embed_dims =  384

- prev m.qkv.out_features =  768
- prev m.qkv.in_features =  384
 - prev m.num_heads =  24
 - prev m.embed_dims =  768
head_embed_dims =  32
0.1767766952966369 768
 - after m.embed_dims =  768

- prev m.qkv.out_features =  768
- prev m.qkv.in_features =  384
 - prev m.num_heads =  24
 - prev m.embed_dims =  768
head_embed_dims =  32
0.1767766952966369 768
 - after m.embed_dims =  768

0.1767766952966369 96
0.1767766952966369 96
0.1767766952966369 192
0.1767766952966369 192
0.1767766952966369 384
0.1767766952966369 384
0.1767766952966369 384
0.1767766952966369 384
0.1767766952966369 384
0.1767766952966369 384
0.1767766952966369 768
0.1767766952966369 768
-----NEW SwinTransformer FORWARD PASS 



		SwinTransformer input shape:  torch.Size([1, 3, 800, 1333])


------------SwinBlockSequence -  torch.Size([1, 66800, 48])
with_cp =  False
x.requires_grad =  True
ShiftWindowMSA = query view: B, L, C=  1 66800 48
	 hw_shape =  (200, 334)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([1392, 49, 48])
B=1392, N=49, C=48
the qkv mod: 48 96 Linear(in_features=48, out_features=96, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([1392, 49, 144])
NEW C= 48
qkv.shape =  torch.Size([1392, 49, 144])
qkv.flatten.shape =  torch.Size([9821952])
self.embed_dims =  96
+++++ reshaping ....  1392 49 3 self.num_heads= 3 16
1392 49 3 3 16  =  9821952
self.num_heads =  3
C =  48
qkv after reshape  torch.Size([1392, 49, 3, 3, 16])
qkv after permute  torch.Size([3, 1392, 3, 49, 16])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f660456e970>
--proj input shape =  torch.Size([1392, 49, 48])
--proj output shape =  torch.Size([1392, 49, 48])
------------WindowMSA - x_shape Output torch.Size([1392, 49, 48]) <UnsafeViewBackward0 object at 0x7f660456e970>
attn_windows.shape before merge =  torch.Size([1392, 49, 48])
x.shape before view =  torch.Size([1, 200, 334, 48])
ShiftWindowMSA = before view B, H * W, C =  1 66800 48
------------ShiftWindowMSA -  <ViewBackward0 object at 0x7f660456e970>
------------SwinBlock -  <AddBackward0 object at 0x7f660456e970>
		block output:  torch.Size([1, 66800, 48])
------------SwinBlockSequence -  torch.Size([1, 66800, 48])
with_cp =  False
x.requires_grad =  True
ShiftWindowMSA = query view: B, L, C=  1 66800 48
	 hw_shape =  (200, 334)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([1392, 49, 48])
B=1392, N=49, C=48
the qkv mod: 48 96 Linear(in_features=48, out_features=96, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([1392, 49, 144])
NEW C= 48
qkv.shape =  torch.Size([1392, 49, 144])
qkv.flatten.shape =  torch.Size([9821952])
self.embed_dims =  96
+++++ reshaping ....  1392 49 3 self.num_heads= 3 16
1392 49 3 3 16  =  9821952
self.num_heads =  3
C =  48
qkv after reshape  torch.Size([1392, 49, 3, 3, 16])
qkv after permute  torch.Size([3, 1392, 3, 49, 16])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f660456e970>
--proj input shape =  torch.Size([1392, 49, 48])
--proj output shape =  torch.Size([1392, 49, 48])
------------WindowMSA - x_shape Output torch.Size([1392, 49, 48]) <UnsafeViewBackward0 object at 0x7f660456e970>
attn_windows.shape before merge =  torch.Size([1392, 49, 48])
x.shape before view =  torch.Size([1, 200, 334, 48])
ShiftWindowMSA = before view B, H * W, C =  1 66800 48
------------ShiftWindowMSA -  <ViewBackward0 object at 0x7f660456e970>
------------SwinBlock -  <AddBackward0 object at 0x7f660456e970>
		block output:  torch.Size([1, 66800, 48])
------------SwinBlockSequence/downsample -  torch.Size([1, 16700, 96]) <UnsafeViewBackward0 object at 0x7f6604731f10>
down_hw_shape =  (100, 167)
x.shape =  torch.Size([1, 16700, 96])
hw_shape =  (100, 167)
stage out.shape =  torch.Size([1, 66800, 48])
out_hw_shape =  (200, 334)
stage =  0  / self.out_indices =  (1, 2, 3)
self.num_features =  [48, 96, 192, 384]
------------SwinBlockSequence -  torch.Size([1, 16700, 96])
with_cp =  False
x.requires_grad =  True
ShiftWindowMSA = query view: B, L, C=  1 16700 96
	 hw_shape =  (100, 167)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([360, 49, 96])
B=360, N=49, C=96
the qkv mod: 96 192 Linear(in_features=96, out_features=192, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([360, 49, 288])
NEW C= 96
qkv.shape =  torch.Size([360, 49, 288])
qkv.flatten.shape =  torch.Size([5080320])
self.embed_dims =  192
+++++ reshaping ....  360 49 3 self.num_heads= 6 16
360 49 3 6 16  =  5080320
self.num_heads =  6
C =  96
qkv after reshape  torch.Size([360, 49, 3, 6, 16])
qkv after permute  torch.Size([3, 360, 6, 49, 16])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f660456e970>
--proj input shape =  torch.Size([360, 49, 96])
--proj output shape =  torch.Size([360, 49, 96])
------------WindowMSA - x_shape Output torch.Size([360, 49, 96]) <UnsafeViewBackward0 object at 0x7f660456e970>
attn_windows.shape before merge =  torch.Size([360, 49, 96])
x.shape before view =  torch.Size([1, 100, 167, 96])
ShiftWindowMSA = before view B, H * W, C =  1 16700 96
------------ShiftWindowMSA -  <ViewBackward0 object at 0x7f660456e970>
------------SwinBlock -  <AddBackward0 object at 0x7f660456e970>
		block output:  torch.Size([1, 16700, 96])
------------SwinBlockSequence -  torch.Size([1, 16700, 96])
with_cp =  False
x.requires_grad =  True
ShiftWindowMSA = query view: B, L, C=  1 16700 96
	 hw_shape =  (100, 167)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([360, 49, 96])
B=360, N=49, C=96
the qkv mod: 96 192 Linear(in_features=96, out_features=192, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([360, 49, 288])
NEW C= 96
qkv.shape =  torch.Size([360, 49, 288])
qkv.flatten.shape =  torch.Size([5080320])
self.embed_dims =  192
+++++ reshaping ....  360 49 3 self.num_heads= 6 16
360 49 3 6 16  =  5080320
self.num_heads =  6
C =  96
qkv after reshape  torch.Size([360, 49, 3, 6, 16])
qkv after permute  torch.Size([3, 360, 6, 49, 16])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f660456e970>
--proj input shape =  torch.Size([360, 49, 96])
--proj output shape =  torch.Size([360, 49, 96])
------------WindowMSA - x_shape Output torch.Size([360, 49, 96]) <UnsafeViewBackward0 object at 0x7f660456e970>
attn_windows.shape before merge =  torch.Size([360, 49, 96])
x.shape before view =  torch.Size([1, 100, 167, 96])
ShiftWindowMSA = before view B, H * W, C =  1 16700 96
------------ShiftWindowMSA -  <ViewBackward0 object at 0x7f660456e970>
------------SwinBlock -  <AddBackward0 object at 0x7f660456e970>
		block output:  torch.Size([1, 16700, 96])
------------SwinBlockSequence/downsample -  torch.Size([1, 4200, 192]) <UnsafeViewBackward0 object at 0x7f660456e910>
down_hw_shape =  (50, 84)
x.shape =  torch.Size([1, 4200, 192])
hw_shape =  (50, 84)
stage out.shape =  torch.Size([1, 16700, 96])
out_hw_shape =  (100, 167)
stage =  1  / self.out_indices =  (1, 2, 3)
self.num_features =  [48, 96, 192, 384]
stage:  1  --- norm_layer out.shape =  torch.Size([1, 16700, 96])
torch.Size([1, 96, 100, 167])
------------SwinBlockSequence -  torch.Size([1, 4200, 192])
with_cp =  False
x.requires_grad =  True
ShiftWindowMSA = query view: B, L, C=  1 4200 192
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 192])
B=96, N=49, C=192
the qkv mod: 192 384 Linear(in_features=192, out_features=384, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 576])
NEW C= 192
qkv.shape =  torch.Size([96, 49, 576])
qkv.flatten.shape =  torch.Size([2709504])
self.embed_dims =  384
+++++ reshaping ....  96 49 3 self.num_heads= 12 16
96 49 3 12 16  =  2709504
self.num_heads =  12
C =  192
qkv after reshape  torch.Size([96, 49, 3, 12, 16])
qkv after permute  torch.Size([3, 96, 12, 49, 16])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f660456e970>
--proj input shape =  torch.Size([96, 49, 192])
--proj output shape =  torch.Size([96, 49, 192])
------------WindowMSA - x_shape Output torch.Size([96, 49, 192]) <UnsafeViewBackward0 object at 0x7f660456e970>
attn_windows.shape before merge =  torch.Size([96, 49, 192])
x.shape before view =  torch.Size([1, 50, 84, 192])
ShiftWindowMSA = before view B, H * W, C =  1 4200 192
------------ShiftWindowMSA -  <ViewBackward0 object at 0x7f660456e970>
------------SwinBlock -  <AddBackward0 object at 0x7f660456e970>
		block output:  torch.Size([1, 4200, 192])
------------SwinBlockSequence -  torch.Size([1, 4200, 192])
with_cp =  False
x.requires_grad =  True
ShiftWindowMSA = query view: B, L, C=  1 4200 192
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 192])
B=96, N=49, C=192
the qkv mod: 192 384 Linear(in_features=192, out_features=384, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 576])
NEW C= 192
qkv.shape =  torch.Size([96, 49, 576])
qkv.flatten.shape =  torch.Size([2709504])
self.embed_dims =  384
+++++ reshaping ....  96 49 3 self.num_heads= 12 16
96 49 3 12 16  =  2709504
self.num_heads =  12
C =  192
qkv after reshape  torch.Size([96, 49, 3, 12, 16])
qkv after permute  torch.Size([3, 96, 12, 49, 16])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f660456e970>
--proj input shape =  torch.Size([96, 49, 192])
--proj output shape =  torch.Size([96, 49, 192])
------------WindowMSA - x_shape Output torch.Size([96, 49, 192]) <UnsafeViewBackward0 object at 0x7f660456e970>
attn_windows.shape before merge =  torch.Size([96, 49, 192])
x.shape before view =  torch.Size([1, 50, 84, 192])
ShiftWindowMSA = before view B, H * W, C =  1 4200 192
------------ShiftWindowMSA -  <ViewBackward0 object at 0x7f660456e970>
------------SwinBlock -  <AddBackward0 object at 0x7f660456e970>
		block output:  torch.Size([1, 4200, 192])
------------SwinBlockSequence -  torch.Size([1, 4200, 192])
with_cp =  False
x.requires_grad =  True
ShiftWindowMSA = query view: B, L, C=  1 4200 192
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 192])
B=96, N=49, C=192
the qkv mod: 192 384 Linear(in_features=192, out_features=384, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 576])
NEW C= 192
qkv.shape =  torch.Size([96, 49, 576])
qkv.flatten.shape =  torch.Size([2709504])
self.embed_dims =  384
+++++ reshaping ....  96 49 3 self.num_heads= 12 16
96 49 3 12 16  =  2709504
self.num_heads =  12
C =  192
qkv after reshape  torch.Size([96, 49, 3, 12, 16])
qkv after permute  torch.Size([3, 96, 12, 49, 16])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f660456e970>
--proj input shape =  torch.Size([96, 49, 192])
--proj output shape =  torch.Size([96, 49, 192])
------------WindowMSA - x_shape Output torch.Size([96, 49, 192]) <UnsafeViewBackward0 object at 0x7f660456e970>
attn_windows.shape before merge =  torch.Size([96, 49, 192])
x.shape before view =  torch.Size([1, 50, 84, 192])
ShiftWindowMSA = before view B, H * W, C =  1 4200 192
------------ShiftWindowMSA -  <ViewBackward0 object at 0x7f660456e970>
------------SwinBlock -  <AddBackward0 object at 0x7f660456e970>
		block output:  torch.Size([1, 4200, 192])
------------SwinBlockSequence -  torch.Size([1, 4200, 192])
with_cp =  False
x.requires_grad =  True
ShiftWindowMSA = query view: B, L, C=  1 4200 192
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 192])
B=96, N=49, C=192
the qkv mod: 192 384 Linear(in_features=192, out_features=384, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 576])
NEW C= 192
qkv.shape =  torch.Size([96, 49, 576])
qkv.flatten.shape =  torch.Size([2709504])
self.embed_dims =  384
+++++ reshaping ....  96 49 3 self.num_heads= 12 16
96 49 3 12 16  =  2709504
self.num_heads =  12
C =  192
qkv after reshape  torch.Size([96, 49, 3, 12, 16])
qkv after permute  torch.Size([3, 96, 12, 49, 16])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f660456e970>
--proj input shape =  torch.Size([96, 49, 192])
--proj output shape =  torch.Size([96, 49, 192])
------------WindowMSA - x_shape Output torch.Size([96, 49, 192]) <UnsafeViewBackward0 object at 0x7f660456e970>
attn_windows.shape before merge =  torch.Size([96, 49, 192])
x.shape before view =  torch.Size([1, 50, 84, 192])
ShiftWindowMSA = before view B, H * W, C =  1 4200 192
------------ShiftWindowMSA -  <ViewBackward0 object at 0x7f660456e970>
------------SwinBlock -  <AddBackward0 object at 0x7f660456e970>
		block output:  torch.Size([1, 4200, 192])
------------SwinBlockSequence -  torch.Size([1, 4200, 192])
with_cp =  False
x.requires_grad =  True
ShiftWindowMSA = query view: B, L, C=  1 4200 192
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 192])
B=96, N=49, C=192
the qkv mod: 192 384 Linear(in_features=192, out_features=384, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 576])
NEW C= 192
qkv.shape =  torch.Size([96, 49, 576])
qkv.flatten.shape =  torch.Size([2709504])
self.embed_dims =  384
+++++ reshaping ....  96 49 3 self.num_heads= 12 16
96 49 3 12 16  =  2709504
self.num_heads =  12
C =  192
qkv after reshape  torch.Size([96, 49, 3, 12, 16])
qkv after permute  torch.Size([3, 96, 12, 49, 16])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f660456e970>
--proj input shape =  torch.Size([96, 49, 192])
--proj output shape =  torch.Size([96, 49, 192])
------------WindowMSA - x_shape Output torch.Size([96, 49, 192]) <UnsafeViewBackward0 object at 0x7f660456e970>
attn_windows.shape before merge =  torch.Size([96, 49, 192])
x.shape before view =  torch.Size([1, 50, 84, 192])
ShiftWindowMSA = before view B, H * W, C =  1 4200 192
------------ShiftWindowMSA -  <ViewBackward0 object at 0x7f660456e970>
------------SwinBlock -  <AddBackward0 object at 0x7f660456e970>
		block output:  torch.Size([1, 4200, 192])
------------SwinBlockSequence -  torch.Size([1, 4200, 192])
with_cp =  False
x.requires_grad =  True
ShiftWindowMSA = query view: B, L, C=  1 4200 192
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 192])
B=96, N=49, C=192
the qkv mod: 192 384 Linear(in_features=192, out_features=384, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 576])
NEW C= 192
qkv.shape =  torch.Size([96, 49, 576])
qkv.flatten.shape =  torch.Size([2709504])
self.embed_dims =  384
+++++ reshaping ....  96 49 3 self.num_heads= 12 16
96 49 3 12 16  =  2709504
self.num_heads =  12
C =  192
qkv after reshape  torch.Size([96, 49, 3, 12, 16])
qkv after permute  torch.Size([3, 96, 12, 49, 16])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f660456e970>
--proj input shape =  torch.Size([96, 49, 192])
--proj output shape =  torch.Size([96, 49, 192])
------------WindowMSA - x_shape Output torch.Size([96, 49, 192]) <UnsafeViewBackward0 object at 0x7f660456e8e0>
attn_windows.shape before merge =  torch.Size([96, 49, 192])
x.shape before view =  torch.Size([1, 50, 84, 192])
ShiftWindowMSA = before view B, H * W, C =  1 4200 192
------------ShiftWindowMSA -  <ViewBackward0 object at 0x7f660456e8e0>
------------SwinBlock -  <AddBackward0 object at 0x7f660456e8e0>
		block output:  torch.Size([1, 4200, 192])
------------SwinBlockSequence/downsample -  torch.Size([1, 1050, 384]) <UnsafeViewBackward0 object at 0x7f6604731f10>
down_hw_shape =  (25, 42)
x.shape =  torch.Size([1, 1050, 384])
hw_shape =  (25, 42)
stage out.shape =  torch.Size([1, 4200, 192])
out_hw_shape =  (50, 84)
stage =  2  / self.out_indices =  (1, 2, 3)
self.num_features =  [48, 96, 192, 384]
stage:  2  --- norm_layer out.shape =  torch.Size([1, 4200, 192])
torch.Size([1, 192, 50, 84])
------------SwinBlockSequence -  torch.Size([1, 1050, 384])
with_cp =  False
x.requires_grad =  True
ShiftWindowMSA = query view: B, L, C=  1 1050 384
	 hw_shape =  (25, 42)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([24, 49, 384])
B=24, N=49, C=384
the qkv mod: 384 768 Linear(in_features=384, out_features=768, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([24, 49, 1152])
NEW C= 384
qkv.shape =  torch.Size([24, 49, 1152])
qkv.flatten.shape =  torch.Size([1354752])
self.embed_dims =  768
+++++ reshaping ....  24 49 3 self.num_heads= 24 16
24 49 3 24 16  =  1354752
self.num_heads =  24
C =  384
qkv after reshape  torch.Size([24, 49, 3, 24, 16])
qkv after permute  torch.Size([3, 24, 24, 49, 16])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f660456e8e0>
--proj input shape =  torch.Size([24, 49, 384])
--proj output shape =  torch.Size([24, 49, 384])
------------WindowMSA - x_shape Output torch.Size([24, 49, 384]) <UnsafeViewBackward0 object at 0x7f660456e8e0>
attn_windows.shape before merge =  torch.Size([24, 49, 384])
x.shape before view =  torch.Size([1, 25, 42, 384])
ShiftWindowMSA = before view B, H * W, C =  1 1050 384
------------ShiftWindowMSA -  <ViewBackward0 object at 0x7f660456e8e0>
------------SwinBlock -  <AddBackward0 object at 0x7f660456e8e0>
		block output:  torch.Size([1, 1050, 384])
------------SwinBlockSequence -  torch.Size([1, 1050, 384])
with_cp =  False
x.requires_grad =  True
ShiftWindowMSA = query view: B, L, C=  1 1050 384
	 hw_shape =  (25, 42)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([24, 49, 384])
B=24, N=49, C=384
the qkv mod: 384 768 Linear(in_features=384, out_features=768, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([24, 49, 1152])
NEW C= 384
qkv.shape =  torch.Size([24, 49, 1152])
qkv.flatten.shape =  torch.Size([1354752])
self.embed_dims =  768
+++++ reshaping ....  24 49 3 self.num_heads= 24 16
24 49 3 24 16  =  1354752
self.num_heads =  24
C =  384
qkv after reshape  torch.Size([24, 49, 3, 24, 16])
qkv after permute  torch.Size([3, 24, 24, 49, 16])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f660456e8e0>
--proj input shape =  torch.Size([24, 49, 384])
--proj output shape =  torch.Size([24, 49, 384])
------------WindowMSA - x_shape Output torch.Size([24, 49, 384]) <UnsafeViewBackward0 object at 0x7f660456e8e0>
attn_windows.shape before merge =  torch.Size([24, 49, 384])
x.shape before view =  torch.Size([1, 25, 42, 384])
ShiftWindowMSA = before view B, H * W, C =  1 1050 384
------------ShiftWindowMSA -  <ViewBackward0 object at 0x7f660456e8e0>
------------SwinBlock -  <AddBackward0 object at 0x7f660456e8e0>
		block output:  torch.Size([1, 1050, 384])
x.shape =  torch.Size([1, 1050, 384])
hw_shape =  (25, 42)
stage out.shape =  torch.Size([1, 1050, 384])
out_hw_shape =  (25, 42)
stage =  3  / self.out_indices =  (1, 2, 3)
self.num_features =  [48, 96, 192, 384]
stage:  3  --- norm_layer out.shape =  torch.Size([1, 1050, 384])
torch.Size([1, 384, 25, 42])
------------SwinTransformer -  [<CloneBackward0 object at 0x7f660456e910>, <CloneBackward0 object at 0x7f660456e8e0>, <CloneBackward0 object at 0x7f660456e970>]



Pruned forward_time =  0.3786489963531494
Pruned fps =  2.6409683100476085
>>>>>>>>>>>>>>>>_freeze_stages =  -1
-----NEW SwinTransformer FORWARD PASS 



		SwinTransformer input shape:  torch.Size([1, 3, 800, 1333])


------------SwinBlockSequence -  torch.Size([1, 66800, 48])
with_cp =  False
x.requires_grad =  False
ShiftWindowMSA = query view: B, L, C=  1 66800 48
	 hw_shape =  (200, 334)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([1392, 49, 48])
B=1392, N=49, C=48
the qkv mod: 48 96 Linear(in_features=48, out_features=96, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([1392, 49, 144])
NEW C= 48
qkv.shape =  torch.Size([1392, 49, 144])
qkv.flatten.shape =  torch.Size([9821952])
self.embed_dims =  96
+++++ reshaping ....  1392 49 3 self.num_heads= 3 16
1392 49 3 3 16  =  9821952
self.num_heads =  3
C =  48
qkv after reshape  torch.Size([1392, 49, 3, 3, 16])
qkv after permute  torch.Size([3, 1392, 3, 49, 16])
		qkv.grad_fn >>>>>>  None
--proj input shape =  torch.Size([1392, 49, 48])
--proj output shape =  torch.Size([1392, 49, 48])
------------WindowMSA - x_shape Output torch.Size([1392, 49, 48]) None
attn_windows.shape before merge =  torch.Size([1392, 49, 48])
x.shape before view =  torch.Size([1, 200, 334, 48])
ShiftWindowMSA = before view B, H * W, C =  1 66800 48
------------ShiftWindowMSA -  None
------------SwinBlock -  None
		block output:  torch.Size([1, 66800, 48])
------------SwinBlockSequence -  torch.Size([1, 66800, 48])
with_cp =  False
x.requires_grad =  False
ShiftWindowMSA = query view: B, L, C=  1 66800 48
	 hw_shape =  (200, 334)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([1392, 49, 48])
B=1392, N=49, C=48
the qkv mod: 48 96 Linear(in_features=48, out_features=96, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([1392, 49, 144])
NEW C= 48
qkv.shape =  torch.Size([1392, 49, 144])
qkv.flatten.shape =  torch.Size([9821952])
self.embed_dims =  96
+++++ reshaping ....  1392 49 3 self.num_heads= 3 16
1392 49 3 3 16  =  9821952
self.num_heads =  3
C =  48
qkv after reshape  torch.Size([1392, 49, 3, 3, 16])
qkv after permute  torch.Size([3, 1392, 3, 49, 16])
		qkv.grad_fn >>>>>>  None
--proj input shape =  torch.Size([1392, 49, 48])
--proj output shape =  torch.Size([1392, 49, 48])
------------WindowMSA - x_shape Output torch.Size([1392, 49, 48]) None
attn_windows.shape before merge =  torch.Size([1392, 49, 48])
x.shape before view =  torch.Size([1, 200, 334, 48])
ShiftWindowMSA = before view B, H * W, C =  1 66800 48
------------ShiftWindowMSA -  None
------------SwinBlock -  None
		block output:  torch.Size([1, 66800, 48])
------------SwinBlockSequence/downsample -  torch.Size([1, 16700, 96]) None
down_hw_shape =  (100, 167)
x.shape =  torch.Size([1, 16700, 96])
hw_shape =  (100, 167)
stage out.shape =  torch.Size([1, 66800, 48])
out_hw_shape =  (200, 334)
stage =  0  / self.out_indices =  (1, 2, 3)
self.num_features =  [48, 96, 192, 384]
------------SwinBlockSequence -  torch.Size([1, 16700, 96])
with_cp =  False
x.requires_grad =  False
ShiftWindowMSA = query view: B, L, C=  1 16700 96
	 hw_shape =  (100, 167)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([360, 49, 96])
B=360, N=49, C=96
the qkv mod: 96 192 Linear(in_features=96, out_features=192, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([360, 49, 288])
NEW C= 96
qkv.shape =  torch.Size([360, 49, 288])
qkv.flatten.shape =  torch.Size([5080320])
self.embed_dims =  192
+++++ reshaping ....  360 49 3 self.num_heads= 6 16
360 49 3 6 16  =  5080320
self.num_heads =  6
C =  96
qkv after reshape  torch.Size([360, 49, 3, 6, 16])
qkv after permute  torch.Size([3, 360, 6, 49, 16])
		qkv.grad_fn >>>>>>  None
--proj input shape =  torch.Size([360, 49, 96])
--proj output shape =  torch.Size([360, 49, 96])
------------WindowMSA - x_shape Output torch.Size([360, 49, 96]) None
attn_windows.shape before merge =  torch.Size([360, 49, 96])
x.shape before view =  torch.Size([1, 100, 167, 96])
ShiftWindowMSA = before view B, H * W, C =  1 16700 96
------------ShiftWindowMSA -  None
------------SwinBlock -  None
		block output:  torch.Size([1, 16700, 96])
------------SwinBlockSequence -  torch.Size([1, 16700, 96])
with_cp =  False
x.requires_grad =  False
ShiftWindowMSA = query view: B, L, C=  1 16700 96
	 hw_shape =  (100, 167)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([360, 49, 96])
B=360, N=49, C=96
the qkv mod: 96 192 Linear(in_features=96, out_features=192, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([360, 49, 288])
NEW C= 96
qkv.shape =  torch.Size([360, 49, 288])
qkv.flatten.shape =  torch.Size([5080320])
self.embed_dims =  192
+++++ reshaping ....  360 49 3 self.num_heads= 6 16
360 49 3 6 16  =  5080320
self.num_heads =  6
C =  96
qkv after reshape  torch.Size([360, 49, 3, 6, 16])
qkv after permute  torch.Size([3, 360, 6, 49, 16])
		qkv.grad_fn >>>>>>  None
--proj input shape =  torch.Size([360, 49, 96])
--proj output shape =  torch.Size([360, 49, 96])
------------WindowMSA - x_shape Output torch.Size([360, 49, 96]) None
attn_windows.shape before merge =  torch.Size([360, 49, 96])
x.shape before view =  torch.Size([1, 100, 167, 96])
ShiftWindowMSA = before view B, H * W, C =  1 16700 96
------------ShiftWindowMSA -  None
------------SwinBlock -  None
		block output:  torch.Size([1, 16700, 96])
------------SwinBlockSequence/downsample -  torch.Size([1, 4200, 192]) None
down_hw_shape =  (50, 84)
x.shape =  torch.Size([1, 4200, 192])
hw_shape =  (50, 84)
stage out.shape =  torch.Size([1, 16700, 96])
out_hw_shape =  (100, 167)
stage =  1  / self.out_indices =  (1, 2, 3)
self.num_features =  [48, 96, 192, 384]
stage:  1  --- norm_layer out.shape =  torch.Size([1, 16700, 96])
torch.Size([1, 96, 100, 167])
------------SwinBlockSequence -  torch.Size([1, 4200, 192])
with_cp =  False
x.requires_grad =  False
ShiftWindowMSA = query view: B, L, C=  1 4200 192
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 192])
B=96, N=49, C=192
the qkv mod: 192 384 Linear(in_features=192, out_features=384, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 576])
NEW C= 192
qkv.shape =  torch.Size([96, 49, 576])
qkv.flatten.shape =  torch.Size([2709504])
self.embed_dims =  384
+++++ reshaping ....  96 49 3 self.num_heads= 12 16
96 49 3 12 16  =  2709504
self.num_heads =  12
C =  192
qkv after reshape  torch.Size([96, 49, 3, 12, 16])
qkv after permute  torch.Size([3, 96, 12, 49, 16])
		qkv.grad_fn >>>>>>  None
--proj input shape =  torch.Size([96, 49, 192])
--proj output shape =  torch.Size([96, 49, 192])
------------WindowMSA - x_shape Output torch.Size([96, 49, 192]) None
attn_windows.shape before merge =  torch.Size([96, 49, 192])
x.shape before view =  torch.Size([1, 50, 84, 192])
ShiftWindowMSA = before view B, H * W, C =  1 4200 192
------------ShiftWindowMSA -  None
------------SwinBlock -  None
		block output:  torch.Size([1, 4200, 192])
------------SwinBlockSequence -  torch.Size([1, 4200, 192])
with_cp =  False
x.requires_grad =  False
ShiftWindowMSA = query view: B, L, C=  1 4200 192
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 192])
B=96, N=49, C=192
the qkv mod: 192 384 Linear(in_features=192, out_features=384, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 576])
NEW C= 192
qkv.shape =  torch.Size([96, 49, 576])
qkv.flatten.shape =  torch.Size([2709504])
self.embed_dims =  384
+++++ reshaping ....  96 49 3 self.num_heads= 12 16
96 49 3 12 16  =  2709504
self.num_heads =  12
C =  192
qkv after reshape  torch.Size([96, 49, 3, 12, 16])
qkv after permute  torch.Size([3, 96, 12, 49, 16])
		qkv.grad_fn >>>>>>  None
--proj input shape =  torch.Size([96, 49, 192])
--proj output shape =  torch.Size([96, 49, 192])
------------WindowMSA - x_shape Output torch.Size([96, 49, 192]) None
attn_windows.shape before merge =  torch.Size([96, 49, 192])
x.shape before view =  torch.Size([1, 50, 84, 192])
ShiftWindowMSA = before view B, H * W, C =  1 4200 192
------------ShiftWindowMSA -  None
------------SwinBlock -  None
		block output:  torch.Size([1, 4200, 192])
------------SwinBlockSequence -  torch.Size([1, 4200, 192])
with_cp =  False
x.requires_grad =  False
ShiftWindowMSA = query view: B, L, C=  1 4200 192
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 192])
B=96, N=49, C=192
the qkv mod: 192 384 Linear(in_features=192, out_features=384, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 576])
NEW C= 192
qkv.shape =  torch.Size([96, 49, 576])
qkv.flatten.shape =  torch.Size([2709504])
self.embed_dims =  384
+++++ reshaping ....  96 49 3 self.num_heads= 12 16
96 49 3 12 16  =  2709504
self.num_heads =  12
C =  192
qkv after reshape  torch.Size([96, 49, 3, 12, 16])
qkv after permute  torch.Size([3, 96, 12, 49, 16])
		qkv.grad_fn >>>>>>  None
--proj input shape =  torch.Size([96, 49, 192])
--proj output shape =  torch.Size([96, 49, 192])
------------WindowMSA - x_shape Output torch.Size([96, 49, 192]) None
attn_windows.shape before merge =  torch.Size([96, 49, 192])
x.shape before view =  torch.Size([1, 50, 84, 192])
ShiftWindowMSA = before view B, H * W, C =  1 4200 192
------------ShiftWindowMSA -  None
------------SwinBlock -  None
		block output:  torch.Size([1, 4200, 192])
------------SwinBlockSequence -  torch.Size([1, 4200, 192])
with_cp =  False
x.requires_grad =  False
ShiftWindowMSA = query view: B, L, C=  1 4200 192
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 192])
B=96, N=49, C=192
the qkv mod: 192 384 Linear(in_features=192, out_features=384, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 576])
NEW C= 192
qkv.shape =  torch.Size([96, 49, 576])
qkv.flatten.shape =  torch.Size([2709504])
self.embed_dims =  384
+++++ reshaping ....  96 49 3 self.num_heads= 12 16
96 49 3 12 16  =  2709504
self.num_heads =  12
C =  192
qkv after reshape  torch.Size([96, 49, 3, 12, 16])
qkv after permute  torch.Size([3, 96, 12, 49, 16])
		qkv.grad_fn >>>>>>  None
--proj input shape =  torch.Size([96, 49, 192])
--proj output shape =  torch.Size([96, 49, 192])
------------WindowMSA - x_shape Output torch.Size([96, 49, 192]) None
attn_windows.shape before merge =  torch.Size([96, 49, 192])
x.shape before view =  torch.Size([1, 50, 84, 192])
ShiftWindowMSA = before view B, H * W, C =  1 4200 192
------------ShiftWindowMSA -  None
------------SwinBlock -  None
		block output:  torch.Size([1, 4200, 192])
------------SwinBlockSequence -  torch.Size([1, 4200, 192])
with_cp =  False
x.requires_grad =  False
ShiftWindowMSA = query view: B, L, C=  1 4200 192
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 192])
B=96, N=49, C=192
the qkv mod: 192 384 Linear(in_features=192, out_features=384, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 576])
NEW C= 192
qkv.shape =  torch.Size([96, 49, 576])
qkv.flatten.shape =  torch.Size([2709504])
self.embed_dims =  384
+++++ reshaping ....  96 49 3 self.num_heads= 12 16
96 49 3 12 16  =  2709504
self.num_heads =  12
C =  192
qkv after reshape  torch.Size([96, 49, 3, 12, 16])
qkv after permute  torch.Size([3, 96, 12, 49, 16])
		qkv.grad_fn >>>>>>  None
--proj input shape =  torch.Size([96, 49, 192])
--proj output shape =  torch.Size([96, 49, 192])
------------WindowMSA - x_shape Output torch.Size([96, 49, 192]) None
attn_windows.shape before merge =  torch.Size([96, 49, 192])
x.shape before view =  torch.Size([1, 50, 84, 192])
ShiftWindowMSA = before view B, H * W, C =  1 4200 192
------------ShiftWindowMSA -  None
------------SwinBlock -  None
		block output:  torch.Size([1, 4200, 192])
------------SwinBlockSequence -  torch.Size([1, 4200, 192])
with_cp =  False
x.requires_grad =  False
ShiftWindowMSA = query view: B, L, C=  1 4200 192
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 192])
B=96, N=49, C=192
the qkv mod: 192 384 Linear(in_features=192, out_features=384, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 576])
NEW C= 192
qkv.shape =  torch.Size([96, 49, 576])
qkv.flatten.shape =  torch.Size([2709504])
self.embed_dims =  384
+++++ reshaping ....  96 49 3 self.num_heads= 12 16
96 49 3 12 16  =  2709504
self.num_heads =  12
C =  192
qkv after reshape  torch.Size([96, 49, 3, 12, 16])
qkv after permute  torch.Size([3, 96, 12, 49, 16])
		qkv.grad_fn >>>>>>  None
--proj input shape =  torch.Size([96, 49, 192])
--proj output shape =  torch.Size([96, 49, 192])
------------WindowMSA - x_shape Output torch.Size([96, 49, 192]) None
attn_windows.shape before merge =  torch.Size([96, 49, 192])
x.shape before view =  torch.Size([1, 50, 84, 192])
ShiftWindowMSA = before view B, H * W, C =  1 4200 192
------------ShiftWindowMSA -  None
------------SwinBlock -  None
		block output:  torch.Size([1, 4200, 192])
------------SwinBlockSequence/downsample -  torch.Size([1, 1050, 384]) None
down_hw_shape =  (25, 42)
x.shape =  torch.Size([1, 1050, 384])
hw_shape =  (25, 42)
stage out.shape =  torch.Size([1, 4200, 192])
out_hw_shape =  (50, 84)
stage =  2  / self.out_indices =  (1, 2, 3)
self.num_features =  [48, 96, 192, 384]
stage:  2  --- norm_layer out.shape =  torch.Size([1, 4200, 192])
torch.Size([1, 192, 50, 84])
------------SwinBlockSequence -  torch.Size([1, 1050, 384])
with_cp =  False
x.requires_grad =  False
ShiftWindowMSA = query view: B, L, C=  1 1050 384
	 hw_shape =  (25, 42)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([24, 49, 384])
B=24, N=49, C=384
the qkv mod: 384 768 Linear(in_features=384, out_features=768, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([24, 49, 1152])
NEW C= 384
qkv.shape =  torch.Size([24, 49, 1152])
qkv.flatten.shape =  torch.Size([1354752])
self.embed_dims =  768
+++++ reshaping ....  24 49 3 self.num_heads= 24 16
24 49 3 24 16  =  1354752
self.num_heads =  24
C =  384
qkv after reshape  torch.Size([24, 49, 3, 24, 16])
qkv after permute  torch.Size([3, 24, 24, 49, 16])
		qkv.grad_fn >>>>>>  None
--proj input shape =  torch.Size([24, 49, 384])
--proj output shape =  torch.Size([24, 49, 384])
------------WindowMSA - x_shape Output torch.Size([24, 49, 384]) None
attn_windows.shape before merge =  torch.Size([24, 49, 384])
x.shape before view =  torch.Size([1, 25, 42, 384])
ShiftWindowMSA = before view B, H * W, C =  1 1050 384
------------ShiftWindowMSA -  None
------------SwinBlock -  None
		block output:  torch.Size([1, 1050, 384])
------------SwinBlockSequence -  torch.Size([1, 1050, 384])
with_cp =  False
x.requires_grad =  False
ShiftWindowMSA = query view: B, L, C=  1 1050 384
	 hw_shape =  (25, 42)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([24, 49, 384])
B=24, N=49, C=384
the qkv mod: 384 768 Linear(in_features=384, out_features=768, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([24, 49, 1152])
NEW C= 384
qkv.shape =  torch.Size([24, 49, 1152])
qkv.flatten.shape =  torch.Size([1354752])
self.embed_dims =  768
+++++ reshaping ....  24 49 3 self.num_heads= 24 16
24 49 3 24 16  =  1354752
self.num_heads =  24
C =  384
qkv after reshape  torch.Size([24, 49, 3, 24, 16])
qkv after permute  torch.Size([3, 24, 24, 49, 16])
		qkv.grad_fn >>>>>>  None
--proj input shape =  torch.Size([24, 49, 384])
--proj output shape =  torch.Size([24, 49, 384])
------------WindowMSA - x_shape Output torch.Size([24, 49, 384]) None
attn_windows.shape before merge =  torch.Size([24, 49, 384])
x.shape before view =  torch.Size([1, 25, 42, 384])
ShiftWindowMSA = before view B, H * W, C =  1 1050 384
------------ShiftWindowMSA -  None
------------SwinBlock -  None
		block output:  torch.Size([1, 1050, 384])
x.shape =  torch.Size([1, 1050, 384])
hw_shape =  (25, 42)
stage out.shape =  torch.Size([1, 1050, 384])
out_hw_shape =  (25, 42)
stage =  3  / self.out_indices =  (1, 2, 3)
self.num_features =  [48, 96, 192, 384]
stage:  3  --- norm_layer out.shape =  torch.Size([1, 1050, 384])
torch.Size([1, 384, 25, 42])
------------SwinTransformer -  [None, None, None]



Base MACs: 52.954591 G, Pruned MACs: 13.389912 G
Base Params: 27.520506 M, Pruned Params: 6.915210 M
