Base forward_time =  1.1920928955078125e-06
Base fps =  838860.8
SwinTransformer(
  (patch_embed): PatchEmbed(
    (adap_padding): AdaptivePadding()
    (projection): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (drop_after_pos): Dropout(p=0.0, inplace=False)
  (stages): ModuleList(
    (0): SwinBlockSequence(
      (blocks): ModuleList(
        (0-1): 2 x SwinBlock(
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): ShiftWindowMSA(
            (w_msa): WindowMSA(
              (qkv): Linear(in_features=96, out_features=288, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=96, out_features=96, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop): DropPath()
          )
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (ffn): FFN(
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=96, out_features=384, bias=True)
                (1): GELU(approximate='none')
                (2): Dropout(p=0.0, inplace=False)
              )
              (1): Linear(in_features=384, out_features=96, bias=True)
              (2): Dropout(p=0.0, inplace=False)
            )
            (dropout_layer): DropPath()
            (gamma2): Identity()
          )
        )
      )
      (downsample): PatchMerging(
        (adap_padding): AdaptivePadding()
        (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (reduction): Linear(in_features=384, out_features=192, bias=False)
      )
    )
    (1): SwinBlockSequence(
      (blocks): ModuleList(
        (0-1): 2 x SwinBlock(
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): ShiftWindowMSA(
            (w_msa): WindowMSA(
              (qkv): Linear(in_features=192, out_features=576, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=192, out_features=192, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop): DropPath()
          )
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (ffn): FFN(
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=192, out_features=768, bias=True)
                (1): GELU(approximate='none')
                (2): Dropout(p=0.0, inplace=False)
              )
              (1): Linear(in_features=768, out_features=192, bias=True)
              (2): Dropout(p=0.0, inplace=False)
            )
            (dropout_layer): DropPath()
            (gamma2): Identity()
          )
        )
      )
      (downsample): PatchMerging(
        (adap_padding): AdaptivePadding()
        (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (reduction): Linear(in_features=768, out_features=384, bias=False)
      )
    )
    (2): SwinBlockSequence(
      (blocks): ModuleList(
        (0-5): 6 x SwinBlock(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): ShiftWindowMSA(
            (w_msa): WindowMSA(
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop): DropPath()
          )
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (ffn): FFN(
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=384, out_features=1536, bias=True)
                (1): GELU(approximate='none')
                (2): Dropout(p=0.0, inplace=False)
              )
              (1): Linear(in_features=1536, out_features=384, bias=True)
              (2): Dropout(p=0.0, inplace=False)
            )
            (dropout_layer): DropPath()
            (gamma2): Identity()
          )
        )
      )
      (downsample): PatchMerging(
        (adap_padding): AdaptivePadding()
        (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        (reduction): Linear(in_features=1536, out_features=768, bias=False)
      )
    )
    (3): SwinBlockSequence(
      (blocks): ModuleList(
        (0-1): 2 x SwinBlock(
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): ShiftWindowMSA(
            (w_msa): WindowMSA(
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop): DropPath()
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (ffn): FFN(
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=768, out_features=3072, bias=True)
                (1): GELU(approximate='none')
                (2): Dropout(p=0.0, inplace=False)
              )
              (1): Linear(in_features=3072, out_features=768, bias=True)
              (2): Dropout(p=0.0, inplace=False)
            )
            (dropout_layer): DropPath()
            (gamma2): Identity()
          )
        )
      )
    )
  )
  (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
  (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
>>>>>>>>>>>>>>>>_freeze_stages =  -1
-----NEW SwinTransformer FORWARD PASS 



		SwinTransformer input shape:  torch.Size([1, 3, 800, 1333])


------------SwinBlockSequence -  torch.Size([1, 66800, 96])
with_cp =  False
x.requires_grad =  False
ShiftWindowMSA = query view: B, L, C=  1 66800 96
	 hw_shape =  (200, 334)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([1392, 49, 96])
B=1392, N=49, C=96
the qkv mod: 96 288 Linear(in_features=96, out_features=288, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([1392, 49, 288])
NEW C= 96
qkv.shape =  torch.Size([1392, 49, 288])
qkv.flatten.shape =  torch.Size([19643904])
self.embed_dims =  96
+++++ reshaping ....  1392 49 3 self.num_heads= 3 32
1392 49 3 3 32  =  19643904
self.num_heads =  3
C =  96
qkv after reshape  torch.Size([1392, 49, 3, 3, 32])
qkv after permute  torch.Size([3, 1392, 3, 49, 32])
		qkv.grad_fn >>>>>>  None
--proj input shape =  torch.Size([1392, 49, 96])
--proj output shape =  torch.Size([1392, 49, 96])
------------WindowMSA - x_shape Output torch.Size([1392, 49, 96]) None
attn_windows.shape before merge =  torch.Size([1392, 49, 96])
x.shape before view =  torch.Size([1, 200, 334, 96])
ShiftWindowMSA = before view B, H * W, C =  1 66800 96
------------ShiftWindowMSA -  None
------------SwinBlock -  None
		block output:  torch.Size([1, 66800, 96])
------------SwinBlockSequence -  torch.Size([1, 66800, 96])
with_cp =  False
x.requires_grad =  False
ShiftWindowMSA = query view: B, L, C=  1 66800 96
	 hw_shape =  (200, 334)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([1392, 49, 96])
B=1392, N=49, C=96
the qkv mod: 96 288 Linear(in_features=96, out_features=288, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([1392, 49, 288])
NEW C= 96
qkv.shape =  torch.Size([1392, 49, 288])
qkv.flatten.shape =  torch.Size([19643904])
self.embed_dims =  96
+++++ reshaping ....  1392 49 3 self.num_heads= 3 32
1392 49 3 3 32  =  19643904
self.num_heads =  3
C =  96
qkv after reshape  torch.Size([1392, 49, 3, 3, 32])
qkv after permute  torch.Size([3, 1392, 3, 49, 32])
		qkv.grad_fn >>>>>>  None
--proj input shape =  torch.Size([1392, 49, 96])
--proj output shape =  torch.Size([1392, 49, 96])
------------WindowMSA - x_shape Output torch.Size([1392, 49, 96]) None
attn_windows.shape before merge =  torch.Size([1392, 49, 96])
x.shape before view =  torch.Size([1, 200, 334, 96])
ShiftWindowMSA = before view B, H * W, C =  1 66800 96
------------ShiftWindowMSA -  None
------------SwinBlock -  None
		block output:  torch.Size([1, 66800, 96])
------------SwinBlockSequence/downsample -  torch.Size([1, 16700, 192]) None
down_hw_shape =  (100, 167)
x.shape =  torch.Size([1, 16700, 192])
hw_shape =  (100, 167)
stage out.shape =  torch.Size([1, 66800, 96])
out_hw_shape =  (200, 334)
stage =  0  / self.out_indices =  (1, 2, 3)
self.num_features =  [96, 192, 384, 768]
------------SwinBlockSequence -  torch.Size([1, 16700, 192])
with_cp =  False
x.requires_grad =  False
ShiftWindowMSA = query view: B, L, C=  1 16700 192
	 hw_shape =  (100, 167)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([360, 49, 192])
B=360, N=49, C=192
the qkv mod: 192 576 Linear(in_features=192, out_features=576, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([360, 49, 576])
NEW C= 192
qkv.shape =  torch.Size([360, 49, 576])
qkv.flatten.shape =  torch.Size([10160640])
self.embed_dims =  192
+++++ reshaping ....  360 49 3 self.num_heads= 6 32
360 49 3 6 32  =  10160640
self.num_heads =  6
C =  192
qkv after reshape  torch.Size([360, 49, 3, 6, 32])
qkv after permute  torch.Size([3, 360, 6, 49, 32])
		qkv.grad_fn >>>>>>  None
--proj input shape =  torch.Size([360, 49, 192])
--proj output shape =  torch.Size([360, 49, 192])
------------WindowMSA - x_shape Output torch.Size([360, 49, 192]) None
attn_windows.shape before merge =  torch.Size([360, 49, 192])
x.shape before view =  torch.Size([1, 100, 167, 192])
ShiftWindowMSA = before view B, H * W, C =  1 16700 192
------------ShiftWindowMSA -  None
------------SwinBlock -  None
		block output:  torch.Size([1, 16700, 192])
------------SwinBlockSequence -  torch.Size([1, 16700, 192])
with_cp =  False
x.requires_grad =  False
ShiftWindowMSA = query view: B, L, C=  1 16700 192
	 hw_shape =  (100, 167)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([360, 49, 192])
B=360, N=49, C=192
the qkv mod: 192 576 Linear(in_features=192, out_features=576, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([360, 49, 576])
NEW C= 192
qkv.shape =  torch.Size([360, 49, 576])
qkv.flatten.shape =  torch.Size([10160640])
self.embed_dims =  192
+++++ reshaping ....  360 49 3 self.num_heads= 6 32
360 49 3 6 32  =  10160640
self.num_heads =  6
C =  192
qkv after reshape  torch.Size([360, 49, 3, 6, 32])
qkv after permute  torch.Size([3, 360, 6, 49, 32])
		qkv.grad_fn >>>>>>  None
--proj input shape =  torch.Size([360, 49, 192])
--proj output shape =  torch.Size([360, 49, 192])
------------WindowMSA - x_shape Output torch.Size([360, 49, 192]) None
attn_windows.shape before merge =  torch.Size([360, 49, 192])
x.shape before view =  torch.Size([1, 100, 167, 192])
ShiftWindowMSA = before view B, H * W, C =  1 16700 192
------------ShiftWindowMSA -  None
------------SwinBlock -  None
		block output:  torch.Size([1, 16700, 192])
------------SwinBlockSequence/downsample -  torch.Size([1, 4200, 384]) None
down_hw_shape =  (50, 84)
x.shape =  torch.Size([1, 4200, 384])
hw_shape =  (50, 84)
stage out.shape =  torch.Size([1, 16700, 192])
out_hw_shape =  (100, 167)
stage =  1  / self.out_indices =  (1, 2, 3)
self.num_features =  [96, 192, 384, 768]
stage:  1  --- norm_layer out.shape =  torch.Size([1, 16700, 192])
torch.Size([1, 192, 100, 167])
------------SwinBlockSequence -  torch.Size([1, 4200, 384])
with_cp =  False
x.requires_grad =  False
ShiftWindowMSA = query view: B, L, C=  1 4200 384
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 384])
B=96, N=49, C=384
the qkv mod: 384 1152 Linear(in_features=384, out_features=1152, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 1152])
NEW C= 384
qkv.shape =  torch.Size([96, 49, 1152])
qkv.flatten.shape =  torch.Size([5419008])
self.embed_dims =  384
+++++ reshaping ....  96 49 3 self.num_heads= 12 32
96 49 3 12 32  =  5419008
self.num_heads =  12
C =  384
qkv after reshape  torch.Size([96, 49, 3, 12, 32])
qkv after permute  torch.Size([3, 96, 12, 49, 32])
		qkv.grad_fn >>>>>>  None
--proj input shape =  torch.Size([96, 49, 384])
--proj output shape =  torch.Size([96, 49, 384])
------------WindowMSA - x_shape Output torch.Size([96, 49, 384]) None
attn_windows.shape before merge =  torch.Size([96, 49, 384])
x.shape before view =  torch.Size([1, 50, 84, 384])
ShiftWindowMSA = before view B, H * W, C =  1 4200 384
------------ShiftWindowMSA -  None
------------SwinBlock -  None
		block output:  torch.Size([1, 4200, 384])
------------SwinBlockSequence -  torch.Size([1, 4200, 384])
with_cp =  False
x.requires_grad =  False
ShiftWindowMSA = query view: B, L, C=  1 4200 384
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 384])
B=96, N=49, C=384
the qkv mod: 384 1152 Linear(in_features=384, out_features=1152, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 1152])
NEW C= 384
qkv.shape =  torch.Size([96, 49, 1152])
qkv.flatten.shape =  torch.Size([5419008])
self.embed_dims =  384
+++++ reshaping ....  96 49 3 self.num_heads= 12 32
96 49 3 12 32  =  5419008
self.num_heads =  12
C =  384
qkv after reshape  torch.Size([96, 49, 3, 12, 32])
qkv after permute  torch.Size([3, 96, 12, 49, 32])
		qkv.grad_fn >>>>>>  None
--proj input shape =  torch.Size([96, 49, 384])
--proj output shape =  torch.Size([96, 49, 384])
------------WindowMSA - x_shape Output torch.Size([96, 49, 384]) None
attn_windows.shape before merge =  torch.Size([96, 49, 384])
x.shape before view =  torch.Size([1, 50, 84, 384])
ShiftWindowMSA = before view B, H * W, C =  1 4200 384
------------ShiftWindowMSA -  None
------------SwinBlock -  None
		block output:  torch.Size([1, 4200, 384])
------------SwinBlockSequence -  torch.Size([1, 4200, 384])
with_cp =  False
x.requires_grad =  False
ShiftWindowMSA = query view: B, L, C=  1 4200 384
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 384])
B=96, N=49, C=384
the qkv mod: 384 1152 Linear(in_features=384, out_features=1152, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 1152])
NEW C= 384
qkv.shape =  torch.Size([96, 49, 1152])
qkv.flatten.shape =  torch.Size([5419008])
self.embed_dims =  384
+++++ reshaping ....  96 49 3 self.num_heads= 12 32
96 49 3 12 32  =  5419008
self.num_heads =  12
C =  384
qkv after reshape  torch.Size([96, 49, 3, 12, 32])
qkv after permute  torch.Size([3, 96, 12, 49, 32])
		qkv.grad_fn >>>>>>  None
--proj input shape =  torch.Size([96, 49, 384])
--proj output shape =  torch.Size([96, 49, 384])
------------WindowMSA - x_shape Output torch.Size([96, 49, 384]) None
attn_windows.shape before merge =  torch.Size([96, 49, 384])
x.shape before view =  torch.Size([1, 50, 84, 384])
ShiftWindowMSA = before view B, H * W, C =  1 4200 384
------------ShiftWindowMSA -  None
------------SwinBlock -  None
		block output:  torch.Size([1, 4200, 384])
------------SwinBlockSequence -  torch.Size([1, 4200, 384])
with_cp =  False
x.requires_grad =  False
ShiftWindowMSA = query view: B, L, C=  1 4200 384
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 384])
B=96, N=49, C=384
the qkv mod: 384 1152 Linear(in_features=384, out_features=1152, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 1152])
NEW C= 384
qkv.shape =  torch.Size([96, 49, 1152])
qkv.flatten.shape =  torch.Size([5419008])
self.embed_dims =  384
+++++ reshaping ....  96 49 3 self.num_heads= 12 32
96 49 3 12 32  =  5419008
self.num_heads =  12
C =  384
qkv after reshape  torch.Size([96, 49, 3, 12, 32])
qkv after permute  torch.Size([3, 96, 12, 49, 32])
		qkv.grad_fn >>>>>>  None
--proj input shape =  torch.Size([96, 49, 384])
--proj output shape =  torch.Size([96, 49, 384])
------------WindowMSA - x_shape Output torch.Size([96, 49, 384]) None
attn_windows.shape before merge =  torch.Size([96, 49, 384])
x.shape before view =  torch.Size([1, 50, 84, 384])
ShiftWindowMSA = before view B, H * W, C =  1 4200 384
------------ShiftWindowMSA -  None
------------SwinBlock -  None
		block output:  torch.Size([1, 4200, 384])
------------SwinBlockSequence -  torch.Size([1, 4200, 384])
with_cp =  False
x.requires_grad =  False
ShiftWindowMSA = query view: B, L, C=  1 4200 384
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 384])
B=96, N=49, C=384
the qkv mod: 384 1152 Linear(in_features=384, out_features=1152, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 1152])
NEW C= 384
qkv.shape =  torch.Size([96, 49, 1152])
qkv.flatten.shape =  torch.Size([5419008])
self.embed_dims =  384
+++++ reshaping ....  96 49 3 self.num_heads= 12 32
96 49 3 12 32  =  5419008
self.num_heads =  12
C =  384
qkv after reshape  torch.Size([96, 49, 3, 12, 32])
qkv after permute  torch.Size([3, 96, 12, 49, 32])
		qkv.grad_fn >>>>>>  None
--proj input shape =  torch.Size([96, 49, 384])
--proj output shape =  torch.Size([96, 49, 384])
------------WindowMSA - x_shape Output torch.Size([96, 49, 384]) None
attn_windows.shape before merge =  torch.Size([96, 49, 384])
x.shape before view =  torch.Size([1, 50, 84, 384])
ShiftWindowMSA = before view B, H * W, C =  1 4200 384
------------ShiftWindowMSA -  None
------------SwinBlock -  None
		block output:  torch.Size([1, 4200, 384])
------------SwinBlockSequence -  torch.Size([1, 4200, 384])
with_cp =  False
x.requires_grad =  False
ShiftWindowMSA = query view: B, L, C=  1 4200 384
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 384])
B=96, N=49, C=384
the qkv mod: 384 1152 Linear(in_features=384, out_features=1152, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 1152])
NEW C= 384
qkv.shape =  torch.Size([96, 49, 1152])
qkv.flatten.shape =  torch.Size([5419008])
self.embed_dims =  384
+++++ reshaping ....  96 49 3 self.num_heads= 12 32
96 49 3 12 32  =  5419008
self.num_heads =  12
C =  384
qkv after reshape  torch.Size([96, 49, 3, 12, 32])
qkv after permute  torch.Size([3, 96, 12, 49, 32])
		qkv.grad_fn >>>>>>  None
--proj input shape =  torch.Size([96, 49, 384])
--proj output shape =  torch.Size([96, 49, 384])
------------WindowMSA - x_shape Output torch.Size([96, 49, 384]) None
attn_windows.shape before merge =  torch.Size([96, 49, 384])
x.shape before view =  torch.Size([1, 50, 84, 384])
ShiftWindowMSA = before view B, H * W, C =  1 4200 384
------------ShiftWindowMSA -  None
------------SwinBlock -  None
		block output:  torch.Size([1, 4200, 384])
------------SwinBlockSequence/downsample -  torch.Size([1, 1050, 768]) None
down_hw_shape =  (25, 42)
x.shape =  torch.Size([1, 1050, 768])
hw_shape =  (25, 42)
stage out.shape =  torch.Size([1, 4200, 384])
out_hw_shape =  (50, 84)
stage =  2  / self.out_indices =  (1, 2, 3)
self.num_features =  [96, 192, 384, 768]
stage:  2  --- norm_layer out.shape =  torch.Size([1, 4200, 384])
torch.Size([1, 384, 50, 84])
------------SwinBlockSequence -  torch.Size([1, 1050, 768])
with_cp =  False
x.requires_grad =  False
ShiftWindowMSA = query view: B, L, C=  1 1050 768
	 hw_shape =  (25, 42)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([24, 49, 768])
B=24, N=49, C=768
the qkv mod: 768 2304 Linear(in_features=768, out_features=2304, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([24, 49, 2304])
NEW C= 768
qkv.shape =  torch.Size([24, 49, 2304])
qkv.flatten.shape =  torch.Size([2709504])
self.embed_dims =  768
+++++ reshaping ....  24 49 3 self.num_heads= 24 32
24 49 3 24 32  =  2709504
self.num_heads =  24
C =  768
qkv after reshape  torch.Size([24, 49, 3, 24, 32])
qkv after permute  torch.Size([3, 24, 24, 49, 32])
		qkv.grad_fn >>>>>>  None
--proj input shape =  torch.Size([24, 49, 768])
--proj output shape =  torch.Size([24, 49, 768])
------------WindowMSA - x_shape Output torch.Size([24, 49, 768]) None
attn_windows.shape before merge =  torch.Size([24, 49, 768])
x.shape before view =  torch.Size([1, 25, 42, 768])
ShiftWindowMSA = before view B, H * W, C =  1 1050 768
------------ShiftWindowMSA -  None
------------SwinBlock -  None
		block output:  torch.Size([1, 1050, 768])
------------SwinBlockSequence -  torch.Size([1, 1050, 768])
with_cp =  False
x.requires_grad =  False
ShiftWindowMSA = query view: B, L, C=  1 1050 768
	 hw_shape =  (25, 42)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([24, 49, 768])
B=24, N=49, C=768
the qkv mod: 768 2304 Linear(in_features=768, out_features=2304, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([24, 49, 2304])
NEW C= 768
qkv.shape =  torch.Size([24, 49, 2304])
qkv.flatten.shape =  torch.Size([2709504])
self.embed_dims =  768
+++++ reshaping ....  24 49 3 self.num_heads= 24 32
24 49 3 24 32  =  2709504
self.num_heads =  24
C =  768
qkv after reshape  torch.Size([24, 49, 3, 24, 32])
qkv after permute  torch.Size([3, 24, 24, 49, 32])
		qkv.grad_fn >>>>>>  None
--proj input shape =  torch.Size([24, 49, 768])
--proj output shape =  torch.Size([24, 49, 768])
------------WindowMSA - x_shape Output torch.Size([24, 49, 768]) None
attn_windows.shape before merge =  torch.Size([24, 49, 768])
x.shape before view =  torch.Size([1, 25, 42, 768])
ShiftWindowMSA = before view B, H * W, C =  1 1050 768
------------ShiftWindowMSA -  None
------------SwinBlock -  None
		block output:  torch.Size([1, 1050, 768])
x.shape =  torch.Size([1, 1050, 768])
hw_shape =  (25, 42)
stage out.shape =  torch.Size([1, 1050, 768])
out_hw_shape =  (25, 42)
stage =  3  / self.out_indices =  (1, 2, 3)
self.num_features =  [96, 192, 384, 768]
stage:  3  --- norm_layer out.shape =  torch.Size([1, 1050, 768])
torch.Size([1, 768, 25, 42])
------------SwinTransformer -  [None, None, None]



Base Macs: 52.954591 M, Base Params: 27.520506 M
m.num_attention_heads ===  3
m.num_attention_heads ===  3
m.num_attention_heads ===  6
m.num_attention_heads ===  6
m.num_attention_heads ===  12
m.num_attention_heads ===  12
m.num_attention_heads ===  12
m.num_attention_heads ===  12
m.num_attention_heads ===  12
m.num_attention_heads ===  12
m.num_attention_heads ===  24
m.num_attention_heads ===  24
			IGNORING:  AdaptivePadding()
			IGNORING:  Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
			IGNORING:  LayerNorm((384,), eps=1e-05, elementwise_affine=True)
			IGNORING:  Linear(in_features=384, out_features=192, bias=False)
			IGNORING:  AdaptivePadding()
			IGNORING:  Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
			IGNORING:  LayerNorm((768,), eps=1e-05, elementwise_affine=True)
			IGNORING:  Linear(in_features=768, out_features=384, bias=False)
			IGNORING:  AdaptivePadding()
			IGNORING:  Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
			IGNORING:  LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
			IGNORING:  Linear(in_features=1536, out_features=768, bias=False)
			IGNORING:  Linear(in_features=96, out_features=288, bias=True)
			IGNORING:  Dropout(p=0.0, inplace=False)
			IGNORING:  Linear(in_features=96, out_features=96, bias=True)
			IGNORING:  Dropout(p=0.0, inplace=False)
			IGNORING:  Softmax(dim=-1)
			IGNORING:  Linear(in_features=96, out_features=288, bias=True)
			IGNORING:  Dropout(p=0.0, inplace=False)
			IGNORING:  Linear(in_features=96, out_features=96, bias=True)
			IGNORING:  Dropout(p=0.0, inplace=False)
			IGNORING:  Softmax(dim=-1)
			IGNORING:  Linear(in_features=192, out_features=576, bias=True)
			IGNORING:  Dropout(p=0.0, inplace=False)
			IGNORING:  Linear(in_features=192, out_features=192, bias=True)
			IGNORING:  Dropout(p=0.0, inplace=False)
			IGNORING:  Softmax(dim=-1)
			IGNORING:  Linear(in_features=192, out_features=576, bias=True)
			IGNORING:  Dropout(p=0.0, inplace=False)
			IGNORING:  Linear(in_features=192, out_features=192, bias=True)
			IGNORING:  Dropout(p=0.0, inplace=False)
			IGNORING:  Softmax(dim=-1)
			IGNORING:  Linear(in_features=384, out_features=1152, bias=True)
			IGNORING:  Dropout(p=0.0, inplace=False)
			IGNORING:  Linear(in_features=384, out_features=384, bias=True)
			IGNORING:  Dropout(p=0.0, inplace=False)
			IGNORING:  Softmax(dim=-1)
			IGNORING:  Linear(in_features=384, out_features=1152, bias=True)
			IGNORING:  Dropout(p=0.0, inplace=False)
			IGNORING:  Linear(in_features=384, out_features=384, bias=True)
			IGNORING:  Dropout(p=0.0, inplace=False)
			IGNORING:  Softmax(dim=-1)
			IGNORING:  Linear(in_features=384, out_features=1152, bias=True)
			IGNORING:  Dropout(p=0.0, inplace=False)
			IGNORING:  Linear(in_features=384, out_features=384, bias=True)
			IGNORING:  Dropout(p=0.0, inplace=False)
			IGNORING:  Softmax(dim=-1)
			IGNORING:  Linear(in_features=384, out_features=1152, bias=True)
			IGNORING:  Dropout(p=0.0, inplace=False)
			IGNORING:  Linear(in_features=384, out_features=384, bias=True)
			IGNORING:  Dropout(p=0.0, inplace=False)
			IGNORING:  Softmax(dim=-1)
			IGNORING:  Linear(in_features=384, out_features=1152, bias=True)
			IGNORING:  Dropout(p=0.0, inplace=False)
			IGNORING:  Linear(in_features=384, out_features=384, bias=True)
			IGNORING:  Dropout(p=0.0, inplace=False)
			IGNORING:  Softmax(dim=-1)
			IGNORING:  Linear(in_features=384, out_features=1152, bias=True)
			IGNORING:  Dropout(p=0.0, inplace=False)
			IGNORING:  Linear(in_features=384, out_features=384, bias=True)
			IGNORING:  Dropout(p=0.0, inplace=False)
			IGNORING:  Softmax(dim=-1)
			IGNORING:  Linear(in_features=768, out_features=2304, bias=True)
			IGNORING:  Dropout(p=0.0, inplace=False)
			IGNORING:  Linear(in_features=768, out_features=768, bias=True)
			IGNORING:  Dropout(p=0.0, inplace=False)
			IGNORING:  Softmax(dim=-1)
			IGNORING:  Linear(in_features=768, out_features=2304, bias=True)
			IGNORING:  Dropout(p=0.0, inplace=False)
			IGNORING:  Linear(in_features=768, out_features=768, bias=True)
			IGNORING:  Dropout(p=0.0, inplace=False)
			IGNORING:  Softmax(dim=-1)
self.IGNORED_LAYERS_IN_TRACING =  72 [AdaptivePadding(), Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2)), LayerNorm((384,), eps=1e-05, elementwise_affine=True), Linear(in_features=384, out_features=192, bias=False), AdaptivePadding(), Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2)), LayerNorm((768,), eps=1e-05, elementwise_affine=True), Linear(in_features=768, out_features=384, bias=False), AdaptivePadding(), Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2)), LayerNorm((1536,), eps=1e-05, elementwise_affine=True), Linear(in_features=1536, out_features=768, bias=False), Linear(in_features=96, out_features=288, bias=True), Dropout(p=0.0, inplace=False), Linear(in_features=96, out_features=96, bias=True), Dropout(p=0.0, inplace=False), Softmax(dim=-1), Linear(in_features=96, out_features=288, bias=True), Dropout(p=0.0, inplace=False), Linear(in_features=96, out_features=96, bias=True), Dropout(p=0.0, inplace=False), Softmax(dim=-1), Linear(in_features=192, out_features=576, bias=True), Dropout(p=0.0, inplace=False), Linear(in_features=192, out_features=192, bias=True), Dropout(p=0.0, inplace=False), Softmax(dim=-1), Linear(in_features=192, out_features=576, bias=True), Dropout(p=0.0, inplace=False), Linear(in_features=192, out_features=192, bias=True), Dropout(p=0.0, inplace=False), Softmax(dim=-1), Linear(in_features=384, out_features=1152, bias=True), Dropout(p=0.0, inplace=False), Linear(in_features=384, out_features=384, bias=True), Dropout(p=0.0, inplace=False), Softmax(dim=-1), Linear(in_features=384, out_features=1152, bias=True), Dropout(p=0.0, inplace=False), Linear(in_features=384, out_features=384, bias=True), Dropout(p=0.0, inplace=False), Softmax(dim=-1), Linear(in_features=384, out_features=1152, bias=True), Dropout(p=0.0, inplace=False), Linear(in_features=384, out_features=384, bias=True), Dropout(p=0.0, inplace=False), Softmax(dim=-1), Linear(in_features=384, out_features=1152, bias=True), Dropout(p=0.0, inplace=False), Linear(in_features=384, out_features=384, bias=True), Dropout(p=0.0, inplace=False), Softmax(dim=-1), Linear(in_features=384, out_features=1152, bias=True), Dropout(p=0.0, inplace=False), Linear(in_features=384, out_features=384, bias=True), Dropout(p=0.0, inplace=False), Softmax(dim=-1), Linear(in_features=384, out_features=1152, bias=True), Dropout(p=0.0, inplace=False), Linear(in_features=384, out_features=384, bias=True), Dropout(p=0.0, inplace=False), Softmax(dim=-1), Linear(in_features=768, out_features=2304, bias=True), Dropout(p=0.0, inplace=False), Linear(in_features=768, out_features=768, bias=True), Dropout(p=0.0, inplace=False), Softmax(dim=-1), Linear(in_features=768, out_features=2304, bias=True), Dropout(p=0.0, inplace=False), Linear(in_features=768, out_features=768, bias=True), Dropout(p=0.0, inplace=False), Softmax(dim=-1)]
patch_embed.projection.weight True
patch_embed.projection.bias True
patch_embed.norm.weight True
patch_embed.norm.bias True
stages.0.blocks.0.norm1.weight True
stages.0.blocks.0.norm1.bias True
stages.0.blocks.0.attn.w_msa.relative_position_bias_table True
stages.0.blocks.0.attn.w_msa.qkv.weight True
stages.0.blocks.0.attn.w_msa.qkv.bias True
stages.0.blocks.0.attn.w_msa.proj.weight True
stages.0.blocks.0.attn.w_msa.proj.bias True
stages.0.blocks.0.norm2.weight True
stages.0.blocks.0.norm2.bias True
stages.0.blocks.0.ffn.layers.0.0.weight True
stages.0.blocks.0.ffn.layers.0.0.bias True
stages.0.blocks.0.ffn.layers.1.weight True
stages.0.blocks.0.ffn.layers.1.bias True
stages.0.blocks.1.norm1.weight True
stages.0.blocks.1.norm1.bias True
stages.0.blocks.1.attn.w_msa.relative_position_bias_table True
stages.0.blocks.1.attn.w_msa.qkv.weight True
stages.0.blocks.1.attn.w_msa.qkv.bias True
stages.0.blocks.1.attn.w_msa.proj.weight True
stages.0.blocks.1.attn.w_msa.proj.bias True
stages.0.blocks.1.norm2.weight True
stages.0.blocks.1.norm2.bias True
stages.0.blocks.1.ffn.layers.0.0.weight True
stages.0.blocks.1.ffn.layers.0.0.bias True
stages.0.blocks.1.ffn.layers.1.weight True
stages.0.blocks.1.ffn.layers.1.bias True
stages.0.downsample.norm.weight True
stages.0.downsample.norm.bias True
stages.0.downsample.reduction.weight True
stages.1.blocks.0.norm1.weight True
stages.1.blocks.0.norm1.bias True
stages.1.blocks.0.attn.w_msa.relative_position_bias_table True
stages.1.blocks.0.attn.w_msa.qkv.weight True
stages.1.blocks.0.attn.w_msa.qkv.bias True
stages.1.blocks.0.attn.w_msa.proj.weight True
stages.1.blocks.0.attn.w_msa.proj.bias True
stages.1.blocks.0.norm2.weight True
stages.1.blocks.0.norm2.bias True
stages.1.blocks.0.ffn.layers.0.0.weight True
stages.1.blocks.0.ffn.layers.0.0.bias True
stages.1.blocks.0.ffn.layers.1.weight True
stages.1.blocks.0.ffn.layers.1.bias True
stages.1.blocks.1.norm1.weight True
stages.1.blocks.1.norm1.bias True
stages.1.blocks.1.attn.w_msa.relative_position_bias_table True
stages.1.blocks.1.attn.w_msa.qkv.weight True
stages.1.blocks.1.attn.w_msa.qkv.bias True
stages.1.blocks.1.attn.w_msa.proj.weight True
stages.1.blocks.1.attn.w_msa.proj.bias True
stages.1.blocks.1.norm2.weight True
stages.1.blocks.1.norm2.bias True
stages.1.blocks.1.ffn.layers.0.0.weight True
stages.1.blocks.1.ffn.layers.0.0.bias True
stages.1.blocks.1.ffn.layers.1.weight True
stages.1.blocks.1.ffn.layers.1.bias True
stages.1.downsample.norm.weight True
stages.1.downsample.norm.bias True
stages.1.downsample.reduction.weight True
stages.2.blocks.0.norm1.weight True
stages.2.blocks.0.norm1.bias True
stages.2.blocks.0.attn.w_msa.relative_position_bias_table True
stages.2.blocks.0.attn.w_msa.qkv.weight True
stages.2.blocks.0.attn.w_msa.qkv.bias True
stages.2.blocks.0.attn.w_msa.proj.weight True
stages.2.blocks.0.attn.w_msa.proj.bias True
stages.2.blocks.0.norm2.weight True
stages.2.blocks.0.norm2.bias True
stages.2.blocks.0.ffn.layers.0.0.weight True
stages.2.blocks.0.ffn.layers.0.0.bias True
stages.2.blocks.0.ffn.layers.1.weight True
stages.2.blocks.0.ffn.layers.1.bias True
stages.2.blocks.1.norm1.weight True
stages.2.blocks.1.norm1.bias True
stages.2.blocks.1.attn.w_msa.relative_position_bias_table True
stages.2.blocks.1.attn.w_msa.qkv.weight True
stages.2.blocks.1.attn.w_msa.qkv.bias True
stages.2.blocks.1.attn.w_msa.proj.weight True
stages.2.blocks.1.attn.w_msa.proj.bias True
stages.2.blocks.1.norm2.weight True
stages.2.blocks.1.norm2.bias True
stages.2.blocks.1.ffn.layers.0.0.weight True
stages.2.blocks.1.ffn.layers.0.0.bias True
stages.2.blocks.1.ffn.layers.1.weight True
stages.2.blocks.1.ffn.layers.1.bias True
stages.2.blocks.2.norm1.weight True
stages.2.blocks.2.norm1.bias True
stages.2.blocks.2.attn.w_msa.relative_position_bias_table True
stages.2.blocks.2.attn.w_msa.qkv.weight True
stages.2.blocks.2.attn.w_msa.qkv.bias True
stages.2.blocks.2.attn.w_msa.proj.weight True
stages.2.blocks.2.attn.w_msa.proj.bias True
stages.2.blocks.2.norm2.weight True
stages.2.blocks.2.norm2.bias True
stages.2.blocks.2.ffn.layers.0.0.weight True
stages.2.blocks.2.ffn.layers.0.0.bias True
stages.2.blocks.2.ffn.layers.1.weight True
stages.2.blocks.2.ffn.layers.1.bias True
stages.2.blocks.3.norm1.weight True
stages.2.blocks.3.norm1.bias True
stages.2.blocks.3.attn.w_msa.relative_position_bias_table True
stages.2.blocks.3.attn.w_msa.qkv.weight True
stages.2.blocks.3.attn.w_msa.qkv.bias True
stages.2.blocks.3.attn.w_msa.proj.weight True
stages.2.blocks.3.attn.w_msa.proj.bias True
stages.2.blocks.3.norm2.weight True
stages.2.blocks.3.norm2.bias True
stages.2.blocks.3.ffn.layers.0.0.weight True
stages.2.blocks.3.ffn.layers.0.0.bias True
stages.2.blocks.3.ffn.layers.1.weight True
stages.2.blocks.3.ffn.layers.1.bias True
stages.2.blocks.4.norm1.weight True
stages.2.blocks.4.norm1.bias True
stages.2.blocks.4.attn.w_msa.relative_position_bias_table True
stages.2.blocks.4.attn.w_msa.qkv.weight True
stages.2.blocks.4.attn.w_msa.qkv.bias True
stages.2.blocks.4.attn.w_msa.proj.weight True
stages.2.blocks.4.attn.w_msa.proj.bias True
stages.2.blocks.4.norm2.weight True
stages.2.blocks.4.norm2.bias True
stages.2.blocks.4.ffn.layers.0.0.weight True
stages.2.blocks.4.ffn.layers.0.0.bias True
stages.2.blocks.4.ffn.layers.1.weight True
stages.2.blocks.4.ffn.layers.1.bias True
stages.2.blocks.5.norm1.weight True
stages.2.blocks.5.norm1.bias True
stages.2.blocks.5.attn.w_msa.relative_position_bias_table True
stages.2.blocks.5.attn.w_msa.qkv.weight True
stages.2.blocks.5.attn.w_msa.qkv.bias True
stages.2.blocks.5.attn.w_msa.proj.weight True
stages.2.blocks.5.attn.w_msa.proj.bias True
stages.2.blocks.5.norm2.weight True
stages.2.blocks.5.norm2.bias True
stages.2.blocks.5.ffn.layers.0.0.weight True
stages.2.blocks.5.ffn.layers.0.0.bias True
stages.2.blocks.5.ffn.layers.1.weight True
stages.2.blocks.5.ffn.layers.1.bias True
stages.2.downsample.norm.weight True
stages.2.downsample.norm.bias True
stages.2.downsample.reduction.weight True
stages.3.blocks.0.norm1.weight True
stages.3.blocks.0.norm1.bias True
stages.3.blocks.0.attn.w_msa.relative_position_bias_table True
stages.3.blocks.0.attn.w_msa.qkv.weight True
stages.3.blocks.0.attn.w_msa.qkv.bias True
stages.3.blocks.0.attn.w_msa.proj.weight True
stages.3.blocks.0.attn.w_msa.proj.bias True
stages.3.blocks.0.norm2.weight True
stages.3.blocks.0.norm2.bias True
stages.3.blocks.0.ffn.layers.0.0.weight True
stages.3.blocks.0.ffn.layers.0.0.bias True
stages.3.blocks.0.ffn.layers.1.weight True
stages.3.blocks.0.ffn.layers.1.bias True
stages.3.blocks.1.norm1.weight True
stages.3.blocks.1.norm1.bias True
stages.3.blocks.1.attn.w_msa.relative_position_bias_table True
stages.3.blocks.1.attn.w_msa.qkv.weight True
stages.3.blocks.1.attn.w_msa.qkv.bias True
stages.3.blocks.1.attn.w_msa.proj.weight True
stages.3.blocks.1.attn.w_msa.proj.bias True
stages.3.blocks.1.norm2.weight True
stages.3.blocks.1.norm2.bias True
stages.3.blocks.1.ffn.layers.0.0.weight True
stages.3.blocks.1.ffn.layers.0.0.bias True
stages.3.blocks.1.ffn.layers.1.weight True
stages.3.blocks.1.ffn.layers.1.bias True
norm1.weight True
norm1.bias True
norm2.weight True
norm2.bias True
norm3.weight True
norm3.bias True
self._param_to_name =  dict_values([])
>>>>>>>>>>>>>>>>_freeze_stages =  -1
[Passed] {<class 'torch.nn.modules.conv.Conv2d'>, <class 'mmcv.cnn.bricks.wrappers.Linear'>, <class 'mmdet.models.layers.transformer.utils.PatchMerging'>, <class 'torch.nn.modules.normalization.LayerNorm'>, <class 'mmdet.models.backbones.swin.WindowMSA'>}
[Failed] {<class 'mmengine.model.base_module.ModuleList'>, <class 'torch.nn.modules.activation.Softmax'>, <class 'mmdet.models.backbones.swin.SwinTransformer'>, <class 'mmdet.models.layers.transformer.utils.PatchEmbed'>, <class 'mmdet.models.backbones.swin.SwinBlock'>, <class 'mmengine.model.base_module.Sequential'>, <class 'mmdet.models.layers.transformer.utils.AdaptivePadding'>, <class 'mmdet.models.backbones.swin.ShiftWindowMSA'>, <class 'torch.nn.modules.activation.GELU'>, <class 'mmcv.cnn.bricks.transformer.FFN'>, <class 'mmdet.models.backbones.swin.SwinBlockSequence'>, <class 'torch.nn.modules.fold.Unfold'>, <class 'mmcv.cnn.bricks.drop.DropPath'>, <class 'torch.nn.modules.normalization.LayerNorm'>, <class 'torch.nn.modules.linear.Linear'>, <class 'torch.nn.modules.dropout.Dropout'>, <class 'torch.nn.modules.linear.Identity'>}
self.IGNORED_LAYERS_IN_TRACING =  [AdaptivePadding(), Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2)), LayerNorm((384,), eps=1e-05, elementwise_affine=True), Linear(in_features=384, out_features=192, bias=False), AdaptivePadding(), Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2)), LayerNorm((768,), eps=1e-05, elementwise_affine=True), Linear(in_features=768, out_features=384, bias=False), AdaptivePadding(), Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2)), LayerNorm((1536,), eps=1e-05, elementwise_affine=True), Linear(in_features=1536, out_features=768, bias=False), Linear(in_features=96, out_features=288, bias=True), Dropout(p=0.0, inplace=False), Linear(in_features=96, out_features=96, bias=True), Dropout(p=0.0, inplace=False), Softmax(dim=-1), Linear(in_features=96, out_features=288, bias=True), Dropout(p=0.0, inplace=False), Linear(in_features=96, out_features=96, bias=True), Dropout(p=0.0, inplace=False), Softmax(dim=-1), Linear(in_features=192, out_features=576, bias=True), Dropout(p=0.0, inplace=False), Linear(in_features=192, out_features=192, bias=True), Dropout(p=0.0, inplace=False), Softmax(dim=-1), Linear(in_features=192, out_features=576, bias=True), Dropout(p=0.0, inplace=False), Linear(in_features=192, out_features=192, bias=True), Dropout(p=0.0, inplace=False), Softmax(dim=-1), Linear(in_features=384, out_features=1152, bias=True), Dropout(p=0.0, inplace=False), Linear(in_features=384, out_features=384, bias=True), Dropout(p=0.0, inplace=False), Softmax(dim=-1), Linear(in_features=384, out_features=1152, bias=True), Dropout(p=0.0, inplace=False), Linear(in_features=384, out_features=384, bias=True), Dropout(p=0.0, inplace=False), Softmax(dim=-1), Linear(in_features=384, out_features=1152, bias=True), Dropout(p=0.0, inplace=False), Linear(in_features=384, out_features=384, bias=True), Dropout(p=0.0, inplace=False), Softmax(dim=-1), Linear(in_features=384, out_features=1152, bias=True), Dropout(p=0.0, inplace=False), Linear(in_features=384, out_features=384, bias=True), Dropout(p=0.0, inplace=False), Softmax(dim=-1), Linear(in_features=384, out_features=1152, bias=True), Dropout(p=0.0, inplace=False), Linear(in_features=384, out_features=384, bias=True), Dropout(p=0.0, inplace=False), Softmax(dim=-1), Linear(in_features=384, out_features=1152, bias=True), Dropout(p=0.0, inplace=False), Linear(in_features=384, out_features=384, bias=True), Dropout(p=0.0, inplace=False), Softmax(dim=-1), Linear(in_features=768, out_features=2304, bias=True), Dropout(p=0.0, inplace=False), Linear(in_features=768, out_features=768, bias=True), Dropout(p=0.0, inplace=False), Softmax(dim=-1), Linear(in_features=768, out_features=2304, bias=True), Dropout(p=0.0, inplace=False), Linear(in_features=768, out_features=768, bias=True), Dropout(p=0.0, inplace=False), Softmax(dim=-1)]
registered_types =  (<class 'torch.nn.modules.conv._ConvNd'>, <class 'torch.nn.modules.linear.Linear'>, <class 'torch.nn.modules.batchnorm._BatchNorm'>, <class 'torch.nn.modules.conv._ConvNd'>, <class 'torch.nn.modules.activation.PReLU'>, <class 'torch.nn.modules.normalization.LayerNorm'>, <class 'torch.nn.modules.sparse.Embedding'>, <class 'torch.nn.parameter.Parameter'>, <class 'torch.nn.modules.activation.MultiheadAttention'>, <class 'torch.nn.modules.rnn.LSTM'>, <class 'torch.nn.modules.normalization.GroupNorm'>, <class 'torch.nn.modules.instancenorm._InstanceNorm'>, <class 'torch_pruning.ops._ConcatOp'>, <class 'torch_pruning.ops._SplitOp'>, <class 'torch_pruning.ops._ElementWiseOp'>, <class 'torch_pruning.ops._ElementWiseOp'>, <class 'torch_pruning.ops._ElementWiseOp'>, <class 'torch_pruning.ops._CustomizedOp'>, <class 'mmdet.models.layers.transformer.utils.PatchMerging'>, <class 'mmdet.models.backbones.swin.WindowMSA'>)
len hooks =  68
pre forward - gradfn2module =  0 dict_keys([])
-----NEW SwinTransformer FORWARD PASS 



		SwinTransformer input shape:  torch.Size([3, 800, 1333])


_record_grad_fn <class 'torch.nn.modules.conv.Conv2d'>, True, grad_fn: <SqueezeBackward1 object at 0x7f5c19b8df10>
-----NEW SwinTransformer FORWARD PASS 



		SwinTransformer input shape:  torch.Size([1, 3, 800, 1333])


_record_grad_fn <class 'torch.nn.modules.conv.Conv2d'>, True, grad_fn: <ConvolutionBackward0 object at 0x7f5c19b8df40>
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f5c19b8df70>
------------SwinBlockSequence -  torch.Size([1, 66800, 96])
with_cp =  False
x.requires_grad =  True
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f5c19b87040>
ShiftWindowMSA = query view: B, L, C=  1 66800 96
	 hw_shape =  (200, 334)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([1392, 49, 96])
B=1392, N=49, C=96
the qkv mod: 96 288 Linear(in_features=96, out_features=288, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([1392, 49, 288])
NEW C= 96
qkv.shape =  torch.Size([1392, 49, 288])
qkv.flatten.shape =  torch.Size([19643904])
self.embed_dims =  96
+++++ reshaping ....  1392 49 3 self.num_heads= 3 32
1392 49 3 3 32  =  19643904
self.num_heads =  3
C =  96
qkv after reshape  torch.Size([1392, 49, 3, 3, 32])
qkv after permute  torch.Size([3, 1392, 3, 49, 32])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f5c19b8dfd0>
--proj input shape =  torch.Size([1392, 49, 96])
--proj output shape =  torch.Size([1392, 49, 96])
------------WindowMSA - x_shape Output torch.Size([1392, 49, 96]) <UnsafeViewBackward0 object at 0x7f5c19b8dfd0>
_record_grad_fn <class 'mmdet.models.backbones.swin.WindowMSA'>, True, grad_fn: <UnsafeViewBackward0 object at 0x7f5c19b87070>
attn_windows.shape before merge =  torch.Size([1392, 49, 96])
x.shape before view =  torch.Size([1, 200, 334, 96])
ShiftWindowMSA = before view B, H * W, C =  1 66800 96
------------ShiftWindowMSA -  <ViewBackward0 object at 0x7f5c19b8dfd0>
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f5c19b870a0>
_record_grad_fn <class 'mmcv.cnn.bricks.wrappers.Linear'>, True, grad_fn: <ViewBackward0 object at 0x7f5c19b870d0>
_record_grad_fn <class 'mmcv.cnn.bricks.wrappers.Linear'>, True, grad_fn: <ViewBackward0 object at 0x7f5c19b87100>
------------SwinBlock -  <AddBackward0 object at 0x7f5c19b8dfd0>
		block output:  torch.Size([1, 66800, 96])
------------SwinBlockSequence -  torch.Size([1, 66800, 96])
with_cp =  False
x.requires_grad =  True
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f5c19b87130>
ShiftWindowMSA = query view: B, L, C=  1 66800 96
	 hw_shape =  (200, 334)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([1392, 49, 96])
B=1392, N=49, C=96
the qkv mod: 96 288 Linear(in_features=96, out_features=288, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([1392, 49, 288])
NEW C= 96
qkv.shape =  torch.Size([1392, 49, 288])
qkv.flatten.shape =  torch.Size([19643904])
self.embed_dims =  96
+++++ reshaping ....  1392 49 3 self.num_heads= 3 32
1392 49 3 3 32  =  19643904
self.num_heads =  3
C =  96
qkv after reshape  torch.Size([1392, 49, 3, 3, 32])
qkv after permute  torch.Size([3, 1392, 3, 49, 32])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f5c19b8dfd0>
--proj input shape =  torch.Size([1392, 49, 96])
--proj output shape =  torch.Size([1392, 49, 96])
------------WindowMSA - x_shape Output torch.Size([1392, 49, 96]) <UnsafeViewBackward0 object at 0x7f5c19b8dfd0>
_record_grad_fn <class 'mmdet.models.backbones.swin.WindowMSA'>, True, grad_fn: <UnsafeViewBackward0 object at 0x7f5c19b87160>
attn_windows.shape before merge =  torch.Size([1392, 49, 96])
x.shape before view =  torch.Size([1, 200, 334, 96])
ShiftWindowMSA = before view B, H * W, C =  1 66800 96
------------ShiftWindowMSA -  <ViewBackward0 object at 0x7f5c19b8dfd0>
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f5c19b87190>
_record_grad_fn <class 'mmcv.cnn.bricks.wrappers.Linear'>, True, grad_fn: <ViewBackward0 object at 0x7f5c19b871c0>
_record_grad_fn <class 'mmcv.cnn.bricks.wrappers.Linear'>, True, grad_fn: <ViewBackward0 object at 0x7f5c19b871f0>
------------SwinBlock -  <AddBackward0 object at 0x7f5c19b8dfd0>
		block output:  torch.Size([1, 66800, 96])
_record_grad_fn <class 'mmdet.models.layers.transformer.utils.PatchMerging'>, True, grad_fn: <UnsafeViewBackward0 object at 0x7f5c19b8dfa0>
------------SwinBlockSequence/downsample -  torch.Size([1, 16700, 192]) <UnsafeViewBackward0 object at 0x7f5c19b8dfa0>
down_hw_shape =  (100, 167)
x.shape =  torch.Size([1, 16700, 192])
hw_shape =  (100, 167)
stage out.shape =  torch.Size([1, 66800, 96])
out_hw_shape =  (200, 334)
stage =  0  / self.out_indices =  (1, 2, 3)
self.num_features =  [96, 192, 384, 768]
------------SwinBlockSequence -  torch.Size([1, 16700, 192])
with_cp =  False
x.requires_grad =  True
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f5c19b87250>
ShiftWindowMSA = query view: B, L, C=  1 16700 192
	 hw_shape =  (100, 167)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([360, 49, 192])
B=360, N=49, C=192
the qkv mod: 192 576 Linear(in_features=192, out_features=576, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([360, 49, 576])
NEW C= 192
qkv.shape =  torch.Size([360, 49, 576])
qkv.flatten.shape =  torch.Size([10160640])
self.embed_dims =  192
+++++ reshaping ....  360 49 3 self.num_heads= 6 32
360 49 3 6 32  =  10160640
self.num_heads =  6
C =  192
qkv after reshape  torch.Size([360, 49, 3, 6, 32])
qkv after permute  torch.Size([3, 360, 6, 49, 32])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f5c19b871f0>
--proj input shape =  torch.Size([360, 49, 192])
--proj output shape =  torch.Size([360, 49, 192])
------------WindowMSA - x_shape Output torch.Size([360, 49, 192]) <UnsafeViewBackward0 object at 0x7f5c19b871f0>
_record_grad_fn <class 'mmdet.models.backbones.swin.WindowMSA'>, True, grad_fn: <UnsafeViewBackward0 object at 0x7f5c19b87280>
attn_windows.shape before merge =  torch.Size([360, 49, 192])
x.shape before view =  torch.Size([1, 100, 167, 192])
ShiftWindowMSA = before view B, H * W, C =  1 16700 192
------------ShiftWindowMSA -  <ViewBackward0 object at 0x7f5c19b871f0>
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f5c19b872b0>
_record_grad_fn <class 'mmcv.cnn.bricks.wrappers.Linear'>, True, grad_fn: <ViewBackward0 object at 0x7f5c19b872e0>
_record_grad_fn <class 'mmcv.cnn.bricks.wrappers.Linear'>, True, grad_fn: <ViewBackward0 object at 0x7f5c19b87310>
------------SwinBlock -  <AddBackward0 object at 0x7f5c19b871f0>
		block output:  torch.Size([1, 16700, 192])
------------SwinBlockSequence -  torch.Size([1, 16700, 192])
with_cp =  False
x.requires_grad =  True
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f5c19b87340>
ShiftWindowMSA = query view: B, L, C=  1 16700 192
	 hw_shape =  (100, 167)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([360, 49, 192])
B=360, N=49, C=192
the qkv mod: 192 576 Linear(in_features=192, out_features=576, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([360, 49, 576])
NEW C= 192
qkv.shape =  torch.Size([360, 49, 576])
qkv.flatten.shape =  torch.Size([10160640])
self.embed_dims =  192
+++++ reshaping ....  360 49 3 self.num_heads= 6 32
360 49 3 6 32  =  10160640
self.num_heads =  6
C =  192
qkv after reshape  torch.Size([360, 49, 3, 6, 32])
qkv after permute  torch.Size([3, 360, 6, 49, 32])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f5c19b871f0>
--proj input shape =  torch.Size([360, 49, 192])
--proj output shape =  torch.Size([360, 49, 192])
------------WindowMSA - x_shape Output torch.Size([360, 49, 192]) <UnsafeViewBackward0 object at 0x7f5c19b871f0>
_record_grad_fn <class 'mmdet.models.backbones.swin.WindowMSA'>, True, grad_fn: <UnsafeViewBackward0 object at 0x7f5c19b87370>
attn_windows.shape before merge =  torch.Size([360, 49, 192])
x.shape before view =  torch.Size([1, 100, 167, 192])
ShiftWindowMSA = before view B, H * W, C =  1 16700 192
------------ShiftWindowMSA -  <ViewBackward0 object at 0x7f5c19b871f0>
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f5c19b873a0>
_record_grad_fn <class 'mmcv.cnn.bricks.wrappers.Linear'>, True, grad_fn: <ViewBackward0 object at 0x7f5c19b873d0>
_record_grad_fn <class 'mmcv.cnn.bricks.wrappers.Linear'>, True, grad_fn: <ViewBackward0 object at 0x7f5c19b87400>
------------SwinBlock -  <AddBackward0 object at 0x7f5c19b871f0>
		block output:  torch.Size([1, 16700, 192])
_record_grad_fn <class 'mmdet.models.layers.transformer.utils.PatchMerging'>, True, grad_fn: <UnsafeViewBackward0 object at 0x7f5c19b8dfd0>
------------SwinBlockSequence/downsample -  torch.Size([1, 4200, 384]) <UnsafeViewBackward0 object at 0x7f5c19b8dfd0>
down_hw_shape =  (50, 84)
x.shape =  torch.Size([1, 4200, 384])
hw_shape =  (50, 84)
stage out.shape =  torch.Size([1, 16700, 192])
out_hw_shape =  (100, 167)
stage =  1  / self.out_indices =  (1, 2, 3)
self.num_features =  [96, 192, 384, 768]
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f5c19b871f0>
stage:  1  --- norm_layer out.shape =  torch.Size([1, 16700, 192])
torch.Size([1, 192, 100, 167])
------------SwinBlockSequence -  torch.Size([1, 4200, 384])
with_cp =  False
x.requires_grad =  True
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f5c19b87490>
ShiftWindowMSA = query view: B, L, C=  1 4200 384
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 384])
B=96, N=49, C=384
the qkv mod: 384 1152 Linear(in_features=384, out_features=1152, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 1152])
NEW C= 384
qkv.shape =  torch.Size([96, 49, 1152])
qkv.flatten.shape =  torch.Size([5419008])
self.embed_dims =  384
+++++ reshaping ....  96 49 3 self.num_heads= 12 32
96 49 3 12 32  =  5419008
self.num_heads =  12
C =  384
qkv after reshape  torch.Size([96, 49, 3, 12, 32])
qkv after permute  torch.Size([3, 96, 12, 49, 32])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f5c19b87460>
--proj input shape =  torch.Size([96, 49, 384])
--proj output shape =  torch.Size([96, 49, 384])
------------WindowMSA - x_shape Output torch.Size([96, 49, 384]) <UnsafeViewBackward0 object at 0x7f5c19b87460>
_record_grad_fn <class 'mmdet.models.backbones.swin.WindowMSA'>, True, grad_fn: <UnsafeViewBackward0 object at 0x7f5c19b874c0>
attn_windows.shape before merge =  torch.Size([96, 49, 384])
x.shape before view =  torch.Size([1, 50, 84, 384])
ShiftWindowMSA = before view B, H * W, C =  1 4200 384
------------ShiftWindowMSA -  <ViewBackward0 object at 0x7f5c19b87460>
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f5c19b874f0>
_record_grad_fn <class 'mmcv.cnn.bricks.wrappers.Linear'>, True, grad_fn: <ViewBackward0 object at 0x7f5c19b87520>
_record_grad_fn <class 'mmcv.cnn.bricks.wrappers.Linear'>, True, grad_fn: <ViewBackward0 object at 0x7f5c19b87550>
------------SwinBlock -  <AddBackward0 object at 0x7f5c19b87460>
		block output:  torch.Size([1, 4200, 384])
------------SwinBlockSequence -  torch.Size([1, 4200, 384])
with_cp =  False
x.requires_grad =  True
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f5c19b87580>
ShiftWindowMSA = query view: B, L, C=  1 4200 384
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 384])
B=96, N=49, C=384
the qkv mod: 384 1152 Linear(in_features=384, out_features=1152, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 1152])
NEW C= 384
qkv.shape =  torch.Size([96, 49, 1152])
qkv.flatten.shape =  torch.Size([5419008])
self.embed_dims =  384
+++++ reshaping ....  96 49 3 self.num_heads= 12 32
96 49 3 12 32  =  5419008
self.num_heads =  12
C =  384
qkv after reshape  torch.Size([96, 49, 3, 12, 32])
qkv after permute  torch.Size([3, 96, 12, 49, 32])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f5c19b87460>
--proj input shape =  torch.Size([96, 49, 384])
--proj output shape =  torch.Size([96, 49, 384])
------------WindowMSA - x_shape Output torch.Size([96, 49, 384]) <UnsafeViewBackward0 object at 0x7f5c19b87460>
_record_grad_fn <class 'mmdet.models.backbones.swin.WindowMSA'>, True, grad_fn: <UnsafeViewBackward0 object at 0x7f5c19b875b0>
attn_windows.shape before merge =  torch.Size([96, 49, 384])
x.shape before view =  torch.Size([1, 50, 84, 384])
ShiftWindowMSA = before view B, H * W, C =  1 4200 384
------------ShiftWindowMSA -  <ViewBackward0 object at 0x7f5c19b87460>
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f5c19b875e0>
_record_grad_fn <class 'mmcv.cnn.bricks.wrappers.Linear'>, True, grad_fn: <ViewBackward0 object at 0x7f5c19b87610>
_record_grad_fn <class 'mmcv.cnn.bricks.wrappers.Linear'>, True, grad_fn: <ViewBackward0 object at 0x7f5c19b87640>
------------SwinBlock -  <AddBackward0 object at 0x7f5c19b87460>
		block output:  torch.Size([1, 4200, 384])
------------SwinBlockSequence -  torch.Size([1, 4200, 384])
with_cp =  False
x.requires_grad =  True
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f5c19b87670>
ShiftWindowMSA = query view: B, L, C=  1 4200 384
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 384])
B=96, N=49, C=384
the qkv mod: 384 1152 Linear(in_features=384, out_features=1152, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 1152])
NEW C= 384
qkv.shape =  torch.Size([96, 49, 1152])
qkv.flatten.shape =  torch.Size([5419008])
self.embed_dims =  384
+++++ reshaping ....  96 49 3 self.num_heads= 12 32
96 49 3 12 32  =  5419008
self.num_heads =  12
C =  384
qkv after reshape  torch.Size([96, 49, 3, 12, 32])
qkv after permute  torch.Size([3, 96, 12, 49, 32])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f5c19b87460>
--proj input shape =  torch.Size([96, 49, 384])
--proj output shape =  torch.Size([96, 49, 384])
------------WindowMSA - x_shape Output torch.Size([96, 49, 384]) <UnsafeViewBackward0 object at 0x7f5c19b87460>
_record_grad_fn <class 'mmdet.models.backbones.swin.WindowMSA'>, True, grad_fn: <UnsafeViewBackward0 object at 0x7f5c19b876a0>
attn_windows.shape before merge =  torch.Size([96, 49, 384])
x.shape before view =  torch.Size([1, 50, 84, 384])
ShiftWindowMSA = before view B, H * W, C =  1 4200 384
------------ShiftWindowMSA -  <ViewBackward0 object at 0x7f5c19b87460>
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f5c19b876d0>
_record_grad_fn <class 'mmcv.cnn.bricks.wrappers.Linear'>, True, grad_fn: <ViewBackward0 object at 0x7f5c19b87700>
_record_grad_fn <class 'mmcv.cnn.bricks.wrappers.Linear'>, True, grad_fn: <ViewBackward0 object at 0x7f5c19b87730>
------------SwinBlock -  <AddBackward0 object at 0x7f5c19b87460>
		block output:  torch.Size([1, 4200, 384])
------------SwinBlockSequence -  torch.Size([1, 4200, 384])
with_cp =  False
x.requires_grad =  True
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f5c19b87760>
ShiftWindowMSA = query view: B, L, C=  1 4200 384
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 384])
B=96, N=49, C=384
the qkv mod: 384 1152 Linear(in_features=384, out_features=1152, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 1152])
NEW C= 384
qkv.shape =  torch.Size([96, 49, 1152])
qkv.flatten.shape =  torch.Size([5419008])
self.embed_dims =  384
+++++ reshaping ....  96 49 3 self.num_heads= 12 32
96 49 3 12 32  =  5419008
self.num_heads =  12
C =  384
qkv after reshape  torch.Size([96, 49, 3, 12, 32])
qkv after permute  torch.Size([3, 96, 12, 49, 32])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f5c19b87460>
--proj input shape =  torch.Size([96, 49, 384])
--proj output shape =  torch.Size([96, 49, 384])
------------WindowMSA - x_shape Output torch.Size([96, 49, 384]) <UnsafeViewBackward0 object at 0x7f5c19b87460>
_record_grad_fn <class 'mmdet.models.backbones.swin.WindowMSA'>, True, grad_fn: <UnsafeViewBackward0 object at 0x7f5c19b87790>
attn_windows.shape before merge =  torch.Size([96, 49, 384])
x.shape before view =  torch.Size([1, 50, 84, 384])
ShiftWindowMSA = before view B, H * W, C =  1 4200 384
------------ShiftWindowMSA -  <ViewBackward0 object at 0x7f5c19b87460>
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f5c19b877c0>
_record_grad_fn <class 'mmcv.cnn.bricks.wrappers.Linear'>, True, grad_fn: <ViewBackward0 object at 0x7f5c19b877f0>
_record_grad_fn <class 'mmcv.cnn.bricks.wrappers.Linear'>, True, grad_fn: <ViewBackward0 object at 0x7f5c19b87820>
------------SwinBlock -  <AddBackward0 object at 0x7f5c19b87460>
		block output:  torch.Size([1, 4200, 384])
------------SwinBlockSequence -  torch.Size([1, 4200, 384])
with_cp =  False
x.requires_grad =  True
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f5c19b87850>
ShiftWindowMSA = query view: B, L, C=  1 4200 384
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 384])
B=96, N=49, C=384
the qkv mod: 384 1152 Linear(in_features=384, out_features=1152, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 1152])
NEW C= 384
qkv.shape =  torch.Size([96, 49, 1152])
qkv.flatten.shape =  torch.Size([5419008])
self.embed_dims =  384
+++++ reshaping ....  96 49 3 self.num_heads= 12 32
96 49 3 12 32  =  5419008
self.num_heads =  12
C =  384
qkv after reshape  torch.Size([96, 49, 3, 12, 32])
qkv after permute  torch.Size([3, 96, 12, 49, 32])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f5c19b87460>
--proj input shape =  torch.Size([96, 49, 384])
--proj output shape =  torch.Size([96, 49, 384])
------------WindowMSA - x_shape Output torch.Size([96, 49, 384]) <UnsafeViewBackward0 object at 0x7f5c19b87460>
_record_grad_fn <class 'mmdet.models.backbones.swin.WindowMSA'>, True, grad_fn: <UnsafeViewBackward0 object at 0x7f5c19b87880>
attn_windows.shape before merge =  torch.Size([96, 49, 384])
x.shape before view =  torch.Size([1, 50, 84, 384])
ShiftWindowMSA = before view B, H * W, C =  1 4200 384
------------ShiftWindowMSA -  <ViewBackward0 object at 0x7f5c19b87460>
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f5c19b878b0>
_record_grad_fn <class 'mmcv.cnn.bricks.wrappers.Linear'>, True, grad_fn: <ViewBackward0 object at 0x7f5c19b878e0>
_record_grad_fn <class 'mmcv.cnn.bricks.wrappers.Linear'>, True, grad_fn: <ViewBackward0 object at 0x7f5c19b87910>
------------SwinBlock -  <AddBackward0 object at 0x7f5c19b87460>
		block output:  torch.Size([1, 4200, 384])
------------SwinBlockSequence -  torch.Size([1, 4200, 384])
with_cp =  False
x.requires_grad =  True
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f5c19b87940>
ShiftWindowMSA = query view: B, L, C=  1 4200 384
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 384])
B=96, N=49, C=384
the qkv mod: 384 1152 Linear(in_features=384, out_features=1152, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 1152])
NEW C= 384
qkv.shape =  torch.Size([96, 49, 1152])
qkv.flatten.shape =  torch.Size([5419008])
self.embed_dims =  384
+++++ reshaping ....  96 49 3 self.num_heads= 12 32
96 49 3 12 32  =  5419008
self.num_heads =  12
C =  384
qkv after reshape  torch.Size([96, 49, 3, 12, 32])
qkv after permute  torch.Size([3, 96, 12, 49, 32])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f5c19b87460>
--proj input shape =  torch.Size([96, 49, 384])
--proj output shape =  torch.Size([96, 49, 384])
------------WindowMSA - x_shape Output torch.Size([96, 49, 384]) <UnsafeViewBackward0 object at 0x7f5c19b87460>
_record_grad_fn <class 'mmdet.models.backbones.swin.WindowMSA'>, True, grad_fn: <UnsafeViewBackward0 object at 0x7f5c19b87970>
attn_windows.shape before merge =  torch.Size([96, 49, 384])
x.shape before view =  torch.Size([1, 50, 84, 384])
ShiftWindowMSA = before view B, H * W, C =  1 4200 384
------------ShiftWindowMSA -  <ViewBackward0 object at 0x7f5c19b87460>
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f5c19b879a0>
_record_grad_fn <class 'mmcv.cnn.bricks.wrappers.Linear'>, True, grad_fn: <ViewBackward0 object at 0x7f5c19b879d0>
_record_grad_fn <class 'mmcv.cnn.bricks.wrappers.Linear'>, True, grad_fn: <ViewBackward0 object at 0x7f5c19b87a00>
------------SwinBlock -  <AddBackward0 object at 0x7f5c19b87460>
		block output:  torch.Size([1, 4200, 384])
_record_grad_fn <class 'mmdet.models.layers.transformer.utils.PatchMerging'>, True, grad_fn: <UnsafeViewBackward0 object at 0x7f5c19b87430>
------------SwinBlockSequence/downsample -  torch.Size([1, 1050, 768]) <UnsafeViewBackward0 object at 0x7f5c19b87430>
down_hw_shape =  (25, 42)
x.shape =  torch.Size([1, 1050, 768])
hw_shape =  (25, 42)
stage out.shape =  torch.Size([1, 4200, 384])
out_hw_shape =  (50, 84)
stage =  2  / self.out_indices =  (1, 2, 3)
self.num_features =  [96, 192, 384, 768]
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f5c19b87460>
stage:  2  --- norm_layer out.shape =  torch.Size([1, 4200, 384])
torch.Size([1, 384, 50, 84])
------------SwinBlockSequence -  torch.Size([1, 1050, 768])
with_cp =  False
x.requires_grad =  True
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f5c19b87a90>
ShiftWindowMSA = query view: B, L, C=  1 1050 768
	 hw_shape =  (25, 42)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([24, 49, 768])
B=24, N=49, C=768
the qkv mod: 768 2304 Linear(in_features=768, out_features=2304, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([24, 49, 2304])
NEW C= 768
qkv.shape =  torch.Size([24, 49, 2304])
qkv.flatten.shape =  torch.Size([2709504])
self.embed_dims =  768
+++++ reshaping ....  24 49 3 self.num_heads= 24 32
24 49 3 24 32  =  2709504
self.num_heads =  24
C =  768
qkv after reshape  torch.Size([24, 49, 3, 24, 32])
qkv after permute  torch.Size([3, 24, 24, 49, 32])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f5c19b87a60>
--proj input shape =  torch.Size([24, 49, 768])
--proj output shape =  torch.Size([24, 49, 768])
------------WindowMSA - x_shape Output torch.Size([24, 49, 768]) <UnsafeViewBackward0 object at 0x7f5c19b87a60>
_record_grad_fn <class 'mmdet.models.backbones.swin.WindowMSA'>, True, grad_fn: <UnsafeViewBackward0 object at 0x7f5c19b87ac0>
attn_windows.shape before merge =  torch.Size([24, 49, 768])
x.shape before view =  torch.Size([1, 25, 42, 768])
ShiftWindowMSA = before view B, H * W, C =  1 1050 768
------------ShiftWindowMSA -  <ViewBackward0 object at 0x7f5c19b87a60>
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f5c19b87af0>
_record_grad_fn <class 'mmcv.cnn.bricks.wrappers.Linear'>, True, grad_fn: <ViewBackward0 object at 0x7f5c19b87b20>
_record_grad_fn <class 'mmcv.cnn.bricks.wrappers.Linear'>, True, grad_fn: <ViewBackward0 object at 0x7f5c19b87b50>
------------SwinBlock -  <AddBackward0 object at 0x7f5c19b87a60>
		block output:  torch.Size([1, 1050, 768])
------------SwinBlockSequence -  torch.Size([1, 1050, 768])
with_cp =  False
x.requires_grad =  True
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f5c19b87b80>
ShiftWindowMSA = query view: B, L, C=  1 1050 768
	 hw_shape =  (25, 42)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([24, 49, 768])
B=24, N=49, C=768
the qkv mod: 768 2304 Linear(in_features=768, out_features=2304, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([24, 49, 2304])
NEW C= 768
qkv.shape =  torch.Size([24, 49, 2304])
qkv.flatten.shape =  torch.Size([2709504])
self.embed_dims =  768
+++++ reshaping ....  24 49 3 self.num_heads= 24 32
24 49 3 24 32  =  2709504
self.num_heads =  24
C =  768
qkv after reshape  torch.Size([24, 49, 3, 24, 32])
qkv after permute  torch.Size([3, 24, 24, 49, 32])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f5c19b87a60>
--proj input shape =  torch.Size([24, 49, 768])
--proj output shape =  torch.Size([24, 49, 768])
------------WindowMSA - x_shape Output torch.Size([24, 49, 768]) <UnsafeViewBackward0 object at 0x7f5c19b87a60>
_record_grad_fn <class 'mmdet.models.backbones.swin.WindowMSA'>, True, grad_fn: <UnsafeViewBackward0 object at 0x7f5c19b87bb0>
attn_windows.shape before merge =  torch.Size([24, 49, 768])
x.shape before view =  torch.Size([1, 25, 42, 768])
ShiftWindowMSA = before view B, H * W, C =  1 1050 768
------------ShiftWindowMSA -  <ViewBackward0 object at 0x7f5c19b87a60>
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f5c19b87be0>
_record_grad_fn <class 'mmcv.cnn.bricks.wrappers.Linear'>, True, grad_fn: <ViewBackward0 object at 0x7f5c19b87c10>
_record_grad_fn <class 'mmcv.cnn.bricks.wrappers.Linear'>, True, grad_fn: <ViewBackward0 object at 0x7f5c19b87c40>
------------SwinBlock -  <AddBackward0 object at 0x7f5c19b87a60>
		block output:  torch.Size([1, 1050, 768])
x.shape =  torch.Size([1, 1050, 768])
hw_shape =  (25, 42)
stage out.shape =  torch.Size([1, 1050, 768])
out_hw_shape =  (25, 42)
stage =  3  / self.out_indices =  (1, 2, 3)
self.num_features =  [96, 192, 384, 768]
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f5c19b87a30>
stage:  3  --- norm_layer out.shape =  torch.Size([1, 1050, 768])
torch.Size([1, 768, 25, 42])
------------SwinTransformer -  [<CloneBackward0 object at 0x7f5c19b87a60>, <CloneBackward0 object at 0x7f5c19b87c70>, <CloneBackward0 object at 0x7f5c19b87ca0>]




visited =  {Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4)): 2, LayerNorm((96,), eps=1e-05, elementwise_affine=True): 1, LayerNorm((96,), eps=1e-05, elementwise_affine=True): 1, WindowMSA(
  (qkv): Linear(in_features=96, out_features=288, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=96, out_features=96, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
): 1, LayerNorm((96,), eps=1e-05, elementwise_affine=True): 1, Linear(in_features=96, out_features=384, bias=True): 1, Linear(in_features=384, out_features=96, bias=True): 1, LayerNorm((96,), eps=1e-05, elementwise_affine=True): 1, WindowMSA(
  (qkv): Linear(in_features=96, out_features=288, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=96, out_features=96, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
): 1, LayerNorm((96,), eps=1e-05, elementwise_affine=True): 1, Linear(in_features=96, out_features=384, bias=True): 1, Linear(in_features=384, out_features=96, bias=True): 1, PatchMerging(
  (adap_padding): AdaptivePadding()
  (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
  (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (reduction): Linear(in_features=384, out_features=192, bias=False)
): 1, LayerNorm((192,), eps=1e-05, elementwise_affine=True): 1, WindowMSA(
  (qkv): Linear(in_features=192, out_features=576, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=192, out_features=192, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
): 1, LayerNorm((192,), eps=1e-05, elementwise_affine=True): 1, Linear(in_features=192, out_features=768, bias=True): 1, Linear(in_features=768, out_features=192, bias=True): 1, LayerNorm((192,), eps=1e-05, elementwise_affine=True): 1, WindowMSA(
  (qkv): Linear(in_features=192, out_features=576, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=192, out_features=192, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
): 1, LayerNorm((192,), eps=1e-05, elementwise_affine=True): 1, Linear(in_features=192, out_features=768, bias=True): 1, Linear(in_features=768, out_features=192, bias=True): 1, PatchMerging(
  (adap_padding): AdaptivePadding()
  (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (reduction): Linear(in_features=768, out_features=384, bias=False)
): 1, LayerNorm((192,), eps=1e-05, elementwise_affine=True): 1, LayerNorm((384,), eps=1e-05, elementwise_affine=True): 1, WindowMSA(
  (qkv): Linear(in_features=384, out_features=1152, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=384, out_features=384, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
): 1, LayerNorm((384,), eps=1e-05, elementwise_affine=True): 1, Linear(in_features=384, out_features=1536, bias=True): 1, Linear(in_features=1536, out_features=384, bias=True): 1, LayerNorm((384,), eps=1e-05, elementwise_affine=True): 1, WindowMSA(
  (qkv): Linear(in_features=384, out_features=1152, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=384, out_features=384, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
): 1, LayerNorm((384,), eps=1e-05, elementwise_affine=True): 1, Linear(in_features=384, out_features=1536, bias=True): 1, Linear(in_features=1536, out_features=384, bias=True): 1, LayerNorm((384,), eps=1e-05, elementwise_affine=True): 1, WindowMSA(
  (qkv): Linear(in_features=384, out_features=1152, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=384, out_features=384, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
): 1, LayerNorm((384,), eps=1e-05, elementwise_affine=True): 1, Linear(in_features=384, out_features=1536, bias=True): 1, Linear(in_features=1536, out_features=384, bias=True): 1, LayerNorm((384,), eps=1e-05, elementwise_affine=True): 1, WindowMSA(
  (qkv): Linear(in_features=384, out_features=1152, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=384, out_features=384, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
): 1, LayerNorm((384,), eps=1e-05, elementwise_affine=True): 1, Linear(in_features=384, out_features=1536, bias=True): 1, Linear(in_features=1536, out_features=384, bias=True): 1, LayerNorm((384,), eps=1e-05, elementwise_affine=True): 1, WindowMSA(
  (qkv): Linear(in_features=384, out_features=1152, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=384, out_features=384, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
): 1, LayerNorm((384,), eps=1e-05, elementwise_affine=True): 1, Linear(in_features=384, out_features=1536, bias=True): 1, Linear(in_features=1536, out_features=384, bias=True): 1, LayerNorm((384,), eps=1e-05, elementwise_affine=True): 1, WindowMSA(
  (qkv): Linear(in_features=384, out_features=1152, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=384, out_features=384, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
): 1, LayerNorm((384,), eps=1e-05, elementwise_affine=True): 1, Linear(in_features=384, out_features=1536, bias=True): 1, Linear(in_features=1536, out_features=384, bias=True): 1, PatchMerging(
  (adap_padding): AdaptivePadding()
  (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
  (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (reduction): Linear(in_features=1536, out_features=768, bias=False)
): 1, LayerNorm((384,), eps=1e-05, elementwise_affine=True): 1, LayerNorm((768,), eps=1e-05, elementwise_affine=True): 1, WindowMSA(
  (qkv): Linear(in_features=768, out_features=2304, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=768, out_features=768, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
): 1, LayerNorm((768,), eps=1e-05, elementwise_affine=True): 1, Linear(in_features=768, out_features=3072, bias=True): 1, Linear(in_features=3072, out_features=768, bias=True): 1, LayerNorm((768,), eps=1e-05, elementwise_affine=True): 1, WindowMSA(
  (qkv): Linear(in_features=768, out_features=2304, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=768, out_features=768, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
): 1, LayerNorm((768,), eps=1e-05, elementwise_affine=True): 1, Linear(in_features=768, out_features=3072, bias=True): 1, Linear(in_features=3072, out_features=768, bias=True): 1, LayerNorm((768,), eps=1e-05, elementwise_affine=True): 1}


reused =  [Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))]

flattened_output =  <class 'torch.Tensor'>
flattened_output len =  3
flattened_output shapes =  [torch.Size([1, 192, 100, 167]), torch.Size([1, 384, 50, 84]), torch.Size([1, 768, 25, 42])]
grad_fn =  [<CloneBackward0 object at 0x7f5c19b87a60>, <CloneBackward0 object at 0x7f5c19b87c70>, <CloneBackward0 object at 0x7f5c19b87ca0>]
post forward - gradfn2module =  69 dict_keys([<SqueezeBackward1 object at 0x7f5c19b8df10>, <ConvolutionBackward0 object at 0x7f5c19b8df40>, <NativeLayerNormBackward0 object at 0x7f5c19b8df70>, <NativeLayerNormBackward0 object at 0x7f5c19b87040>, <UnsafeViewBackward0 object at 0x7f5c19b87070>, <NativeLayerNormBackward0 object at 0x7f5c19b870a0>, <ViewBackward0 object at 0x7f5c19b870d0>, <ViewBackward0 object at 0x7f5c19b87100>, <NativeLayerNormBackward0 object at 0x7f5c19b87130>, <UnsafeViewBackward0 object at 0x7f5c19b87160>, <NativeLayerNormBackward0 object at 0x7f5c19b87190>, <ViewBackward0 object at 0x7f5c19b871c0>, <ViewBackward0 object at 0x7f5c19b87220>, <UnsafeViewBackward0 object at 0x7f5c19b8dfa0>, <NativeLayerNormBackward0 object at 0x7f5c19b87250>, <UnsafeViewBackward0 object at 0x7f5c19b87280>, <NativeLayerNormBackward0 object at 0x7f5c19b872b0>, <ViewBackward0 object at 0x7f5c19b872e0>, <ViewBackward0 object at 0x7f5c19b87310>, <NativeLayerNormBackward0 object at 0x7f5c19b87340>, <UnsafeViewBackward0 object at 0x7f5c19b87370>, <NativeLayerNormBackward0 object at 0x7f5c19b873a0>, <ViewBackward0 object at 0x7f5c19b873d0>, <ViewBackward0 object at 0x7f5c19b87400>, <UnsafeViewBackward0 object at 0x7f5c19b8dfd0>, <NativeLayerNormBackward0 object at 0x7f5c19b871f0>, <NativeLayerNormBackward0 object at 0x7f5c19b87490>, <UnsafeViewBackward0 object at 0x7f5c19b874c0>, <NativeLayerNormBackward0 object at 0x7f5c19b874f0>, <ViewBackward0 object at 0x7f5c19b87520>, <ViewBackward0 object at 0x7f5c19b87550>, <NativeLayerNormBackward0 object at 0x7f5c19b87580>, <UnsafeViewBackward0 object at 0x7f5c19b875b0>, <NativeLayerNormBackward0 object at 0x7f5c19b875e0>, <ViewBackward0 object at 0x7f5c19b87610>, <ViewBackward0 object at 0x7f5c19b87640>, <NativeLayerNormBackward0 object at 0x7f5c19b87670>, <UnsafeViewBackward0 object at 0x7f5c19b876a0>, <NativeLayerNormBackward0 object at 0x7f5c19b876d0>, <ViewBackward0 object at 0x7f5c19b87700>, <ViewBackward0 object at 0x7f5c19b87730>, <NativeLayerNormBackward0 object at 0x7f5c19b87760>, <UnsafeViewBackward0 object at 0x7f5c19b87790>, <NativeLayerNormBackward0 object at 0x7f5c19b877c0>, <ViewBackward0 object at 0x7f5c19b877f0>, <ViewBackward0 object at 0x7f5c19b87820>, <NativeLayerNormBackward0 object at 0x7f5c19b87850>, <UnsafeViewBackward0 object at 0x7f5c19b87880>, <NativeLayerNormBackward0 object at 0x7f5c19b878b0>, <ViewBackward0 object at 0x7f5c19b878e0>, <ViewBackward0 object at 0x7f5c19b87910>, <NativeLayerNormBackward0 object at 0x7f5c19b87940>, <UnsafeViewBackward0 object at 0x7f5c19b87970>, <NativeLayerNormBackward0 object at 0x7f5c19b879a0>, <ViewBackward0 object at 0x7f5c19b879d0>, <ViewBackward0 object at 0x7f5c19b87a00>, <UnsafeViewBackward0 object at 0x7f5c19b87430>, <NativeLayerNormBackward0 object at 0x7f5c19b87460>, <NativeLayerNormBackward0 object at 0x7f5c19b87a90>, <UnsafeViewBackward0 object at 0x7f5c19b87ac0>, <NativeLayerNormBackward0 object at 0x7f5c19b87af0>, <ViewBackward0 object at 0x7f5c19b87b20>, <ViewBackward0 object at 0x7f5c19b87b50>, <NativeLayerNormBackward0 object at 0x7f5c19b87b80>, <UnsafeViewBackward0 object at 0x7f5c19b87bb0>, <NativeLayerNormBackward0 object at 0x7f5c19b87be0>, <ViewBackward0 object at 0x7f5c19b87c10>, <ViewBackward0 object at 0x7f5c19b87c40>, <NativeLayerNormBackward0 object at 0x7f5c19b87a30>])

 module2node ==  dict_values([<Node: (_ElementWiseOp_0(CloneBackward0))>, <Node: (_ElementWiseOp_1(PermuteBackward0))>, <Node: (_Reshape_2())>, <Node: (norm1 (LayerNorm((192,), eps=1e-05, elementwise_affine=True)))>, <Node: (_ElementWiseOp_3(AddBackward0))>, <Node: (_ElementWiseOp_4(AddBackward0))>, <Node: (stages.1.blocks.1.ffn.layers.1 (Linear(in_features=768, out_features=192, bias=True)))>, <Node: (_ElementWiseOp_5(AddmmBackward0))>, <Node: (_Reshape_6())>, <Node: (_ElementWiseOp_7(TBackward0))>, <Node: (_ElementWiseOp_8(GeluBackward0))>, <Node: (stages.1.blocks.1.ffn.layers.0.0 (Linear(in_features=192, out_features=768, bias=True)))>, <Node: (_ElementWiseOp_9(AddmmBackward0))>, <Node: (_Reshape_10())>, <Node: (_ElementWiseOp_11(TBackward0))>, <Node: (stages.1.blocks.1.norm2 (LayerNorm((192,), eps=1e-05, elementwise_affine=True)))>, <Node: (_Reshape_12())>, <Node: (_ElementWiseOp_13(AddBackward0))>, <Node: (_ElementWiseOp_14(AddBackward0))>, <Node: (stages.1.blocks.0.ffn.layers.1 (Linear(in_features=768, out_features=192, bias=True)))>, <Node: (_ElementWiseOp_15(AddmmBackward0))>, <Node: (_Reshape_16())>, <Node: (_ElementWiseOp_17(TBackward0))>, <Node: (_ElementWiseOp_18(GeluBackward0))>, <Node: (stages.1.blocks.0.ffn.layers.0.0 (Linear(in_features=192, out_features=768, bias=True)))>, <Node: (_ElementWiseOp_19(AddmmBackward0))>, <Node: (_Reshape_20())>, <Node: (_ElementWiseOp_21(TBackward0))>, <Node: (stages.1.blocks.0.norm2 (LayerNorm((192,), eps=1e-05, elementwise_affine=True)))>, <Node: (_Reshape_22())>, <Node: (stages.0.downsample (PatchMerging(
  (adap_padding): AdaptivePadding()
  (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
  (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (reduction): Linear(in_features=384, out_features=192, bias=False)
)))>, <Node: (_ElementWiseOp_23(MmBackward0))>, <Node: (_Reshape_24())>, <Node: (_ElementWiseOp_25(TBackward0))>, <Node: (_ElementWiseOp_26(NativeLayerNormBackward0))>, <Node: (_ElementWiseOp_27(TransposeBackward0))>, <Node: (_ElementWiseOp_28(Im2ColBackward0))>, <Node: (_ElementWiseOp_29(PermuteBackward0))>, <Node: (_Reshape_30())>, <Node: (_ElementWiseOp_31(AddBackward0))>, <Node: (_ElementWiseOp_32(AddBackward0))>, <Node: (stages.0.blocks.1.ffn.layers.1 (Linear(in_features=384, out_features=96, bias=True)))>, <Node: (_ElementWiseOp_33(AddmmBackward0))>, <Node: (_Reshape_34())>, <Node: (_ElementWiseOp_35(TBackward0))>, <Node: (_ElementWiseOp_36(GeluBackward0))>, <Node: (stages.0.blocks.1.ffn.layers.0.0 (Linear(in_features=96, out_features=384, bias=True)))>, <Node: (_ElementWiseOp_37(AddmmBackward0))>, <Node: (_Reshape_38())>, <Node: (_ElementWiseOp_39(TBackward0))>, <Node: (stages.0.blocks.1.norm2 (LayerNorm((96,), eps=1e-05, elementwise_affine=True)))>, <Node: (_Reshape_40())>, <Node: (_ElementWiseOp_41(AddBackward0))>, <Node: (_ElementWiseOp_42(AddBackward0))>, <Node: (stages.0.blocks.0.ffn.layers.1 (Linear(in_features=384, out_features=96, bias=True)))>, <Node: (_ElementWiseOp_43(AddmmBackward0))>, <Node: (_Reshape_44())>, <Node: (_ElementWiseOp_45(TBackward0))>, <Node: (_ElementWiseOp_46(GeluBackward0))>, <Node: (stages.0.blocks.0.ffn.layers.0.0 (Linear(in_features=96, out_features=384, bias=True)))>, <Node: (_ElementWiseOp_47(AddmmBackward0))>, <Node: (_Reshape_48())>, <Node: (_ElementWiseOp_49(TBackward0))>, <Node: (stages.0.blocks.0.norm2 (LayerNorm((96,), eps=1e-05, elementwise_affine=True)))>, <Node: (_Reshape_50())>, <Node: (patch_embed.norm (LayerNorm((96,), eps=1e-05, elementwise_affine=True)))>, <Node: (_ElementWiseOp_51(TransposeBackward0))>, <Node: (_Reshape_52())>, <Node: (patch_embed.projection (Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))))>, <Node: (_ElementWiseOp_53(CloneBackward0))>, <Node: (_ElementWiseOp_54(SliceBackward0))>, <Node: (_ElementWiseOp_55(SliceBackward0))>, <Node: (_ElementWiseOp_56(SliceBackward0))>, <Node: (_ElementWiseOp_57(SliceBackward0))>, <Node: (_Reshape_58())>, <Node: (_ElementWiseOp_59(CloneBackward0))>, <Node: (_ElementWiseOp_60(PermuteBackward0))>, <Node: (_Reshape_61())>, <Node: (_Reshape_62())>, <Node: (stages.0.blocks.0.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=96, out_features=288, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=96, out_features=96, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)))>, <Node: (_ElementWiseOp_63(CloneBackward0))>, <Node: (_ElementWiseOp_64(TransposeBackward0))>, <Node: (_Reshape_65())>, <Node: (_ElementWiseOp_66(BmmBackward0))>, <Node: (_Reshape_67())>, <Node: (_Reshape_68())>, <Node: (_ElementWiseOp_69(CloneBackward0))>, <Node: (_ElementWiseOp_70(ExpandBackward0))>, <Node: (_ElementWiseOp_71(SelectBackward0))>, <Node: (_ElementWiseOp_72(PermuteBackward0))>, <Node: (_Reshape_73())>, <Node: (_Reshape_74())>, <Node: (_ElementWiseOp_75(AddmmBackward0))>, <Node: (_Reshape_76())>, <Node: (_ElementWiseOp_77(TBackward0))>, <Node: (_Reshape_78())>, <Node: (_Reshape_79())>, <Node: (_ElementWiseOp_80(CloneBackward0))>, <Node: (_ElementWiseOp_81(PermuteBackward0))>, <Node: (_Reshape_82())>, <Node: (_ElementWiseOp_83(ConstantPadNdBackward0))>, <Node: (_Reshape_84())>, <Node: (stages.0.blocks.0.norm1 (LayerNorm((96,), eps=1e-05, elementwise_affine=True)))>, <Node: (_ElementWiseOp_85(ExpandBackward0))>, <Node: (_ElementWiseOp_86(SoftmaxBackward0))>, <Node: (_ElementWiseOp_87(AddBackward0))>, <Node: (_Reshape_88())>, <Node: (_ElementWiseOp_89(UnsqueezeBackward0))>, <Node: (_ElementWiseOp_90(CloneBackward0))>, <Node: (_ElementWiseOp_91(PermuteBackward0))>, <Node: (_Reshape_92())>, <Node: (_ElementWiseOp_93(IndexBackward0))>, <Node: (_ElementWiseOp_94(BmmBackward0))>, <Node: (_Reshape_95())>, <Node: (_Reshape_96())>, <Node: (_ElementWiseOp_97(CloneBackward0))>, <Node: (_ElementWiseOp_98(ExpandBackward0))>, <Node: (_ElementWiseOp_99(TransposeBackward0))>, <Node: (_ElementWiseOp_100(SelectBackward0))>, <Node: (_ElementWiseOp_101(CloneBackward0))>, <Node: (_ElementWiseOp_102(ExpandBackward0))>, <Node: (_ElementWiseOp_103(MulBackward0))>, <Node: (_ElementWiseOp_104(SelectBackward0))>, <Node: (_ElementWiseOp_105(CloneBackward0))>, <Node: (_ElementWiseOp_106(SliceBackward0))>, <Node: (_ElementWiseOp_107(SliceBackward0))>, <Node: (_ElementWiseOp_108(SliceBackward0))>, <Node: (_ElementWiseOp_109(SliceBackward0))>, <Node: (_ElementWiseOp_110(RollBackward0))>, <Node: (_Reshape_111())>, <Node: (_ElementWiseOp_112(CloneBackward0))>, <Node: (_ElementWiseOp_113(PermuteBackward0))>, <Node: (_Reshape_114())>, <Node: (_Reshape_115())>, <Node: (stages.0.blocks.1.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=96, out_features=288, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=96, out_features=96, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)))>, <Node: (_ElementWiseOp_116(CloneBackward0))>, <Node: (_ElementWiseOp_117(TransposeBackward0))>, <Node: (_Reshape_118())>, <Node: (_ElementWiseOp_119(BmmBackward0))>, <Node: (_Reshape_120())>, <Node: (_Reshape_121())>, <Node: (_ElementWiseOp_122(CloneBackward0))>, <Node: (_ElementWiseOp_123(ExpandBackward0))>, <Node: (_ElementWiseOp_124(SelectBackward0))>, <Node: (_ElementWiseOp_125(PermuteBackward0))>, <Node: (_Reshape_126())>, <Node: (_Reshape_127())>, <Node: (_ElementWiseOp_128(AddmmBackward0))>, <Node: (_Reshape_129())>, <Node: (_ElementWiseOp_130(TBackward0))>, <Node: (_Reshape_131())>, <Node: (_Reshape_132())>, <Node: (_ElementWiseOp_133(CloneBackward0))>, <Node: (_ElementWiseOp_134(PermuteBackward0))>, <Node: (_Reshape_135())>, <Node: (_ElementWiseOp_136(RollBackward0))>, <Node: (_ElementWiseOp_137(ConstantPadNdBackward0))>, <Node: (_Reshape_138())>, <Node: (stages.0.blocks.1.norm1 (LayerNorm((96,), eps=1e-05, elementwise_affine=True)))>, <Node: (_ElementWiseOp_139(ExpandBackward0))>, <Node: (_ElementWiseOp_140(SoftmaxBackward0))>, <Node: (_Reshape_141())>, <Node: (_ElementWiseOp_142(AddBackward0))>, <Node: (_Reshape_143())>, <Node: (_ElementWiseOp_144(AddBackward0))>, <Node: (_Reshape_145())>, <Node: (_ElementWiseOp_146(UnsqueezeBackward0))>, <Node: (_ElementWiseOp_147(CloneBackward0))>, <Node: (_ElementWiseOp_148(PermuteBackward0))>, <Node: (_Reshape_149())>, <Node: (_ElementWiseOp_150(IndexBackward0))>, <Node: (_ElementWiseOp_151(BmmBackward0))>, <Node: (_Reshape_152())>, <Node: (_Reshape_153())>, <Node: (_ElementWiseOp_154(CloneBackward0))>, <Node: (_ElementWiseOp_155(ExpandBackward0))>, <Node: (_ElementWiseOp_156(TransposeBackward0))>, <Node: (_ElementWiseOp_157(SelectBackward0))>, <Node: (_ElementWiseOp_158(CloneBackward0))>, <Node: (_ElementWiseOp_159(ExpandBackward0))>, <Node: (_ElementWiseOp_160(MulBackward0))>, <Node: (_ElementWiseOp_161(SelectBackward0))>, <Node: (_ElementWiseOp_162(CloneBackward0))>, <Node: (_ElementWiseOp_163(SliceBackward0))>, <Node: (_ElementWiseOp_164(SliceBackward0))>, <Node: (_ElementWiseOp_165(SliceBackward0))>, <Node: (_ElementWiseOp_166(SliceBackward0))>, <Node: (_Reshape_167())>, <Node: (_ElementWiseOp_168(CloneBackward0))>, <Node: (_ElementWiseOp_169(PermuteBackward0))>, <Node: (_Reshape_170())>, <Node: (_Reshape_171())>, <Node: (stages.1.blocks.0.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=192, out_features=576, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=192, out_features=192, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)))>, <Node: (_ElementWiseOp_172(CloneBackward0))>, <Node: (_ElementWiseOp_173(TransposeBackward0))>, <Node: (_Reshape_174())>, <Node: (_ElementWiseOp_175(BmmBackward0))>, <Node: (_Reshape_176())>, <Node: (_Reshape_177())>, <Node: (_ElementWiseOp_178(CloneBackward0))>, <Node: (_ElementWiseOp_179(ExpandBackward0))>, <Node: (_ElementWiseOp_180(SelectBackward0))>, <Node: (_ElementWiseOp_181(PermuteBackward0))>, <Node: (_Reshape_182())>, <Node: (_Reshape_183())>, <Node: (_ElementWiseOp_184(AddmmBackward0))>, <Node: (_Reshape_185())>, <Node: (_ElementWiseOp_186(TBackward0))>, <Node: (_Reshape_187())>, <Node: (_Reshape_188())>, <Node: (_ElementWiseOp_189(CloneBackward0))>, <Node: (_ElementWiseOp_190(PermuteBackward0))>, <Node: (_Reshape_191())>, <Node: (_ElementWiseOp_192(ConstantPadNdBackward0))>, <Node: (_Reshape_193())>, <Node: (stages.1.blocks.0.norm1 (LayerNorm((192,), eps=1e-05, elementwise_affine=True)))>, <Node: (_ElementWiseOp_194(ExpandBackward0))>, <Node: (_ElementWiseOp_195(SoftmaxBackward0))>, <Node: (_ElementWiseOp_196(AddBackward0))>, <Node: (_Reshape_197())>, <Node: (_ElementWiseOp_198(UnsqueezeBackward0))>, <Node: (_ElementWiseOp_199(CloneBackward0))>, <Node: (_ElementWiseOp_200(PermuteBackward0))>, <Node: (_Reshape_201())>, <Node: (_ElementWiseOp_202(IndexBackward0))>, <Node: (_ElementWiseOp_203(BmmBackward0))>, <Node: (_Reshape_204())>, <Node: (_Reshape_205())>, <Node: (_ElementWiseOp_206(CloneBackward0))>, <Node: (_ElementWiseOp_207(ExpandBackward0))>, <Node: (_ElementWiseOp_208(TransposeBackward0))>, <Node: (_ElementWiseOp_209(SelectBackward0))>, <Node: (_ElementWiseOp_210(CloneBackward0))>, <Node: (_ElementWiseOp_211(ExpandBackward0))>, <Node: (_ElementWiseOp_212(MulBackward0))>, <Node: (_ElementWiseOp_213(SelectBackward0))>, <Node: (_ElementWiseOp_214(CloneBackward0))>, <Node: (_ElementWiseOp_215(SliceBackward0))>, <Node: (_ElementWiseOp_216(SliceBackward0))>, <Node: (_ElementWiseOp_217(SliceBackward0))>, <Node: (_ElementWiseOp_218(SliceBackward0))>, <Node: (_ElementWiseOp_219(RollBackward0))>, <Node: (_Reshape_220())>, <Node: (_ElementWiseOp_221(CloneBackward0))>, <Node: (_ElementWiseOp_222(PermuteBackward0))>, <Node: (_Reshape_223())>, <Node: (_Reshape_224())>, <Node: (stages.1.blocks.1.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=192, out_features=576, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=192, out_features=192, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)))>, <Node: (_ElementWiseOp_225(CloneBackward0))>, <Node: (_ElementWiseOp_226(TransposeBackward0))>, <Node: (_Reshape_227())>, <Node: (_ElementWiseOp_228(BmmBackward0))>, <Node: (_Reshape_229())>, <Node: (_Reshape_230())>, <Node: (_ElementWiseOp_231(CloneBackward0))>, <Node: (_ElementWiseOp_232(ExpandBackward0))>, <Node: (_ElementWiseOp_233(SelectBackward0))>, <Node: (_ElementWiseOp_234(PermuteBackward0))>, <Node: (_Reshape_235())>, <Node: (_Reshape_236())>, <Node: (_ElementWiseOp_237(AddmmBackward0))>, <Node: (_Reshape_238())>, <Node: (_ElementWiseOp_239(TBackward0))>, <Node: (_Reshape_240())>, <Node: (_Reshape_241())>, <Node: (_ElementWiseOp_242(CloneBackward0))>, <Node: (_ElementWiseOp_243(PermuteBackward0))>, <Node: (_Reshape_244())>, <Node: (_ElementWiseOp_245(RollBackward0))>, <Node: (_ElementWiseOp_246(ConstantPadNdBackward0))>, <Node: (_Reshape_247())>, <Node: (stages.1.blocks.1.norm1 (LayerNorm((192,), eps=1e-05, elementwise_affine=True)))>, <Node: (_ElementWiseOp_248(ExpandBackward0))>, <Node: (_ElementWiseOp_249(SoftmaxBackward0))>, <Node: (_Reshape_250())>, <Node: (_ElementWiseOp_251(AddBackward0))>, <Node: (_Reshape_252())>, <Node: (_ElementWiseOp_253(AddBackward0))>, <Node: (_Reshape_254())>, <Node: (_ElementWiseOp_255(UnsqueezeBackward0))>, <Node: (_ElementWiseOp_256(CloneBackward0))>, <Node: (_ElementWiseOp_257(PermuteBackward0))>, <Node: (_Reshape_258())>, <Node: (_ElementWiseOp_259(IndexBackward0))>, <Node: (_ElementWiseOp_260(BmmBackward0))>, <Node: (_Reshape_261())>, <Node: (_Reshape_262())>, <Node: (_ElementWiseOp_263(CloneBackward0))>, <Node: (_ElementWiseOp_264(ExpandBackward0))>, <Node: (_ElementWiseOp_265(TransposeBackward0))>, <Node: (_ElementWiseOp_266(SelectBackward0))>, <Node: (_ElementWiseOp_267(CloneBackward0))>, <Node: (_ElementWiseOp_268(ExpandBackward0))>, <Node: (_ElementWiseOp_269(MulBackward0))>, <Node: (_ElementWiseOp_270(SelectBackward0))>, <Node: (_ElementWiseOp_271(CloneBackward0))>, <Node: (_ElementWiseOp_272(PermuteBackward0))>, <Node: (_Reshape_273())>, <Node: (norm2 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)))>, <Node: (_ElementWiseOp_274(AddBackward0))>, <Node: (_ElementWiseOp_275(AddBackward0))>, <Node: (stages.2.blocks.5.ffn.layers.1 (Linear(in_features=1536, out_features=384, bias=True)))>, <Node: (_ElementWiseOp_276(AddmmBackward0))>, <Node: (_Reshape_277())>, <Node: (_ElementWiseOp_278(TBackward0))>, <Node: (_ElementWiseOp_279(GeluBackward0))>, <Node: (stages.2.blocks.5.ffn.layers.0.0 (Linear(in_features=384, out_features=1536, bias=True)))>, <Node: (_ElementWiseOp_280(AddmmBackward0))>, <Node: (_Reshape_281())>, <Node: (_ElementWiseOp_282(TBackward0))>, <Node: (stages.2.blocks.5.norm2 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)))>, <Node: (_Reshape_283())>, <Node: (_ElementWiseOp_284(AddBackward0))>, <Node: (_ElementWiseOp_285(AddBackward0))>, <Node: (stages.2.blocks.4.ffn.layers.1 (Linear(in_features=1536, out_features=384, bias=True)))>, <Node: (_ElementWiseOp_286(AddmmBackward0))>, <Node: (_Reshape_287())>, <Node: (_ElementWiseOp_288(TBackward0))>, <Node: (_ElementWiseOp_289(GeluBackward0))>, <Node: (stages.2.blocks.4.ffn.layers.0.0 (Linear(in_features=384, out_features=1536, bias=True)))>, <Node: (_ElementWiseOp_290(AddmmBackward0))>, <Node: (_Reshape_291())>, <Node: (_ElementWiseOp_292(TBackward0))>, <Node: (stages.2.blocks.4.norm2 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)))>, <Node: (_Reshape_293())>, <Node: (_ElementWiseOp_294(AddBackward0))>, <Node: (_ElementWiseOp_295(AddBackward0))>, <Node: (stages.2.blocks.3.ffn.layers.1 (Linear(in_features=1536, out_features=384, bias=True)))>, <Node: (_ElementWiseOp_296(AddmmBackward0))>, <Node: (_Reshape_297())>, <Node: (_ElementWiseOp_298(TBackward0))>, <Node: (_ElementWiseOp_299(GeluBackward0))>, <Node: (stages.2.blocks.3.ffn.layers.0.0 (Linear(in_features=384, out_features=1536, bias=True)))>, <Node: (_ElementWiseOp_300(AddmmBackward0))>, <Node: (_Reshape_301())>, <Node: (_ElementWiseOp_302(TBackward0))>, <Node: (stages.2.blocks.3.norm2 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)))>, <Node: (_Reshape_303())>, <Node: (_ElementWiseOp_304(AddBackward0))>, <Node: (_ElementWiseOp_305(AddBackward0))>, <Node: (stages.2.blocks.2.ffn.layers.1 (Linear(in_features=1536, out_features=384, bias=True)))>, <Node: (_ElementWiseOp_306(AddmmBackward0))>, <Node: (_Reshape_307())>, <Node: (_ElementWiseOp_308(TBackward0))>, <Node: (_ElementWiseOp_309(GeluBackward0))>, <Node: (stages.2.blocks.2.ffn.layers.0.0 (Linear(in_features=384, out_features=1536, bias=True)))>, <Node: (_ElementWiseOp_310(AddmmBackward0))>, <Node: (_Reshape_311())>, <Node: (_ElementWiseOp_312(TBackward0))>, <Node: (stages.2.blocks.2.norm2 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)))>, <Node: (_Reshape_313())>, <Node: (_ElementWiseOp_314(AddBackward0))>, <Node: (_ElementWiseOp_315(AddBackward0))>, <Node: (stages.2.blocks.1.ffn.layers.1 (Linear(in_features=1536, out_features=384, bias=True)))>, <Node: (_ElementWiseOp_316(AddmmBackward0))>, <Node: (_Reshape_317())>, <Node: (_ElementWiseOp_318(TBackward0))>, <Node: (_ElementWiseOp_319(GeluBackward0))>, <Node: (stages.2.blocks.1.ffn.layers.0.0 (Linear(in_features=384, out_features=1536, bias=True)))>, <Node: (_ElementWiseOp_320(AddmmBackward0))>, <Node: (_Reshape_321())>, <Node: (_ElementWiseOp_322(TBackward0))>, <Node: (stages.2.blocks.1.norm2 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)))>, <Node: (_Reshape_323())>, <Node: (_ElementWiseOp_324(AddBackward0))>, <Node: (_ElementWiseOp_325(AddBackward0))>, <Node: (stages.2.blocks.0.ffn.layers.1 (Linear(in_features=1536, out_features=384, bias=True)))>, <Node: (_ElementWiseOp_326(AddmmBackward0))>, <Node: (_Reshape_327())>, <Node: (_ElementWiseOp_328(TBackward0))>, <Node: (_ElementWiseOp_329(GeluBackward0))>, <Node: (stages.2.blocks.0.ffn.layers.0.0 (Linear(in_features=384, out_features=1536, bias=True)))>, <Node: (_ElementWiseOp_330(AddmmBackward0))>, <Node: (_Reshape_331())>, <Node: (_ElementWiseOp_332(TBackward0))>, <Node: (stages.2.blocks.0.norm2 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)))>, <Node: (_Reshape_333())>, <Node: (stages.1.downsample (PatchMerging(
  (adap_padding): AdaptivePadding()
  (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (reduction): Linear(in_features=768, out_features=384, bias=False)
)))>, <Node: (_ElementWiseOp_334(MmBackward0))>, <Node: (_Reshape_335())>, <Node: (_ElementWiseOp_336(TBackward0))>, <Node: (_ElementWiseOp_337(NativeLayerNormBackward0))>, <Node: (_ElementWiseOp_338(TransposeBackward0))>, <Node: (_ElementWiseOp_339(Im2ColBackward0))>, <Node: (_ElementWiseOp_340(ConstantPadNdBackward0))>, <Node: (_ElementWiseOp_341(PermuteBackward0))>, <Node: (_Reshape_342())>, <Node: (_ElementWiseOp_343(SliceBackward0))>, <Node: (_ElementWiseOp_344(SliceBackward0))>, <Node: (_ElementWiseOp_345(SliceBackward0))>, <Node: (_Reshape_346())>, <Node: (_ElementWiseOp_347(CloneBackward0))>, <Node: (_ElementWiseOp_348(PermuteBackward0))>, <Node: (_Reshape_349())>, <Node: (_Reshape_350())>, <Node: (stages.2.blocks.0.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=384, out_features=1152, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=384, out_features=384, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)))>, <Node: (_ElementWiseOp_351(CloneBackward0))>, <Node: (_ElementWiseOp_352(TransposeBackward0))>, <Node: (_Reshape_353())>, <Node: (_ElementWiseOp_354(BmmBackward0))>, <Node: (_Reshape_355())>, <Node: (_Reshape_356())>, <Node: (_ElementWiseOp_357(CloneBackward0))>, <Node: (_ElementWiseOp_358(ExpandBackward0))>, <Node: (_ElementWiseOp_359(SelectBackward0))>, <Node: (_ElementWiseOp_360(PermuteBackward0))>, <Node: (_Reshape_361())>, <Node: (_Reshape_362())>, <Node: (_ElementWiseOp_363(AddmmBackward0))>, <Node: (_Reshape_364())>, <Node: (_ElementWiseOp_365(TBackward0))>, <Node: (_Reshape_366())>, <Node: (_Reshape_367())>, <Node: (_ElementWiseOp_368(CloneBackward0))>, <Node: (_ElementWiseOp_369(PermuteBackward0))>, <Node: (_Reshape_370())>, <Node: (_ElementWiseOp_371(ConstantPadNdBackward0))>, <Node: (_Reshape_372())>, <Node: (stages.2.blocks.0.norm1 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)))>, <Node: (_ElementWiseOp_373(ExpandBackward0))>, <Node: (_ElementWiseOp_374(SoftmaxBackward0))>, <Node: (_ElementWiseOp_375(AddBackward0))>, <Node: (_Reshape_376())>, <Node: (_ElementWiseOp_377(UnsqueezeBackward0))>, <Node: (_ElementWiseOp_378(CloneBackward0))>, <Node: (_ElementWiseOp_379(PermuteBackward0))>, <Node: (_Reshape_380())>, <Node: (_ElementWiseOp_381(IndexBackward0))>, <Node: (_ElementWiseOp_382(BmmBackward0))>, <Node: (_Reshape_383())>, <Node: (_Reshape_384())>, <Node: (_ElementWiseOp_385(CloneBackward0))>, <Node: (_ElementWiseOp_386(ExpandBackward0))>, <Node: (_ElementWiseOp_387(TransposeBackward0))>, <Node: (_ElementWiseOp_388(SelectBackward0))>, <Node: (_ElementWiseOp_389(CloneBackward0))>, <Node: (_ElementWiseOp_390(ExpandBackward0))>, <Node: (_ElementWiseOp_391(MulBackward0))>, <Node: (_ElementWiseOp_392(SelectBackward0))>, <Node: (_ElementWiseOp_393(SliceBackward0))>, <Node: (_ElementWiseOp_394(SliceBackward0))>, <Node: (_ElementWiseOp_395(SliceBackward0))>, <Node: (_ElementWiseOp_396(RollBackward0))>, <Node: (_Reshape_397())>, <Node: (_ElementWiseOp_398(CloneBackward0))>, <Node: (_ElementWiseOp_399(PermuteBackward0))>, <Node: (_Reshape_400())>, <Node: (_Reshape_401())>, <Node: (stages.2.blocks.1.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=384, out_features=1152, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=384, out_features=384, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)))>, <Node: (_ElementWiseOp_402(CloneBackward0))>, <Node: (_ElementWiseOp_403(TransposeBackward0))>, <Node: (_Reshape_404())>, <Node: (_ElementWiseOp_405(BmmBackward0))>, <Node: (_Reshape_406())>, <Node: (_Reshape_407())>, <Node: (_ElementWiseOp_408(CloneBackward0))>, <Node: (_ElementWiseOp_409(ExpandBackward0))>, <Node: (_ElementWiseOp_410(SelectBackward0))>, <Node: (_ElementWiseOp_411(PermuteBackward0))>, <Node: (_Reshape_412())>, <Node: (_Reshape_413())>, <Node: (_ElementWiseOp_414(AddmmBackward0))>, <Node: (_Reshape_415())>, <Node: (_ElementWiseOp_416(TBackward0))>, <Node: (_Reshape_417())>, <Node: (_Reshape_418())>, <Node: (_ElementWiseOp_419(CloneBackward0))>, <Node: (_ElementWiseOp_420(PermuteBackward0))>, <Node: (_Reshape_421())>, <Node: (_ElementWiseOp_422(RollBackward0))>, <Node: (_ElementWiseOp_423(ConstantPadNdBackward0))>, <Node: (_Reshape_424())>, <Node: (stages.2.blocks.1.norm1 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)))>, <Node: (_ElementWiseOp_425(ExpandBackward0))>, <Node: (_ElementWiseOp_426(SoftmaxBackward0))>, <Node: (_Reshape_427())>, <Node: (_ElementWiseOp_428(AddBackward0))>, <Node: (_Reshape_429())>, <Node: (_ElementWiseOp_430(AddBackward0))>, <Node: (_Reshape_431())>, <Node: (_ElementWiseOp_432(UnsqueezeBackward0))>, <Node: (_ElementWiseOp_433(CloneBackward0))>, <Node: (_ElementWiseOp_434(PermuteBackward0))>, <Node: (_Reshape_435())>, <Node: (_ElementWiseOp_436(IndexBackward0))>, <Node: (_ElementWiseOp_437(BmmBackward0))>, <Node: (_Reshape_438())>, <Node: (_Reshape_439())>, <Node: (_ElementWiseOp_440(CloneBackward0))>, <Node: (_ElementWiseOp_441(ExpandBackward0))>, <Node: (_ElementWiseOp_442(TransposeBackward0))>, <Node: (_ElementWiseOp_443(SelectBackward0))>, <Node: (_ElementWiseOp_444(CloneBackward0))>, <Node: (_ElementWiseOp_445(ExpandBackward0))>, <Node: (_ElementWiseOp_446(MulBackward0))>, <Node: (_ElementWiseOp_447(SelectBackward0))>, <Node: (_ElementWiseOp_448(SliceBackward0))>, <Node: (_ElementWiseOp_449(SliceBackward0))>, <Node: (_ElementWiseOp_450(SliceBackward0))>, <Node: (_Reshape_451())>, <Node: (_ElementWiseOp_452(CloneBackward0))>, <Node: (_ElementWiseOp_453(PermuteBackward0))>, <Node: (_Reshape_454())>, <Node: (_Reshape_455())>, <Node: (stages.2.blocks.2.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=384, out_features=1152, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=384, out_features=384, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)))>, <Node: (_ElementWiseOp_456(CloneBackward0))>, <Node: (_ElementWiseOp_457(TransposeBackward0))>, <Node: (_Reshape_458())>, <Node: (_ElementWiseOp_459(BmmBackward0))>, <Node: (_Reshape_460())>, <Node: (_Reshape_461())>, <Node: (_ElementWiseOp_462(CloneBackward0))>, <Node: (_ElementWiseOp_463(ExpandBackward0))>, <Node: (_ElementWiseOp_464(SelectBackward0))>, <Node: (_ElementWiseOp_465(PermuteBackward0))>, <Node: (_Reshape_466())>, <Node: (_Reshape_467())>, <Node: (_ElementWiseOp_468(AddmmBackward0))>, <Node: (_Reshape_469())>, <Node: (_ElementWiseOp_470(TBackward0))>, <Node: (_Reshape_471())>, <Node: (_Reshape_472())>, <Node: (_ElementWiseOp_473(CloneBackward0))>, <Node: (_ElementWiseOp_474(PermuteBackward0))>, <Node: (_Reshape_475())>, <Node: (_ElementWiseOp_476(ConstantPadNdBackward0))>, <Node: (_Reshape_477())>, <Node: (stages.2.blocks.2.norm1 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)))>, <Node: (_ElementWiseOp_478(ExpandBackward0))>, <Node: (_ElementWiseOp_479(SoftmaxBackward0))>, <Node: (_ElementWiseOp_480(AddBackward0))>, <Node: (_Reshape_481())>, <Node: (_ElementWiseOp_482(UnsqueezeBackward0))>, <Node: (_ElementWiseOp_483(CloneBackward0))>, <Node: (_ElementWiseOp_484(PermuteBackward0))>, <Node: (_Reshape_485())>, <Node: (_ElementWiseOp_486(IndexBackward0))>, <Node: (_ElementWiseOp_487(BmmBackward0))>, <Node: (_Reshape_488())>, <Node: (_Reshape_489())>, <Node: (_ElementWiseOp_490(CloneBackward0))>, <Node: (_ElementWiseOp_491(ExpandBackward0))>, <Node: (_ElementWiseOp_492(TransposeBackward0))>, <Node: (_ElementWiseOp_493(SelectBackward0))>, <Node: (_ElementWiseOp_494(CloneBackward0))>, <Node: (_ElementWiseOp_495(ExpandBackward0))>, <Node: (_ElementWiseOp_496(MulBackward0))>, <Node: (_ElementWiseOp_497(SelectBackward0))>, <Node: (_ElementWiseOp_498(SliceBackward0))>, <Node: (_ElementWiseOp_499(SliceBackward0))>, <Node: (_ElementWiseOp_500(SliceBackward0))>, <Node: (_ElementWiseOp_501(RollBackward0))>, <Node: (_Reshape_502())>, <Node: (_ElementWiseOp_503(CloneBackward0))>, <Node: (_ElementWiseOp_504(PermuteBackward0))>, <Node: (_Reshape_505())>, <Node: (_Reshape_506())>, <Node: (stages.2.blocks.3.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=384, out_features=1152, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=384, out_features=384, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)))>, <Node: (_ElementWiseOp_507(CloneBackward0))>, <Node: (_ElementWiseOp_508(TransposeBackward0))>, <Node: (_Reshape_509())>, <Node: (_ElementWiseOp_510(BmmBackward0))>, <Node: (_Reshape_511())>, <Node: (_Reshape_512())>, <Node: (_ElementWiseOp_513(CloneBackward0))>, <Node: (_ElementWiseOp_514(ExpandBackward0))>, <Node: (_ElementWiseOp_515(SelectBackward0))>, <Node: (_ElementWiseOp_516(PermuteBackward0))>, <Node: (_Reshape_517())>, <Node: (_Reshape_518())>, <Node: (_ElementWiseOp_519(AddmmBackward0))>, <Node: (_Reshape_520())>, <Node: (_ElementWiseOp_521(TBackward0))>, <Node: (_Reshape_522())>, <Node: (_Reshape_523())>, <Node: (_ElementWiseOp_524(CloneBackward0))>, <Node: (_ElementWiseOp_525(PermuteBackward0))>, <Node: (_Reshape_526())>, <Node: (_ElementWiseOp_527(RollBackward0))>, <Node: (_ElementWiseOp_528(ConstantPadNdBackward0))>, <Node: (_Reshape_529())>, <Node: (stages.2.blocks.3.norm1 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)))>, <Node: (_ElementWiseOp_530(ExpandBackward0))>, <Node: (_ElementWiseOp_531(SoftmaxBackward0))>, <Node: (_Reshape_532())>, <Node: (_ElementWiseOp_533(AddBackward0))>, <Node: (_Reshape_534())>, <Node: (_ElementWiseOp_535(AddBackward0))>, <Node: (_Reshape_536())>, <Node: (_ElementWiseOp_537(UnsqueezeBackward0))>, <Node: (_ElementWiseOp_538(CloneBackward0))>, <Node: (_ElementWiseOp_539(PermuteBackward0))>, <Node: (_Reshape_540())>, <Node: (_ElementWiseOp_541(IndexBackward0))>, <Node: (_ElementWiseOp_542(BmmBackward0))>, <Node: (_Reshape_543())>, <Node: (_Reshape_544())>, <Node: (_ElementWiseOp_545(CloneBackward0))>, <Node: (_ElementWiseOp_546(ExpandBackward0))>, <Node: (_ElementWiseOp_547(TransposeBackward0))>, <Node: (_ElementWiseOp_548(SelectBackward0))>, <Node: (_ElementWiseOp_549(CloneBackward0))>, <Node: (_ElementWiseOp_550(ExpandBackward0))>, <Node: (_ElementWiseOp_551(MulBackward0))>, <Node: (_ElementWiseOp_552(SelectBackward0))>, <Node: (_ElementWiseOp_553(SliceBackward0))>, <Node: (_ElementWiseOp_554(SliceBackward0))>, <Node: (_ElementWiseOp_555(SliceBackward0))>, <Node: (_Reshape_556())>, <Node: (_ElementWiseOp_557(CloneBackward0))>, <Node: (_ElementWiseOp_558(PermuteBackward0))>, <Node: (_Reshape_559())>, <Node: (_Reshape_560())>, <Node: (stages.2.blocks.4.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=384, out_features=1152, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=384, out_features=384, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)))>, <Node: (_ElementWiseOp_561(CloneBackward0))>, <Node: (_ElementWiseOp_562(TransposeBackward0))>, <Node: (_Reshape_563())>, <Node: (_ElementWiseOp_564(BmmBackward0))>, <Node: (_Reshape_565())>, <Node: (_Reshape_566())>, <Node: (_ElementWiseOp_567(CloneBackward0))>, <Node: (_ElementWiseOp_568(ExpandBackward0))>, <Node: (_ElementWiseOp_569(SelectBackward0))>, <Node: (_ElementWiseOp_570(PermuteBackward0))>, <Node: (_Reshape_571())>, <Node: (_Reshape_572())>, <Node: (_ElementWiseOp_573(AddmmBackward0))>, <Node: (_Reshape_574())>, <Node: (_ElementWiseOp_575(TBackward0))>, <Node: (_Reshape_576())>, <Node: (_Reshape_577())>, <Node: (_ElementWiseOp_578(CloneBackward0))>, <Node: (_ElementWiseOp_579(PermuteBackward0))>, <Node: (_Reshape_580())>, <Node: (_ElementWiseOp_581(ConstantPadNdBackward0))>, <Node: (_Reshape_582())>, <Node: (stages.2.blocks.4.norm1 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)))>, <Node: (_ElementWiseOp_583(ExpandBackward0))>, <Node: (_ElementWiseOp_584(SoftmaxBackward0))>, <Node: (_ElementWiseOp_585(AddBackward0))>, <Node: (_Reshape_586())>, <Node: (_ElementWiseOp_587(UnsqueezeBackward0))>, <Node: (_ElementWiseOp_588(CloneBackward0))>, <Node: (_ElementWiseOp_589(PermuteBackward0))>, <Node: (_Reshape_590())>, <Node: (_ElementWiseOp_591(IndexBackward0))>, <Node: (_ElementWiseOp_592(BmmBackward0))>, <Node: (_Reshape_593())>, <Node: (_Reshape_594())>, <Node: (_ElementWiseOp_595(CloneBackward0))>, <Node: (_ElementWiseOp_596(ExpandBackward0))>, <Node: (_ElementWiseOp_597(TransposeBackward0))>, <Node: (_ElementWiseOp_598(SelectBackward0))>, <Node: (_ElementWiseOp_599(CloneBackward0))>, <Node: (_ElementWiseOp_600(ExpandBackward0))>, <Node: (_ElementWiseOp_601(MulBackward0))>, <Node: (_ElementWiseOp_602(SelectBackward0))>, <Node: (_ElementWiseOp_603(SliceBackward0))>, <Node: (_ElementWiseOp_604(SliceBackward0))>, <Node: (_ElementWiseOp_605(SliceBackward0))>, <Node: (_ElementWiseOp_606(RollBackward0))>, <Node: (_Reshape_607())>, <Node: (_ElementWiseOp_608(CloneBackward0))>, <Node: (_ElementWiseOp_609(PermuteBackward0))>, <Node: (_Reshape_610())>, <Node: (_Reshape_611())>, <Node: (stages.2.blocks.5.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=384, out_features=1152, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=384, out_features=384, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)))>, <Node: (_ElementWiseOp_612(CloneBackward0))>, <Node: (_ElementWiseOp_613(TransposeBackward0))>, <Node: (_Reshape_614())>, <Node: (_ElementWiseOp_615(BmmBackward0))>, <Node: (_Reshape_616())>, <Node: (_Reshape_617())>, <Node: (_ElementWiseOp_618(CloneBackward0))>, <Node: (_ElementWiseOp_619(ExpandBackward0))>, <Node: (_ElementWiseOp_620(SelectBackward0))>, <Node: (_ElementWiseOp_621(PermuteBackward0))>, <Node: (_Reshape_622())>, <Node: (_Reshape_623())>, <Node: (_ElementWiseOp_624(AddmmBackward0))>, <Node: (_Reshape_625())>, <Node: (_ElementWiseOp_626(TBackward0))>, <Node: (_Reshape_627())>, <Node: (_Reshape_628())>, <Node: (_ElementWiseOp_629(CloneBackward0))>, <Node: (_ElementWiseOp_630(PermuteBackward0))>, <Node: (_Reshape_631())>, <Node: (_ElementWiseOp_632(RollBackward0))>, <Node: (_ElementWiseOp_633(ConstantPadNdBackward0))>, <Node: (_Reshape_634())>, <Node: (stages.2.blocks.5.norm1 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)))>, <Node: (_ElementWiseOp_635(ExpandBackward0))>, <Node: (_ElementWiseOp_636(SoftmaxBackward0))>, <Node: (_Reshape_637())>, <Node: (_ElementWiseOp_638(AddBackward0))>, <Node: (_Reshape_639())>, <Node: (_ElementWiseOp_640(AddBackward0))>, <Node: (_Reshape_641())>, <Node: (_ElementWiseOp_642(UnsqueezeBackward0))>, <Node: (_ElementWiseOp_643(CloneBackward0))>, <Node: (_ElementWiseOp_644(PermuteBackward0))>, <Node: (_Reshape_645())>, <Node: (_ElementWiseOp_646(IndexBackward0))>, <Node: (_ElementWiseOp_647(BmmBackward0))>, <Node: (_Reshape_648())>, <Node: (_Reshape_649())>, <Node: (_ElementWiseOp_650(CloneBackward0))>, <Node: (_ElementWiseOp_651(ExpandBackward0))>, <Node: (_ElementWiseOp_652(TransposeBackward0))>, <Node: (_ElementWiseOp_653(SelectBackward0))>, <Node: (_ElementWiseOp_654(CloneBackward0))>, <Node: (_ElementWiseOp_655(ExpandBackward0))>, <Node: (_ElementWiseOp_656(MulBackward0))>, <Node: (_ElementWiseOp_657(SelectBackward0))>, <Node: (_ElementWiseOp_658(CloneBackward0))>, <Node: (_ElementWiseOp_659(PermuteBackward0))>, <Node: (_Reshape_660())>, <Node: (norm3 (LayerNorm((768,), eps=1e-05, elementwise_affine=True)))>, <Node: (_ElementWiseOp_661(AddBackward0))>, <Node: (_ElementWiseOp_662(AddBackward0))>, <Node: (stages.3.blocks.1.ffn.layers.1 (Linear(in_features=3072, out_features=768, bias=True)))>, <Node: (_ElementWiseOp_663(AddmmBackward0))>, <Node: (_Reshape_664())>, <Node: (_ElementWiseOp_665(TBackward0))>, <Node: (_ElementWiseOp_666(GeluBackward0))>, <Node: (stages.3.blocks.1.ffn.layers.0.0 (Linear(in_features=768, out_features=3072, bias=True)))>, <Node: (_ElementWiseOp_667(AddmmBackward0))>, <Node: (_Reshape_668())>, <Node: (_ElementWiseOp_669(TBackward0))>, <Node: (stages.3.blocks.1.norm2 (LayerNorm((768,), eps=1e-05, elementwise_affine=True)))>, <Node: (_Reshape_670())>, <Node: (_ElementWiseOp_671(AddBackward0))>, <Node: (_ElementWiseOp_672(AddBackward0))>, <Node: (stages.3.blocks.0.ffn.layers.1 (Linear(in_features=3072, out_features=768, bias=True)))>, <Node: (_ElementWiseOp_673(AddmmBackward0))>, <Node: (_Reshape_674())>, <Node: (_ElementWiseOp_675(TBackward0))>, <Node: (_ElementWiseOp_676(GeluBackward0))>, <Node: (stages.3.blocks.0.ffn.layers.0.0 (Linear(in_features=768, out_features=3072, bias=True)))>, <Node: (_ElementWiseOp_677(AddmmBackward0))>, <Node: (_Reshape_678())>, <Node: (_ElementWiseOp_679(TBackward0))>, <Node: (stages.3.blocks.0.norm2 (LayerNorm((768,), eps=1e-05, elementwise_affine=True)))>, <Node: (_Reshape_680())>, <Node: (stages.2.downsample (PatchMerging(
  (adap_padding): AdaptivePadding()
  (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
  (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (reduction): Linear(in_features=1536, out_features=768, bias=False)
)))>, <Node: (_ElementWiseOp_681(MmBackward0))>, <Node: (_Reshape_682())>, <Node: (_ElementWiseOp_683(TBackward0))>, <Node: (_ElementWiseOp_684(NativeLayerNormBackward0))>, <Node: (_ElementWiseOp_685(TransposeBackward0))>, <Node: (_ElementWiseOp_686(Im2ColBackward0))>, <Node: (_ElementWiseOp_687(PermuteBackward0))>, <Node: (_Reshape_688())>, <Node: (_ElementWiseOp_689(SliceBackward0))>, <Node: (_ElementWiseOp_690(SliceBackward0))>, <Node: (_ElementWiseOp_691(SliceBackward0))>, <Node: (_Reshape_692())>, <Node: (_ElementWiseOp_693(CloneBackward0))>, <Node: (_ElementWiseOp_694(PermuteBackward0))>, <Node: (_Reshape_695())>, <Node: (_Reshape_696())>, <Node: (stages.3.blocks.0.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=768, out_features=2304, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=768, out_features=768, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)))>, <Node: (_ElementWiseOp_697(CloneBackward0))>, <Node: (_ElementWiseOp_698(TransposeBackward0))>, <Node: (_Reshape_699())>, <Node: (_ElementWiseOp_700(BmmBackward0))>, <Node: (_Reshape_701())>, <Node: (_Reshape_702())>, <Node: (_ElementWiseOp_703(CloneBackward0))>, <Node: (_ElementWiseOp_704(ExpandBackward0))>, <Node: (_ElementWiseOp_705(SelectBackward0))>, <Node: (_ElementWiseOp_706(PermuteBackward0))>, <Node: (_Reshape_707())>, <Node: (_Reshape_708())>, <Node: (_ElementWiseOp_709(AddmmBackward0))>, <Node: (_Reshape_710())>, <Node: (_ElementWiseOp_711(TBackward0))>, <Node: (_Reshape_712())>, <Node: (_Reshape_713())>, <Node: (_ElementWiseOp_714(CloneBackward0))>, <Node: (_ElementWiseOp_715(PermuteBackward0))>, <Node: (_Reshape_716())>, <Node: (_ElementWiseOp_717(ConstantPadNdBackward0))>, <Node: (_Reshape_718())>, <Node: (stages.3.blocks.0.norm1 (LayerNorm((768,), eps=1e-05, elementwise_affine=True)))>, <Node: (_ElementWiseOp_719(ExpandBackward0))>, <Node: (_ElementWiseOp_720(SoftmaxBackward0))>, <Node: (_ElementWiseOp_721(AddBackward0))>, <Node: (_Reshape_722())>, <Node: (_ElementWiseOp_723(UnsqueezeBackward0))>, <Node: (_ElementWiseOp_724(CloneBackward0))>, <Node: (_ElementWiseOp_725(PermuteBackward0))>, <Node: (_Reshape_726())>, <Node: (_ElementWiseOp_727(IndexBackward0))>, <Node: (_ElementWiseOp_728(BmmBackward0))>, <Node: (_Reshape_729())>, <Node: (_Reshape_730())>, <Node: (_ElementWiseOp_731(CloneBackward0))>, <Node: (_ElementWiseOp_732(ExpandBackward0))>, <Node: (_ElementWiseOp_733(TransposeBackward0))>, <Node: (_ElementWiseOp_734(SelectBackward0))>, <Node: (_ElementWiseOp_735(CloneBackward0))>, <Node: (_ElementWiseOp_736(ExpandBackward0))>, <Node: (_ElementWiseOp_737(MulBackward0))>, <Node: (_ElementWiseOp_738(SelectBackward0))>, <Node: (_ElementWiseOp_739(SliceBackward0))>, <Node: (_ElementWiseOp_740(SliceBackward0))>, <Node: (_ElementWiseOp_741(SliceBackward0))>, <Node: (_ElementWiseOp_742(RollBackward0))>, <Node: (_Reshape_743())>, <Node: (_ElementWiseOp_744(CloneBackward0))>, <Node: (_ElementWiseOp_745(PermuteBackward0))>, <Node: (_Reshape_746())>, <Node: (_Reshape_747())>, <Node: (stages.3.blocks.1.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=768, out_features=2304, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=768, out_features=768, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)))>, <Node: (_ElementWiseOp_748(CloneBackward0))>, <Node: (_ElementWiseOp_749(TransposeBackward0))>, <Node: (_Reshape_750())>, <Node: (_ElementWiseOp_751(BmmBackward0))>, <Node: (_Reshape_752())>, <Node: (_Reshape_753())>, <Node: (_ElementWiseOp_754(CloneBackward0))>, <Node: (_ElementWiseOp_755(ExpandBackward0))>, <Node: (_ElementWiseOp_756(SelectBackward0))>, <Node: (_ElementWiseOp_757(PermuteBackward0))>, <Node: (_Reshape_758())>, <Node: (_Reshape_759())>, <Node: (_ElementWiseOp_760(AddmmBackward0))>, <Node: (_Reshape_761())>, <Node: (_ElementWiseOp_762(TBackward0))>, <Node: (_Reshape_763())>, <Node: (_Reshape_764())>, <Node: (_ElementWiseOp_765(CloneBackward0))>, <Node: (_ElementWiseOp_766(PermuteBackward0))>, <Node: (_Reshape_767())>, <Node: (_ElementWiseOp_768(RollBackward0))>, <Node: (_ElementWiseOp_769(ConstantPadNdBackward0))>, <Node: (_Reshape_770())>, <Node: (stages.3.blocks.1.norm1 (LayerNorm((768,), eps=1e-05, elementwise_affine=True)))>, <Node: (_ElementWiseOp_771(ExpandBackward0))>, <Node: (_ElementWiseOp_772(SoftmaxBackward0))>, <Node: (_Reshape_773())>, <Node: (_ElementWiseOp_774(AddBackward0))>, <Node: (_Reshape_775())>, <Node: (_ElementWiseOp_776(AddBackward0))>, <Node: (_Reshape_777())>, <Node: (_ElementWiseOp_778(UnsqueezeBackward0))>, <Node: (_ElementWiseOp_779(CloneBackward0))>, <Node: (_ElementWiseOp_780(PermuteBackward0))>, <Node: (_Reshape_781())>, <Node: (_ElementWiseOp_782(IndexBackward0))>, <Node: (_ElementWiseOp_783(BmmBackward0))>, <Node: (_Reshape_784())>, <Node: (_Reshape_785())>, <Node: (_ElementWiseOp_786(CloneBackward0))>, <Node: (_ElementWiseOp_787(ExpandBackward0))>, <Node: (_ElementWiseOp_788(TransposeBackward0))>, <Node: (_ElementWiseOp_789(SelectBackward0))>, <Node: (_ElementWiseOp_790(CloneBackward0))>, <Node: (_ElementWiseOp_791(ExpandBackward0))>, <Node: (_ElementWiseOp_792(MulBackward0))>, <Node: (_ElementWiseOp_793(SelectBackward0))>]) 

PatchMergingPruner () get_out_channels/  192 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
PatchMergingPruner () get_in_channels/  96 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
WindowMSAPruner layer.embed_dims =  96
WindowMSAPruner layer.qkv.in_features =  96
WindowMSAPruner layer.embed_dims =  96
WindowMSAPruner layer.qkv.in_features =  96
WindowMSAPruner layer.embed_dims =  192
WindowMSAPruner layer.qkv.in_features =  192
WindowMSAPruner layer.embed_dims =  192
WindowMSAPruner layer.qkv.in_features =  192
PatchMergingPruner () get_out_channels/  384 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
PatchMergingPruner () get_in_channels/  192 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
WindowMSAPruner layer.embed_dims =  384
WindowMSAPruner layer.qkv.in_features =  384
WindowMSAPruner layer.embed_dims =  384
WindowMSAPruner layer.qkv.in_features =  384
WindowMSAPruner layer.embed_dims =  384
WindowMSAPruner layer.qkv.in_features =  384
WindowMSAPruner layer.embed_dims =  384
WindowMSAPruner layer.qkv.in_features =  384
WindowMSAPruner layer.embed_dims =  384
WindowMSAPruner layer.qkv.in_features =  384
WindowMSAPruner layer.embed_dims =  384
WindowMSAPruner layer.qkv.in_features =  384
PatchMergingPruner () get_out_channels/  768 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
PatchMergingPruner () get_in_channels/  384 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
WindowMSAPruner layer.embed_dims =  768
WindowMSAPruner layer.qkv.in_features =  768
WindowMSAPruner layer.embed_dims =  768
WindowMSAPruner layer.qkv.in_features =  768
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
	get_all_groups -- Adding layer:  <class 'torch.nn.modules.normalization.LayerNorm'>
PatchMergingPruner () get_in_channels/  192 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
WindowMSAPruner layer.qkv.in_features =  192
PatchMergingPruner () get_out_channels/  192 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
WindowMSAPruner layer.qkv.in_features =  192
WindowMSAPruner layer.embed_dims =  192
WindowMSAPruner layer.embed_dims =  192
		prune_local()/  _check_pruning_ratio OK
idxs =  48
prunable_chs =  192
idxs =  48
prunable_chs =  192
PatchMergingPruner () get_in_channels/  192 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
idxs =  48
prunable_chs =  192
idxs =  48
prunable_chs =  192
idxs =  48
prunable_chs =  192
WindowMSAPruner layer.qkv.in_features =  192
PatchMergingPruner () get_out_channels/  192 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
idxs =  48
prunable_chs =  192
idxs =  48
prunable_chs =  192
idxs =  48
prunable_chs =  192
WindowMSAPruner layer.qkv.in_features =  192
WindowMSAPruner layer.embed_dims =  192
idxs =  48
prunable_chs =  192
WindowMSAPruner layer.embed_dims =  192
idxs =  48
prunable_chs =  192
pruner.step 
--------------------------------
          Pruning Group
--------------------------------
[0] prune_out_channels on norm1 (LayerNorm((192,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on norm1 (LayerNorm((192,), eps=1e-05, elementwise_affine=True)), #idxs=48
[1] prune_out_channels on norm1 (LayerNorm((192,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _ElementWiseOp_3(AddBackward0), #idxs=48
[2] prune_out_channels on norm1 (LayerNorm((192,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_2(), #idxs=48
[3] prune_out_channels on _Reshape_2() => prune_out_channels on _ElementWiseOp_1(PermuteBackward0), #idxs=48
[4] prune_out_channels on _ElementWiseOp_1(PermuteBackward0) => prune_out_channels on _ElementWiseOp_0(CloneBackward0), #idxs=48
[5] prune_out_channels on _ElementWiseOp_3(AddBackward0) => prune_out_channels on _ElementWiseOp_4(AddBackward0), #idxs=48
[6] prune_out_channels on _ElementWiseOp_3(AddBackward0) => prune_out_channels on stages.1.blocks.1.ffn.layers.1 (Linear(in_features=768, out_features=192, bias=True)), #idxs=48
[7] prune_out_channels on _ElementWiseOp_3(AddBackward0) => prune_out_channels on _Reshape_342(), #idxs=48
[8] prune_out_channels on _Reshape_342() => prune_out_channels on _ElementWiseOp_341(PermuteBackward0), #idxs=48
[9] prune_out_channels on _ElementWiseOp_341(PermuteBackward0) => prune_out_channels on _ElementWiseOp_340(ConstantPadNdBackward0), #idxs=48
[10] prune_out_channels on _ElementWiseOp_340(ConstantPadNdBackward0) => prune_out_channels on _ElementWiseOp_339(Im2ColBackward0), #idxs=48
[11] prune_out_channels on _ElementWiseOp_339(Im2ColBackward0) => prune_out_channels on _ElementWiseOp_338(TransposeBackward0), #idxs=48
[12] prune_out_channels on _ElementWiseOp_338(TransposeBackward0) => prune_out_channels on _ElementWiseOp_337(NativeLayerNormBackward0), #idxs=48
[13] prune_out_channels on _ElementWiseOp_337(NativeLayerNormBackward0) => prune_out_channels on _Reshape_335(), #idxs=48
[14] prune_out_channels on _Reshape_335() => prune_out_channels on _ElementWiseOp_334(MmBackward0), #idxs=48
[15] prune_out_channels on _ElementWiseOp_334(MmBackward0) => prune_out_channels on _ElementWiseOp_336(TBackward0), #idxs=48
[16] prune_out_channels on _ElementWiseOp_334(MmBackward0) => prune_in_channels on stages.1.downsample (PatchMerging(
  (adap_padding): AdaptivePadding()
  (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (reduction): Linear(in_features=768, out_features=384, bias=False)
)), #idxs=48
[17] prune_out_channels on _ElementWiseOp_4(AddBackward0) => prune_out_channels on _Reshape_12(), #idxs=48
[18] prune_out_channels on _ElementWiseOp_4(AddBackward0) => prune_out_channels on _ElementWiseOp_13(AddBackward0), #idxs=48
[19] prune_out_channels on _ElementWiseOp_4(AddBackward0) => prune_out_channels on stages.1.blocks.1.norm2 (LayerNorm((192,), eps=1e-05, elementwise_affine=True)), #idxs=48
[20] prune_out_channels on stages.1.blocks.1.norm2 (LayerNorm((192,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_10(), #idxs=48
[21] prune_out_channels on _Reshape_10() => prune_out_channels on _ElementWiseOp_9(AddmmBackward0), #idxs=48
[22] prune_out_channels on _ElementWiseOp_9(AddmmBackward0) => prune_out_channels on _ElementWiseOp_11(TBackward0), #idxs=48
[23] prune_out_channels on _ElementWiseOp_9(AddmmBackward0) => prune_in_channels on stages.1.blocks.1.ffn.layers.0.0 (Linear(in_features=192, out_features=768, bias=True)), #idxs=48
[24] prune_out_channels on _ElementWiseOp_13(AddBackward0) => prune_out_channels on _ElementWiseOp_14(AddBackward0), #idxs=48
[25] prune_out_channels on _ElementWiseOp_13(AddBackward0) => prune_out_channels on stages.1.blocks.0.ffn.layers.1 (Linear(in_features=768, out_features=192, bias=True)), #idxs=48
[26] prune_out_channels on _ElementWiseOp_13(AddBackward0) => prune_out_channels on stages.1.blocks.1.norm1 (LayerNorm((192,), eps=1e-05, elementwise_affine=True)), #idxs=48
[27] prune_out_channels on stages.1.blocks.1.norm1 (LayerNorm((192,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_247(), #idxs=48
[28] prune_out_channels on _Reshape_247() => prune_out_channels on _ElementWiseOp_246(ConstantPadNdBackward0), #idxs=48
[29] prune_out_channels on _ElementWiseOp_246(ConstantPadNdBackward0) => prune_out_channels on _ElementWiseOp_245(RollBackward0), #idxs=48
[30] prune_out_channels on _ElementWiseOp_245(RollBackward0) => prune_out_channels on _Reshape_244(), #idxs=48
[31] prune_out_channels on _Reshape_244() => prune_out_channels on _ElementWiseOp_243(PermuteBackward0), #idxs=48
[32] prune_out_channels on _ElementWiseOp_243(PermuteBackward0) => prune_out_channels on _ElementWiseOp_242(CloneBackward0), #idxs=48
[33] prune_out_channels on _ElementWiseOp_242(CloneBackward0) => prune_out_channels on _Reshape_241(), #idxs=48
[34] prune_out_channels on _Reshape_241() => prune_out_channels on _Reshape_240(), #idxs=48
[35] prune_out_channels on _Reshape_240() => prune_out_channels on _Reshape_238(), #idxs=48
[36] prune_out_channels on _Reshape_238() => prune_out_channels on _ElementWiseOp_237(AddmmBackward0), #idxs=48
[37] prune_out_channels on _ElementWiseOp_237(AddmmBackward0) => prune_out_channels on _ElementWiseOp_239(TBackward0), #idxs=48
[38] prune_out_channels on _ElementWiseOp_237(AddmmBackward0) => prune_out_channels on _Reshape_236(), #idxs=48
[39] prune_out_channels on _Reshape_236() => prune_out_channels on _Reshape_235(), #idxs=48
[40] prune_out_channels on _Reshape_235() => prune_out_channels on _ElementWiseOp_234(PermuteBackward0), #idxs=48
[41] prune_out_channels on _ElementWiseOp_234(PermuteBackward0) => prune_out_channels on _ElementWiseOp_233(SelectBackward0), #idxs=48
[42] prune_out_channels on _ElementWiseOp_234(PermuteBackward0) => prune_out_channels on _ElementWiseOp_266(SelectBackward0), #idxs=48
[43] prune_out_channels on _ElementWiseOp_234(PermuteBackward0) => prune_out_channels on _ElementWiseOp_270(SelectBackward0), #idxs=48
[44] prune_out_channels on _ElementWiseOp_270(SelectBackward0) => prune_out_channels on _ElementWiseOp_269(MulBackward0), #idxs=48
[45] prune_out_channels on _ElementWiseOp_269(MulBackward0) => prune_out_channels on _ElementWiseOp_268(ExpandBackward0), #idxs=48
[46] prune_out_channels on _ElementWiseOp_268(ExpandBackward0) => prune_out_channels on _ElementWiseOp_267(CloneBackward0), #idxs=48
[47] prune_out_channels on _ElementWiseOp_267(CloneBackward0) => prune_out_channels on _Reshape_261(), #idxs=48
[48] prune_out_channels on _Reshape_261() => prune_out_channels on _ElementWiseOp_260(BmmBackward0), #idxs=48
[49] prune_out_channels on _ElementWiseOp_260(BmmBackward0) => prune_out_channels on _Reshape_262(), #idxs=48
[50] prune_out_channels on _ElementWiseOp_260(BmmBackward0) => prune_out_channels on _Reshape_254(), #idxs=48
[51] prune_out_channels on _Reshape_254() => prune_out_channels on _ElementWiseOp_253(AddBackward0), #idxs=48
[52] prune_out_channels on _ElementWiseOp_253(AddBackward0) => prune_out_channels on _ElementWiseOp_255(UnsqueezeBackward0), #idxs=48
[53] prune_out_channels on _ElementWiseOp_253(AddBackward0) => prune_out_channels on _Reshape_252(), #idxs=48
[54] prune_out_channels on _Reshape_252() => prune_out_channels on _ElementWiseOp_251(AddBackward0), #idxs=48
[55] prune_out_channels on _ElementWiseOp_251(AddBackward0) => prune_out_channels on _Reshape_250(), #idxs=48
[56] prune_out_channels on _Reshape_250() => prune_out_channels on _ElementWiseOp_249(SoftmaxBackward0), #idxs=48
[57] prune_out_channels on _ElementWiseOp_249(SoftmaxBackward0) => prune_out_channels on _ElementWiseOp_248(ExpandBackward0), #idxs=48
[58] prune_out_channels on _ElementWiseOp_248(ExpandBackward0) => prune_out_channels on _Reshape_229(), #idxs=48
[59] prune_out_channels on _Reshape_229() => prune_out_channels on _ElementWiseOp_228(BmmBackward0), #idxs=48
[60] prune_out_channels on _ElementWiseOp_228(BmmBackward0) => prune_out_channels on _Reshape_230(), #idxs=48
[61] prune_out_channels on _ElementWiseOp_228(BmmBackward0) => prune_out_channels on _Reshape_227(), #idxs=48
[62] prune_out_channels on _Reshape_227() => prune_out_channels on _ElementWiseOp_226(TransposeBackward0), #idxs=48
[63] prune_out_channels on _ElementWiseOp_226(TransposeBackward0) => prune_out_channels on _ElementWiseOp_225(CloneBackward0), #idxs=48
[64] prune_out_channels on _ElementWiseOp_225(CloneBackward0) => prune_in_channels on stages.1.blocks.1.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=192, out_features=576, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=192, out_features=192, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)), #idxs=48
[65] prune_out_channels on _Reshape_230() => prune_out_channels on _ElementWiseOp_231(CloneBackward0), #idxs=48
[66] prune_out_channels on _ElementWiseOp_231(CloneBackward0) => prune_out_channels on _ElementWiseOp_232(ExpandBackward0), #idxs=48
[67] prune_out_channels on _ElementWiseOp_255(UnsqueezeBackward0) => prune_out_channels on _ElementWiseOp_256(CloneBackward0), #idxs=48
[68] prune_out_channels on _ElementWiseOp_256(CloneBackward0) => prune_out_channels on _ElementWiseOp_257(PermuteBackward0), #idxs=48
[69] prune_out_channels on _ElementWiseOp_257(PermuteBackward0) => prune_out_channels on _Reshape_258(), #idxs=48
[70] prune_out_channels on _Reshape_258() => prune_out_channels on _ElementWiseOp_259(IndexBackward0), #idxs=48
[71] prune_out_channels on _Reshape_262() => prune_out_channels on _ElementWiseOp_263(CloneBackward0), #idxs=48
[72] prune_out_channels on _ElementWiseOp_263(CloneBackward0) => prune_out_channels on _ElementWiseOp_264(ExpandBackward0), #idxs=48
[73] prune_out_channels on _ElementWiseOp_264(ExpandBackward0) => prune_out_channels on _ElementWiseOp_265(TransposeBackward0), #idxs=48
[74] prune_out_channels on _ElementWiseOp_14(AddBackward0) => prune_out_channels on _Reshape_22(), #idxs=48
[75] prune_out_channels on _ElementWiseOp_14(AddBackward0) => prune_out_channels on stages.0.downsample (PatchMerging(
  (adap_padding): AdaptivePadding()
  (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
  (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (reduction): Linear(in_features=384, out_features=192, bias=False)
)), #idxs=48
[76] prune_out_channels on _ElementWiseOp_14(AddBackward0) => prune_out_channels on stages.1.blocks.0.norm2 (LayerNorm((192,), eps=1e-05, elementwise_affine=True)), #idxs=48
[77] prune_out_channels on stages.1.blocks.0.norm2 (LayerNorm((192,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_20(), #idxs=48
[78] prune_out_channels on _Reshape_20() => prune_out_channels on _ElementWiseOp_19(AddmmBackward0), #idxs=48
[79] prune_out_channels on _ElementWiseOp_19(AddmmBackward0) => prune_out_channels on _ElementWiseOp_21(TBackward0), #idxs=48
[80] prune_out_channels on _ElementWiseOp_19(AddmmBackward0) => prune_in_channels on stages.1.blocks.0.ffn.layers.0.0 (Linear(in_features=192, out_features=768, bias=True)), #idxs=48
[81] prune_out_channels on stages.0.downsample (PatchMerging(
  (adap_padding): AdaptivePadding()
  (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
  (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (reduction): Linear(in_features=384, out_features=192, bias=False)
)) => prune_out_channels on stages.1.blocks.0.norm1 (LayerNorm((192,), eps=1e-05, elementwise_affine=True)), #idxs=48
[82] prune_out_channels on stages.1.blocks.0.norm1 (LayerNorm((192,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_193(), #idxs=48
[83] prune_out_channels on _Reshape_193() => prune_out_channels on _ElementWiseOp_192(ConstantPadNdBackward0), #idxs=48
[84] prune_out_channels on _ElementWiseOp_192(ConstantPadNdBackward0) => prune_out_channels on _Reshape_191(), #idxs=48
[85] prune_out_channels on _Reshape_191() => prune_out_channels on _ElementWiseOp_190(PermuteBackward0), #idxs=48
[86] prune_out_channels on _ElementWiseOp_190(PermuteBackward0) => prune_out_channels on _ElementWiseOp_189(CloneBackward0), #idxs=48
[87] prune_out_channels on _ElementWiseOp_189(CloneBackward0) => prune_out_channels on _Reshape_188(), #idxs=48
[88] prune_out_channels on _Reshape_188() => prune_out_channels on _Reshape_187(), #idxs=48
[89] prune_out_channels on _Reshape_187() => prune_out_channels on _Reshape_185(), #idxs=48
[90] prune_out_channels on _Reshape_185() => prune_out_channels on _ElementWiseOp_184(AddmmBackward0), #idxs=48
[91] prune_out_channels on _ElementWiseOp_184(AddmmBackward0) => prune_out_channels on _ElementWiseOp_186(TBackward0), #idxs=48
[92] prune_out_channels on _ElementWiseOp_184(AddmmBackward0) => prune_out_channels on _Reshape_183(), #idxs=48
[93] prune_out_channels on _Reshape_183() => prune_out_channels on _Reshape_182(), #idxs=48
[94] prune_out_channels on _Reshape_182() => prune_out_channels on _ElementWiseOp_181(PermuteBackward0), #idxs=48
[95] prune_out_channels on _ElementWiseOp_181(PermuteBackward0) => prune_out_channels on _ElementWiseOp_180(SelectBackward0), #idxs=48
[96] prune_out_channels on _ElementWiseOp_181(PermuteBackward0) => prune_out_channels on _ElementWiseOp_209(SelectBackward0), #idxs=48
[97] prune_out_channels on _ElementWiseOp_181(PermuteBackward0) => prune_out_channels on _ElementWiseOp_213(SelectBackward0), #idxs=48
[98] prune_out_channels on _ElementWiseOp_213(SelectBackward0) => prune_out_channels on _ElementWiseOp_212(MulBackward0), #idxs=48
[99] prune_out_channels on _ElementWiseOp_212(MulBackward0) => prune_out_channels on _ElementWiseOp_211(ExpandBackward0), #idxs=48
[100] prune_out_channels on _ElementWiseOp_211(ExpandBackward0) => prune_out_channels on _ElementWiseOp_210(CloneBackward0), #idxs=48
[101] prune_out_channels on _ElementWiseOp_210(CloneBackward0) => prune_out_channels on _Reshape_204(), #idxs=48
[102] prune_out_channels on _Reshape_204() => prune_out_channels on _ElementWiseOp_203(BmmBackward0), #idxs=48
[103] prune_out_channels on _ElementWiseOp_203(BmmBackward0) => prune_out_channels on _Reshape_205(), #idxs=48
[104] prune_out_channels on _ElementWiseOp_203(BmmBackward0) => prune_out_channels on _Reshape_197(), #idxs=48
[105] prune_out_channels on _Reshape_197() => prune_out_channels on _ElementWiseOp_196(AddBackward0), #idxs=48
[106] prune_out_channels on _ElementWiseOp_196(AddBackward0) => prune_out_channels on _ElementWiseOp_198(UnsqueezeBackward0), #idxs=48
[107] prune_out_channels on _ElementWiseOp_196(AddBackward0) => prune_out_channels on _ElementWiseOp_195(SoftmaxBackward0), #idxs=48
[108] prune_out_channels on _ElementWiseOp_195(SoftmaxBackward0) => prune_out_channels on _ElementWiseOp_194(ExpandBackward0), #idxs=48
[109] prune_out_channels on _ElementWiseOp_194(ExpandBackward0) => prune_out_channels on _Reshape_176(), #idxs=48
[110] prune_out_channels on _Reshape_176() => prune_out_channels on _ElementWiseOp_175(BmmBackward0), #idxs=48
[111] prune_out_channels on _ElementWiseOp_175(BmmBackward0) => prune_out_channels on _Reshape_177(), #idxs=48
[112] prune_out_channels on _ElementWiseOp_175(BmmBackward0) => prune_out_channels on _Reshape_174(), #idxs=48
[113] prune_out_channels on _Reshape_174() => prune_out_channels on _ElementWiseOp_173(TransposeBackward0), #idxs=48
[114] prune_out_channels on _ElementWiseOp_173(TransposeBackward0) => prune_out_channels on _ElementWiseOp_172(CloneBackward0), #idxs=48
[115] prune_out_channels on _ElementWiseOp_172(CloneBackward0) => prune_in_channels on stages.1.blocks.0.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=192, out_features=576, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=192, out_features=192, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)), #idxs=48
[116] prune_out_channels on _Reshape_177() => prune_out_channels on _ElementWiseOp_178(CloneBackward0), #idxs=48
[117] prune_out_channels on _ElementWiseOp_178(CloneBackward0) => prune_out_channels on _ElementWiseOp_179(ExpandBackward0), #idxs=48
[118] prune_out_channels on _ElementWiseOp_198(UnsqueezeBackward0) => prune_out_channels on _ElementWiseOp_199(CloneBackward0), #idxs=48
[119] prune_out_channels on _ElementWiseOp_199(CloneBackward0) => prune_out_channels on _ElementWiseOp_200(PermuteBackward0), #idxs=48
[120] prune_out_channels on _ElementWiseOp_200(PermuteBackward0) => prune_out_channels on _Reshape_201(), #idxs=48
[121] prune_out_channels on _Reshape_201() => prune_out_channels on _ElementWiseOp_202(IndexBackward0), #idxs=48
[122] prune_out_channels on _Reshape_205() => prune_out_channels on _ElementWiseOp_206(CloneBackward0), #idxs=48
[123] prune_out_channels on _ElementWiseOp_206(CloneBackward0) => prune_out_channels on _ElementWiseOp_207(ExpandBackward0), #idxs=48
[124] prune_out_channels on _ElementWiseOp_207(ExpandBackward0) => prune_out_channels on _ElementWiseOp_208(TransposeBackward0), #idxs=48
[125] prune_out_channels on _Reshape_22() => prune_out_channels on _ElementWiseOp_162(CloneBackward0), #idxs=48
[126] prune_out_channels on _ElementWiseOp_162(CloneBackward0) => prune_out_channels on _ElementWiseOp_163(SliceBackward0), #idxs=48
[127] prune_out_channels on _ElementWiseOp_163(SliceBackward0) => prune_out_channels on _ElementWiseOp_164(SliceBackward0), #idxs=48
[128] prune_out_channels on _ElementWiseOp_164(SliceBackward0) => prune_out_channels on _ElementWiseOp_165(SliceBackward0), #idxs=48
[129] prune_out_channels on _ElementWiseOp_165(SliceBackward0) => prune_out_channels on _ElementWiseOp_166(SliceBackward0), #idxs=48
[130] prune_out_channels on _ElementWiseOp_166(SliceBackward0) => prune_out_channels on _Reshape_167(), #idxs=48
[131] prune_out_channels on _Reshape_167() => prune_out_channels on _ElementWiseOp_168(CloneBackward0), #idxs=48
[132] prune_out_channels on _ElementWiseOp_168(CloneBackward0) => prune_out_channels on _ElementWiseOp_169(PermuteBackward0), #idxs=48
[133] prune_out_channels on _ElementWiseOp_169(PermuteBackward0) => prune_out_channels on _Reshape_170(), #idxs=48
[134] prune_out_channels on _Reshape_170() => prune_out_channels on _Reshape_171(), #idxs=48
[135] prune_out_channels on _Reshape_171() => prune_out_channels on stages.1.blocks.0.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=192, out_features=576, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=192, out_features=192, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)), #idxs=48
[136] prune_out_channels on _Reshape_12() => prune_out_channels on _ElementWiseOp_214(CloneBackward0), #idxs=48
[137] prune_out_channels on _ElementWiseOp_214(CloneBackward0) => prune_out_channels on _ElementWiseOp_215(SliceBackward0), #idxs=48
[138] prune_out_channels on _ElementWiseOp_215(SliceBackward0) => prune_out_channels on _ElementWiseOp_216(SliceBackward0), #idxs=48
[139] prune_out_channels on _ElementWiseOp_216(SliceBackward0) => prune_out_channels on _ElementWiseOp_217(SliceBackward0), #idxs=48
[140] prune_out_channels on _ElementWiseOp_217(SliceBackward0) => prune_out_channels on _ElementWiseOp_218(SliceBackward0), #idxs=48
[141] prune_out_channels on _ElementWiseOp_218(SliceBackward0) => prune_out_channels on _ElementWiseOp_219(RollBackward0), #idxs=48
[142] prune_out_channels on _ElementWiseOp_219(RollBackward0) => prune_out_channels on _Reshape_220(), #idxs=48
[143] prune_out_channels on _Reshape_220() => prune_out_channels on _ElementWiseOp_221(CloneBackward0), #idxs=48
[144] prune_out_channels on _ElementWiseOp_221(CloneBackward0) => prune_out_channels on _ElementWiseOp_222(PermuteBackward0), #idxs=48
[145] prune_out_channels on _ElementWiseOp_222(PermuteBackward0) => prune_out_channels on _Reshape_223(), #idxs=48
[146] prune_out_channels on _Reshape_223() => prune_out_channels on _Reshape_224(), #idxs=48
[147] prune_out_channels on _Reshape_224() => prune_out_channels on stages.1.blocks.1.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=192, out_features=576, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=192, out_features=192, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)), #idxs=48
--------------------------------

PatchMergingPruner () prune_in_channels/  768 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
PatchMergingPruner () prune_in_channels/ dim =  192
len indx =  48
idxs_repeated =  192
WindowMSAPruner prune_in_channels() /  48
PatchMergingPruner () prune_out_channels/  192 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
WindowMSAPruner prune_in_channels() /  48
WindowMSAPruner prune_out_channels() /  48
WindowMSAPruner prune_out_channels/ dim =  192
	len indx =  48
WindowMSAPruner - idxs =  [2, 12, 21, 32, 33, 42, 48, 50, 51, 53, 54, 56, 57, 64, 76, 81, 82, 85, 86, 91, 92, 103, 104, 106, 108, 109, 113, 115, 117, 118, 125, 129, 130, 132, 140, 149, 150, 154, 155, 159, 161, 163, 165, 170, 175, 180, 186, 190]
	idxs_repeated =  192
WindowMSAPruner - idxs_repeated =  [2, 12, 21, 32, 33, 42, 48, 50, 51, 53, 54, 56, 57, 64, 76, 81, 82, 85, 86, 91, 92, 103, 104, 106, 108, 109, 113, 115, 117, 118, 125, 129, 130, 132, 140, 149, 150, 154, 155, 159, 161, 163, 165, 170, 175, 180, 186, 190, 194, 204, 213, 224, 225, 234, 240, 242, 243, 245, 246, 248, 249, 256, 268, 273, 274, 277, 278, 283, 284, 295, 296, 298, 300, 301, 305, 307, 309, 310, 317, 321, 322, 324, 332, 341, 342, 346, 347, 351, 353, 355, 357, 362, 367, 372, 378, 382, 386, 396, 405, 416, 417, 426, 432, 434, 435, 437, 438, 440, 441, 448, 460, 465, 466, 469, 470, 475, 476, 487, 488, 490, 492, 493, 497, 499, 501, 502, 509, 513, 514, 516, 524, 533, 534, 538, 539, 543, 545, 547, 549, 554, 559, 564, 570, 574, 578, 588, 597, 608, 609, 618, 624, 626, 627, 629, 630, 632, 633, 640, 652, 657, 658, 661, 662, 667, 668, 679, 680, 682, 684, 685, 689, 691, 693, 694, 701, 705, 706, 708, 716, 725, 726, 730, 731, 735, 737, 739, 741, 746, 751, 756, 762, 766]
WindowMSAPruner prune_out_channels idxs_repeated =  192
WindowMSAPruner prune_out_channels() /  48
WindowMSAPruner prune_out_channels/ dim =  192
	len indx =  48
WindowMSAPruner - idxs =  [2, 12, 21, 32, 33, 42, 48, 50, 51, 53, 54, 56, 57, 64, 76, 81, 82, 85, 86, 91, 92, 103, 104, 106, 108, 109, 113, 115, 117, 118, 125, 129, 130, 132, 140, 149, 150, 154, 155, 159, 161, 163, 165, 170, 175, 180, 186, 190]
	idxs_repeated =  192
WindowMSAPruner - idxs_repeated =  [2, 12, 21, 32, 33, 42, 48, 50, 51, 53, 54, 56, 57, 64, 76, 81, 82, 85, 86, 91, 92, 103, 104, 106, 108, 109, 113, 115, 117, 118, 125, 129, 130, 132, 140, 149, 150, 154, 155, 159, 161, 163, 165, 170, 175, 180, 186, 190, 194, 204, 213, 224, 225, 234, 240, 242, 243, 245, 246, 248, 249, 256, 268, 273, 274, 277, 278, 283, 284, 295, 296, 298, 300, 301, 305, 307, 309, 310, 317, 321, 322, 324, 332, 341, 342, 346, 347, 351, 353, 355, 357, 362, 367, 372, 378, 382, 386, 396, 405, 416, 417, 426, 432, 434, 435, 437, 438, 440, 441, 448, 460, 465, 466, 469, 470, 475, 476, 487, 488, 490, 492, 493, 497, 499, 501, 502, 509, 513, 514, 516, 524, 533, 534, 538, 539, 543, 545, 547, 549, 554, 559, 564, 570, 574, 578, 588, 597, 608, 609, 618, 624, 626, 627, 629, 630, 632, 633, 640, 652, 657, 658, 661, 662, 667, 668, 679, 680, 682, 684, 685, 689, 691, 693, 694, 701, 705, 706, 708, 716, 725, 726, 730, 731, 735, 737, 739, 741, 746, 751, 756, 762, 766]
WindowMSAPruner prune_out_channels idxs_repeated =  192
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
	get_all_groups -- Adding layer:  <class 'mmcv.cnn.bricks.wrappers.Linear'>
		prune_local()/  _check_pruning_ratio OK
idxs =  192
prunable_chs =  768
pruner.step 
--------------------------------
          Pruning Group
--------------------------------
[0] prune_out_channels on stages.1.blocks.1.ffn.layers.0.0 (Linear(in_features=144, out_features=768, bias=True)) => prune_out_channels on stages.1.blocks.1.ffn.layers.0.0 (Linear(in_features=144, out_features=768, bias=True)), #idxs=192
[1] prune_out_channels on stages.1.blocks.1.ffn.layers.0.0 (Linear(in_features=144, out_features=768, bias=True)) => prune_out_channels on _ElementWiseOp_8(GeluBackward0), #idxs=192
[2] prune_out_channels on _ElementWiseOp_8(GeluBackward0) => prune_out_channels on _Reshape_6(), #idxs=192
[3] prune_out_channels on _Reshape_6() => prune_out_channels on _ElementWiseOp_5(AddmmBackward0), #idxs=192
[4] prune_out_channels on _ElementWiseOp_5(AddmmBackward0) => prune_out_channels on _ElementWiseOp_7(TBackward0), #idxs=192
[5] prune_out_channels on _ElementWiseOp_5(AddmmBackward0) => prune_in_channels on stages.1.blocks.1.ffn.layers.1 (Linear(in_features=768, out_features=144, bias=True)), #idxs=192
--------------------------------

<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
continue 4
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
	get_all_groups -- Adding layer:  <class 'mmcv.cnn.bricks.wrappers.Linear'>
		prune_local()/  _check_pruning_ratio OK
idxs =  192
prunable_chs =  768
pruner.step 
--------------------------------
          Pruning Group
--------------------------------
[0] prune_out_channels on stages.1.blocks.0.ffn.layers.0.0 (Linear(in_features=144, out_features=768, bias=True)) => prune_out_channels on stages.1.blocks.0.ffn.layers.0.0 (Linear(in_features=144, out_features=768, bias=True)), #idxs=192
[1] prune_out_channels on stages.1.blocks.0.ffn.layers.0.0 (Linear(in_features=144, out_features=768, bias=True)) => prune_out_channels on _ElementWiseOp_18(GeluBackward0), #idxs=192
[2] prune_out_channels on _ElementWiseOp_18(GeluBackward0) => prune_out_channels on _Reshape_16(), #idxs=192
[3] prune_out_channels on _Reshape_16() => prune_out_channels on _ElementWiseOp_15(AddmmBackward0), #idxs=192
[4] prune_out_channels on _ElementWiseOp_15(AddmmBackward0) => prune_out_channels on _ElementWiseOp_17(TBackward0), #idxs=192
[5] prune_out_channels on _ElementWiseOp_15(AddmmBackward0) => prune_in_channels on stages.1.blocks.0.ffn.layers.1 (Linear(in_features=768, out_features=144, bias=True)), #idxs=192
--------------------------------

<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
continue 4
<class 'torch_pruning.ops._ReshapeOp'> continue 2
PatchMergingPruner () get_out_channels/  144 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
	get_all_groups -- Adding layer:  <class 'mmcv.cnn.bricks.wrappers.Linear'>
PatchMergingPruner () get_in_channels/  96 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
WindowMSAPruner layer.qkv.in_features =  96
WindowMSAPruner layer.qkv.in_features =  96
WindowMSAPruner layer.embed_dims =  96
WindowMSAPruner layer.embed_dims =  96
		prune_local()/  _check_pruning_ratio OK
idxs =  24
prunable_chs =  96
PatchMergingPruner () get_in_channels/  96 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
idxs =  24
prunable_chs =  96
idxs =  24
prunable_chs =  96
idxs =  24
prunable_chs =  96
WindowMSAPruner layer.qkv.in_features =  96
idxs =  24
prunable_chs =  96
idxs =  24
prunable_chs =  96
idxs =  24
prunable_chs =  96
WindowMSAPruner layer.qkv.in_features =  96
idxs =  24
prunable_chs =  96
WindowMSAPruner layer.embed_dims =  96
idxs =  24
prunable_chs =  96
WindowMSAPruner layer.embed_dims =  96
idxs =  24
prunable_chs =  96
pruner.step 
--------------------------------
          Pruning Group
--------------------------------
[0] prune_out_channels on stages.0.blocks.1.ffn.layers.1 (Linear(in_features=384, out_features=96, bias=True)) => prune_out_channels on stages.0.blocks.1.ffn.layers.1 (Linear(in_features=384, out_features=96, bias=True)), #idxs=24
[1] prune_out_channels on stages.0.blocks.1.ffn.layers.1 (Linear(in_features=384, out_features=96, bias=True)) => prune_out_channels on _ElementWiseOp_31(AddBackward0), #idxs=24
[2] prune_out_channels on _ElementWiseOp_31(AddBackward0) => prune_out_channels on _ElementWiseOp_32(AddBackward0), #idxs=24
[3] prune_out_channels on _ElementWiseOp_31(AddBackward0) => prune_out_channels on _Reshape_30(), #idxs=24
[4] prune_out_channels on _Reshape_30() => prune_out_channels on _ElementWiseOp_29(PermuteBackward0), #idxs=24
[5] prune_out_channels on _ElementWiseOp_29(PermuteBackward0) => prune_out_channels on _ElementWiseOp_28(Im2ColBackward0), #idxs=24
[6] prune_out_channels on _ElementWiseOp_28(Im2ColBackward0) => prune_out_channels on _ElementWiseOp_27(TransposeBackward0), #idxs=24
[7] prune_out_channels on _ElementWiseOp_27(TransposeBackward0) => prune_out_channels on _ElementWiseOp_26(NativeLayerNormBackward0), #idxs=24
[8] prune_out_channels on _ElementWiseOp_26(NativeLayerNormBackward0) => prune_out_channels on _Reshape_24(), #idxs=24
[9] prune_out_channels on _Reshape_24() => prune_out_channels on _ElementWiseOp_23(MmBackward0), #idxs=24
[10] prune_out_channels on _ElementWiseOp_23(MmBackward0) => prune_out_channels on _ElementWiseOp_25(TBackward0), #idxs=24
[11] prune_out_channels on _ElementWiseOp_23(MmBackward0) => prune_in_channels on stages.0.downsample (PatchMerging(
  (adap_padding): AdaptivePadding()
  (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
  (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (reduction): Linear(in_features=384, out_features=144, bias=False)
)), #idxs=24
[12] prune_out_channels on _ElementWiseOp_32(AddBackward0) => prune_out_channels on _Reshape_40(), #idxs=24
[13] prune_out_channels on _ElementWiseOp_32(AddBackward0) => prune_out_channels on _ElementWiseOp_41(AddBackward0), #idxs=24
[14] prune_out_channels on _ElementWiseOp_32(AddBackward0) => prune_out_channels on stages.0.blocks.1.norm2 (LayerNorm((96,), eps=1e-05, elementwise_affine=True)), #idxs=24
[15] prune_out_channels on stages.0.blocks.1.norm2 (LayerNorm((96,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_38(), #idxs=24
[16] prune_out_channels on _Reshape_38() => prune_out_channels on _ElementWiseOp_37(AddmmBackward0), #idxs=24
[17] prune_out_channels on _ElementWiseOp_37(AddmmBackward0) => prune_out_channels on _ElementWiseOp_39(TBackward0), #idxs=24
[18] prune_out_channels on _ElementWiseOp_37(AddmmBackward0) => prune_in_channels on stages.0.blocks.1.ffn.layers.0.0 (Linear(in_features=96, out_features=384, bias=True)), #idxs=24
[19] prune_out_channels on _ElementWiseOp_41(AddBackward0) => prune_out_channels on _ElementWiseOp_42(AddBackward0), #idxs=24
[20] prune_out_channels on _ElementWiseOp_41(AddBackward0) => prune_out_channels on stages.0.blocks.0.ffn.layers.1 (Linear(in_features=384, out_features=96, bias=True)), #idxs=24
[21] prune_out_channels on _ElementWiseOp_41(AddBackward0) => prune_out_channels on stages.0.blocks.1.norm1 (LayerNorm((96,), eps=1e-05, elementwise_affine=True)), #idxs=24
[22] prune_out_channels on stages.0.blocks.1.norm1 (LayerNorm((96,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_138(), #idxs=24
[23] prune_out_channels on _Reshape_138() => prune_out_channels on _ElementWiseOp_137(ConstantPadNdBackward0), #idxs=24
[24] prune_out_channels on _ElementWiseOp_137(ConstantPadNdBackward0) => prune_out_channels on _ElementWiseOp_136(RollBackward0), #idxs=24
[25] prune_out_channels on _ElementWiseOp_136(RollBackward0) => prune_out_channels on _Reshape_135(), #idxs=24
[26] prune_out_channels on _Reshape_135() => prune_out_channels on _ElementWiseOp_134(PermuteBackward0), #idxs=24
[27] prune_out_channels on _ElementWiseOp_134(PermuteBackward0) => prune_out_channels on _ElementWiseOp_133(CloneBackward0), #idxs=24
[28] prune_out_channels on _ElementWiseOp_133(CloneBackward0) => prune_out_channels on _Reshape_132(), #idxs=24
[29] prune_out_channels on _Reshape_132() => prune_out_channels on _Reshape_131(), #idxs=24
[30] prune_out_channels on _Reshape_131() => prune_out_channels on _Reshape_129(), #idxs=24
[31] prune_out_channels on _Reshape_129() => prune_out_channels on _ElementWiseOp_128(AddmmBackward0), #idxs=24
[32] prune_out_channels on _ElementWiseOp_128(AddmmBackward0) => prune_out_channels on _ElementWiseOp_130(TBackward0), #idxs=24
[33] prune_out_channels on _ElementWiseOp_128(AddmmBackward0) => prune_out_channels on _Reshape_127(), #idxs=24
[34] prune_out_channels on _Reshape_127() => prune_out_channels on _Reshape_126(), #idxs=24
[35] prune_out_channels on _Reshape_126() => prune_out_channels on _ElementWiseOp_125(PermuteBackward0), #idxs=24
[36] prune_out_channels on _ElementWiseOp_125(PermuteBackward0) => prune_out_channels on _ElementWiseOp_124(SelectBackward0), #idxs=24
[37] prune_out_channels on _ElementWiseOp_125(PermuteBackward0) => prune_out_channels on _ElementWiseOp_157(SelectBackward0), #idxs=24
[38] prune_out_channels on _ElementWiseOp_125(PermuteBackward0) => prune_out_channels on _ElementWiseOp_161(SelectBackward0), #idxs=24
[39] prune_out_channels on _ElementWiseOp_161(SelectBackward0) => prune_out_channels on _ElementWiseOp_160(MulBackward0), #idxs=24
[40] prune_out_channels on _ElementWiseOp_160(MulBackward0) => prune_out_channels on _ElementWiseOp_159(ExpandBackward0), #idxs=24
[41] prune_out_channels on _ElementWiseOp_159(ExpandBackward0) => prune_out_channels on _ElementWiseOp_158(CloneBackward0), #idxs=24
[42] prune_out_channels on _ElementWiseOp_158(CloneBackward0) => prune_out_channels on _Reshape_152(), #idxs=24
[43] prune_out_channels on _Reshape_152() => prune_out_channels on _ElementWiseOp_151(BmmBackward0), #idxs=24
[44] prune_out_channels on _ElementWiseOp_151(BmmBackward0) => prune_out_channels on _Reshape_153(), #idxs=24
[45] prune_out_channels on _ElementWiseOp_151(BmmBackward0) => prune_out_channels on _Reshape_145(), #idxs=24
[46] prune_out_channels on _Reshape_145() => prune_out_channels on _ElementWiseOp_144(AddBackward0), #idxs=24
[47] prune_out_channels on _ElementWiseOp_144(AddBackward0) => prune_out_channels on _ElementWiseOp_146(UnsqueezeBackward0), #idxs=24
[48] prune_out_channels on _ElementWiseOp_144(AddBackward0) => prune_out_channels on _Reshape_143(), #idxs=24
[49] prune_out_channels on _Reshape_143() => prune_out_channels on _ElementWiseOp_142(AddBackward0), #idxs=24
[50] prune_out_channels on _ElementWiseOp_142(AddBackward0) => prune_out_channels on _Reshape_141(), #idxs=24
[51] prune_out_channels on _Reshape_141() => prune_out_channels on _ElementWiseOp_140(SoftmaxBackward0), #idxs=24
[52] prune_out_channels on _ElementWiseOp_140(SoftmaxBackward0) => prune_out_channels on _ElementWiseOp_139(ExpandBackward0), #idxs=24
[53] prune_out_channels on _ElementWiseOp_139(ExpandBackward0) => prune_out_channels on _Reshape_120(), #idxs=24
[54] prune_out_channels on _Reshape_120() => prune_out_channels on _ElementWiseOp_119(BmmBackward0), #idxs=24
[55] prune_out_channels on _ElementWiseOp_119(BmmBackward0) => prune_out_channels on _Reshape_121(), #idxs=24
[56] prune_out_channels on _ElementWiseOp_119(BmmBackward0) => prune_out_channels on _Reshape_118(), #idxs=24
[57] prune_out_channels on _Reshape_118() => prune_out_channels on _ElementWiseOp_117(TransposeBackward0), #idxs=24
[58] prune_out_channels on _ElementWiseOp_117(TransposeBackward0) => prune_out_channels on _ElementWiseOp_116(CloneBackward0), #idxs=24
[59] prune_out_channels on _ElementWiseOp_116(CloneBackward0) => prune_in_channels on stages.0.blocks.1.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=96, out_features=288, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=96, out_features=96, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)), #idxs=24
[60] prune_out_channels on _Reshape_121() => prune_out_channels on _ElementWiseOp_122(CloneBackward0), #idxs=24
[61] prune_out_channels on _ElementWiseOp_122(CloneBackward0) => prune_out_channels on _ElementWiseOp_123(ExpandBackward0), #idxs=24
[62] prune_out_channels on _ElementWiseOp_146(UnsqueezeBackward0) => prune_out_channels on _ElementWiseOp_147(CloneBackward0), #idxs=24
[63] prune_out_channels on _ElementWiseOp_147(CloneBackward0) => prune_out_channels on _ElementWiseOp_148(PermuteBackward0), #idxs=24
[64] prune_out_channels on _ElementWiseOp_148(PermuteBackward0) => prune_out_channels on _Reshape_149(), #idxs=24
[65] prune_out_channels on _Reshape_149() => prune_out_channels on _ElementWiseOp_150(IndexBackward0), #idxs=24
[66] prune_out_channels on _Reshape_153() => prune_out_channels on _ElementWiseOp_154(CloneBackward0), #idxs=24
[67] prune_out_channels on _ElementWiseOp_154(CloneBackward0) => prune_out_channels on _ElementWiseOp_155(ExpandBackward0), #idxs=24
[68] prune_out_channels on _ElementWiseOp_155(ExpandBackward0) => prune_out_channels on _ElementWiseOp_156(TransposeBackward0), #idxs=24
[69] prune_out_channels on _ElementWiseOp_42(AddBackward0) => prune_out_channels on _Reshape_50(), #idxs=24
[70] prune_out_channels on _ElementWiseOp_42(AddBackward0) => prune_out_channels on patch_embed.norm (LayerNorm((96,), eps=1e-05, elementwise_affine=True)), #idxs=24
[71] prune_out_channels on _ElementWiseOp_42(AddBackward0) => prune_out_channels on stages.0.blocks.0.norm2 (LayerNorm((96,), eps=1e-05, elementwise_affine=True)), #idxs=24
[72] prune_out_channels on stages.0.blocks.0.norm2 (LayerNorm((96,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_48(), #idxs=24
[73] prune_out_channels on _Reshape_48() => prune_out_channels on _ElementWiseOp_47(AddmmBackward0), #idxs=24
[74] prune_out_channels on _ElementWiseOp_47(AddmmBackward0) => prune_out_channels on _ElementWiseOp_49(TBackward0), #idxs=24
[75] prune_out_channels on _ElementWiseOp_47(AddmmBackward0) => prune_in_channels on stages.0.blocks.0.ffn.layers.0.0 (Linear(in_features=96, out_features=384, bias=True)), #idxs=24
[76] prune_out_channels on patch_embed.norm (LayerNorm((96,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _ElementWiseOp_51(TransposeBackward0), #idxs=24
[77] prune_out_channels on patch_embed.norm (LayerNorm((96,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on stages.0.blocks.0.norm1 (LayerNorm((96,), eps=1e-05, elementwise_affine=True)), #idxs=24
[78] prune_out_channels on stages.0.blocks.0.norm1 (LayerNorm((96,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_84(), #idxs=24
[79] prune_out_channels on _Reshape_84() => prune_out_channels on _ElementWiseOp_83(ConstantPadNdBackward0), #idxs=24
[80] prune_out_channels on _ElementWiseOp_83(ConstantPadNdBackward0) => prune_out_channels on _Reshape_82(), #idxs=24
[81] prune_out_channels on _Reshape_82() => prune_out_channels on _ElementWiseOp_81(PermuteBackward0), #idxs=24
[82] prune_out_channels on _ElementWiseOp_81(PermuteBackward0) => prune_out_channels on _ElementWiseOp_80(CloneBackward0), #idxs=24
[83] prune_out_channels on _ElementWiseOp_80(CloneBackward0) => prune_out_channels on _Reshape_79(), #idxs=24
[84] prune_out_channels on _Reshape_79() => prune_out_channels on _Reshape_78(), #idxs=24
[85] prune_out_channels on _Reshape_78() => prune_out_channels on _Reshape_76(), #idxs=24
[86] prune_out_channels on _Reshape_76() => prune_out_channels on _ElementWiseOp_75(AddmmBackward0), #idxs=24
[87] prune_out_channels on _ElementWiseOp_75(AddmmBackward0) => prune_out_channels on _ElementWiseOp_77(TBackward0), #idxs=24
[88] prune_out_channels on _ElementWiseOp_75(AddmmBackward0) => prune_out_channels on _Reshape_74(), #idxs=24
[89] prune_out_channels on _Reshape_74() => prune_out_channels on _Reshape_73(), #idxs=24
[90] prune_out_channels on _Reshape_73() => prune_out_channels on _ElementWiseOp_72(PermuteBackward0), #idxs=24
[91] prune_out_channels on _ElementWiseOp_72(PermuteBackward0) => prune_out_channels on _ElementWiseOp_71(SelectBackward0), #idxs=24
[92] prune_out_channels on _ElementWiseOp_72(PermuteBackward0) => prune_out_channels on _ElementWiseOp_100(SelectBackward0), #idxs=24
[93] prune_out_channels on _ElementWiseOp_72(PermuteBackward0) => prune_out_channels on _ElementWiseOp_104(SelectBackward0), #idxs=24
[94] prune_out_channels on _ElementWiseOp_104(SelectBackward0) => prune_out_channels on _ElementWiseOp_103(MulBackward0), #idxs=24
[95] prune_out_channels on _ElementWiseOp_103(MulBackward0) => prune_out_channels on _ElementWiseOp_102(ExpandBackward0), #idxs=24
[96] prune_out_channels on _ElementWiseOp_102(ExpandBackward0) => prune_out_channels on _ElementWiseOp_101(CloneBackward0), #idxs=24
[97] prune_out_channels on _ElementWiseOp_101(CloneBackward0) => prune_out_channels on _Reshape_95(), #idxs=24
[98] prune_out_channels on _Reshape_95() => prune_out_channels on _ElementWiseOp_94(BmmBackward0), #idxs=24
[99] prune_out_channels on _ElementWiseOp_94(BmmBackward0) => prune_out_channels on _Reshape_96(), #idxs=24
[100] prune_out_channels on _ElementWiseOp_94(BmmBackward0) => prune_out_channels on _Reshape_88(), #idxs=24
[101] prune_out_channels on _Reshape_88() => prune_out_channels on _ElementWiseOp_87(AddBackward0), #idxs=24
[102] prune_out_channels on _ElementWiseOp_87(AddBackward0) => prune_out_channels on _ElementWiseOp_89(UnsqueezeBackward0), #idxs=24
[103] prune_out_channels on _ElementWiseOp_87(AddBackward0) => prune_out_channels on _ElementWiseOp_86(SoftmaxBackward0), #idxs=24
[104] prune_out_channels on _ElementWiseOp_86(SoftmaxBackward0) => prune_out_channels on _ElementWiseOp_85(ExpandBackward0), #idxs=24
[105] prune_out_channels on _ElementWiseOp_85(ExpandBackward0) => prune_out_channels on _Reshape_67(), #idxs=24
[106] prune_out_channels on _Reshape_67() => prune_out_channels on _ElementWiseOp_66(BmmBackward0), #idxs=24
[107] prune_out_channels on _ElementWiseOp_66(BmmBackward0) => prune_out_channels on _Reshape_68(), #idxs=24
[108] prune_out_channels on _ElementWiseOp_66(BmmBackward0) => prune_out_channels on _Reshape_65(), #idxs=24
[109] prune_out_channels on _Reshape_65() => prune_out_channels on _ElementWiseOp_64(TransposeBackward0), #idxs=24
[110] prune_out_channels on _ElementWiseOp_64(TransposeBackward0) => prune_out_channels on _ElementWiseOp_63(CloneBackward0), #idxs=24
[111] prune_out_channels on _ElementWiseOp_63(CloneBackward0) => prune_in_channels on stages.0.blocks.0.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=96, out_features=288, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=96, out_features=96, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)), #idxs=24
[112] prune_out_channels on _Reshape_68() => prune_out_channels on _ElementWiseOp_69(CloneBackward0), #idxs=24
[113] prune_out_channels on _ElementWiseOp_69(CloneBackward0) => prune_out_channels on _ElementWiseOp_70(ExpandBackward0), #idxs=24
[114] prune_out_channels on _ElementWiseOp_89(UnsqueezeBackward0) => prune_out_channels on _ElementWiseOp_90(CloneBackward0), #idxs=24
[115] prune_out_channels on _ElementWiseOp_90(CloneBackward0) => prune_out_channels on _ElementWiseOp_91(PermuteBackward0), #idxs=24
[116] prune_out_channels on _ElementWiseOp_91(PermuteBackward0) => prune_out_channels on _Reshape_92(), #idxs=24
[117] prune_out_channels on _Reshape_92() => prune_out_channels on _ElementWiseOp_93(IndexBackward0), #idxs=24
[118] prune_out_channels on _Reshape_96() => prune_out_channels on _ElementWiseOp_97(CloneBackward0), #idxs=24
[119] prune_out_channels on _ElementWiseOp_97(CloneBackward0) => prune_out_channels on _ElementWiseOp_98(ExpandBackward0), #idxs=24
[120] prune_out_channels on _ElementWiseOp_98(ExpandBackward0) => prune_out_channels on _ElementWiseOp_99(TransposeBackward0), #idxs=24
[121] prune_out_channels on _ElementWiseOp_51(TransposeBackward0) => prune_out_channels on _Reshape_52(), #idxs=24
[122] prune_out_channels on _Reshape_52() => prune_out_channels on patch_embed.projection (Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))), #idxs=24
[123] prune_out_channels on _Reshape_50() => prune_out_channels on _ElementWiseOp_53(CloneBackward0), #idxs=24
[124] prune_out_channels on _ElementWiseOp_53(CloneBackward0) => prune_out_channels on _ElementWiseOp_54(SliceBackward0), #idxs=24
[125] prune_out_channels on _ElementWiseOp_54(SliceBackward0) => prune_out_channels on _ElementWiseOp_55(SliceBackward0), #idxs=24
[126] prune_out_channels on _ElementWiseOp_55(SliceBackward0) => prune_out_channels on _ElementWiseOp_56(SliceBackward0), #idxs=24
[127] prune_out_channels on _ElementWiseOp_56(SliceBackward0) => prune_out_channels on _ElementWiseOp_57(SliceBackward0), #idxs=24
[128] prune_out_channels on _ElementWiseOp_57(SliceBackward0) => prune_out_channels on _Reshape_58(), #idxs=24
[129] prune_out_channels on _Reshape_58() => prune_out_channels on _ElementWiseOp_59(CloneBackward0), #idxs=24
[130] prune_out_channels on _ElementWiseOp_59(CloneBackward0) => prune_out_channels on _ElementWiseOp_60(PermuteBackward0), #idxs=24
[131] prune_out_channels on _ElementWiseOp_60(PermuteBackward0) => prune_out_channels on _Reshape_61(), #idxs=24
[132] prune_out_channels on _Reshape_61() => prune_out_channels on _Reshape_62(), #idxs=24
[133] prune_out_channels on _Reshape_62() => prune_out_channels on stages.0.blocks.0.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=96, out_features=288, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=96, out_features=96, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)), #idxs=24
[134] prune_out_channels on _Reshape_40() => prune_out_channels on _ElementWiseOp_105(CloneBackward0), #idxs=24
[135] prune_out_channels on _ElementWiseOp_105(CloneBackward0) => prune_out_channels on _ElementWiseOp_106(SliceBackward0), #idxs=24
[136] prune_out_channels on _ElementWiseOp_106(SliceBackward0) => prune_out_channels on _ElementWiseOp_107(SliceBackward0), #idxs=24
[137] prune_out_channels on _ElementWiseOp_107(SliceBackward0) => prune_out_channels on _ElementWiseOp_108(SliceBackward0), #idxs=24
[138] prune_out_channels on _ElementWiseOp_108(SliceBackward0) => prune_out_channels on _ElementWiseOp_109(SliceBackward0), #idxs=24
[139] prune_out_channels on _ElementWiseOp_109(SliceBackward0) => prune_out_channels on _ElementWiseOp_110(RollBackward0), #idxs=24
[140] prune_out_channels on _ElementWiseOp_110(RollBackward0) => prune_out_channels on _Reshape_111(), #idxs=24
[141] prune_out_channels on _Reshape_111() => prune_out_channels on _ElementWiseOp_112(CloneBackward0), #idxs=24
[142] prune_out_channels on _ElementWiseOp_112(CloneBackward0) => prune_out_channels on _ElementWiseOp_113(PermuteBackward0), #idxs=24
[143] prune_out_channels on _ElementWiseOp_113(PermuteBackward0) => prune_out_channels on _Reshape_114(), #idxs=24
[144] prune_out_channels on _Reshape_114() => prune_out_channels on _Reshape_115(), #idxs=24
[145] prune_out_channels on _Reshape_115() => prune_out_channels on stages.0.blocks.1.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=96, out_features=288, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=96, out_features=96, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)), #idxs=24
--------------------------------

PatchMergingPruner () prune_in_channels/  384 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
PatchMergingPruner () prune_in_channels/ dim =  96
len indx =  24
idxs_repeated =  96
WindowMSAPruner prune_in_channels() /  24
WindowMSAPruner prune_in_channels() /  24
WindowMSAPruner prune_out_channels() /  24
WindowMSAPruner prune_out_channels/ dim =  96
	len indx =  24
WindowMSAPruner - idxs =  [1, 4, 10, 12, 26, 31, 32, 38, 39, 42, 50, 51, 54, 55, 60, 67, 69, 74, 77, 83, 87, 91, 94, 95]
	idxs_repeated =  96
WindowMSAPruner - idxs_repeated =  [1, 4, 10, 12, 26, 31, 32, 38, 39, 42, 50, 51, 54, 55, 60, 67, 69, 74, 77, 83, 87, 91, 94, 95, 97, 100, 106, 108, 122, 127, 128, 134, 135, 138, 146, 147, 150, 151, 156, 163, 165, 170, 173, 179, 183, 187, 190, 191, 193, 196, 202, 204, 218, 223, 224, 230, 231, 234, 242, 243, 246, 247, 252, 259, 261, 266, 269, 275, 279, 283, 286, 287, 289, 292, 298, 300, 314, 319, 320, 326, 327, 330, 338, 339, 342, 343, 348, 355, 357, 362, 365, 371, 375, 379, 382, 383]
WindowMSAPruner prune_out_channels idxs_repeated =  96
WindowMSAPruner prune_out_channels() /  24
WindowMSAPruner prune_out_channels/ dim =  96
	len indx =  24
WindowMSAPruner - idxs =  [1, 4, 10, 12, 26, 31, 32, 38, 39, 42, 50, 51, 54, 55, 60, 67, 69, 74, 77, 83, 87, 91, 94, 95]
	idxs_repeated =  96
WindowMSAPruner - idxs_repeated =  [1, 4, 10, 12, 26, 31, 32, 38, 39, 42, 50, 51, 54, 55, 60, 67, 69, 74, 77, 83, 87, 91, 94, 95, 97, 100, 106, 108, 122, 127, 128, 134, 135, 138, 146, 147, 150, 151, 156, 163, 165, 170, 173, 179, 183, 187, 190, 191, 193, 196, 202, 204, 218, 223, 224, 230, 231, 234, 242, 243, 246, 247, 252, 259, 261, 266, 269, 275, 279, 283, 286, 287, 289, 292, 298, 300, 314, 319, 320, 326, 327, 330, 338, 339, 342, 343, 348, 355, 357, 362, 365, 371, 375, 379, 382, 383]
WindowMSAPruner prune_out_channels idxs_repeated =  96
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
	get_all_groups -- Adding layer:  <class 'mmcv.cnn.bricks.wrappers.Linear'>
		prune_local()/  _check_pruning_ratio OK
idxs =  96
prunable_chs =  384
pruner.step 
--------------------------------
          Pruning Group
--------------------------------
[0] prune_out_channels on stages.0.blocks.1.ffn.layers.0.0 (Linear(in_features=72, out_features=384, bias=True)) => prune_out_channels on stages.0.blocks.1.ffn.layers.0.0 (Linear(in_features=72, out_features=384, bias=True)), #idxs=96
[1] prune_out_channels on stages.0.blocks.1.ffn.layers.0.0 (Linear(in_features=72, out_features=384, bias=True)) => prune_out_channels on _ElementWiseOp_36(GeluBackward0), #idxs=96
[2] prune_out_channels on _ElementWiseOp_36(GeluBackward0) => prune_out_channels on _Reshape_34(), #idxs=96
[3] prune_out_channels on _Reshape_34() => prune_out_channels on _ElementWiseOp_33(AddmmBackward0), #idxs=96
[4] prune_out_channels on _ElementWiseOp_33(AddmmBackward0) => prune_out_channels on _ElementWiseOp_35(TBackward0), #idxs=96
[5] prune_out_channels on _ElementWiseOp_33(AddmmBackward0) => prune_in_channels on stages.0.blocks.1.ffn.layers.1 (Linear(in_features=384, out_features=72, bias=True)), #idxs=96
--------------------------------

<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
continue 4
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
	get_all_groups -- Adding layer:  <class 'mmcv.cnn.bricks.wrappers.Linear'>
		prune_local()/  _check_pruning_ratio OK
idxs =  96
prunable_chs =  384
pruner.step 
--------------------------------
          Pruning Group
--------------------------------
[0] prune_out_channels on stages.0.blocks.0.ffn.layers.0.0 (Linear(in_features=72, out_features=384, bias=True)) => prune_out_channels on stages.0.blocks.0.ffn.layers.0.0 (Linear(in_features=72, out_features=384, bias=True)), #idxs=96
[1] prune_out_channels on stages.0.blocks.0.ffn.layers.0.0 (Linear(in_features=72, out_features=384, bias=True)) => prune_out_channels on _ElementWiseOp_46(GeluBackward0), #idxs=96
[2] prune_out_channels on _ElementWiseOp_46(GeluBackward0) => prune_out_channels on _Reshape_44(), #idxs=96
[3] prune_out_channels on _Reshape_44() => prune_out_channels on _ElementWiseOp_43(AddmmBackward0), #idxs=96
[4] prune_out_channels on _ElementWiseOp_43(AddmmBackward0) => prune_out_channels on _ElementWiseOp_45(TBackward0), #idxs=96
[5] prune_out_channels on _ElementWiseOp_43(AddmmBackward0) => prune_in_channels on stages.0.blocks.0.ffn.layers.1 (Linear(in_features=384, out_features=72, bias=True)), #idxs=96
--------------------------------

<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
continue 4
<class 'torch_pruning.ops._ReshapeOp'> continue 2
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch.nn.modules.conv.Conv2d'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
WindowMSAPruner layer.embed_dims =  72
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
WindowMSAPruner layer.embed_dims =  72
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
WindowMSAPruner layer.embed_dims =  144
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
WindowMSAPruner layer.embed_dims =  144
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
	get_all_groups -- Adding layer:  <class 'torch.nn.modules.normalization.LayerNorm'>
PatchMergingPruner () get_in_channels/  384 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
WindowMSAPruner layer.qkv.in_features =  384
WindowMSAPruner layer.qkv.in_features =  384
WindowMSAPruner layer.qkv.in_features =  384
WindowMSAPruner layer.qkv.in_features =  384
WindowMSAPruner layer.qkv.in_features =  384
PatchMergingPruner () get_out_channels/  384 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
WindowMSAPruner layer.qkv.in_features =  384
WindowMSAPruner layer.embed_dims =  384
WindowMSAPruner layer.embed_dims =  384
WindowMSAPruner layer.embed_dims =  384
WindowMSAPruner layer.embed_dims =  384
WindowMSAPruner layer.embed_dims =  384
WindowMSAPruner layer.embed_dims =  384
		prune_local()/  _check_pruning_ratio OK
idxs =  96
prunable_chs =  384
idxs =  96
prunable_chs =  384
PatchMergingPruner () get_in_channels/  384 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
idxs =  96
prunable_chs =  384
idxs =  96
prunable_chs =  384
idxs =  96
prunable_chs =  384
WindowMSAPruner layer.qkv.in_features =  384
idxs =  96
prunable_chs =  384
idxs =  96
prunable_chs =  384
idxs =  96
prunable_chs =  384
WindowMSAPruner layer.qkv.in_features =  384
idxs =  96
prunable_chs =  384
idxs =  96
prunable_chs =  384
idxs =  96
prunable_chs =  384
WindowMSAPruner layer.qkv.in_features =  384
idxs =  96
prunable_chs =  384
idxs =  96
prunable_chs =  384
idxs =  96
prunable_chs =  384
WindowMSAPruner layer.qkv.in_features =  384
idxs =  96
prunable_chs =  384
idxs =  96
prunable_chs =  384
idxs =  96
prunable_chs =  384
WindowMSAPruner layer.qkv.in_features =  384
PatchMergingPruner () get_out_channels/  384 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
idxs =  96
prunable_chs =  384
idxs =  96
prunable_chs =  384
idxs =  96
prunable_chs =  384
WindowMSAPruner layer.qkv.in_features =  384
WindowMSAPruner layer.embed_dims =  384
idxs =  96
prunable_chs =  384
WindowMSAPruner layer.embed_dims =  384
idxs =  96
prunable_chs =  384
WindowMSAPruner layer.embed_dims =  384
idxs =  96
prunable_chs =  384
WindowMSAPruner layer.embed_dims =  384
idxs =  96
prunable_chs =  384
WindowMSAPruner layer.embed_dims =  384
idxs =  96
prunable_chs =  384
WindowMSAPruner layer.embed_dims =  384
idxs =  96
prunable_chs =  384
pruner.step 
--------------------------------
          Pruning Group
--------------------------------
[0] prune_out_channels on norm2 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on norm2 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)), #idxs=96
[1] prune_out_channels on norm2 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _ElementWiseOp_274(AddBackward0), #idxs=96
[2] prune_out_channels on norm2 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_273(), #idxs=96
[3] prune_out_channels on _Reshape_273() => prune_out_channels on _ElementWiseOp_272(PermuteBackward0), #idxs=96
[4] prune_out_channels on _ElementWiseOp_272(PermuteBackward0) => prune_out_channels on _ElementWiseOp_271(CloneBackward0), #idxs=96
[5] prune_out_channels on _ElementWiseOp_274(AddBackward0) => prune_out_channels on _ElementWiseOp_275(AddBackward0), #idxs=96
[6] prune_out_channels on _ElementWiseOp_274(AddBackward0) => prune_out_channels on stages.2.blocks.5.ffn.layers.1 (Linear(in_features=1536, out_features=384, bias=True)), #idxs=96
[7] prune_out_channels on _ElementWiseOp_274(AddBackward0) => prune_out_channels on _Reshape_688(), #idxs=96
[8] prune_out_channels on _Reshape_688() => prune_out_channels on _ElementWiseOp_687(PermuteBackward0), #idxs=96
[9] prune_out_channels on _ElementWiseOp_687(PermuteBackward0) => prune_out_channels on _ElementWiseOp_686(Im2ColBackward0), #idxs=96
[10] prune_out_channels on _ElementWiseOp_686(Im2ColBackward0) => prune_out_channels on _ElementWiseOp_685(TransposeBackward0), #idxs=96
[11] prune_out_channels on _ElementWiseOp_685(TransposeBackward0) => prune_out_channels on _ElementWiseOp_684(NativeLayerNormBackward0), #idxs=96
[12] prune_out_channels on _ElementWiseOp_684(NativeLayerNormBackward0) => prune_out_channels on _Reshape_682(), #idxs=96
[13] prune_out_channels on _Reshape_682() => prune_out_channels on _ElementWiseOp_681(MmBackward0), #idxs=96
[14] prune_out_channels on _ElementWiseOp_681(MmBackward0) => prune_out_channels on _ElementWiseOp_683(TBackward0), #idxs=96
[15] prune_out_channels on _ElementWiseOp_681(MmBackward0) => prune_in_channels on stages.2.downsample (PatchMerging(
  (adap_padding): AdaptivePadding()
  (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
  (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (reduction): Linear(in_features=1536, out_features=768, bias=False)
)), #idxs=96
[16] prune_out_channels on _ElementWiseOp_275(AddBackward0) => prune_out_channels on _Reshape_283(), #idxs=96
[17] prune_out_channels on _ElementWiseOp_275(AddBackward0) => prune_out_channels on _ElementWiseOp_284(AddBackward0), #idxs=96
[18] prune_out_channels on _ElementWiseOp_275(AddBackward0) => prune_out_channels on stages.2.blocks.5.norm2 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)), #idxs=96
[19] prune_out_channels on stages.2.blocks.5.norm2 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_281(), #idxs=96
[20] prune_out_channels on _Reshape_281() => prune_out_channels on _ElementWiseOp_280(AddmmBackward0), #idxs=96
[21] prune_out_channels on _ElementWiseOp_280(AddmmBackward0) => prune_out_channels on _ElementWiseOp_282(TBackward0), #idxs=96
[22] prune_out_channels on _ElementWiseOp_280(AddmmBackward0) => prune_in_channels on stages.2.blocks.5.ffn.layers.0.0 (Linear(in_features=384, out_features=1536, bias=True)), #idxs=96
[23] prune_out_channels on _ElementWiseOp_284(AddBackward0) => prune_out_channels on _ElementWiseOp_285(AddBackward0), #idxs=96
[24] prune_out_channels on _ElementWiseOp_284(AddBackward0) => prune_out_channels on stages.2.blocks.4.ffn.layers.1 (Linear(in_features=1536, out_features=384, bias=True)), #idxs=96
[25] prune_out_channels on _ElementWiseOp_284(AddBackward0) => prune_out_channels on stages.2.blocks.5.norm1 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)), #idxs=96
[26] prune_out_channels on stages.2.blocks.5.norm1 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_634(), #idxs=96
[27] prune_out_channels on _Reshape_634() => prune_out_channels on _ElementWiseOp_633(ConstantPadNdBackward0), #idxs=96
[28] prune_out_channels on _ElementWiseOp_633(ConstantPadNdBackward0) => prune_out_channels on _ElementWiseOp_632(RollBackward0), #idxs=96
[29] prune_out_channels on _ElementWiseOp_632(RollBackward0) => prune_out_channels on _Reshape_631(), #idxs=96
[30] prune_out_channels on _Reshape_631() => prune_out_channels on _ElementWiseOp_630(PermuteBackward0), #idxs=96
[31] prune_out_channels on _ElementWiseOp_630(PermuteBackward0) => prune_out_channels on _ElementWiseOp_629(CloneBackward0), #idxs=96
[32] prune_out_channels on _ElementWiseOp_629(CloneBackward0) => prune_out_channels on _Reshape_628(), #idxs=96
[33] prune_out_channels on _Reshape_628() => prune_out_channels on _Reshape_627(), #idxs=96
[34] prune_out_channels on _Reshape_627() => prune_out_channels on _Reshape_625(), #idxs=96
[35] prune_out_channels on _Reshape_625() => prune_out_channels on _ElementWiseOp_624(AddmmBackward0), #idxs=96
[36] prune_out_channels on _ElementWiseOp_624(AddmmBackward0) => prune_out_channels on _ElementWiseOp_626(TBackward0), #idxs=96
[37] prune_out_channels on _ElementWiseOp_624(AddmmBackward0) => prune_out_channels on _Reshape_623(), #idxs=96
[38] prune_out_channels on _Reshape_623() => prune_out_channels on _Reshape_622(), #idxs=96
[39] prune_out_channels on _Reshape_622() => prune_out_channels on _ElementWiseOp_621(PermuteBackward0), #idxs=96
[40] prune_out_channels on _ElementWiseOp_621(PermuteBackward0) => prune_out_channels on _ElementWiseOp_620(SelectBackward0), #idxs=96
[41] prune_out_channels on _ElementWiseOp_621(PermuteBackward0) => prune_out_channels on _ElementWiseOp_653(SelectBackward0), #idxs=96
[42] prune_out_channels on _ElementWiseOp_621(PermuteBackward0) => prune_out_channels on _ElementWiseOp_657(SelectBackward0), #idxs=96
[43] prune_out_channels on _ElementWiseOp_657(SelectBackward0) => prune_out_channels on _ElementWiseOp_656(MulBackward0), #idxs=96
[44] prune_out_channels on _ElementWiseOp_656(MulBackward0) => prune_out_channels on _ElementWiseOp_655(ExpandBackward0), #idxs=96
[45] prune_out_channels on _ElementWiseOp_655(ExpandBackward0) => prune_out_channels on _ElementWiseOp_654(CloneBackward0), #idxs=96
[46] prune_out_channels on _ElementWiseOp_654(CloneBackward0) => prune_out_channels on _Reshape_648(), #idxs=96
[47] prune_out_channels on _Reshape_648() => prune_out_channels on _ElementWiseOp_647(BmmBackward0), #idxs=96
[48] prune_out_channels on _ElementWiseOp_647(BmmBackward0) => prune_out_channels on _Reshape_649(), #idxs=96
[49] prune_out_channels on _ElementWiseOp_647(BmmBackward0) => prune_out_channels on _Reshape_641(), #idxs=96
[50] prune_out_channels on _Reshape_641() => prune_out_channels on _ElementWiseOp_640(AddBackward0), #idxs=96
[51] prune_out_channels on _ElementWiseOp_640(AddBackward0) => prune_out_channels on _ElementWiseOp_642(UnsqueezeBackward0), #idxs=96
[52] prune_out_channels on _ElementWiseOp_640(AddBackward0) => prune_out_channels on _Reshape_639(), #idxs=96
[53] prune_out_channels on _Reshape_639() => prune_out_channels on _ElementWiseOp_638(AddBackward0), #idxs=96
[54] prune_out_channels on _ElementWiseOp_638(AddBackward0) => prune_out_channels on _Reshape_637(), #idxs=96
[55] prune_out_channels on _Reshape_637() => prune_out_channels on _ElementWiseOp_636(SoftmaxBackward0), #idxs=96
[56] prune_out_channels on _ElementWiseOp_636(SoftmaxBackward0) => prune_out_channels on _ElementWiseOp_635(ExpandBackward0), #idxs=96
[57] prune_out_channels on _ElementWiseOp_635(ExpandBackward0) => prune_out_channels on _Reshape_616(), #idxs=96
[58] prune_out_channels on _Reshape_616() => prune_out_channels on _ElementWiseOp_615(BmmBackward0), #idxs=96
[59] prune_out_channels on _ElementWiseOp_615(BmmBackward0) => prune_out_channels on _Reshape_617(), #idxs=96
[60] prune_out_channels on _ElementWiseOp_615(BmmBackward0) => prune_out_channels on _Reshape_614(), #idxs=96
[61] prune_out_channels on _Reshape_614() => prune_out_channels on _ElementWiseOp_613(TransposeBackward0), #idxs=96
[62] prune_out_channels on _ElementWiseOp_613(TransposeBackward0) => prune_out_channels on _ElementWiseOp_612(CloneBackward0), #idxs=96
[63] prune_out_channels on _ElementWiseOp_612(CloneBackward0) => prune_in_channels on stages.2.blocks.5.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=384, out_features=1152, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=384, out_features=384, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)), #idxs=96
[64] prune_out_channels on _Reshape_617() => prune_out_channels on _ElementWiseOp_618(CloneBackward0), #idxs=96
[65] prune_out_channels on _ElementWiseOp_618(CloneBackward0) => prune_out_channels on _ElementWiseOp_619(ExpandBackward0), #idxs=96
[66] prune_out_channels on _ElementWiseOp_642(UnsqueezeBackward0) => prune_out_channels on _ElementWiseOp_643(CloneBackward0), #idxs=96
[67] prune_out_channels on _ElementWiseOp_643(CloneBackward0) => prune_out_channels on _ElementWiseOp_644(PermuteBackward0), #idxs=96
[68] prune_out_channels on _ElementWiseOp_644(PermuteBackward0) => prune_out_channels on _Reshape_645(), #idxs=96
[69] prune_out_channels on _Reshape_645() => prune_out_channels on _ElementWiseOp_646(IndexBackward0), #idxs=96
[70] prune_out_channels on _Reshape_649() => prune_out_channels on _ElementWiseOp_650(CloneBackward0), #idxs=96
[71] prune_out_channels on _ElementWiseOp_650(CloneBackward0) => prune_out_channels on _ElementWiseOp_651(ExpandBackward0), #idxs=96
[72] prune_out_channels on _ElementWiseOp_651(ExpandBackward0) => prune_out_channels on _ElementWiseOp_652(TransposeBackward0), #idxs=96
[73] prune_out_channels on _ElementWiseOp_285(AddBackward0) => prune_out_channels on _Reshape_293(), #idxs=96
[74] prune_out_channels on _ElementWiseOp_285(AddBackward0) => prune_out_channels on _ElementWiseOp_294(AddBackward0), #idxs=96
[75] prune_out_channels on _ElementWiseOp_285(AddBackward0) => prune_out_channels on stages.2.blocks.4.norm2 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)), #idxs=96
[76] prune_out_channels on stages.2.blocks.4.norm2 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_291(), #idxs=96
[77] prune_out_channels on _Reshape_291() => prune_out_channels on _ElementWiseOp_290(AddmmBackward0), #idxs=96
[78] prune_out_channels on _ElementWiseOp_290(AddmmBackward0) => prune_out_channels on _ElementWiseOp_292(TBackward0), #idxs=96
[79] prune_out_channels on _ElementWiseOp_290(AddmmBackward0) => prune_in_channels on stages.2.blocks.4.ffn.layers.0.0 (Linear(in_features=384, out_features=1536, bias=True)), #idxs=96
[80] prune_out_channels on _ElementWiseOp_294(AddBackward0) => prune_out_channels on _ElementWiseOp_295(AddBackward0), #idxs=96
[81] prune_out_channels on _ElementWiseOp_294(AddBackward0) => prune_out_channels on stages.2.blocks.3.ffn.layers.1 (Linear(in_features=1536, out_features=384, bias=True)), #idxs=96
[82] prune_out_channels on _ElementWiseOp_294(AddBackward0) => prune_out_channels on stages.2.blocks.4.norm1 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)), #idxs=96
[83] prune_out_channels on stages.2.blocks.4.norm1 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_582(), #idxs=96
[84] prune_out_channels on _Reshape_582() => prune_out_channels on _ElementWiseOp_581(ConstantPadNdBackward0), #idxs=96
[85] prune_out_channels on _ElementWiseOp_581(ConstantPadNdBackward0) => prune_out_channels on _Reshape_580(), #idxs=96
[86] prune_out_channels on _Reshape_580() => prune_out_channels on _ElementWiseOp_579(PermuteBackward0), #idxs=96
[87] prune_out_channels on _ElementWiseOp_579(PermuteBackward0) => prune_out_channels on _ElementWiseOp_578(CloneBackward0), #idxs=96
[88] prune_out_channels on _ElementWiseOp_578(CloneBackward0) => prune_out_channels on _Reshape_577(), #idxs=96
[89] prune_out_channels on _Reshape_577() => prune_out_channels on _Reshape_576(), #idxs=96
[90] prune_out_channels on _Reshape_576() => prune_out_channels on _Reshape_574(), #idxs=96
[91] prune_out_channels on _Reshape_574() => prune_out_channels on _ElementWiseOp_573(AddmmBackward0), #idxs=96
[92] prune_out_channels on _ElementWiseOp_573(AddmmBackward0) => prune_out_channels on _ElementWiseOp_575(TBackward0), #idxs=96
[93] prune_out_channels on _ElementWiseOp_573(AddmmBackward0) => prune_out_channels on _Reshape_572(), #idxs=96
[94] prune_out_channels on _Reshape_572() => prune_out_channels on _Reshape_571(), #idxs=96
[95] prune_out_channels on _Reshape_571() => prune_out_channels on _ElementWiseOp_570(PermuteBackward0), #idxs=96
[96] prune_out_channels on _ElementWiseOp_570(PermuteBackward0) => prune_out_channels on _ElementWiseOp_569(SelectBackward0), #idxs=96
[97] prune_out_channels on _ElementWiseOp_570(PermuteBackward0) => prune_out_channels on _ElementWiseOp_598(SelectBackward0), #idxs=96
[98] prune_out_channels on _ElementWiseOp_570(PermuteBackward0) => prune_out_channels on _ElementWiseOp_602(SelectBackward0), #idxs=96
[99] prune_out_channels on _ElementWiseOp_602(SelectBackward0) => prune_out_channels on _ElementWiseOp_601(MulBackward0), #idxs=96
[100] prune_out_channels on _ElementWiseOp_601(MulBackward0) => prune_out_channels on _ElementWiseOp_600(ExpandBackward0), #idxs=96
[101] prune_out_channels on _ElementWiseOp_600(ExpandBackward0) => prune_out_channels on _ElementWiseOp_599(CloneBackward0), #idxs=96
[102] prune_out_channels on _ElementWiseOp_599(CloneBackward0) => prune_out_channels on _Reshape_593(), #idxs=96
[103] prune_out_channels on _Reshape_593() => prune_out_channels on _ElementWiseOp_592(BmmBackward0), #idxs=96
[104] prune_out_channels on _ElementWiseOp_592(BmmBackward0) => prune_out_channels on _Reshape_594(), #idxs=96
[105] prune_out_channels on _ElementWiseOp_592(BmmBackward0) => prune_out_channels on _Reshape_586(), #idxs=96
[106] prune_out_channels on _Reshape_586() => prune_out_channels on _ElementWiseOp_585(AddBackward0), #idxs=96
[107] prune_out_channels on _ElementWiseOp_585(AddBackward0) => prune_out_channels on _ElementWiseOp_587(UnsqueezeBackward0), #idxs=96
[108] prune_out_channels on _ElementWiseOp_585(AddBackward0) => prune_out_channels on _ElementWiseOp_584(SoftmaxBackward0), #idxs=96
[109] prune_out_channels on _ElementWiseOp_584(SoftmaxBackward0) => prune_out_channels on _ElementWiseOp_583(ExpandBackward0), #idxs=96
[110] prune_out_channels on _ElementWiseOp_583(ExpandBackward0) => prune_out_channels on _Reshape_565(), #idxs=96
[111] prune_out_channels on _Reshape_565() => prune_out_channels on _ElementWiseOp_564(BmmBackward0), #idxs=96
[112] prune_out_channels on _ElementWiseOp_564(BmmBackward0) => prune_out_channels on _Reshape_566(), #idxs=96
[113] prune_out_channels on _ElementWiseOp_564(BmmBackward0) => prune_out_channels on _Reshape_563(), #idxs=96
[114] prune_out_channels on _Reshape_563() => prune_out_channels on _ElementWiseOp_562(TransposeBackward0), #idxs=96
[115] prune_out_channels on _ElementWiseOp_562(TransposeBackward0) => prune_out_channels on _ElementWiseOp_561(CloneBackward0), #idxs=96
[116] prune_out_channels on _ElementWiseOp_561(CloneBackward0) => prune_in_channels on stages.2.blocks.4.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=384, out_features=1152, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=384, out_features=384, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)), #idxs=96
[117] prune_out_channels on _Reshape_566() => prune_out_channels on _ElementWiseOp_567(CloneBackward0), #idxs=96
[118] prune_out_channels on _ElementWiseOp_567(CloneBackward0) => prune_out_channels on _ElementWiseOp_568(ExpandBackward0), #idxs=96
[119] prune_out_channels on _ElementWiseOp_587(UnsqueezeBackward0) => prune_out_channels on _ElementWiseOp_588(CloneBackward0), #idxs=96
[120] prune_out_channels on _ElementWiseOp_588(CloneBackward0) => prune_out_channels on _ElementWiseOp_589(PermuteBackward0), #idxs=96
[121] prune_out_channels on _ElementWiseOp_589(PermuteBackward0) => prune_out_channels on _Reshape_590(), #idxs=96
[122] prune_out_channels on _Reshape_590() => prune_out_channels on _ElementWiseOp_591(IndexBackward0), #idxs=96
[123] prune_out_channels on _Reshape_594() => prune_out_channels on _ElementWiseOp_595(CloneBackward0), #idxs=96
[124] prune_out_channels on _ElementWiseOp_595(CloneBackward0) => prune_out_channels on _ElementWiseOp_596(ExpandBackward0), #idxs=96
[125] prune_out_channels on _ElementWiseOp_596(ExpandBackward0) => prune_out_channels on _ElementWiseOp_597(TransposeBackward0), #idxs=96
[126] prune_out_channels on _ElementWiseOp_295(AddBackward0) => prune_out_channels on _Reshape_303(), #idxs=96
[127] prune_out_channels on _ElementWiseOp_295(AddBackward0) => prune_out_channels on _ElementWiseOp_304(AddBackward0), #idxs=96
[128] prune_out_channels on _ElementWiseOp_295(AddBackward0) => prune_out_channels on stages.2.blocks.3.norm2 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)), #idxs=96
[129] prune_out_channels on stages.2.blocks.3.norm2 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_301(), #idxs=96
[130] prune_out_channels on _Reshape_301() => prune_out_channels on _ElementWiseOp_300(AddmmBackward0), #idxs=96
[131] prune_out_channels on _ElementWiseOp_300(AddmmBackward0) => prune_out_channels on _ElementWiseOp_302(TBackward0), #idxs=96
[132] prune_out_channels on _ElementWiseOp_300(AddmmBackward0) => prune_in_channels on stages.2.blocks.3.ffn.layers.0.0 (Linear(in_features=384, out_features=1536, bias=True)), #idxs=96
[133] prune_out_channels on _ElementWiseOp_304(AddBackward0) => prune_out_channels on _ElementWiseOp_305(AddBackward0), #idxs=96
[134] prune_out_channels on _ElementWiseOp_304(AddBackward0) => prune_out_channels on stages.2.blocks.2.ffn.layers.1 (Linear(in_features=1536, out_features=384, bias=True)), #idxs=96
[135] prune_out_channels on _ElementWiseOp_304(AddBackward0) => prune_out_channels on stages.2.blocks.3.norm1 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)), #idxs=96
[136] prune_out_channels on stages.2.blocks.3.norm1 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_529(), #idxs=96
[137] prune_out_channels on _Reshape_529() => prune_out_channels on _ElementWiseOp_528(ConstantPadNdBackward0), #idxs=96
[138] prune_out_channels on _ElementWiseOp_528(ConstantPadNdBackward0) => prune_out_channels on _ElementWiseOp_527(RollBackward0), #idxs=96
[139] prune_out_channels on _ElementWiseOp_527(RollBackward0) => prune_out_channels on _Reshape_526(), #idxs=96
[140] prune_out_channels on _Reshape_526() => prune_out_channels on _ElementWiseOp_525(PermuteBackward0), #idxs=96
[141] prune_out_channels on _ElementWiseOp_525(PermuteBackward0) => prune_out_channels on _ElementWiseOp_524(CloneBackward0), #idxs=96
[142] prune_out_channels on _ElementWiseOp_524(CloneBackward0) => prune_out_channels on _Reshape_523(), #idxs=96
[143] prune_out_channels on _Reshape_523() => prune_out_channels on _Reshape_522(), #idxs=96
[144] prune_out_channels on _Reshape_522() => prune_out_channels on _Reshape_520(), #idxs=96
[145] prune_out_channels on _Reshape_520() => prune_out_channels on _ElementWiseOp_519(AddmmBackward0), #idxs=96
[146] prune_out_channels on _ElementWiseOp_519(AddmmBackward0) => prune_out_channels on _ElementWiseOp_521(TBackward0), #idxs=96
[147] prune_out_channels on _ElementWiseOp_519(AddmmBackward0) => prune_out_channels on _Reshape_518(), #idxs=96
[148] prune_out_channels on _Reshape_518() => prune_out_channels on _Reshape_517(), #idxs=96
[149] prune_out_channels on _Reshape_517() => prune_out_channels on _ElementWiseOp_516(PermuteBackward0), #idxs=96
[150] prune_out_channels on _ElementWiseOp_516(PermuteBackward0) => prune_out_channels on _ElementWiseOp_515(SelectBackward0), #idxs=96
[151] prune_out_channels on _ElementWiseOp_516(PermuteBackward0) => prune_out_channels on _ElementWiseOp_548(SelectBackward0), #idxs=96
[152] prune_out_channels on _ElementWiseOp_516(PermuteBackward0) => prune_out_channels on _ElementWiseOp_552(SelectBackward0), #idxs=96
[153] prune_out_channels on _ElementWiseOp_552(SelectBackward0) => prune_out_channels on _ElementWiseOp_551(MulBackward0), #idxs=96
[154] prune_out_channels on _ElementWiseOp_551(MulBackward0) => prune_out_channels on _ElementWiseOp_550(ExpandBackward0), #idxs=96
[155] prune_out_channels on _ElementWiseOp_550(ExpandBackward0) => prune_out_channels on _ElementWiseOp_549(CloneBackward0), #idxs=96
[156] prune_out_channels on _ElementWiseOp_549(CloneBackward0) => prune_out_channels on _Reshape_543(), #idxs=96
[157] prune_out_channels on _Reshape_543() => prune_out_channels on _ElementWiseOp_542(BmmBackward0), #idxs=96
[158] prune_out_channels on _ElementWiseOp_542(BmmBackward0) => prune_out_channels on _Reshape_544(), #idxs=96
[159] prune_out_channels on _ElementWiseOp_542(BmmBackward0) => prune_out_channels on _Reshape_536(), #idxs=96
[160] prune_out_channels on _Reshape_536() => prune_out_channels on _ElementWiseOp_535(AddBackward0), #idxs=96
[161] prune_out_channels on _ElementWiseOp_535(AddBackward0) => prune_out_channels on _ElementWiseOp_537(UnsqueezeBackward0), #idxs=96
[162] prune_out_channels on _ElementWiseOp_535(AddBackward0) => prune_out_channels on _Reshape_534(), #idxs=96
[163] prune_out_channels on _Reshape_534() => prune_out_channels on _ElementWiseOp_533(AddBackward0), #idxs=96
[164] prune_out_channels on _ElementWiseOp_533(AddBackward0) => prune_out_channels on _Reshape_532(), #idxs=96
[165] prune_out_channels on _Reshape_532() => prune_out_channels on _ElementWiseOp_531(SoftmaxBackward0), #idxs=96
[166] prune_out_channels on _ElementWiseOp_531(SoftmaxBackward0) => prune_out_channels on _ElementWiseOp_530(ExpandBackward0), #idxs=96
[167] prune_out_channels on _ElementWiseOp_530(ExpandBackward0) => prune_out_channels on _Reshape_511(), #idxs=96
[168] prune_out_channels on _Reshape_511() => prune_out_channels on _ElementWiseOp_510(BmmBackward0), #idxs=96
[169] prune_out_channels on _ElementWiseOp_510(BmmBackward0) => prune_out_channels on _Reshape_512(), #idxs=96
[170] prune_out_channels on _ElementWiseOp_510(BmmBackward0) => prune_out_channels on _Reshape_509(), #idxs=96
[171] prune_out_channels on _Reshape_509() => prune_out_channels on _ElementWiseOp_508(TransposeBackward0), #idxs=96
[172] prune_out_channels on _ElementWiseOp_508(TransposeBackward0) => prune_out_channels on _ElementWiseOp_507(CloneBackward0), #idxs=96
[173] prune_out_channels on _ElementWiseOp_507(CloneBackward0) => prune_in_channels on stages.2.blocks.3.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=384, out_features=1152, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=384, out_features=384, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)), #idxs=96
[174] prune_out_channels on _Reshape_512() => prune_out_channels on _ElementWiseOp_513(CloneBackward0), #idxs=96
[175] prune_out_channels on _ElementWiseOp_513(CloneBackward0) => prune_out_channels on _ElementWiseOp_514(ExpandBackward0), #idxs=96
[176] prune_out_channels on _ElementWiseOp_537(UnsqueezeBackward0) => prune_out_channels on _ElementWiseOp_538(CloneBackward0), #idxs=96
[177] prune_out_channels on _ElementWiseOp_538(CloneBackward0) => prune_out_channels on _ElementWiseOp_539(PermuteBackward0), #idxs=96
[178] prune_out_channels on _ElementWiseOp_539(PermuteBackward0) => prune_out_channels on _Reshape_540(), #idxs=96
[179] prune_out_channels on _Reshape_540() => prune_out_channels on _ElementWiseOp_541(IndexBackward0), #idxs=96
[180] prune_out_channels on _Reshape_544() => prune_out_channels on _ElementWiseOp_545(CloneBackward0), #idxs=96
[181] prune_out_channels on _ElementWiseOp_545(CloneBackward0) => prune_out_channels on _ElementWiseOp_546(ExpandBackward0), #idxs=96
[182] prune_out_channels on _ElementWiseOp_546(ExpandBackward0) => prune_out_channels on _ElementWiseOp_547(TransposeBackward0), #idxs=96
[183] prune_out_channels on _ElementWiseOp_305(AddBackward0) => prune_out_channels on _Reshape_313(), #idxs=96
[184] prune_out_channels on _ElementWiseOp_305(AddBackward0) => prune_out_channels on _ElementWiseOp_314(AddBackward0), #idxs=96
[185] prune_out_channels on _ElementWiseOp_305(AddBackward0) => prune_out_channels on stages.2.blocks.2.norm2 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)), #idxs=96
[186] prune_out_channels on stages.2.blocks.2.norm2 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_311(), #idxs=96
[187] prune_out_channels on _Reshape_311() => prune_out_channels on _ElementWiseOp_310(AddmmBackward0), #idxs=96
[188] prune_out_channels on _ElementWiseOp_310(AddmmBackward0) => prune_out_channels on _ElementWiseOp_312(TBackward0), #idxs=96
[189] prune_out_channels on _ElementWiseOp_310(AddmmBackward0) => prune_in_channels on stages.2.blocks.2.ffn.layers.0.0 (Linear(in_features=384, out_features=1536, bias=True)), #idxs=96
[190] prune_out_channels on _ElementWiseOp_314(AddBackward0) => prune_out_channels on _ElementWiseOp_315(AddBackward0), #idxs=96
[191] prune_out_channels on _ElementWiseOp_314(AddBackward0) => prune_out_channels on stages.2.blocks.1.ffn.layers.1 (Linear(in_features=1536, out_features=384, bias=True)), #idxs=96
[192] prune_out_channels on _ElementWiseOp_314(AddBackward0) => prune_out_channels on stages.2.blocks.2.norm1 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)), #idxs=96
[193] prune_out_channels on stages.2.blocks.2.norm1 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_477(), #idxs=96
[194] prune_out_channels on _Reshape_477() => prune_out_channels on _ElementWiseOp_476(ConstantPadNdBackward0), #idxs=96
[195] prune_out_channels on _ElementWiseOp_476(ConstantPadNdBackward0) => prune_out_channels on _Reshape_475(), #idxs=96
[196] prune_out_channels on _Reshape_475() => prune_out_channels on _ElementWiseOp_474(PermuteBackward0), #idxs=96
[197] prune_out_channels on _ElementWiseOp_474(PermuteBackward0) => prune_out_channels on _ElementWiseOp_473(CloneBackward0), #idxs=96
[198] prune_out_channels on _ElementWiseOp_473(CloneBackward0) => prune_out_channels on _Reshape_472(), #idxs=96
[199] prune_out_channels on _Reshape_472() => prune_out_channels on _Reshape_471(), #idxs=96
[200] prune_out_channels on _Reshape_471() => prune_out_channels on _Reshape_469(), #idxs=96
[201] prune_out_channels on _Reshape_469() => prune_out_channels on _ElementWiseOp_468(AddmmBackward0), #idxs=96
[202] prune_out_channels on _ElementWiseOp_468(AddmmBackward0) => prune_out_channels on _ElementWiseOp_470(TBackward0), #idxs=96
[203] prune_out_channels on _ElementWiseOp_468(AddmmBackward0) => prune_out_channels on _Reshape_467(), #idxs=96
[204] prune_out_channels on _Reshape_467() => prune_out_channels on _Reshape_466(), #idxs=96
[205] prune_out_channels on _Reshape_466() => prune_out_channels on _ElementWiseOp_465(PermuteBackward0), #idxs=96
[206] prune_out_channels on _ElementWiseOp_465(PermuteBackward0) => prune_out_channels on _ElementWiseOp_464(SelectBackward0), #idxs=96
[207] prune_out_channels on _ElementWiseOp_465(PermuteBackward0) => prune_out_channels on _ElementWiseOp_493(SelectBackward0), #idxs=96
[208] prune_out_channels on _ElementWiseOp_465(PermuteBackward0) => prune_out_channels on _ElementWiseOp_497(SelectBackward0), #idxs=96
[209] prune_out_channels on _ElementWiseOp_497(SelectBackward0) => prune_out_channels on _ElementWiseOp_496(MulBackward0), #idxs=96
[210] prune_out_channels on _ElementWiseOp_496(MulBackward0) => prune_out_channels on _ElementWiseOp_495(ExpandBackward0), #idxs=96
[211] prune_out_channels on _ElementWiseOp_495(ExpandBackward0) => prune_out_channels on _ElementWiseOp_494(CloneBackward0), #idxs=96
[212] prune_out_channels on _ElementWiseOp_494(CloneBackward0) => prune_out_channels on _Reshape_488(), #idxs=96
[213] prune_out_channels on _Reshape_488() => prune_out_channels on _ElementWiseOp_487(BmmBackward0), #idxs=96
[214] prune_out_channels on _ElementWiseOp_487(BmmBackward0) => prune_out_channels on _Reshape_489(), #idxs=96
[215] prune_out_channels on _ElementWiseOp_487(BmmBackward0) => prune_out_channels on _Reshape_481(), #idxs=96
[216] prune_out_channels on _Reshape_481() => prune_out_channels on _ElementWiseOp_480(AddBackward0), #idxs=96
[217] prune_out_channels on _ElementWiseOp_480(AddBackward0) => prune_out_channels on _ElementWiseOp_482(UnsqueezeBackward0), #idxs=96
[218] prune_out_channels on _ElementWiseOp_480(AddBackward0) => prune_out_channels on _ElementWiseOp_479(SoftmaxBackward0), #idxs=96
[219] prune_out_channels on _ElementWiseOp_479(SoftmaxBackward0) => prune_out_channels on _ElementWiseOp_478(ExpandBackward0), #idxs=96
[220] prune_out_channels on _ElementWiseOp_478(ExpandBackward0) => prune_out_channels on _Reshape_460(), #idxs=96
[221] prune_out_channels on _Reshape_460() => prune_out_channels on _ElementWiseOp_459(BmmBackward0), #idxs=96
[222] prune_out_channels on _ElementWiseOp_459(BmmBackward0) => prune_out_channels on _Reshape_461(), #idxs=96
[223] prune_out_channels on _ElementWiseOp_459(BmmBackward0) => prune_out_channels on _Reshape_458(), #idxs=96
[224] prune_out_channels on _Reshape_458() => prune_out_channels on _ElementWiseOp_457(TransposeBackward0), #idxs=96
[225] prune_out_channels on _ElementWiseOp_457(TransposeBackward0) => prune_out_channels on _ElementWiseOp_456(CloneBackward0), #idxs=96
[226] prune_out_channels on _ElementWiseOp_456(CloneBackward0) => prune_in_channels on stages.2.blocks.2.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=384, out_features=1152, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=384, out_features=384, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)), #idxs=96
[227] prune_out_channels on _Reshape_461() => prune_out_channels on _ElementWiseOp_462(CloneBackward0), #idxs=96
[228] prune_out_channels on _ElementWiseOp_462(CloneBackward0) => prune_out_channels on _ElementWiseOp_463(ExpandBackward0), #idxs=96
[229] prune_out_channels on _ElementWiseOp_482(UnsqueezeBackward0) => prune_out_channels on _ElementWiseOp_483(CloneBackward0), #idxs=96
[230] prune_out_channels on _ElementWiseOp_483(CloneBackward0) => prune_out_channels on _ElementWiseOp_484(PermuteBackward0), #idxs=96
[231] prune_out_channels on _ElementWiseOp_484(PermuteBackward0) => prune_out_channels on _Reshape_485(), #idxs=96
[232] prune_out_channels on _Reshape_485() => prune_out_channels on _ElementWiseOp_486(IndexBackward0), #idxs=96
[233] prune_out_channels on _Reshape_489() => prune_out_channels on _ElementWiseOp_490(CloneBackward0), #idxs=96
[234] prune_out_channels on _ElementWiseOp_490(CloneBackward0) => prune_out_channels on _ElementWiseOp_491(ExpandBackward0), #idxs=96
[235] prune_out_channels on _ElementWiseOp_491(ExpandBackward0) => prune_out_channels on _ElementWiseOp_492(TransposeBackward0), #idxs=96
[236] prune_out_channels on _ElementWiseOp_315(AddBackward0) => prune_out_channels on _Reshape_323(), #idxs=96
[237] prune_out_channels on _ElementWiseOp_315(AddBackward0) => prune_out_channels on _ElementWiseOp_324(AddBackward0), #idxs=96
[238] prune_out_channels on _ElementWiseOp_315(AddBackward0) => prune_out_channels on stages.2.blocks.1.norm2 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)), #idxs=96
[239] prune_out_channels on stages.2.blocks.1.norm2 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_321(), #idxs=96
[240] prune_out_channels on _Reshape_321() => prune_out_channels on _ElementWiseOp_320(AddmmBackward0), #idxs=96
[241] prune_out_channels on _ElementWiseOp_320(AddmmBackward0) => prune_out_channels on _ElementWiseOp_322(TBackward0), #idxs=96
[242] prune_out_channels on _ElementWiseOp_320(AddmmBackward0) => prune_in_channels on stages.2.blocks.1.ffn.layers.0.0 (Linear(in_features=384, out_features=1536, bias=True)), #idxs=96
[243] prune_out_channels on _ElementWiseOp_324(AddBackward0) => prune_out_channels on _ElementWiseOp_325(AddBackward0), #idxs=96
[244] prune_out_channels on _ElementWiseOp_324(AddBackward0) => prune_out_channels on stages.2.blocks.0.ffn.layers.1 (Linear(in_features=1536, out_features=384, bias=True)), #idxs=96
[245] prune_out_channels on _ElementWiseOp_324(AddBackward0) => prune_out_channels on stages.2.blocks.1.norm1 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)), #idxs=96
[246] prune_out_channels on stages.2.blocks.1.norm1 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_424(), #idxs=96
[247] prune_out_channels on _Reshape_424() => prune_out_channels on _ElementWiseOp_423(ConstantPadNdBackward0), #idxs=96
[248] prune_out_channels on _ElementWiseOp_423(ConstantPadNdBackward0) => prune_out_channels on _ElementWiseOp_422(RollBackward0), #idxs=96
[249] prune_out_channels on _ElementWiseOp_422(RollBackward0) => prune_out_channels on _Reshape_421(), #idxs=96
[250] prune_out_channels on _Reshape_421() => prune_out_channels on _ElementWiseOp_420(PermuteBackward0), #idxs=96
[251] prune_out_channels on _ElementWiseOp_420(PermuteBackward0) => prune_out_channels on _ElementWiseOp_419(CloneBackward0), #idxs=96
[252] prune_out_channels on _ElementWiseOp_419(CloneBackward0) => prune_out_channels on _Reshape_418(), #idxs=96
[253] prune_out_channels on _Reshape_418() => prune_out_channels on _Reshape_417(), #idxs=96
[254] prune_out_channels on _Reshape_417() => prune_out_channels on _Reshape_415(), #idxs=96
[255] prune_out_channels on _Reshape_415() => prune_out_channels on _ElementWiseOp_414(AddmmBackward0), #idxs=96
[256] prune_out_channels on _ElementWiseOp_414(AddmmBackward0) => prune_out_channels on _ElementWiseOp_416(TBackward0), #idxs=96
[257] prune_out_channels on _ElementWiseOp_414(AddmmBackward0) => prune_out_channels on _Reshape_413(), #idxs=96
[258] prune_out_channels on _Reshape_413() => prune_out_channels on _Reshape_412(), #idxs=96
[259] prune_out_channels on _Reshape_412() => prune_out_channels on _ElementWiseOp_411(PermuteBackward0), #idxs=96
[260] prune_out_channels on _ElementWiseOp_411(PermuteBackward0) => prune_out_channels on _ElementWiseOp_410(SelectBackward0), #idxs=96
[261] prune_out_channels on _ElementWiseOp_411(PermuteBackward0) => prune_out_channels on _ElementWiseOp_443(SelectBackward0), #idxs=96
[262] prune_out_channels on _ElementWiseOp_411(PermuteBackward0) => prune_out_channels on _ElementWiseOp_447(SelectBackward0), #idxs=96
[263] prune_out_channels on _ElementWiseOp_447(SelectBackward0) => prune_out_channels on _ElementWiseOp_446(MulBackward0), #idxs=96
[264] prune_out_channels on _ElementWiseOp_446(MulBackward0) => prune_out_channels on _ElementWiseOp_445(ExpandBackward0), #idxs=96
[265] prune_out_channels on _ElementWiseOp_445(ExpandBackward0) => prune_out_channels on _ElementWiseOp_444(CloneBackward0), #idxs=96
[266] prune_out_channels on _ElementWiseOp_444(CloneBackward0) => prune_out_channels on _Reshape_438(), #idxs=96
[267] prune_out_channels on _Reshape_438() => prune_out_channels on _ElementWiseOp_437(BmmBackward0), #idxs=96
[268] prune_out_channels on _ElementWiseOp_437(BmmBackward0) => prune_out_channels on _Reshape_439(), #idxs=96
[269] prune_out_channels on _ElementWiseOp_437(BmmBackward0) => prune_out_channels on _Reshape_431(), #idxs=96
[270] prune_out_channels on _Reshape_431() => prune_out_channels on _ElementWiseOp_430(AddBackward0), #idxs=96
[271] prune_out_channels on _ElementWiseOp_430(AddBackward0) => prune_out_channels on _ElementWiseOp_432(UnsqueezeBackward0), #idxs=96
[272] prune_out_channels on _ElementWiseOp_430(AddBackward0) => prune_out_channels on _Reshape_429(), #idxs=96
[273] prune_out_channels on _Reshape_429() => prune_out_channels on _ElementWiseOp_428(AddBackward0), #idxs=96
[274] prune_out_channels on _ElementWiseOp_428(AddBackward0) => prune_out_channels on _Reshape_427(), #idxs=96
[275] prune_out_channels on _Reshape_427() => prune_out_channels on _ElementWiseOp_426(SoftmaxBackward0), #idxs=96
[276] prune_out_channels on _ElementWiseOp_426(SoftmaxBackward0) => prune_out_channels on _ElementWiseOp_425(ExpandBackward0), #idxs=96
[277] prune_out_channels on _ElementWiseOp_425(ExpandBackward0) => prune_out_channels on _Reshape_406(), #idxs=96
[278] prune_out_channels on _Reshape_406() => prune_out_channels on _ElementWiseOp_405(BmmBackward0), #idxs=96
[279] prune_out_channels on _ElementWiseOp_405(BmmBackward0) => prune_out_channels on _Reshape_407(), #idxs=96
[280] prune_out_channels on _ElementWiseOp_405(BmmBackward0) => prune_out_channels on _Reshape_404(), #idxs=96
[281] prune_out_channels on _Reshape_404() => prune_out_channels on _ElementWiseOp_403(TransposeBackward0), #idxs=96
[282] prune_out_channels on _ElementWiseOp_403(TransposeBackward0) => prune_out_channels on _ElementWiseOp_402(CloneBackward0), #idxs=96
[283] prune_out_channels on _ElementWiseOp_402(CloneBackward0) => prune_in_channels on stages.2.blocks.1.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=384, out_features=1152, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=384, out_features=384, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)), #idxs=96
[284] prune_out_channels on _Reshape_407() => prune_out_channels on _ElementWiseOp_408(CloneBackward0), #idxs=96
[285] prune_out_channels on _ElementWiseOp_408(CloneBackward0) => prune_out_channels on _ElementWiseOp_409(ExpandBackward0), #idxs=96
[286] prune_out_channels on _ElementWiseOp_432(UnsqueezeBackward0) => prune_out_channels on _ElementWiseOp_433(CloneBackward0), #idxs=96
[287] prune_out_channels on _ElementWiseOp_433(CloneBackward0) => prune_out_channels on _ElementWiseOp_434(PermuteBackward0), #idxs=96
[288] prune_out_channels on _ElementWiseOp_434(PermuteBackward0) => prune_out_channels on _Reshape_435(), #idxs=96
[289] prune_out_channels on _Reshape_435() => prune_out_channels on _ElementWiseOp_436(IndexBackward0), #idxs=96
[290] prune_out_channels on _Reshape_439() => prune_out_channels on _ElementWiseOp_440(CloneBackward0), #idxs=96
[291] prune_out_channels on _ElementWiseOp_440(CloneBackward0) => prune_out_channels on _ElementWiseOp_441(ExpandBackward0), #idxs=96
[292] prune_out_channels on _ElementWiseOp_441(ExpandBackward0) => prune_out_channels on _ElementWiseOp_442(TransposeBackward0), #idxs=96
[293] prune_out_channels on _ElementWiseOp_325(AddBackward0) => prune_out_channels on _Reshape_333(), #idxs=96
[294] prune_out_channels on _ElementWiseOp_325(AddBackward0) => prune_out_channels on stages.1.downsample (PatchMerging(
  (adap_padding): AdaptivePadding()
  (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
  (norm): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
  (reduction): Linear(in_features=576, out_features=384, bias=False)
)), #idxs=96
[295] prune_out_channels on _ElementWiseOp_325(AddBackward0) => prune_out_channels on stages.2.blocks.0.norm2 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)), #idxs=96
[296] prune_out_channels on stages.2.blocks.0.norm2 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_331(), #idxs=96
[297] prune_out_channels on _Reshape_331() => prune_out_channels on _ElementWiseOp_330(AddmmBackward0), #idxs=96
[298] prune_out_channels on _ElementWiseOp_330(AddmmBackward0) => prune_out_channels on _ElementWiseOp_332(TBackward0), #idxs=96
[299] prune_out_channels on _ElementWiseOp_330(AddmmBackward0) => prune_in_channels on stages.2.blocks.0.ffn.layers.0.0 (Linear(in_features=384, out_features=1536, bias=True)), #idxs=96
[300] prune_out_channels on stages.1.downsample (PatchMerging(
  (adap_padding): AdaptivePadding()
  (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
  (norm): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
  (reduction): Linear(in_features=576, out_features=384, bias=False)
)) => prune_out_channels on stages.2.blocks.0.norm1 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)), #idxs=96
[301] prune_out_channels on stages.2.blocks.0.norm1 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_372(), #idxs=96
[302] prune_out_channels on _Reshape_372() => prune_out_channels on _ElementWiseOp_371(ConstantPadNdBackward0), #idxs=96
[303] prune_out_channels on _ElementWiseOp_371(ConstantPadNdBackward0) => prune_out_channels on _Reshape_370(), #idxs=96
[304] prune_out_channels on _Reshape_370() => prune_out_channels on _ElementWiseOp_369(PermuteBackward0), #idxs=96
[305] prune_out_channels on _ElementWiseOp_369(PermuteBackward0) => prune_out_channels on _ElementWiseOp_368(CloneBackward0), #idxs=96
[306] prune_out_channels on _ElementWiseOp_368(CloneBackward0) => prune_out_channels on _Reshape_367(), #idxs=96
[307] prune_out_channels on _Reshape_367() => prune_out_channels on _Reshape_366(), #idxs=96
[308] prune_out_channels on _Reshape_366() => prune_out_channels on _Reshape_364(), #idxs=96
[309] prune_out_channels on _Reshape_364() => prune_out_channels on _ElementWiseOp_363(AddmmBackward0), #idxs=96
[310] prune_out_channels on _ElementWiseOp_363(AddmmBackward0) => prune_out_channels on _ElementWiseOp_365(TBackward0), #idxs=96
[311] prune_out_channels on _ElementWiseOp_363(AddmmBackward0) => prune_out_channels on _Reshape_362(), #idxs=96
[312] prune_out_channels on _Reshape_362() => prune_out_channels on _Reshape_361(), #idxs=96
[313] prune_out_channels on _Reshape_361() => prune_out_channels on _ElementWiseOp_360(PermuteBackward0), #idxs=96
[314] prune_out_channels on _ElementWiseOp_360(PermuteBackward0) => prune_out_channels on _ElementWiseOp_359(SelectBackward0), #idxs=96
[315] prune_out_channels on _ElementWiseOp_360(PermuteBackward0) => prune_out_channels on _ElementWiseOp_388(SelectBackward0), #idxs=96
[316] prune_out_channels on _ElementWiseOp_360(PermuteBackward0) => prune_out_channels on _ElementWiseOp_392(SelectBackward0), #idxs=96
[317] prune_out_channels on _ElementWiseOp_392(SelectBackward0) => prune_out_channels on _ElementWiseOp_391(MulBackward0), #idxs=96
[318] prune_out_channels on _ElementWiseOp_391(MulBackward0) => prune_out_channels on _ElementWiseOp_390(ExpandBackward0), #idxs=96
[319] prune_out_channels on _ElementWiseOp_390(ExpandBackward0) => prune_out_channels on _ElementWiseOp_389(CloneBackward0), #idxs=96
[320] prune_out_channels on _ElementWiseOp_389(CloneBackward0) => prune_out_channels on _Reshape_383(), #idxs=96
[321] prune_out_channels on _Reshape_383() => prune_out_channels on _ElementWiseOp_382(BmmBackward0), #idxs=96
[322] prune_out_channels on _ElementWiseOp_382(BmmBackward0) => prune_out_channels on _Reshape_384(), #idxs=96
[323] prune_out_channels on _ElementWiseOp_382(BmmBackward0) => prune_out_channels on _Reshape_376(), #idxs=96
[324] prune_out_channels on _Reshape_376() => prune_out_channels on _ElementWiseOp_375(AddBackward0), #idxs=96
[325] prune_out_channels on _ElementWiseOp_375(AddBackward0) => prune_out_channels on _ElementWiseOp_377(UnsqueezeBackward0), #idxs=96
[326] prune_out_channels on _ElementWiseOp_375(AddBackward0) => prune_out_channels on _ElementWiseOp_374(SoftmaxBackward0), #idxs=96
[327] prune_out_channels on _ElementWiseOp_374(SoftmaxBackward0) => prune_out_channels on _ElementWiseOp_373(ExpandBackward0), #idxs=96
[328] prune_out_channels on _ElementWiseOp_373(ExpandBackward0) => prune_out_channels on _Reshape_355(), #idxs=96
[329] prune_out_channels on _Reshape_355() => prune_out_channels on _ElementWiseOp_354(BmmBackward0), #idxs=96
[330] prune_out_channels on _ElementWiseOp_354(BmmBackward0) => prune_out_channels on _Reshape_356(), #idxs=96
[331] prune_out_channels on _ElementWiseOp_354(BmmBackward0) => prune_out_channels on _Reshape_353(), #idxs=96
[332] prune_out_channels on _Reshape_353() => prune_out_channels on _ElementWiseOp_352(TransposeBackward0), #idxs=96
[333] prune_out_channels on _ElementWiseOp_352(TransposeBackward0) => prune_out_channels on _ElementWiseOp_351(CloneBackward0), #idxs=96
[334] prune_out_channels on _ElementWiseOp_351(CloneBackward0) => prune_in_channels on stages.2.blocks.0.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=384, out_features=1152, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=384, out_features=384, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)), #idxs=96
[335] prune_out_channels on _Reshape_356() => prune_out_channels on _ElementWiseOp_357(CloneBackward0), #idxs=96
[336] prune_out_channels on _ElementWiseOp_357(CloneBackward0) => prune_out_channels on _ElementWiseOp_358(ExpandBackward0), #idxs=96
[337] prune_out_channels on _ElementWiseOp_377(UnsqueezeBackward0) => prune_out_channels on _ElementWiseOp_378(CloneBackward0), #idxs=96
[338] prune_out_channels on _ElementWiseOp_378(CloneBackward0) => prune_out_channels on _ElementWiseOp_379(PermuteBackward0), #idxs=96
[339] prune_out_channels on _ElementWiseOp_379(PermuteBackward0) => prune_out_channels on _Reshape_380(), #idxs=96
[340] prune_out_channels on _Reshape_380() => prune_out_channels on _ElementWiseOp_381(IndexBackward0), #idxs=96
[341] prune_out_channels on _Reshape_384() => prune_out_channels on _ElementWiseOp_385(CloneBackward0), #idxs=96
[342] prune_out_channels on _ElementWiseOp_385(CloneBackward0) => prune_out_channels on _ElementWiseOp_386(ExpandBackward0), #idxs=96
[343] prune_out_channels on _ElementWiseOp_386(ExpandBackward0) => prune_out_channels on _ElementWiseOp_387(TransposeBackward0), #idxs=96
[344] prune_out_channels on _Reshape_333() => prune_out_channels on _ElementWiseOp_343(SliceBackward0), #idxs=96
[345] prune_out_channels on _ElementWiseOp_343(SliceBackward0) => prune_out_channels on _ElementWiseOp_344(SliceBackward0), #idxs=96
[346] prune_out_channels on _ElementWiseOp_344(SliceBackward0) => prune_out_channels on _ElementWiseOp_345(SliceBackward0), #idxs=96
[347] prune_out_channels on _ElementWiseOp_345(SliceBackward0) => prune_out_channels on _Reshape_346(), #idxs=96
[348] prune_out_channels on _Reshape_346() => prune_out_channels on _ElementWiseOp_347(CloneBackward0), #idxs=96
[349] prune_out_channels on _ElementWiseOp_347(CloneBackward0) => prune_out_channels on _ElementWiseOp_348(PermuteBackward0), #idxs=96
[350] prune_out_channels on _ElementWiseOp_348(PermuteBackward0) => prune_out_channels on _Reshape_349(), #idxs=96
[351] prune_out_channels on _Reshape_349() => prune_out_channels on _Reshape_350(), #idxs=96
[352] prune_out_channels on _Reshape_350() => prune_out_channels on stages.2.blocks.0.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=384, out_features=1152, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=384, out_features=384, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)), #idxs=96
[353] prune_out_channels on _Reshape_323() => prune_out_channels on _ElementWiseOp_393(SliceBackward0), #idxs=96
[354] prune_out_channels on _ElementWiseOp_393(SliceBackward0) => prune_out_channels on _ElementWiseOp_394(SliceBackward0), #idxs=96
[355] prune_out_channels on _ElementWiseOp_394(SliceBackward0) => prune_out_channels on _ElementWiseOp_395(SliceBackward0), #idxs=96
[356] prune_out_channels on _ElementWiseOp_395(SliceBackward0) => prune_out_channels on _ElementWiseOp_396(RollBackward0), #idxs=96
[357] prune_out_channels on _ElementWiseOp_396(RollBackward0) => prune_out_channels on _Reshape_397(), #idxs=96
[358] prune_out_channels on _Reshape_397() => prune_out_channels on _ElementWiseOp_398(CloneBackward0), #idxs=96
[359] prune_out_channels on _ElementWiseOp_398(CloneBackward0) => prune_out_channels on _ElementWiseOp_399(PermuteBackward0), #idxs=96
[360] prune_out_channels on _ElementWiseOp_399(PermuteBackward0) => prune_out_channels on _Reshape_400(), #idxs=96
[361] prune_out_channels on _Reshape_400() => prune_out_channels on _Reshape_401(), #idxs=96
[362] prune_out_channels on _Reshape_401() => prune_out_channels on stages.2.blocks.1.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=384, out_features=1152, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=384, out_features=384, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)), #idxs=96
[363] prune_out_channels on _Reshape_313() => prune_out_channels on _ElementWiseOp_448(SliceBackward0), #idxs=96
[364] prune_out_channels on _ElementWiseOp_448(SliceBackward0) => prune_out_channels on _ElementWiseOp_449(SliceBackward0), #idxs=96
[365] prune_out_channels on _ElementWiseOp_449(SliceBackward0) => prune_out_channels on _ElementWiseOp_450(SliceBackward0), #idxs=96
[366] prune_out_channels on _ElementWiseOp_450(SliceBackward0) => prune_out_channels on _Reshape_451(), #idxs=96
[367] prune_out_channels on _Reshape_451() => prune_out_channels on _ElementWiseOp_452(CloneBackward0), #idxs=96
[368] prune_out_channels on _ElementWiseOp_452(CloneBackward0) => prune_out_channels on _ElementWiseOp_453(PermuteBackward0), #idxs=96
[369] prune_out_channels on _ElementWiseOp_453(PermuteBackward0) => prune_out_channels on _Reshape_454(), #idxs=96
[370] prune_out_channels on _Reshape_454() => prune_out_channels on _Reshape_455(), #idxs=96
[371] prune_out_channels on _Reshape_455() => prune_out_channels on stages.2.blocks.2.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=384, out_features=1152, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=384, out_features=384, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)), #idxs=96
[372] prune_out_channels on _Reshape_303() => prune_out_channels on _ElementWiseOp_498(SliceBackward0), #idxs=96
[373] prune_out_channels on _ElementWiseOp_498(SliceBackward0) => prune_out_channels on _ElementWiseOp_499(SliceBackward0), #idxs=96
[374] prune_out_channels on _ElementWiseOp_499(SliceBackward0) => prune_out_channels on _ElementWiseOp_500(SliceBackward0), #idxs=96
[375] prune_out_channels on _ElementWiseOp_500(SliceBackward0) => prune_out_channels on _ElementWiseOp_501(RollBackward0), #idxs=96
[376] prune_out_channels on _ElementWiseOp_501(RollBackward0) => prune_out_channels on _Reshape_502(), #idxs=96
[377] prune_out_channels on _Reshape_502() => prune_out_channels on _ElementWiseOp_503(CloneBackward0), #idxs=96
[378] prune_out_channels on _ElementWiseOp_503(CloneBackward0) => prune_out_channels on _ElementWiseOp_504(PermuteBackward0), #idxs=96
[379] prune_out_channels on _ElementWiseOp_504(PermuteBackward0) => prune_out_channels on _Reshape_505(), #idxs=96
[380] prune_out_channels on _Reshape_505() => prune_out_channels on _Reshape_506(), #idxs=96
[381] prune_out_channels on _Reshape_506() => prune_out_channels on stages.2.blocks.3.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=384, out_features=1152, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=384, out_features=384, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)), #idxs=96
[382] prune_out_channels on _Reshape_293() => prune_out_channels on _ElementWiseOp_553(SliceBackward0), #idxs=96
[383] prune_out_channels on _ElementWiseOp_553(SliceBackward0) => prune_out_channels on _ElementWiseOp_554(SliceBackward0), #idxs=96
[384] prune_out_channels on _ElementWiseOp_554(SliceBackward0) => prune_out_channels on _ElementWiseOp_555(SliceBackward0), #idxs=96
[385] prune_out_channels on _ElementWiseOp_555(SliceBackward0) => prune_out_channels on _Reshape_556(), #idxs=96
[386] prune_out_channels on _Reshape_556() => prune_out_channels on _ElementWiseOp_557(CloneBackward0), #idxs=96
[387] prune_out_channels on _ElementWiseOp_557(CloneBackward0) => prune_out_channels on _ElementWiseOp_558(PermuteBackward0), #idxs=96
[388] prune_out_channels on _ElementWiseOp_558(PermuteBackward0) => prune_out_channels on _Reshape_559(), #idxs=96
[389] prune_out_channels on _Reshape_559() => prune_out_channels on _Reshape_560(), #idxs=96
[390] prune_out_channels on _Reshape_560() => prune_out_channels on stages.2.blocks.4.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=384, out_features=1152, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=384, out_features=384, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)), #idxs=96
[391] prune_out_channels on _Reshape_283() => prune_out_channels on _ElementWiseOp_603(SliceBackward0), #idxs=96
[392] prune_out_channels on _ElementWiseOp_603(SliceBackward0) => prune_out_channels on _ElementWiseOp_604(SliceBackward0), #idxs=96
[393] prune_out_channels on _ElementWiseOp_604(SliceBackward0) => prune_out_channels on _ElementWiseOp_605(SliceBackward0), #idxs=96
[394] prune_out_channels on _ElementWiseOp_605(SliceBackward0) => prune_out_channels on _ElementWiseOp_606(RollBackward0), #idxs=96
[395] prune_out_channels on _ElementWiseOp_606(RollBackward0) => prune_out_channels on _Reshape_607(), #idxs=96
[396] prune_out_channels on _Reshape_607() => prune_out_channels on _ElementWiseOp_608(CloneBackward0), #idxs=96
[397] prune_out_channels on _ElementWiseOp_608(CloneBackward0) => prune_out_channels on _ElementWiseOp_609(PermuteBackward0), #idxs=96
[398] prune_out_channels on _ElementWiseOp_609(PermuteBackward0) => prune_out_channels on _Reshape_610(), #idxs=96
[399] prune_out_channels on _Reshape_610() => prune_out_channels on _Reshape_611(), #idxs=96
[400] prune_out_channels on _Reshape_611() => prune_out_channels on stages.2.blocks.5.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=384, out_features=1152, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=384, out_features=384, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)), #idxs=96
--------------------------------

PatchMergingPruner () prune_in_channels/  1536 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
PatchMergingPruner () prune_in_channels/ dim =  384
len indx =  96
idxs_repeated =  384
WindowMSAPruner prune_in_channels() /  96
WindowMSAPruner prune_in_channels() /  96
WindowMSAPruner prune_in_channels() /  96
WindowMSAPruner prune_in_channels() /  96
WindowMSAPruner prune_in_channels() /  96
PatchMergingPruner () prune_out_channels/  384 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
WindowMSAPruner prune_in_channels() /  96
WindowMSAPruner prune_out_channels() /  96
WindowMSAPruner prune_out_channels/ dim =  384
	len indx =  96
WindowMSAPruner - idxs =  [2, 3, 14, 23, 37, 40, 41, 50, 51, 52, 53, 54, 60, 64, 75, 79, 81, 86, 87, 88, 98, 108, 113, 114, 115, 119, 120, 122, 124, 129, 135, 137, 138, 140, 142, 145, 150, 161, 165, 168, 169, 172, 180, 182, 183, 187, 189, 193, 202, 204, 208, 209, 211, 216, 218, 221, 222, 234, 235, 238, 245, 252, 253, 254, 258, 259, 263, 265, 269, 271, 274, 277, 283, 300, 304, 305, 307, 309, 310, 311, 314, 315, 324, 326, 332, 333, 338, 343, 347, 353, 355, 359, 361, 368, 372, 377]
	idxs_repeated =  384
WindowMSAPruner - idxs_repeated =  [2, 3, 14, 23, 37, 40, 41, 50, 51, 52, 53, 54, 60, 64, 75, 79, 81, 86, 87, 88, 98, 108, 113, 114, 115, 119, 120, 122, 124, 129, 135, 137, 138, 140, 142, 145, 150, 161, 165, 168, 169, 172, 180, 182, 183, 187, 189, 193, 202, 204, 208, 209, 211, 216, 218, 221, 222, 234, 235, 238, 245, 252, 253, 254, 258, 259, 263, 265, 269, 271, 274, 277, 283, 300, 304, 305, 307, 309, 310, 311, 314, 315, 324, 326, 332, 333, 338, 343, 347, 353, 355, 359, 361, 368, 372, 377, 386, 387, 398, 407, 421, 424, 425, 434, 435, 436, 437, 438, 444, 448, 459, 463, 465, 470, 471, 472, 482, 492, 497, 498, 499, 503, 504, 506, 508, 513, 519, 521, 522, 524, 526, 529, 534, 545, 549, 552, 553, 556, 564, 566, 567, 571, 573, 577, 586, 588, 592, 593, 595, 600, 602, 605, 606, 618, 619, 622, 629, 636, 637, 638, 642, 643, 647, 649, 653, 655, 658, 661, 667, 684, 688, 689, 691, 693, 694, 695, 698, 699, 708, 710, 716, 717, 722, 727, 731, 737, 739, 743, 745, 752, 756, 761, 770, 771, 782, 791, 805, 808, 809, 818, 819, 820, 821, 822, 828, 832, 843, 847, 849, 854, 855, 856, 866, 876, 881, 882, 883, 887, 888, 890, 892, 897, 903, 905, 906, 908, 910, 913, 918, 929, 933, 936, 937, 940, 948, 950, 951, 955, 957, 961, 970, 972, 976, 977, 979, 984, 986, 989, 990, 1002, 1003, 1006, 1013, 1020, 1021, 1022, 1026, 1027, 1031, 1033, 1037, 1039, 1042, 1045, 1051, 1068, 1072, 1073, 1075, 1077, 1078, 1079, 1082, 1083, 1092, 1094, 1100, 1101, 1106, 1111, 1115, 1121, 1123, 1127, 1129, 1136, 1140, 1145, 1154, 1155, 1166, 1175, 1189, 1192, 1193, 1202, 1203, 1204, 1205, 1206, 1212, 1216, 1227, 1231, 1233, 1238, 1239, 1240, 1250, 1260, 1265, 1266, 1267, 1271, 1272, 1274, 1276, 1281, 1287, 1289, 1290, 1292, 1294, 1297, 1302, 1313, 1317, 1320, 1321, 1324, 1332, 1334, 1335, 1339, 1341, 1345, 1354, 1356, 1360, 1361, 1363, 1368, 1370, 1373, 1374, 1386, 1387, 1390, 1397, 1404, 1405, 1406, 1410, 1411, 1415, 1417, 1421, 1423, 1426, 1429, 1435, 1452, 1456, 1457, 1459, 1461, 1462, 1463, 1466, 1467, 1476, 1478, 1484, 1485, 1490, 1495, 1499, 1505, 1507, 1511, 1513, 1520, 1524, 1529]
WindowMSAPruner prune_out_channels idxs_repeated =  384
WindowMSAPruner prune_out_channels() /  96
WindowMSAPruner prune_out_channels/ dim =  384
	len indx =  96
WindowMSAPruner - idxs =  [2, 3, 14, 23, 37, 40, 41, 50, 51, 52, 53, 54, 60, 64, 75, 79, 81, 86, 87, 88, 98, 108, 113, 114, 115, 119, 120, 122, 124, 129, 135, 137, 138, 140, 142, 145, 150, 161, 165, 168, 169, 172, 180, 182, 183, 187, 189, 193, 202, 204, 208, 209, 211, 216, 218, 221, 222, 234, 235, 238, 245, 252, 253, 254, 258, 259, 263, 265, 269, 271, 274, 277, 283, 300, 304, 305, 307, 309, 310, 311, 314, 315, 324, 326, 332, 333, 338, 343, 347, 353, 355, 359, 361, 368, 372, 377]
	idxs_repeated =  384
WindowMSAPruner - idxs_repeated =  [2, 3, 14, 23, 37, 40, 41, 50, 51, 52, 53, 54, 60, 64, 75, 79, 81, 86, 87, 88, 98, 108, 113, 114, 115, 119, 120, 122, 124, 129, 135, 137, 138, 140, 142, 145, 150, 161, 165, 168, 169, 172, 180, 182, 183, 187, 189, 193, 202, 204, 208, 209, 211, 216, 218, 221, 222, 234, 235, 238, 245, 252, 253, 254, 258, 259, 263, 265, 269, 271, 274, 277, 283, 300, 304, 305, 307, 309, 310, 311, 314, 315, 324, 326, 332, 333, 338, 343, 347, 353, 355, 359, 361, 368, 372, 377, 386, 387, 398, 407, 421, 424, 425, 434, 435, 436, 437, 438, 444, 448, 459, 463, 465, 470, 471, 472, 482, 492, 497, 498, 499, 503, 504, 506, 508, 513, 519, 521, 522, 524, 526, 529, 534, 545, 549, 552, 553, 556, 564, 566, 567, 571, 573, 577, 586, 588, 592, 593, 595, 600, 602, 605, 606, 618, 619, 622, 629, 636, 637, 638, 642, 643, 647, 649, 653, 655, 658, 661, 667, 684, 688, 689, 691, 693, 694, 695, 698, 699, 708, 710, 716, 717, 722, 727, 731, 737, 739, 743, 745, 752, 756, 761, 770, 771, 782, 791, 805, 808, 809, 818, 819, 820, 821, 822, 828, 832, 843, 847, 849, 854, 855, 856, 866, 876, 881, 882, 883, 887, 888, 890, 892, 897, 903, 905, 906, 908, 910, 913, 918, 929, 933, 936, 937, 940, 948, 950, 951, 955, 957, 961, 970, 972, 976, 977, 979, 984, 986, 989, 990, 1002, 1003, 1006, 1013, 1020, 1021, 1022, 1026, 1027, 1031, 1033, 1037, 1039, 1042, 1045, 1051, 1068, 1072, 1073, 1075, 1077, 1078, 1079, 1082, 1083, 1092, 1094, 1100, 1101, 1106, 1111, 1115, 1121, 1123, 1127, 1129, 1136, 1140, 1145, 1154, 1155, 1166, 1175, 1189, 1192, 1193, 1202, 1203, 1204, 1205, 1206, 1212, 1216, 1227, 1231, 1233, 1238, 1239, 1240, 1250, 1260, 1265, 1266, 1267, 1271, 1272, 1274, 1276, 1281, 1287, 1289, 1290, 1292, 1294, 1297, 1302, 1313, 1317, 1320, 1321, 1324, 1332, 1334, 1335, 1339, 1341, 1345, 1354, 1356, 1360, 1361, 1363, 1368, 1370, 1373, 1374, 1386, 1387, 1390, 1397, 1404, 1405, 1406, 1410, 1411, 1415, 1417, 1421, 1423, 1426, 1429, 1435, 1452, 1456, 1457, 1459, 1461, 1462, 1463, 1466, 1467, 1476, 1478, 1484, 1485, 1490, 1495, 1499, 1505, 1507, 1511, 1513, 1520, 1524, 1529]
WindowMSAPruner prune_out_channels idxs_repeated =  384
WindowMSAPruner prune_out_channels() /  96
WindowMSAPruner prune_out_channels/ dim =  384
	len indx =  96
WindowMSAPruner - idxs =  [2, 3, 14, 23, 37, 40, 41, 50, 51, 52, 53, 54, 60, 64, 75, 79, 81, 86, 87, 88, 98, 108, 113, 114, 115, 119, 120, 122, 124, 129, 135, 137, 138, 140, 142, 145, 150, 161, 165, 168, 169, 172, 180, 182, 183, 187, 189, 193, 202, 204, 208, 209, 211, 216, 218, 221, 222, 234, 235, 238, 245, 252, 253, 254, 258, 259, 263, 265, 269, 271, 274, 277, 283, 300, 304, 305, 307, 309, 310, 311, 314, 315, 324, 326, 332, 333, 338, 343, 347, 353, 355, 359, 361, 368, 372, 377]
	idxs_repeated =  384
WindowMSAPruner - idxs_repeated =  [2, 3, 14, 23, 37, 40, 41, 50, 51, 52, 53, 54, 60, 64, 75, 79, 81, 86, 87, 88, 98, 108, 113, 114, 115, 119, 120, 122, 124, 129, 135, 137, 138, 140, 142, 145, 150, 161, 165, 168, 169, 172, 180, 182, 183, 187, 189, 193, 202, 204, 208, 209, 211, 216, 218, 221, 222, 234, 235, 238, 245, 252, 253, 254, 258, 259, 263, 265, 269, 271, 274, 277, 283, 300, 304, 305, 307, 309, 310, 311, 314, 315, 324, 326, 332, 333, 338, 343, 347, 353, 355, 359, 361, 368, 372, 377, 386, 387, 398, 407, 421, 424, 425, 434, 435, 436, 437, 438, 444, 448, 459, 463, 465, 470, 471, 472, 482, 492, 497, 498, 499, 503, 504, 506, 508, 513, 519, 521, 522, 524, 526, 529, 534, 545, 549, 552, 553, 556, 564, 566, 567, 571, 573, 577, 586, 588, 592, 593, 595, 600, 602, 605, 606, 618, 619, 622, 629, 636, 637, 638, 642, 643, 647, 649, 653, 655, 658, 661, 667, 684, 688, 689, 691, 693, 694, 695, 698, 699, 708, 710, 716, 717, 722, 727, 731, 737, 739, 743, 745, 752, 756, 761, 770, 771, 782, 791, 805, 808, 809, 818, 819, 820, 821, 822, 828, 832, 843, 847, 849, 854, 855, 856, 866, 876, 881, 882, 883, 887, 888, 890, 892, 897, 903, 905, 906, 908, 910, 913, 918, 929, 933, 936, 937, 940, 948, 950, 951, 955, 957, 961, 970, 972, 976, 977, 979, 984, 986, 989, 990, 1002, 1003, 1006, 1013, 1020, 1021, 1022, 1026, 1027, 1031, 1033, 1037, 1039, 1042, 1045, 1051, 1068, 1072, 1073, 1075, 1077, 1078, 1079, 1082, 1083, 1092, 1094, 1100, 1101, 1106, 1111, 1115, 1121, 1123, 1127, 1129, 1136, 1140, 1145, 1154, 1155, 1166, 1175, 1189, 1192, 1193, 1202, 1203, 1204, 1205, 1206, 1212, 1216, 1227, 1231, 1233, 1238, 1239, 1240, 1250, 1260, 1265, 1266, 1267, 1271, 1272, 1274, 1276, 1281, 1287, 1289, 1290, 1292, 1294, 1297, 1302, 1313, 1317, 1320, 1321, 1324, 1332, 1334, 1335, 1339, 1341, 1345, 1354, 1356, 1360, 1361, 1363, 1368, 1370, 1373, 1374, 1386, 1387, 1390, 1397, 1404, 1405, 1406, 1410, 1411, 1415, 1417, 1421, 1423, 1426, 1429, 1435, 1452, 1456, 1457, 1459, 1461, 1462, 1463, 1466, 1467, 1476, 1478, 1484, 1485, 1490, 1495, 1499, 1505, 1507, 1511, 1513, 1520, 1524, 1529]
WindowMSAPruner prune_out_channels idxs_repeated =  384
WindowMSAPruner prune_out_channels() /  96
WindowMSAPruner prune_out_channels/ dim =  384
	len indx =  96
WindowMSAPruner - idxs =  [2, 3, 14, 23, 37, 40, 41, 50, 51, 52, 53, 54, 60, 64, 75, 79, 81, 86, 87, 88, 98, 108, 113, 114, 115, 119, 120, 122, 124, 129, 135, 137, 138, 140, 142, 145, 150, 161, 165, 168, 169, 172, 180, 182, 183, 187, 189, 193, 202, 204, 208, 209, 211, 216, 218, 221, 222, 234, 235, 238, 245, 252, 253, 254, 258, 259, 263, 265, 269, 271, 274, 277, 283, 300, 304, 305, 307, 309, 310, 311, 314, 315, 324, 326, 332, 333, 338, 343, 347, 353, 355, 359, 361, 368, 372, 377]
	idxs_repeated =  384
WindowMSAPruner - idxs_repeated =  [2, 3, 14, 23, 37, 40, 41, 50, 51, 52, 53, 54, 60, 64, 75, 79, 81, 86, 87, 88, 98, 108, 113, 114, 115, 119, 120, 122, 124, 129, 135, 137, 138, 140, 142, 145, 150, 161, 165, 168, 169, 172, 180, 182, 183, 187, 189, 193, 202, 204, 208, 209, 211, 216, 218, 221, 222, 234, 235, 238, 245, 252, 253, 254, 258, 259, 263, 265, 269, 271, 274, 277, 283, 300, 304, 305, 307, 309, 310, 311, 314, 315, 324, 326, 332, 333, 338, 343, 347, 353, 355, 359, 361, 368, 372, 377, 386, 387, 398, 407, 421, 424, 425, 434, 435, 436, 437, 438, 444, 448, 459, 463, 465, 470, 471, 472, 482, 492, 497, 498, 499, 503, 504, 506, 508, 513, 519, 521, 522, 524, 526, 529, 534, 545, 549, 552, 553, 556, 564, 566, 567, 571, 573, 577, 586, 588, 592, 593, 595, 600, 602, 605, 606, 618, 619, 622, 629, 636, 637, 638, 642, 643, 647, 649, 653, 655, 658, 661, 667, 684, 688, 689, 691, 693, 694, 695, 698, 699, 708, 710, 716, 717, 722, 727, 731, 737, 739, 743, 745, 752, 756, 761, 770, 771, 782, 791, 805, 808, 809, 818, 819, 820, 821, 822, 828, 832, 843, 847, 849, 854, 855, 856, 866, 876, 881, 882, 883, 887, 888, 890, 892, 897, 903, 905, 906, 908, 910, 913, 918, 929, 933, 936, 937, 940, 948, 950, 951, 955, 957, 961, 970, 972, 976, 977, 979, 984, 986, 989, 990, 1002, 1003, 1006, 1013, 1020, 1021, 1022, 1026, 1027, 1031, 1033, 1037, 1039, 1042, 1045, 1051, 1068, 1072, 1073, 1075, 1077, 1078, 1079, 1082, 1083, 1092, 1094, 1100, 1101, 1106, 1111, 1115, 1121, 1123, 1127, 1129, 1136, 1140, 1145, 1154, 1155, 1166, 1175, 1189, 1192, 1193, 1202, 1203, 1204, 1205, 1206, 1212, 1216, 1227, 1231, 1233, 1238, 1239, 1240, 1250, 1260, 1265, 1266, 1267, 1271, 1272, 1274, 1276, 1281, 1287, 1289, 1290, 1292, 1294, 1297, 1302, 1313, 1317, 1320, 1321, 1324, 1332, 1334, 1335, 1339, 1341, 1345, 1354, 1356, 1360, 1361, 1363, 1368, 1370, 1373, 1374, 1386, 1387, 1390, 1397, 1404, 1405, 1406, 1410, 1411, 1415, 1417, 1421, 1423, 1426, 1429, 1435, 1452, 1456, 1457, 1459, 1461, 1462, 1463, 1466, 1467, 1476, 1478, 1484, 1485, 1490, 1495, 1499, 1505, 1507, 1511, 1513, 1520, 1524, 1529]
WindowMSAPruner prune_out_channels idxs_repeated =  384
WindowMSAPruner prune_out_channels() /  96
WindowMSAPruner prune_out_channels/ dim =  384
	len indx =  96
WindowMSAPruner - idxs =  [2, 3, 14, 23, 37, 40, 41, 50, 51, 52, 53, 54, 60, 64, 75, 79, 81, 86, 87, 88, 98, 108, 113, 114, 115, 119, 120, 122, 124, 129, 135, 137, 138, 140, 142, 145, 150, 161, 165, 168, 169, 172, 180, 182, 183, 187, 189, 193, 202, 204, 208, 209, 211, 216, 218, 221, 222, 234, 235, 238, 245, 252, 253, 254, 258, 259, 263, 265, 269, 271, 274, 277, 283, 300, 304, 305, 307, 309, 310, 311, 314, 315, 324, 326, 332, 333, 338, 343, 347, 353, 355, 359, 361, 368, 372, 377]
	idxs_repeated =  384
WindowMSAPruner - idxs_repeated =  [2, 3, 14, 23, 37, 40, 41, 50, 51, 52, 53, 54, 60, 64, 75, 79, 81, 86, 87, 88, 98, 108, 113, 114, 115, 119, 120, 122, 124, 129, 135, 137, 138, 140, 142, 145, 150, 161, 165, 168, 169, 172, 180, 182, 183, 187, 189, 193, 202, 204, 208, 209, 211, 216, 218, 221, 222, 234, 235, 238, 245, 252, 253, 254, 258, 259, 263, 265, 269, 271, 274, 277, 283, 300, 304, 305, 307, 309, 310, 311, 314, 315, 324, 326, 332, 333, 338, 343, 347, 353, 355, 359, 361, 368, 372, 377, 386, 387, 398, 407, 421, 424, 425, 434, 435, 436, 437, 438, 444, 448, 459, 463, 465, 470, 471, 472, 482, 492, 497, 498, 499, 503, 504, 506, 508, 513, 519, 521, 522, 524, 526, 529, 534, 545, 549, 552, 553, 556, 564, 566, 567, 571, 573, 577, 586, 588, 592, 593, 595, 600, 602, 605, 606, 618, 619, 622, 629, 636, 637, 638, 642, 643, 647, 649, 653, 655, 658, 661, 667, 684, 688, 689, 691, 693, 694, 695, 698, 699, 708, 710, 716, 717, 722, 727, 731, 737, 739, 743, 745, 752, 756, 761, 770, 771, 782, 791, 805, 808, 809, 818, 819, 820, 821, 822, 828, 832, 843, 847, 849, 854, 855, 856, 866, 876, 881, 882, 883, 887, 888, 890, 892, 897, 903, 905, 906, 908, 910, 913, 918, 929, 933, 936, 937, 940, 948, 950, 951, 955, 957, 961, 970, 972, 976, 977, 979, 984, 986, 989, 990, 1002, 1003, 1006, 1013, 1020, 1021, 1022, 1026, 1027, 1031, 1033, 1037, 1039, 1042, 1045, 1051, 1068, 1072, 1073, 1075, 1077, 1078, 1079, 1082, 1083, 1092, 1094, 1100, 1101, 1106, 1111, 1115, 1121, 1123, 1127, 1129, 1136, 1140, 1145, 1154, 1155, 1166, 1175, 1189, 1192, 1193, 1202, 1203, 1204, 1205, 1206, 1212, 1216, 1227, 1231, 1233, 1238, 1239, 1240, 1250, 1260, 1265, 1266, 1267, 1271, 1272, 1274, 1276, 1281, 1287, 1289, 1290, 1292, 1294, 1297, 1302, 1313, 1317, 1320, 1321, 1324, 1332, 1334, 1335, 1339, 1341, 1345, 1354, 1356, 1360, 1361, 1363, 1368, 1370, 1373, 1374, 1386, 1387, 1390, 1397, 1404, 1405, 1406, 1410, 1411, 1415, 1417, 1421, 1423, 1426, 1429, 1435, 1452, 1456, 1457, 1459, 1461, 1462, 1463, 1466, 1467, 1476, 1478, 1484, 1485, 1490, 1495, 1499, 1505, 1507, 1511, 1513, 1520, 1524, 1529]
WindowMSAPruner prune_out_channels idxs_repeated =  384
WindowMSAPruner prune_out_channels() /  96
WindowMSAPruner prune_out_channels/ dim =  384
	len indx =  96
WindowMSAPruner - idxs =  [2, 3, 14, 23, 37, 40, 41, 50, 51, 52, 53, 54, 60, 64, 75, 79, 81, 86, 87, 88, 98, 108, 113, 114, 115, 119, 120, 122, 124, 129, 135, 137, 138, 140, 142, 145, 150, 161, 165, 168, 169, 172, 180, 182, 183, 187, 189, 193, 202, 204, 208, 209, 211, 216, 218, 221, 222, 234, 235, 238, 245, 252, 253, 254, 258, 259, 263, 265, 269, 271, 274, 277, 283, 300, 304, 305, 307, 309, 310, 311, 314, 315, 324, 326, 332, 333, 338, 343, 347, 353, 355, 359, 361, 368, 372, 377]
	idxs_repeated =  384
WindowMSAPruner - idxs_repeated =  [2, 3, 14, 23, 37, 40, 41, 50, 51, 52, 53, 54, 60, 64, 75, 79, 81, 86, 87, 88, 98, 108, 113, 114, 115, 119, 120, 122, 124, 129, 135, 137, 138, 140, 142, 145, 150, 161, 165, 168, 169, 172, 180, 182, 183, 187, 189, 193, 202, 204, 208, 209, 211, 216, 218, 221, 222, 234, 235, 238, 245, 252, 253, 254, 258, 259, 263, 265, 269, 271, 274, 277, 283, 300, 304, 305, 307, 309, 310, 311, 314, 315, 324, 326, 332, 333, 338, 343, 347, 353, 355, 359, 361, 368, 372, 377, 386, 387, 398, 407, 421, 424, 425, 434, 435, 436, 437, 438, 444, 448, 459, 463, 465, 470, 471, 472, 482, 492, 497, 498, 499, 503, 504, 506, 508, 513, 519, 521, 522, 524, 526, 529, 534, 545, 549, 552, 553, 556, 564, 566, 567, 571, 573, 577, 586, 588, 592, 593, 595, 600, 602, 605, 606, 618, 619, 622, 629, 636, 637, 638, 642, 643, 647, 649, 653, 655, 658, 661, 667, 684, 688, 689, 691, 693, 694, 695, 698, 699, 708, 710, 716, 717, 722, 727, 731, 737, 739, 743, 745, 752, 756, 761, 770, 771, 782, 791, 805, 808, 809, 818, 819, 820, 821, 822, 828, 832, 843, 847, 849, 854, 855, 856, 866, 876, 881, 882, 883, 887, 888, 890, 892, 897, 903, 905, 906, 908, 910, 913, 918, 929, 933, 936, 937, 940, 948, 950, 951, 955, 957, 961, 970, 972, 976, 977, 979, 984, 986, 989, 990, 1002, 1003, 1006, 1013, 1020, 1021, 1022, 1026, 1027, 1031, 1033, 1037, 1039, 1042, 1045, 1051, 1068, 1072, 1073, 1075, 1077, 1078, 1079, 1082, 1083, 1092, 1094, 1100, 1101, 1106, 1111, 1115, 1121, 1123, 1127, 1129, 1136, 1140, 1145, 1154, 1155, 1166, 1175, 1189, 1192, 1193, 1202, 1203, 1204, 1205, 1206, 1212, 1216, 1227, 1231, 1233, 1238, 1239, 1240, 1250, 1260, 1265, 1266, 1267, 1271, 1272, 1274, 1276, 1281, 1287, 1289, 1290, 1292, 1294, 1297, 1302, 1313, 1317, 1320, 1321, 1324, 1332, 1334, 1335, 1339, 1341, 1345, 1354, 1356, 1360, 1361, 1363, 1368, 1370, 1373, 1374, 1386, 1387, 1390, 1397, 1404, 1405, 1406, 1410, 1411, 1415, 1417, 1421, 1423, 1426, 1429, 1435, 1452, 1456, 1457, 1459, 1461, 1462, 1463, 1466, 1467, 1476, 1478, 1484, 1485, 1490, 1495, 1499, 1505, 1507, 1511, 1513, 1520, 1524, 1529]
WindowMSAPruner prune_out_channels idxs_repeated =  384
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
	get_all_groups -- Adding layer:  <class 'mmcv.cnn.bricks.wrappers.Linear'>
		prune_local()/  _check_pruning_ratio OK
idxs =  384
prunable_chs =  1536
pruner.step 
--------------------------------
          Pruning Group
--------------------------------
[0] prune_out_channels on stages.2.blocks.5.ffn.layers.0.0 (Linear(in_features=288, out_features=1536, bias=True)) => prune_out_channels on stages.2.blocks.5.ffn.layers.0.0 (Linear(in_features=288, out_features=1536, bias=True)), #idxs=384
[1] prune_out_channels on stages.2.blocks.5.ffn.layers.0.0 (Linear(in_features=288, out_features=1536, bias=True)) => prune_out_channels on _ElementWiseOp_279(GeluBackward0), #idxs=384
[2] prune_out_channels on _ElementWiseOp_279(GeluBackward0) => prune_out_channels on _Reshape_277(), #idxs=384
[3] prune_out_channels on _Reshape_277() => prune_out_channels on _ElementWiseOp_276(AddmmBackward0), #idxs=384
[4] prune_out_channels on _ElementWiseOp_276(AddmmBackward0) => prune_out_channels on _ElementWiseOp_278(TBackward0), #idxs=384
[5] prune_out_channels on _ElementWiseOp_276(AddmmBackward0) => prune_in_channels on stages.2.blocks.5.ffn.layers.1 (Linear(in_features=1536, out_features=288, bias=True)), #idxs=384
--------------------------------

<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
continue 4
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
	get_all_groups -- Adding layer:  <class 'mmcv.cnn.bricks.wrappers.Linear'>
		prune_local()/  _check_pruning_ratio OK
idxs =  384
prunable_chs =  1536
pruner.step 
--------------------------------
          Pruning Group
--------------------------------
[0] prune_out_channels on stages.2.blocks.4.ffn.layers.0.0 (Linear(in_features=288, out_features=1536, bias=True)) => prune_out_channels on stages.2.blocks.4.ffn.layers.0.0 (Linear(in_features=288, out_features=1536, bias=True)), #idxs=384
[1] prune_out_channels on stages.2.blocks.4.ffn.layers.0.0 (Linear(in_features=288, out_features=1536, bias=True)) => prune_out_channels on _ElementWiseOp_289(GeluBackward0), #idxs=384
[2] prune_out_channels on _ElementWiseOp_289(GeluBackward0) => prune_out_channels on _Reshape_287(), #idxs=384
[3] prune_out_channels on _Reshape_287() => prune_out_channels on _ElementWiseOp_286(AddmmBackward0), #idxs=384
[4] prune_out_channels on _ElementWiseOp_286(AddmmBackward0) => prune_out_channels on _ElementWiseOp_288(TBackward0), #idxs=384
[5] prune_out_channels on _ElementWiseOp_286(AddmmBackward0) => prune_in_channels on stages.2.blocks.4.ffn.layers.1 (Linear(in_features=1536, out_features=288, bias=True)), #idxs=384
--------------------------------

<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
continue 4
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
	get_all_groups -- Adding layer:  <class 'mmcv.cnn.bricks.wrappers.Linear'>
		prune_local()/  _check_pruning_ratio OK
idxs =  384
prunable_chs =  1536
pruner.step 
--------------------------------
          Pruning Group
--------------------------------
[0] prune_out_channels on stages.2.blocks.3.ffn.layers.0.0 (Linear(in_features=288, out_features=1536, bias=True)) => prune_out_channels on stages.2.blocks.3.ffn.layers.0.0 (Linear(in_features=288, out_features=1536, bias=True)), #idxs=384
[1] prune_out_channels on stages.2.blocks.3.ffn.layers.0.0 (Linear(in_features=288, out_features=1536, bias=True)) => prune_out_channels on _ElementWiseOp_299(GeluBackward0), #idxs=384
[2] prune_out_channels on _ElementWiseOp_299(GeluBackward0) => prune_out_channels on _Reshape_297(), #idxs=384
[3] prune_out_channels on _Reshape_297() => prune_out_channels on _ElementWiseOp_296(AddmmBackward0), #idxs=384
[4] prune_out_channels on _ElementWiseOp_296(AddmmBackward0) => prune_out_channels on _ElementWiseOp_298(TBackward0), #idxs=384
[5] prune_out_channels on _ElementWiseOp_296(AddmmBackward0) => prune_in_channels on stages.2.blocks.3.ffn.layers.1 (Linear(in_features=1536, out_features=288, bias=True)), #idxs=384
--------------------------------

<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
continue 4
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
	get_all_groups -- Adding layer:  <class 'mmcv.cnn.bricks.wrappers.Linear'>
		prune_local()/  _check_pruning_ratio OK
idxs =  384
prunable_chs =  1536
pruner.step 
--------------------------------
          Pruning Group
--------------------------------
[0] prune_out_channels on stages.2.blocks.2.ffn.layers.0.0 (Linear(in_features=288, out_features=1536, bias=True)) => prune_out_channels on stages.2.blocks.2.ffn.layers.0.0 (Linear(in_features=288, out_features=1536, bias=True)), #idxs=384
[1] prune_out_channels on stages.2.blocks.2.ffn.layers.0.0 (Linear(in_features=288, out_features=1536, bias=True)) => prune_out_channels on _ElementWiseOp_309(GeluBackward0), #idxs=384
[2] prune_out_channels on _ElementWiseOp_309(GeluBackward0) => prune_out_channels on _Reshape_307(), #idxs=384
[3] prune_out_channels on _Reshape_307() => prune_out_channels on _ElementWiseOp_306(AddmmBackward0), #idxs=384
[4] prune_out_channels on _ElementWiseOp_306(AddmmBackward0) => prune_out_channels on _ElementWiseOp_308(TBackward0), #idxs=384
[5] prune_out_channels on _ElementWiseOp_306(AddmmBackward0) => prune_in_channels on stages.2.blocks.2.ffn.layers.1 (Linear(in_features=1536, out_features=288, bias=True)), #idxs=384
--------------------------------

<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
continue 4
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
	get_all_groups -- Adding layer:  <class 'mmcv.cnn.bricks.wrappers.Linear'>
		prune_local()/  _check_pruning_ratio OK
idxs =  384
prunable_chs =  1536
pruner.step 
--------------------------------
          Pruning Group
--------------------------------
[0] prune_out_channels on stages.2.blocks.1.ffn.layers.0.0 (Linear(in_features=288, out_features=1536, bias=True)) => prune_out_channels on stages.2.blocks.1.ffn.layers.0.0 (Linear(in_features=288, out_features=1536, bias=True)), #idxs=384
[1] prune_out_channels on stages.2.blocks.1.ffn.layers.0.0 (Linear(in_features=288, out_features=1536, bias=True)) => prune_out_channels on _ElementWiseOp_319(GeluBackward0), #idxs=384
[2] prune_out_channels on _ElementWiseOp_319(GeluBackward0) => prune_out_channels on _Reshape_317(), #idxs=384
[3] prune_out_channels on _Reshape_317() => prune_out_channels on _ElementWiseOp_316(AddmmBackward0), #idxs=384
[4] prune_out_channels on _ElementWiseOp_316(AddmmBackward0) => prune_out_channels on _ElementWiseOp_318(TBackward0), #idxs=384
[5] prune_out_channels on _ElementWiseOp_316(AddmmBackward0) => prune_in_channels on stages.2.blocks.1.ffn.layers.1 (Linear(in_features=1536, out_features=288, bias=True)), #idxs=384
--------------------------------

<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
continue 4
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
	get_all_groups -- Adding layer:  <class 'mmcv.cnn.bricks.wrappers.Linear'>
		prune_local()/  _check_pruning_ratio OK
idxs =  384
prunable_chs =  1536
pruner.step 
--------------------------------
          Pruning Group
--------------------------------
[0] prune_out_channels on stages.2.blocks.0.ffn.layers.0.0 (Linear(in_features=288, out_features=1536, bias=True)) => prune_out_channels on stages.2.blocks.0.ffn.layers.0.0 (Linear(in_features=288, out_features=1536, bias=True)), #idxs=384
[1] prune_out_channels on stages.2.blocks.0.ffn.layers.0.0 (Linear(in_features=288, out_features=1536, bias=True)) => prune_out_channels on _ElementWiseOp_329(GeluBackward0), #idxs=384
[2] prune_out_channels on _ElementWiseOp_329(GeluBackward0) => prune_out_channels on _Reshape_327(), #idxs=384
[3] prune_out_channels on _Reshape_327() => prune_out_channels on _ElementWiseOp_326(AddmmBackward0), #idxs=384
[4] prune_out_channels on _ElementWiseOp_326(AddmmBackward0) => prune_out_channels on _ElementWiseOp_328(TBackward0), #idxs=384
[5] prune_out_channels on _ElementWiseOp_326(AddmmBackward0) => prune_in_channels on stages.2.blocks.0.ffn.layers.1 (Linear(in_features=1536, out_features=288, bias=True)), #idxs=384
--------------------------------

<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
continue 4
<class 'torch_pruning.ops._ReshapeOp'> continue 2
PatchMergingPruner () get_out_channels/  288 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
WindowMSAPruner layer.embed_dims =  288
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
WindowMSAPruner layer.embed_dims =  288
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
WindowMSAPruner layer.embed_dims =  288
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
WindowMSAPruner layer.embed_dims =  288
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
WindowMSAPruner layer.embed_dims =  288
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
WindowMSAPruner layer.embed_dims =  288
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
	get_all_groups -- Adding layer:  <class 'torch.nn.modules.normalization.LayerNorm'>
WindowMSAPruner layer.qkv.in_features =  768
PatchMergingPruner () get_out_channels/  768 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
WindowMSAPruner layer.qkv.in_features =  768
WindowMSAPruner layer.embed_dims =  768
WindowMSAPruner layer.embed_dims =  768
		prune_local()/  _check_pruning_ratio OK
idxs =  192
prunable_chs =  768
idxs =  192
prunable_chs =  768
idxs =  192
prunable_chs =  768
idxs =  192
prunable_chs =  768
idxs =  192
prunable_chs =  768
WindowMSAPruner layer.qkv.in_features =  768
PatchMergingPruner () get_out_channels/  768 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
idxs =  192
prunable_chs =  768
idxs =  192
prunable_chs =  768
idxs =  192
prunable_chs =  768
WindowMSAPruner layer.qkv.in_features =  768
WindowMSAPruner layer.embed_dims =  768
idxs =  192
prunable_chs =  768
WindowMSAPruner layer.embed_dims =  768
idxs =  192
prunable_chs =  768
pruner.step 
--------------------------------
          Pruning Group
--------------------------------
[0] prune_out_channels on norm3 (LayerNorm((768,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on norm3 (LayerNorm((768,), eps=1e-05, elementwise_affine=True)), #idxs=192
[1] prune_out_channels on norm3 (LayerNorm((768,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _ElementWiseOp_661(AddBackward0), #idxs=192
[2] prune_out_channels on norm3 (LayerNorm((768,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_660(), #idxs=192
[3] prune_out_channels on _Reshape_660() => prune_out_channels on _ElementWiseOp_659(PermuteBackward0), #idxs=192
[4] prune_out_channels on _ElementWiseOp_659(PermuteBackward0) => prune_out_channels on _ElementWiseOp_658(CloneBackward0), #idxs=192
[5] prune_out_channels on _ElementWiseOp_661(AddBackward0) => prune_out_channels on _ElementWiseOp_662(AddBackward0), #idxs=192
[6] prune_out_channels on _ElementWiseOp_661(AddBackward0) => prune_out_channels on stages.3.blocks.1.ffn.layers.1 (Linear(in_features=3072, out_features=768, bias=True)), #idxs=192
[7] prune_out_channels on _ElementWiseOp_662(AddBackward0) => prune_out_channels on _Reshape_670(), #idxs=192
[8] prune_out_channels on _ElementWiseOp_662(AddBackward0) => prune_out_channels on _ElementWiseOp_671(AddBackward0), #idxs=192
[9] prune_out_channels on _ElementWiseOp_662(AddBackward0) => prune_out_channels on stages.3.blocks.1.norm2 (LayerNorm((768,), eps=1e-05, elementwise_affine=True)), #idxs=192
[10] prune_out_channels on stages.3.blocks.1.norm2 (LayerNorm((768,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_668(), #idxs=192
[11] prune_out_channels on _Reshape_668() => prune_out_channels on _ElementWiseOp_667(AddmmBackward0), #idxs=192
[12] prune_out_channels on _ElementWiseOp_667(AddmmBackward0) => prune_out_channels on _ElementWiseOp_669(TBackward0), #idxs=192
[13] prune_out_channels on _ElementWiseOp_667(AddmmBackward0) => prune_in_channels on stages.3.blocks.1.ffn.layers.0.0 (Linear(in_features=768, out_features=3072, bias=True)), #idxs=192
[14] prune_out_channels on _ElementWiseOp_671(AddBackward0) => prune_out_channels on _ElementWiseOp_672(AddBackward0), #idxs=192
[15] prune_out_channels on _ElementWiseOp_671(AddBackward0) => prune_out_channels on stages.3.blocks.0.ffn.layers.1 (Linear(in_features=3072, out_features=768, bias=True)), #idxs=192
[16] prune_out_channels on _ElementWiseOp_671(AddBackward0) => prune_out_channels on stages.3.blocks.1.norm1 (LayerNorm((768,), eps=1e-05, elementwise_affine=True)), #idxs=192
[17] prune_out_channels on stages.3.blocks.1.norm1 (LayerNorm((768,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_770(), #idxs=192
[18] prune_out_channels on _Reshape_770() => prune_out_channels on _ElementWiseOp_769(ConstantPadNdBackward0), #idxs=192
[19] prune_out_channels on _ElementWiseOp_769(ConstantPadNdBackward0) => prune_out_channels on _ElementWiseOp_768(RollBackward0), #idxs=192
[20] prune_out_channels on _ElementWiseOp_768(RollBackward0) => prune_out_channels on _Reshape_767(), #idxs=192
[21] prune_out_channels on _Reshape_767() => prune_out_channels on _ElementWiseOp_766(PermuteBackward0), #idxs=192
[22] prune_out_channels on _ElementWiseOp_766(PermuteBackward0) => prune_out_channels on _ElementWiseOp_765(CloneBackward0), #idxs=192
[23] prune_out_channels on _ElementWiseOp_765(CloneBackward0) => prune_out_channels on _Reshape_764(), #idxs=192
[24] prune_out_channels on _Reshape_764() => prune_out_channels on _Reshape_763(), #idxs=192
[25] prune_out_channels on _Reshape_763() => prune_out_channels on _Reshape_761(), #idxs=192
[26] prune_out_channels on _Reshape_761() => prune_out_channels on _ElementWiseOp_760(AddmmBackward0), #idxs=192
[27] prune_out_channels on _ElementWiseOp_760(AddmmBackward0) => prune_out_channels on _ElementWiseOp_762(TBackward0), #idxs=192
[28] prune_out_channels on _ElementWiseOp_760(AddmmBackward0) => prune_out_channels on _Reshape_759(), #idxs=192
[29] prune_out_channels on _Reshape_759() => prune_out_channels on _Reshape_758(), #idxs=192
[30] prune_out_channels on _Reshape_758() => prune_out_channels on _ElementWiseOp_757(PermuteBackward0), #idxs=192
[31] prune_out_channels on _ElementWiseOp_757(PermuteBackward0) => prune_out_channels on _ElementWiseOp_756(SelectBackward0), #idxs=192
[32] prune_out_channels on _ElementWiseOp_757(PermuteBackward0) => prune_out_channels on _ElementWiseOp_789(SelectBackward0), #idxs=192
[33] prune_out_channels on _ElementWiseOp_757(PermuteBackward0) => prune_out_channels on _ElementWiseOp_793(SelectBackward0), #idxs=192
[34] prune_out_channels on _ElementWiseOp_793(SelectBackward0) => prune_out_channels on _ElementWiseOp_792(MulBackward0), #idxs=192
[35] prune_out_channels on _ElementWiseOp_792(MulBackward0) => prune_out_channels on _ElementWiseOp_791(ExpandBackward0), #idxs=192
[36] prune_out_channels on _ElementWiseOp_791(ExpandBackward0) => prune_out_channels on _ElementWiseOp_790(CloneBackward0), #idxs=192
[37] prune_out_channels on _ElementWiseOp_790(CloneBackward0) => prune_out_channels on _Reshape_784(), #idxs=192
[38] prune_out_channels on _Reshape_784() => prune_out_channels on _ElementWiseOp_783(BmmBackward0), #idxs=192
[39] prune_out_channels on _ElementWiseOp_783(BmmBackward0) => prune_out_channels on _Reshape_785(), #idxs=192
[40] prune_out_channels on _ElementWiseOp_783(BmmBackward0) => prune_out_channels on _Reshape_777(), #idxs=192
[41] prune_out_channels on _Reshape_777() => prune_out_channels on _ElementWiseOp_776(AddBackward0), #idxs=192
[42] prune_out_channels on _ElementWiseOp_776(AddBackward0) => prune_out_channels on _ElementWiseOp_778(UnsqueezeBackward0), #idxs=192
[43] prune_out_channels on _ElementWiseOp_776(AddBackward0) => prune_out_channels on _Reshape_775(), #idxs=192
[44] prune_out_channels on _Reshape_775() => prune_out_channels on _ElementWiseOp_774(AddBackward0), #idxs=192
[45] prune_out_channels on _ElementWiseOp_774(AddBackward0) => prune_out_channels on _Reshape_773(), #idxs=192
[46] prune_out_channels on _Reshape_773() => prune_out_channels on _ElementWiseOp_772(SoftmaxBackward0), #idxs=192
[47] prune_out_channels on _ElementWiseOp_772(SoftmaxBackward0) => prune_out_channels on _ElementWiseOp_771(ExpandBackward0), #idxs=192
[48] prune_out_channels on _ElementWiseOp_771(ExpandBackward0) => prune_out_channels on _Reshape_752(), #idxs=192
[49] prune_out_channels on _Reshape_752() => prune_out_channels on _ElementWiseOp_751(BmmBackward0), #idxs=192
[50] prune_out_channels on _ElementWiseOp_751(BmmBackward0) => prune_out_channels on _Reshape_753(), #idxs=192
[51] prune_out_channels on _ElementWiseOp_751(BmmBackward0) => prune_out_channels on _Reshape_750(), #idxs=192
[52] prune_out_channels on _Reshape_750() => prune_out_channels on _ElementWiseOp_749(TransposeBackward0), #idxs=192
[53] prune_out_channels on _ElementWiseOp_749(TransposeBackward0) => prune_out_channels on _ElementWiseOp_748(CloneBackward0), #idxs=192
[54] prune_out_channels on _ElementWiseOp_748(CloneBackward0) => prune_in_channels on stages.3.blocks.1.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=768, out_features=2304, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=768, out_features=768, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)), #idxs=192
[55] prune_out_channels on _Reshape_753() => prune_out_channels on _ElementWiseOp_754(CloneBackward0), #idxs=192
[56] prune_out_channels on _ElementWiseOp_754(CloneBackward0) => prune_out_channels on _ElementWiseOp_755(ExpandBackward0), #idxs=192
[57] prune_out_channels on _ElementWiseOp_778(UnsqueezeBackward0) => prune_out_channels on _ElementWiseOp_779(CloneBackward0), #idxs=192
[58] prune_out_channels on _ElementWiseOp_779(CloneBackward0) => prune_out_channels on _ElementWiseOp_780(PermuteBackward0), #idxs=192
[59] prune_out_channels on _ElementWiseOp_780(PermuteBackward0) => prune_out_channels on _Reshape_781(), #idxs=192
[60] prune_out_channels on _Reshape_781() => prune_out_channels on _ElementWiseOp_782(IndexBackward0), #idxs=192
[61] prune_out_channels on _Reshape_785() => prune_out_channels on _ElementWiseOp_786(CloneBackward0), #idxs=192
[62] prune_out_channels on _ElementWiseOp_786(CloneBackward0) => prune_out_channels on _ElementWiseOp_787(ExpandBackward0), #idxs=192
[63] prune_out_channels on _ElementWiseOp_787(ExpandBackward0) => prune_out_channels on _ElementWiseOp_788(TransposeBackward0), #idxs=192
[64] prune_out_channels on _ElementWiseOp_672(AddBackward0) => prune_out_channels on _Reshape_680(), #idxs=192
[65] prune_out_channels on _ElementWiseOp_672(AddBackward0) => prune_out_channels on stages.2.downsample (PatchMerging(
  (adap_padding): AdaptivePadding()
  (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
  (norm): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)
  (reduction): Linear(in_features=1152, out_features=768, bias=False)
)), #idxs=192
[66] prune_out_channels on _ElementWiseOp_672(AddBackward0) => prune_out_channels on stages.3.blocks.0.norm2 (LayerNorm((768,), eps=1e-05, elementwise_affine=True)), #idxs=192
[67] prune_out_channels on stages.3.blocks.0.norm2 (LayerNorm((768,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_678(), #idxs=192
[68] prune_out_channels on _Reshape_678() => prune_out_channels on _ElementWiseOp_677(AddmmBackward0), #idxs=192
[69] prune_out_channels on _ElementWiseOp_677(AddmmBackward0) => prune_out_channels on _ElementWiseOp_679(TBackward0), #idxs=192
[70] prune_out_channels on _ElementWiseOp_677(AddmmBackward0) => prune_in_channels on stages.3.blocks.0.ffn.layers.0.0 (Linear(in_features=768, out_features=3072, bias=True)), #idxs=192
[71] prune_out_channels on stages.2.downsample (PatchMerging(
  (adap_padding): AdaptivePadding()
  (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
  (norm): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)
  (reduction): Linear(in_features=1152, out_features=768, bias=False)
)) => prune_out_channels on stages.3.blocks.0.norm1 (LayerNorm((768,), eps=1e-05, elementwise_affine=True)), #idxs=192
[72] prune_out_channels on stages.3.blocks.0.norm1 (LayerNorm((768,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_718(), #idxs=192
[73] prune_out_channels on _Reshape_718() => prune_out_channels on _ElementWiseOp_717(ConstantPadNdBackward0), #idxs=192
[74] prune_out_channels on _ElementWiseOp_717(ConstantPadNdBackward0) => prune_out_channels on _Reshape_716(), #idxs=192
[75] prune_out_channels on _Reshape_716() => prune_out_channels on _ElementWiseOp_715(PermuteBackward0), #idxs=192
[76] prune_out_channels on _ElementWiseOp_715(PermuteBackward0) => prune_out_channels on _ElementWiseOp_714(CloneBackward0), #idxs=192
[77] prune_out_channels on _ElementWiseOp_714(CloneBackward0) => prune_out_channels on _Reshape_713(), #idxs=192
[78] prune_out_channels on _Reshape_713() => prune_out_channels on _Reshape_712(), #idxs=192
[79] prune_out_channels on _Reshape_712() => prune_out_channels on _Reshape_710(), #idxs=192
[80] prune_out_channels on _Reshape_710() => prune_out_channels on _ElementWiseOp_709(AddmmBackward0), #idxs=192
[81] prune_out_channels on _ElementWiseOp_709(AddmmBackward0) => prune_out_channels on _ElementWiseOp_711(TBackward0), #idxs=192
[82] prune_out_channels on _ElementWiseOp_709(AddmmBackward0) => prune_out_channels on _Reshape_708(), #idxs=192
[83] prune_out_channels on _Reshape_708() => prune_out_channels on _Reshape_707(), #idxs=192
[84] prune_out_channels on _Reshape_707() => prune_out_channels on _ElementWiseOp_706(PermuteBackward0), #idxs=192
[85] prune_out_channels on _ElementWiseOp_706(PermuteBackward0) => prune_out_channels on _ElementWiseOp_705(SelectBackward0), #idxs=192
[86] prune_out_channels on _ElementWiseOp_706(PermuteBackward0) => prune_out_channels on _ElementWiseOp_734(SelectBackward0), #idxs=192
[87] prune_out_channels on _ElementWiseOp_706(PermuteBackward0) => prune_out_channels on _ElementWiseOp_738(SelectBackward0), #idxs=192
[88] prune_out_channels on _ElementWiseOp_738(SelectBackward0) => prune_out_channels on _ElementWiseOp_737(MulBackward0), #idxs=192
[89] prune_out_channels on _ElementWiseOp_737(MulBackward0) => prune_out_channels on _ElementWiseOp_736(ExpandBackward0), #idxs=192
[90] prune_out_channels on _ElementWiseOp_736(ExpandBackward0) => prune_out_channels on _ElementWiseOp_735(CloneBackward0), #idxs=192
[91] prune_out_channels on _ElementWiseOp_735(CloneBackward0) => prune_out_channels on _Reshape_729(), #idxs=192
[92] prune_out_channels on _Reshape_729() => prune_out_channels on _ElementWiseOp_728(BmmBackward0), #idxs=192
[93] prune_out_channels on _ElementWiseOp_728(BmmBackward0) => prune_out_channels on _Reshape_730(), #idxs=192
[94] prune_out_channels on _ElementWiseOp_728(BmmBackward0) => prune_out_channels on _Reshape_722(), #idxs=192
[95] prune_out_channels on _Reshape_722() => prune_out_channels on _ElementWiseOp_721(AddBackward0), #idxs=192
[96] prune_out_channels on _ElementWiseOp_721(AddBackward0) => prune_out_channels on _ElementWiseOp_723(UnsqueezeBackward0), #idxs=192
[97] prune_out_channels on _ElementWiseOp_721(AddBackward0) => prune_out_channels on _ElementWiseOp_720(SoftmaxBackward0), #idxs=192
[98] prune_out_channels on _ElementWiseOp_720(SoftmaxBackward0) => prune_out_channels on _ElementWiseOp_719(ExpandBackward0), #idxs=192
[99] prune_out_channels on _ElementWiseOp_719(ExpandBackward0) => prune_out_channels on _Reshape_701(), #idxs=192
[100] prune_out_channels on _Reshape_701() => prune_out_channels on _ElementWiseOp_700(BmmBackward0), #idxs=192
[101] prune_out_channels on _ElementWiseOp_700(BmmBackward0) => prune_out_channels on _Reshape_702(), #idxs=192
[102] prune_out_channels on _ElementWiseOp_700(BmmBackward0) => prune_out_channels on _Reshape_699(), #idxs=192
[103] prune_out_channels on _Reshape_699() => prune_out_channels on _ElementWiseOp_698(TransposeBackward0), #idxs=192
[104] prune_out_channels on _ElementWiseOp_698(TransposeBackward0) => prune_out_channels on _ElementWiseOp_697(CloneBackward0), #idxs=192
[105] prune_out_channels on _ElementWiseOp_697(CloneBackward0) => prune_in_channels on stages.3.blocks.0.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=768, out_features=2304, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=768, out_features=768, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)), #idxs=192
[106] prune_out_channels on _Reshape_702() => prune_out_channels on _ElementWiseOp_703(CloneBackward0), #idxs=192
[107] prune_out_channels on _ElementWiseOp_703(CloneBackward0) => prune_out_channels on _ElementWiseOp_704(ExpandBackward0), #idxs=192
[108] prune_out_channels on _ElementWiseOp_723(UnsqueezeBackward0) => prune_out_channels on _ElementWiseOp_724(CloneBackward0), #idxs=192
[109] prune_out_channels on _ElementWiseOp_724(CloneBackward0) => prune_out_channels on _ElementWiseOp_725(PermuteBackward0), #idxs=192
[110] prune_out_channels on _ElementWiseOp_725(PermuteBackward0) => prune_out_channels on _Reshape_726(), #idxs=192
[111] prune_out_channels on _Reshape_726() => prune_out_channels on _ElementWiseOp_727(IndexBackward0), #idxs=192
[112] prune_out_channels on _Reshape_730() => prune_out_channels on _ElementWiseOp_731(CloneBackward0), #idxs=192
[113] prune_out_channels on _ElementWiseOp_731(CloneBackward0) => prune_out_channels on _ElementWiseOp_732(ExpandBackward0), #idxs=192
[114] prune_out_channels on _ElementWiseOp_732(ExpandBackward0) => prune_out_channels on _ElementWiseOp_733(TransposeBackward0), #idxs=192
[115] prune_out_channels on _Reshape_680() => prune_out_channels on _ElementWiseOp_689(SliceBackward0), #idxs=192
[116] prune_out_channels on _ElementWiseOp_689(SliceBackward0) => prune_out_channels on _ElementWiseOp_690(SliceBackward0), #idxs=192
[117] prune_out_channels on _ElementWiseOp_690(SliceBackward0) => prune_out_channels on _ElementWiseOp_691(SliceBackward0), #idxs=192
[118] prune_out_channels on _ElementWiseOp_691(SliceBackward0) => prune_out_channels on _Reshape_692(), #idxs=192
[119] prune_out_channels on _Reshape_692() => prune_out_channels on _ElementWiseOp_693(CloneBackward0), #idxs=192
[120] prune_out_channels on _ElementWiseOp_693(CloneBackward0) => prune_out_channels on _ElementWiseOp_694(PermuteBackward0), #idxs=192
[121] prune_out_channels on _ElementWiseOp_694(PermuteBackward0) => prune_out_channels on _Reshape_695(), #idxs=192
[122] prune_out_channels on _Reshape_695() => prune_out_channels on _Reshape_696(), #idxs=192
[123] prune_out_channels on _Reshape_696() => prune_out_channels on stages.3.blocks.0.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=768, out_features=2304, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=768, out_features=768, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)), #idxs=192
[124] prune_out_channels on _Reshape_670() => prune_out_channels on _ElementWiseOp_739(SliceBackward0), #idxs=192
[125] prune_out_channels on _ElementWiseOp_739(SliceBackward0) => prune_out_channels on _ElementWiseOp_740(SliceBackward0), #idxs=192
[126] prune_out_channels on _ElementWiseOp_740(SliceBackward0) => prune_out_channels on _ElementWiseOp_741(SliceBackward0), #idxs=192
[127] prune_out_channels on _ElementWiseOp_741(SliceBackward0) => prune_out_channels on _ElementWiseOp_742(RollBackward0), #idxs=192
[128] prune_out_channels on _ElementWiseOp_742(RollBackward0) => prune_out_channels on _Reshape_743(), #idxs=192
[129] prune_out_channels on _Reshape_743() => prune_out_channels on _ElementWiseOp_744(CloneBackward0), #idxs=192
[130] prune_out_channels on _ElementWiseOp_744(CloneBackward0) => prune_out_channels on _ElementWiseOp_745(PermuteBackward0), #idxs=192
[131] prune_out_channels on _ElementWiseOp_745(PermuteBackward0) => prune_out_channels on _Reshape_746(), #idxs=192
[132] prune_out_channels on _Reshape_746() => prune_out_channels on _Reshape_747(), #idxs=192
[133] prune_out_channels on _Reshape_747() => prune_out_channels on stages.3.blocks.1.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=768, out_features=2304, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=768, out_features=768, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)), #idxs=192
--------------------------------

WindowMSAPruner prune_in_channels() /  192
PatchMergingPruner () prune_out_channels/  768 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
WindowMSAPruner prune_in_channels() /  192
WindowMSAPruner prune_out_channels() /  192
WindowMSAPruner prune_out_channels/ dim =  768
	len indx =  192
WindowMSAPruner - idxs =  [2, 5, 7, 9, 21, 23, 26, 30, 31, 32, 33, 36, 46, 48, 49, 52, 59, 63, 69, 80, 85, 90, 91, 95, 96, 102, 103, 111, 114, 115, 123, 139, 140, 144, 145, 148, 152, 154, 155, 160, 163, 171, 174, 175, 180, 181, 185, 188, 195, 212, 214, 217, 219, 222, 229, 230, 236, 239, 241, 245, 255, 260, 265, 270, 272, 277, 278, 281, 283, 284, 291, 296, 302, 306, 308, 310, 313, 317, 320, 325, 328, 331, 333, 337, 343, 345, 350, 356, 357, 361, 366, 370, 371, 381, 382, 383, 386, 387, 391, 392, 393, 397, 399, 401, 406, 409, 416, 417, 418, 424, 427, 430, 431, 434, 435, 439, 447, 448, 451, 461, 462, 468, 469, 475, 477, 483, 484, 486, 487, 494, 497, 498, 500, 505, 506, 521, 525, 531, 532, 540, 543, 544, 551, 552, 557, 561, 564, 568, 572, 573, 574, 576, 578, 587, 595, 601, 602, 603, 605, 611, 623, 632, 634, 638, 641, 647, 667, 673, 674, 675, 683, 686, 688, 689, 694, 709, 720, 723, 724, 726, 728, 731, 732, 734, 741, 743, 751, 753, 761, 764, 765, 766]
	idxs_repeated =  768
WindowMSAPruner - idxs_repeated =  [2, 5, 7, 9, 21, 23, 26, 30, 31, 32, 33, 36, 46, 48, 49, 52, 59, 63, 69, 80, 85, 90, 91, 95, 96, 102, 103, 111, 114, 115, 123, 139, 140, 144, 145, 148, 152, 154, 155, 160, 163, 171, 174, 175, 180, 181, 185, 188, 195, 212, 214, 217, 219, 222, 229, 230, 236, 239, 241, 245, 255, 260, 265, 270, 272, 277, 278, 281, 283, 284, 291, 296, 302, 306, 308, 310, 313, 317, 320, 325, 328, 331, 333, 337, 343, 345, 350, 356, 357, 361, 366, 370, 371, 381, 382, 383, 386, 387, 391, 392, 393, 397, 399, 401, 406, 409, 416, 417, 418, 424, 427, 430, 431, 434, 435, 439, 447, 448, 451, 461, 462, 468, 469, 475, 477, 483, 484, 486, 487, 494, 497, 498, 500, 505, 506, 521, 525, 531, 532, 540, 543, 544, 551, 552, 557, 561, 564, 568, 572, 573, 574, 576, 578, 587, 595, 601, 602, 603, 605, 611, 623, 632, 634, 638, 641, 647, 667, 673, 674, 675, 683, 686, 688, 689, 694, 709, 720, 723, 724, 726, 728, 731, 732, 734, 741, 743, 751, 753, 761, 764, 765, 766, 770, 773, 775, 777, 789, 791, 794, 798, 799, 800, 801, 804, 814, 816, 817, 820, 827, 831, 837, 848, 853, 858, 859, 863, 864, 870, 871, 879, 882, 883, 891, 907, 908, 912, 913, 916, 920, 922, 923, 928, 931, 939, 942, 943, 948, 949, 953, 956, 963, 980, 982, 985, 987, 990, 997, 998, 1004, 1007, 1009, 1013, 1023, 1028, 1033, 1038, 1040, 1045, 1046, 1049, 1051, 1052, 1059, 1064, 1070, 1074, 1076, 1078, 1081, 1085, 1088, 1093, 1096, 1099, 1101, 1105, 1111, 1113, 1118, 1124, 1125, 1129, 1134, 1138, 1139, 1149, 1150, 1151, 1154, 1155, 1159, 1160, 1161, 1165, 1167, 1169, 1174, 1177, 1184, 1185, 1186, 1192, 1195, 1198, 1199, 1202, 1203, 1207, 1215, 1216, 1219, 1229, 1230, 1236, 1237, 1243, 1245, 1251, 1252, 1254, 1255, 1262, 1265, 1266, 1268, 1273, 1274, 1289, 1293, 1299, 1300, 1308, 1311, 1312, 1319, 1320, 1325, 1329, 1332, 1336, 1340, 1341, 1342, 1344, 1346, 1355, 1363, 1369, 1370, 1371, 1373, 1379, 1391, 1400, 1402, 1406, 1409, 1415, 1435, 1441, 1442, 1443, 1451, 1454, 1456, 1457, 1462, 1477, 1488, 1491, 1492, 1494, 1496, 1499, 1500, 1502, 1509, 1511, 1519, 1521, 1529, 1532, 1533, 1534, 1538, 1541, 1543, 1545, 1557, 1559, 1562, 1566, 1567, 1568, 1569, 1572, 1582, 1584, 1585, 1588, 1595, 1599, 1605, 1616, 1621, 1626, 1627, 1631, 1632, 1638, 1639, 1647, 1650, 1651, 1659, 1675, 1676, 1680, 1681, 1684, 1688, 1690, 1691, 1696, 1699, 1707, 1710, 1711, 1716, 1717, 1721, 1724, 1731, 1748, 1750, 1753, 1755, 1758, 1765, 1766, 1772, 1775, 1777, 1781, 1791, 1796, 1801, 1806, 1808, 1813, 1814, 1817, 1819, 1820, 1827, 1832, 1838, 1842, 1844, 1846, 1849, 1853, 1856, 1861, 1864, 1867, 1869, 1873, 1879, 1881, 1886, 1892, 1893, 1897, 1902, 1906, 1907, 1917, 1918, 1919, 1922, 1923, 1927, 1928, 1929, 1933, 1935, 1937, 1942, 1945, 1952, 1953, 1954, 1960, 1963, 1966, 1967, 1970, 1971, 1975, 1983, 1984, 1987, 1997, 1998, 2004, 2005, 2011, 2013, 2019, 2020, 2022, 2023, 2030, 2033, 2034, 2036, 2041, 2042, 2057, 2061, 2067, 2068, 2076, 2079, 2080, 2087, 2088, 2093, 2097, 2100, 2104, 2108, 2109, 2110, 2112, 2114, 2123, 2131, 2137, 2138, 2139, 2141, 2147, 2159, 2168, 2170, 2174, 2177, 2183, 2203, 2209, 2210, 2211, 2219, 2222, 2224, 2225, 2230, 2245, 2256, 2259, 2260, 2262, 2264, 2267, 2268, 2270, 2277, 2279, 2287, 2289, 2297, 2300, 2301, 2302, 2306, 2309, 2311, 2313, 2325, 2327, 2330, 2334, 2335, 2336, 2337, 2340, 2350, 2352, 2353, 2356, 2363, 2367, 2373, 2384, 2389, 2394, 2395, 2399, 2400, 2406, 2407, 2415, 2418, 2419, 2427, 2443, 2444, 2448, 2449, 2452, 2456, 2458, 2459, 2464, 2467, 2475, 2478, 2479, 2484, 2485, 2489, 2492, 2499, 2516, 2518, 2521, 2523, 2526, 2533, 2534, 2540, 2543, 2545, 2549, 2559, 2564, 2569, 2574, 2576, 2581, 2582, 2585, 2587, 2588, 2595, 2600, 2606, 2610, 2612, 2614, 2617, 2621, 2624, 2629, 2632, 2635, 2637, 2641, 2647, 2649, 2654, 2660, 2661, 2665, 2670, 2674, 2675, 2685, 2686, 2687, 2690, 2691, 2695, 2696, 2697, 2701, 2703, 2705, 2710, 2713, 2720, 2721, 2722, 2728, 2731, 2734, 2735, 2738, 2739, 2743, 2751, 2752, 2755, 2765, 2766, 2772, 2773, 2779, 2781, 2787, 2788, 2790, 2791, 2798, 2801, 2802, 2804, 2809, 2810, 2825, 2829, 2835, 2836, 2844, 2847, 2848, 2855, 2856, 2861, 2865, 2868, 2872, 2876, 2877, 2878, 2880, 2882, 2891, 2899, 2905, 2906, 2907, 2909, 2915, 2927, 2936, 2938, 2942, 2945, 2951, 2971, 2977, 2978, 2979, 2987, 2990, 2992, 2993, 2998, 3013, 3024, 3027, 3028, 3030, 3032, 3035, 3036, 3038, 3045, 3047, 3055, 3057, 3065, 3068, 3069, 3070]
WindowMSAPruner prune_out_channels idxs_repeated =  768
WindowMSAPruner prune_out_channels() /  192
WindowMSAPruner prune_out_channels/ dim =  768
	len indx =  192
WindowMSAPruner - idxs =  [2, 5, 7, 9, 21, 23, 26, 30, 31, 32, 33, 36, 46, 48, 49, 52, 59, 63, 69, 80, 85, 90, 91, 95, 96, 102, 103, 111, 114, 115, 123, 139, 140, 144, 145, 148, 152, 154, 155, 160, 163, 171, 174, 175, 180, 181, 185, 188, 195, 212, 214, 217, 219, 222, 229, 230, 236, 239, 241, 245, 255, 260, 265, 270, 272, 277, 278, 281, 283, 284, 291, 296, 302, 306, 308, 310, 313, 317, 320, 325, 328, 331, 333, 337, 343, 345, 350, 356, 357, 361, 366, 370, 371, 381, 382, 383, 386, 387, 391, 392, 393, 397, 399, 401, 406, 409, 416, 417, 418, 424, 427, 430, 431, 434, 435, 439, 447, 448, 451, 461, 462, 468, 469, 475, 477, 483, 484, 486, 487, 494, 497, 498, 500, 505, 506, 521, 525, 531, 532, 540, 543, 544, 551, 552, 557, 561, 564, 568, 572, 573, 574, 576, 578, 587, 595, 601, 602, 603, 605, 611, 623, 632, 634, 638, 641, 647, 667, 673, 674, 675, 683, 686, 688, 689, 694, 709, 720, 723, 724, 726, 728, 731, 732, 734, 741, 743, 751, 753, 761, 764, 765, 766]
	idxs_repeated =  768
WindowMSAPruner - idxs_repeated =  [2, 5, 7, 9, 21, 23, 26, 30, 31, 32, 33, 36, 46, 48, 49, 52, 59, 63, 69, 80, 85, 90, 91, 95, 96, 102, 103, 111, 114, 115, 123, 139, 140, 144, 145, 148, 152, 154, 155, 160, 163, 171, 174, 175, 180, 181, 185, 188, 195, 212, 214, 217, 219, 222, 229, 230, 236, 239, 241, 245, 255, 260, 265, 270, 272, 277, 278, 281, 283, 284, 291, 296, 302, 306, 308, 310, 313, 317, 320, 325, 328, 331, 333, 337, 343, 345, 350, 356, 357, 361, 366, 370, 371, 381, 382, 383, 386, 387, 391, 392, 393, 397, 399, 401, 406, 409, 416, 417, 418, 424, 427, 430, 431, 434, 435, 439, 447, 448, 451, 461, 462, 468, 469, 475, 477, 483, 484, 486, 487, 494, 497, 498, 500, 505, 506, 521, 525, 531, 532, 540, 543, 544, 551, 552, 557, 561, 564, 568, 572, 573, 574, 576, 578, 587, 595, 601, 602, 603, 605, 611, 623, 632, 634, 638, 641, 647, 667, 673, 674, 675, 683, 686, 688, 689, 694, 709, 720, 723, 724, 726, 728, 731, 732, 734, 741, 743, 751, 753, 761, 764, 765, 766, 770, 773, 775, 777, 789, 791, 794, 798, 799, 800, 801, 804, 814, 816, 817, 820, 827, 831, 837, 848, 853, 858, 859, 863, 864, 870, 871, 879, 882, 883, 891, 907, 908, 912, 913, 916, 920, 922, 923, 928, 931, 939, 942, 943, 948, 949, 953, 956, 963, 980, 982, 985, 987, 990, 997, 998, 1004, 1007, 1009, 1013, 1023, 1028, 1033, 1038, 1040, 1045, 1046, 1049, 1051, 1052, 1059, 1064, 1070, 1074, 1076, 1078, 1081, 1085, 1088, 1093, 1096, 1099, 1101, 1105, 1111, 1113, 1118, 1124, 1125, 1129, 1134, 1138, 1139, 1149, 1150, 1151, 1154, 1155, 1159, 1160, 1161, 1165, 1167, 1169, 1174, 1177, 1184, 1185, 1186, 1192, 1195, 1198, 1199, 1202, 1203, 1207, 1215, 1216, 1219, 1229, 1230, 1236, 1237, 1243, 1245, 1251, 1252, 1254, 1255, 1262, 1265, 1266, 1268, 1273, 1274, 1289, 1293, 1299, 1300, 1308, 1311, 1312, 1319, 1320, 1325, 1329, 1332, 1336, 1340, 1341, 1342, 1344, 1346, 1355, 1363, 1369, 1370, 1371, 1373, 1379, 1391, 1400, 1402, 1406, 1409, 1415, 1435, 1441, 1442, 1443, 1451, 1454, 1456, 1457, 1462, 1477, 1488, 1491, 1492, 1494, 1496, 1499, 1500, 1502, 1509, 1511, 1519, 1521, 1529, 1532, 1533, 1534, 1538, 1541, 1543, 1545, 1557, 1559, 1562, 1566, 1567, 1568, 1569, 1572, 1582, 1584, 1585, 1588, 1595, 1599, 1605, 1616, 1621, 1626, 1627, 1631, 1632, 1638, 1639, 1647, 1650, 1651, 1659, 1675, 1676, 1680, 1681, 1684, 1688, 1690, 1691, 1696, 1699, 1707, 1710, 1711, 1716, 1717, 1721, 1724, 1731, 1748, 1750, 1753, 1755, 1758, 1765, 1766, 1772, 1775, 1777, 1781, 1791, 1796, 1801, 1806, 1808, 1813, 1814, 1817, 1819, 1820, 1827, 1832, 1838, 1842, 1844, 1846, 1849, 1853, 1856, 1861, 1864, 1867, 1869, 1873, 1879, 1881, 1886, 1892, 1893, 1897, 1902, 1906, 1907, 1917, 1918, 1919, 1922, 1923, 1927, 1928, 1929, 1933, 1935, 1937, 1942, 1945, 1952, 1953, 1954, 1960, 1963, 1966, 1967, 1970, 1971, 1975, 1983, 1984, 1987, 1997, 1998, 2004, 2005, 2011, 2013, 2019, 2020, 2022, 2023, 2030, 2033, 2034, 2036, 2041, 2042, 2057, 2061, 2067, 2068, 2076, 2079, 2080, 2087, 2088, 2093, 2097, 2100, 2104, 2108, 2109, 2110, 2112, 2114, 2123, 2131, 2137, 2138, 2139, 2141, 2147, 2159, 2168, 2170, 2174, 2177, 2183, 2203, 2209, 2210, 2211, 2219, 2222, 2224, 2225, 2230, 2245, 2256, 2259, 2260, 2262, 2264, 2267, 2268, 2270, 2277, 2279, 2287, 2289, 2297, 2300, 2301, 2302, 2306, 2309, 2311, 2313, 2325, 2327, 2330, 2334, 2335, 2336, 2337, 2340, 2350, 2352, 2353, 2356, 2363, 2367, 2373, 2384, 2389, 2394, 2395, 2399, 2400, 2406, 2407, 2415, 2418, 2419, 2427, 2443, 2444, 2448, 2449, 2452, 2456, 2458, 2459, 2464, 2467, 2475, 2478, 2479, 2484, 2485, 2489, 2492, 2499, 2516, 2518, 2521, 2523, 2526, 2533, 2534, 2540, 2543, 2545, 2549, 2559, 2564, 2569, 2574, 2576, 2581, 2582, 2585, 2587, 2588, 2595, 2600, 2606, 2610, 2612, 2614, 2617, 2621, 2624, 2629, 2632, 2635, 2637, 2641, 2647, 2649, 2654, 2660, 2661, 2665, 2670, 2674, 2675, 2685, 2686, 2687, 2690, 2691, 2695, 2696, 2697, 2701, 2703, 2705, 2710, 2713, 2720, 2721, 2722, 2728, 2731, 2734, 2735, 2738, 2739, 2743, 2751, 2752, 2755, 2765, 2766, 2772, 2773, 2779, 2781, 2787, 2788, 2790, 2791, 2798, 2801, 2802, 2804, 2809, 2810, 2825, 2829, 2835, 2836, 2844, 2847, 2848, 2855, 2856, 2861, 2865, 2868, 2872, 2876, 2877, 2878, 2880, 2882, 2891, 2899, 2905, 2906, 2907, 2909, 2915, 2927, 2936, 2938, 2942, 2945, 2951, 2971, 2977, 2978, 2979, 2987, 2990, 2992, 2993, 2998, 3013, 3024, 3027, 3028, 3030, 3032, 3035, 3036, 3038, 3045, 3047, 3055, 3057, 3065, 3068, 3069, 3070]
WindowMSAPruner prune_out_channels idxs_repeated =  768
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
	get_all_groups -- Adding layer:  <class 'mmcv.cnn.bricks.wrappers.Linear'>
		prune_local()/  _check_pruning_ratio OK
idxs =  768
prunable_chs =  3072
pruner.step 
--------------------------------
          Pruning Group
--------------------------------
[0] prune_out_channels on stages.3.blocks.1.ffn.layers.0.0 (Linear(in_features=576, out_features=3072, bias=True)) => prune_out_channels on stages.3.blocks.1.ffn.layers.0.0 (Linear(in_features=576, out_features=3072, bias=True)), #idxs=768
[1] prune_out_channels on stages.3.blocks.1.ffn.layers.0.0 (Linear(in_features=576, out_features=3072, bias=True)) => prune_out_channels on _ElementWiseOp_666(GeluBackward0), #idxs=768
[2] prune_out_channels on _ElementWiseOp_666(GeluBackward0) => prune_out_channels on _Reshape_664(), #idxs=768
[3] prune_out_channels on _Reshape_664() => prune_out_channels on _ElementWiseOp_663(AddmmBackward0), #idxs=768
[4] prune_out_channels on _ElementWiseOp_663(AddmmBackward0) => prune_out_channels on _ElementWiseOp_665(TBackward0), #idxs=768
[5] prune_out_channels on _ElementWiseOp_663(AddmmBackward0) => prune_in_channels on stages.3.blocks.1.ffn.layers.1 (Linear(in_features=3072, out_features=576, bias=True)), #idxs=768
--------------------------------

<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
continue 4
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
	get_all_groups -- Adding layer:  <class 'mmcv.cnn.bricks.wrappers.Linear'>
		prune_local()/  _check_pruning_ratio OK
idxs =  768
prunable_chs =  3072
pruner.step 
--------------------------------
          Pruning Group
--------------------------------
[0] prune_out_channels on stages.3.blocks.0.ffn.layers.0.0 (Linear(in_features=576, out_features=3072, bias=True)) => prune_out_channels on stages.3.blocks.0.ffn.layers.0.0 (Linear(in_features=576, out_features=3072, bias=True)), #idxs=768
[1] prune_out_channels on stages.3.blocks.0.ffn.layers.0.0 (Linear(in_features=576, out_features=3072, bias=True)) => prune_out_channels on _ElementWiseOp_676(GeluBackward0), #idxs=768
[2] prune_out_channels on _ElementWiseOp_676(GeluBackward0) => prune_out_channels on _Reshape_674(), #idxs=768
[3] prune_out_channels on _Reshape_674() => prune_out_channels on _ElementWiseOp_673(AddmmBackward0), #idxs=768
[4] prune_out_channels on _ElementWiseOp_673(AddmmBackward0) => prune_out_channels on _ElementWiseOp_675(TBackward0), #idxs=768
[5] prune_out_channels on _ElementWiseOp_673(AddmmBackward0) => prune_in_channels on stages.3.blocks.0.ffn.layers.1 (Linear(in_features=3072, out_features=576, bias=True)), #idxs=768
--------------------------------

<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
continue 4
<class 'torch_pruning.ops._ReshapeOp'> continue 2
PatchMergingPruner () get_out_channels/  576 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
WindowMSAPruner layer.embed_dims =  576
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
WindowMSAPruner layer.embed_dims =  576
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
SwinTransformer(
  (patch_embed): PatchEmbed(
    (adap_padding): AdaptivePadding()
    (projection): Conv2d(3, 72, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((72,), eps=1e-05, elementwise_affine=True)
  )
  (drop_after_pos): Dropout(p=0.0, inplace=False)
  (stages): ModuleList(
    (0): SwinBlockSequence(
      (blocks): ModuleList(
        (0-1): 2 x SwinBlock(
          (norm1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)
          (attn): ShiftWindowMSA(
            (w_msa): WindowMSA(
              (qkv): Linear(in_features=72, out_features=192, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=72, out_features=72, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop): DropPath()
          )
          (norm2): LayerNorm((72,), eps=1e-05, elementwise_affine=True)
          (ffn): FFN(
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=72, out_features=288, bias=True)
                (1): GELU(approximate='none')
                (2): Dropout(p=0.0, inplace=False)
              )
              (1): Linear(in_features=288, out_features=72, bias=True)
              (2): Dropout(p=0.0, inplace=False)
            )
            (dropout_layer): DropPath()
            (gamma2): Identity()
          )
        )
      )
      (downsample): PatchMerging(
        (adap_padding): AdaptivePadding()
        (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
        (norm): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
        (reduction): Linear(in_features=288, out_features=144, bias=False)
      )
    )
    (1): SwinBlockSequence(
      (blocks): ModuleList(
        (0-1): 2 x SwinBlock(
          (norm1): LayerNorm((144,), eps=1e-05, elementwise_affine=True)
          (attn): ShiftWindowMSA(
            (w_msa): WindowMSA(
              (qkv): Linear(in_features=144, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=144, out_features=144, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop): DropPath()
          )
          (norm2): LayerNorm((144,), eps=1e-05, elementwise_affine=True)
          (ffn): FFN(
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=144, out_features=576, bias=True)
                (1): GELU(approximate='none')
                (2): Dropout(p=0.0, inplace=False)
              )
              (1): Linear(in_features=576, out_features=144, bias=True)
              (2): Dropout(p=0.0, inplace=False)
            )
            (dropout_layer): DropPath()
            (gamma2): Identity()
          )
        )
      )
      (downsample): PatchMerging(
        (adap_padding): AdaptivePadding()
        (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
        (norm): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
        (reduction): Linear(in_features=576, out_features=288, bias=False)
      )
    )
    (2): SwinBlockSequence(
      (blocks): ModuleList(
        (0-5): 6 x SwinBlock(
          (norm1): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
          (attn): ShiftWindowMSA(
            (w_msa): WindowMSA(
              (qkv): Linear(in_features=288, out_features=768, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=288, out_features=288, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop): DropPath()
          )
          (norm2): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
          (ffn): FFN(
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=288, out_features=1152, bias=True)
                (1): GELU(approximate='none')
                (2): Dropout(p=0.0, inplace=False)
              )
              (1): Linear(in_features=1152, out_features=288, bias=True)
              (2): Dropout(p=0.0, inplace=False)
            )
            (dropout_layer): DropPath()
            (gamma2): Identity()
          )
        )
      )
      (downsample): PatchMerging(
        (adap_padding): AdaptivePadding()
        (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
        (norm): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)
        (reduction): Linear(in_features=1152, out_features=576, bias=False)
      )
    )
    (3): SwinBlockSequence(
      (blocks): ModuleList(
        (0-1): 2 x SwinBlock(
          (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (attn): ShiftWindowMSA(
            (w_msa): WindowMSA(
              (qkv): Linear(in_features=576, out_features=1536, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=576, out_features=576, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop): DropPath()
          )
          (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (ffn): FFN(
            (layers): Sequential(
              (0): Sequential(
                (0): Linear(in_features=576, out_features=2304, bias=True)
                (1): GELU(approximate='none')
                (2): Dropout(p=0.0, inplace=False)
              )
              (1): Linear(in_features=2304, out_features=576, bias=True)
              (2): Dropout(p=0.0, inplace=False)
            )
            (dropout_layer): DropPath()
            (gamma2): Identity()
          )
        )
      )
    )
  )
  (norm1): LayerNorm((144,), eps=1e-05, elementwise_affine=True)
  (norm2): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
  (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
)
- prev m.qkv.out_features =  192
- prev m.qkv.in_features =  72
 - prev m.num_heads =  3
 - prev m.embed_dims =  96
head_embed_dims =  32
0.1767766952966369 96
 - after m.embed_dims =  96

- prev m.qkv.out_features =  192
- prev m.qkv.in_features =  72
 - prev m.num_heads =  3
 - prev m.embed_dims =  96
head_embed_dims =  32
0.1767766952966369 96
 - after m.embed_dims =  96

- prev m.qkv.out_features =  384
- prev m.qkv.in_features =  144
 - prev m.num_heads =  6
 - prev m.embed_dims =  192
head_embed_dims =  32
0.1767766952966369 192
 - after m.embed_dims =  192

- prev m.qkv.out_features =  384
- prev m.qkv.in_features =  144
 - prev m.num_heads =  6
 - prev m.embed_dims =  192
head_embed_dims =  32
0.1767766952966369 192
 - after m.embed_dims =  192

- prev m.qkv.out_features =  768
- prev m.qkv.in_features =  288
 - prev m.num_heads =  12
 - prev m.embed_dims =  384
head_embed_dims =  32
0.1767766952966369 384
 - after m.embed_dims =  384

- prev m.qkv.out_features =  768
- prev m.qkv.in_features =  288
 - prev m.num_heads =  12
 - prev m.embed_dims =  384
head_embed_dims =  32
0.1767766952966369 384
 - after m.embed_dims =  384

- prev m.qkv.out_features =  768
- prev m.qkv.in_features =  288
 - prev m.num_heads =  12
 - prev m.embed_dims =  384
head_embed_dims =  32
0.1767766952966369 384
 - after m.embed_dims =  384

- prev m.qkv.out_features =  768
- prev m.qkv.in_features =  288
 - prev m.num_heads =  12
 - prev m.embed_dims =  384
head_embed_dims =  32
0.1767766952966369 384
 - after m.embed_dims =  384

- prev m.qkv.out_features =  768
- prev m.qkv.in_features =  288
 - prev m.num_heads =  12
 - prev m.embed_dims =  384
head_embed_dims =  32
0.1767766952966369 384
 - after m.embed_dims =  384

- prev m.qkv.out_features =  768
- prev m.qkv.in_features =  288
 - prev m.num_heads =  12
 - prev m.embed_dims =  384
head_embed_dims =  32
0.1767766952966369 384
 - after m.embed_dims =  384

- prev m.qkv.out_features =  1536
- prev m.qkv.in_features =  576
 - prev m.num_heads =  24
 - prev m.embed_dims =  768
head_embed_dims =  32
0.1767766952966369 768
 - after m.embed_dims =  768

- prev m.qkv.out_features =  1536
- prev m.qkv.in_features =  576
 - prev m.num_heads =  24
 - prev m.embed_dims =  768
head_embed_dims =  32
0.1767766952966369 768
 - after m.embed_dims =  768

0.1767766952966369 96
0.1767766952966369 96
0.1767766952966369 192
0.1767766952966369 192
0.1767766952966369 384
0.1767766952966369 384
0.1767766952966369 384
0.1767766952966369 384
0.1767766952966369 384
0.1767766952966369 384
0.1767766952966369 768
0.1767766952966369 768
-----NEW SwinTransformer FORWARD PASS 



		SwinTransformer input shape:  torch.Size([1, 3, 800, 1333])


------------SwinBlockSequence -  torch.Size([1, 66800, 72])
with_cp =  False
x.requires_grad =  True
ShiftWindowMSA = query view: B, L, C=  1 66800 72
	 hw_shape =  (200, 334)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([1392, 49, 72])
B=1392, N=49, C=72
the qkv mod: 72 192 Linear(in_features=72, out_features=192, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([1392, 49, 216])
NEW C= 72
qkv.shape =  torch.Size([1392, 49, 216])
qkv.flatten.shape =  torch.Size([14732928])
self.embed_dims =  96
+++++ reshaping ....  1392 49 3 self.num_heads= 3 24
1392 49 3 3 24  =  14732928
self.num_heads =  3
C =  72
qkv after reshape  torch.Size([1392, 49, 3, 3, 24])
qkv after permute  torch.Size([3, 1392, 3, 49, 24])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f5c195ac9a0>
--proj input shape =  torch.Size([1392, 49, 72])
--proj output shape =  torch.Size([1392, 49, 72])
------------WindowMSA - x_shape Output torch.Size([1392, 49, 72]) <UnsafeViewBackward0 object at 0x7f5c195ac9a0>
attn_windows.shape before merge =  torch.Size([1392, 49, 72])
x.shape before view =  torch.Size([1, 200, 334, 72])
ShiftWindowMSA = before view B, H * W, C =  1 66800 72
------------ShiftWindowMSA -  <ViewBackward0 object at 0x7f5c195ac9a0>
------------SwinBlock -  <AddBackward0 object at 0x7f5c195ac9a0>
		block output:  torch.Size([1, 66800, 72])
------------SwinBlockSequence -  torch.Size([1, 66800, 72])
with_cp =  False
x.requires_grad =  True
ShiftWindowMSA = query view: B, L, C=  1 66800 72
	 hw_shape =  (200, 334)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([1392, 49, 72])
B=1392, N=49, C=72
the qkv mod: 72 192 Linear(in_features=72, out_features=192, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([1392, 49, 216])
NEW C= 72
qkv.shape =  torch.Size([1392, 49, 216])
qkv.flatten.shape =  torch.Size([14732928])
self.embed_dims =  96
+++++ reshaping ....  1392 49 3 self.num_heads= 3 24
1392 49 3 3 24  =  14732928
self.num_heads =  3
C =  72
qkv after reshape  torch.Size([1392, 49, 3, 3, 24])
qkv after permute  torch.Size([3, 1392, 3, 49, 24])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f5c195ac9a0>
--proj input shape =  torch.Size([1392, 49, 72])
--proj output shape =  torch.Size([1392, 49, 72])
------------WindowMSA - x_shape Output torch.Size([1392, 49, 72]) <UnsafeViewBackward0 object at 0x7f5c195ac9a0>
attn_windows.shape before merge =  torch.Size([1392, 49, 72])
x.shape before view =  torch.Size([1, 200, 334, 72])
ShiftWindowMSA = before view B, H * W, C =  1 66800 72
------------ShiftWindowMSA -  <ViewBackward0 object at 0x7f5c195ac9a0>
------------SwinBlock -  <AddBackward0 object at 0x7f5c195ac9a0>
		block output:  torch.Size([1, 66800, 72])
------------SwinBlockSequence/downsample -  torch.Size([1, 16700, 144]) <UnsafeViewBackward0 object at 0x7f5c1972ff40>
down_hw_shape =  (100, 167)
x.shape =  torch.Size([1, 16700, 144])
hw_shape =  (100, 167)
stage out.shape =  torch.Size([1, 66800, 72])
out_hw_shape =  (200, 334)
stage =  0  / self.out_indices =  (1, 2, 3)
self.num_features =  [72, 144, 288, 576]
------------SwinBlockSequence -  torch.Size([1, 16700, 144])
with_cp =  False
x.requires_grad =  True
ShiftWindowMSA = query view: B, L, C=  1 16700 144
	 hw_shape =  (100, 167)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([360, 49, 144])
B=360, N=49, C=144
the qkv mod: 144 384 Linear(in_features=144, out_features=384, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([360, 49, 432])
NEW C= 144
qkv.shape =  torch.Size([360, 49, 432])
qkv.flatten.shape =  torch.Size([7620480])
self.embed_dims =  192
+++++ reshaping ....  360 49 3 self.num_heads= 6 24
360 49 3 6 24  =  7620480
self.num_heads =  6
C =  144
qkv after reshape  torch.Size([360, 49, 3, 6, 24])
qkv after permute  torch.Size([3, 360, 6, 49, 24])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f5c195ac9a0>
--proj input shape =  torch.Size([360, 49, 144])
--proj output shape =  torch.Size([360, 49, 144])
------------WindowMSA - x_shape Output torch.Size([360, 49, 144]) <UnsafeViewBackward0 object at 0x7f5c195ac9a0>
attn_windows.shape before merge =  torch.Size([360, 49, 144])
x.shape before view =  torch.Size([1, 100, 167, 144])
ShiftWindowMSA = before view B, H * W, C =  1 16700 144
------------ShiftWindowMSA -  <ViewBackward0 object at 0x7f5c195ac9a0>
------------SwinBlock -  <AddBackward0 object at 0x7f5c195ac9a0>
		block output:  torch.Size([1, 16700, 144])
------------SwinBlockSequence -  torch.Size([1, 16700, 144])
with_cp =  False
x.requires_grad =  True
ShiftWindowMSA = query view: B, L, C=  1 16700 144
	 hw_shape =  (100, 167)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([360, 49, 144])
B=360, N=49, C=144
the qkv mod: 144 384 Linear(in_features=144, out_features=384, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([360, 49, 432])
NEW C= 144
qkv.shape =  torch.Size([360, 49, 432])
qkv.flatten.shape =  torch.Size([7620480])
self.embed_dims =  192
+++++ reshaping ....  360 49 3 self.num_heads= 6 24
360 49 3 6 24  =  7620480
self.num_heads =  6
C =  144
qkv after reshape  torch.Size([360, 49, 3, 6, 24])
qkv after permute  torch.Size([3, 360, 6, 49, 24])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f5c195ac9a0>
--proj input shape =  torch.Size([360, 49, 144])
--proj output shape =  torch.Size([360, 49, 144])
------------WindowMSA - x_shape Output torch.Size([360, 49, 144]) <UnsafeViewBackward0 object at 0x7f5c195ac9a0>
attn_windows.shape before merge =  torch.Size([360, 49, 144])
x.shape before view =  torch.Size([1, 100, 167, 144])
ShiftWindowMSA = before view B, H * W, C =  1 16700 144
------------ShiftWindowMSA -  <ViewBackward0 object at 0x7f5c195ac9a0>
------------SwinBlock -  <AddBackward0 object at 0x7f5c195ac9a0>
		block output:  torch.Size([1, 16700, 144])
------------SwinBlockSequence/downsample -  torch.Size([1, 4200, 288]) <UnsafeViewBackward0 object at 0x7f5c1972ff40>
down_hw_shape =  (50, 84)
x.shape =  torch.Size([1, 4200, 288])
hw_shape =  (50, 84)
stage out.shape =  torch.Size([1, 16700, 144])
out_hw_shape =  (100, 167)
stage =  1  / self.out_indices =  (1, 2, 3)
self.num_features =  [72, 144, 288, 576]
stage:  1  --- norm_layer out.shape =  torch.Size([1, 16700, 144])
torch.Size([1, 144, 100, 167])
------------SwinBlockSequence -  torch.Size([1, 4200, 288])
with_cp =  False
x.requires_grad =  True
ShiftWindowMSA = query view: B, L, C=  1 4200 288
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 288])
B=96, N=49, C=288
the qkv mod: 288 768 Linear(in_features=288, out_features=768, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 864])
NEW C= 288
qkv.shape =  torch.Size([96, 49, 864])
qkv.flatten.shape =  torch.Size([4064256])
self.embed_dims =  384
+++++ reshaping ....  96 49 3 self.num_heads= 12 24
96 49 3 12 24  =  4064256
self.num_heads =  12
C =  288
qkv after reshape  torch.Size([96, 49, 3, 12, 24])
qkv after permute  torch.Size([3, 96, 12, 49, 24])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f5c195ac9a0>
--proj input shape =  torch.Size([96, 49, 288])
--proj output shape =  torch.Size([96, 49, 288])
------------WindowMSA - x_shape Output torch.Size([96, 49, 288]) <UnsafeViewBackward0 object at 0x7f5c195ac9a0>
attn_windows.shape before merge =  torch.Size([96, 49, 288])
x.shape before view =  torch.Size([1, 50, 84, 288])
ShiftWindowMSA = before view B, H * W, C =  1 4200 288
------------ShiftWindowMSA -  <ViewBackward0 object at 0x7f5c195ac9a0>
------------SwinBlock -  <AddBackward0 object at 0x7f5c195ac9a0>
		block output:  torch.Size([1, 4200, 288])
------------SwinBlockSequence -  torch.Size([1, 4200, 288])
with_cp =  False
x.requires_grad =  True
ShiftWindowMSA = query view: B, L, C=  1 4200 288
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 288])
B=96, N=49, C=288
the qkv mod: 288 768 Linear(in_features=288, out_features=768, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 864])
NEW C= 288
qkv.shape =  torch.Size([96, 49, 864])
qkv.flatten.shape =  torch.Size([4064256])
self.embed_dims =  384
+++++ reshaping ....  96 49 3 self.num_heads= 12 24
96 49 3 12 24  =  4064256
self.num_heads =  12
C =  288
qkv after reshape  torch.Size([96, 49, 3, 12, 24])
qkv after permute  torch.Size([3, 96, 12, 49, 24])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f5c195ac9a0>
--proj input shape =  torch.Size([96, 49, 288])
--proj output shape =  torch.Size([96, 49, 288])
------------WindowMSA - x_shape Output torch.Size([96, 49, 288]) <UnsafeViewBackward0 object at 0x7f5c195ac9a0>
attn_windows.shape before merge =  torch.Size([96, 49, 288])
x.shape before view =  torch.Size([1, 50, 84, 288])
ShiftWindowMSA = before view B, H * W, C =  1 4200 288
------------ShiftWindowMSA -  <ViewBackward0 object at 0x7f5c195ac9a0>
------------SwinBlock -  <AddBackward0 object at 0x7f5c195ac9a0>
		block output:  torch.Size([1, 4200, 288])
------------SwinBlockSequence -  torch.Size([1, 4200, 288])
with_cp =  False
x.requires_grad =  True
ShiftWindowMSA = query view: B, L, C=  1 4200 288
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 288])
B=96, N=49, C=288
the qkv mod: 288 768 Linear(in_features=288, out_features=768, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 864])
NEW C= 288
qkv.shape =  torch.Size([96, 49, 864])
qkv.flatten.shape =  torch.Size([4064256])
self.embed_dims =  384
+++++ reshaping ....  96 49 3 self.num_heads= 12 24
96 49 3 12 24  =  4064256
self.num_heads =  12
C =  288
qkv after reshape  torch.Size([96, 49, 3, 12, 24])
qkv after permute  torch.Size([3, 96, 12, 49, 24])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f5c195ac9a0>
--proj input shape =  torch.Size([96, 49, 288])
--proj output shape =  torch.Size([96, 49, 288])
------------WindowMSA - x_shape Output torch.Size([96, 49, 288]) <UnsafeViewBackward0 object at 0x7f5c195ac9a0>
attn_windows.shape before merge =  torch.Size([96, 49, 288])
x.shape before view =  torch.Size([1, 50, 84, 288])
ShiftWindowMSA = before view B, H * W, C =  1 4200 288
------------ShiftWindowMSA -  <ViewBackward0 object at 0x7f5c195ac9a0>
------------SwinBlock -  <AddBackward0 object at 0x7f5c195ac9a0>
		block output:  torch.Size([1, 4200, 288])
------------SwinBlockSequence -  torch.Size([1, 4200, 288])
with_cp =  False
x.requires_grad =  True
ShiftWindowMSA = query view: B, L, C=  1 4200 288
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 288])
B=96, N=49, C=288
the qkv mod: 288 768 Linear(in_features=288, out_features=768, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 864])
NEW C= 288
qkv.shape =  torch.Size([96, 49, 864])
qkv.flatten.shape =  torch.Size([4064256])
self.embed_dims =  384
+++++ reshaping ....  96 49 3 self.num_heads= 12 24
96 49 3 12 24  =  4064256
self.num_heads =  12
C =  288
qkv after reshape  torch.Size([96, 49, 3, 12, 24])
qkv after permute  torch.Size([3, 96, 12, 49, 24])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f5c195ac9a0>
--proj input shape =  torch.Size([96, 49, 288])
--proj output shape =  torch.Size([96, 49, 288])
------------WindowMSA - x_shape Output torch.Size([96, 49, 288]) <UnsafeViewBackward0 object at 0x7f5c195ac9a0>
attn_windows.shape before merge =  torch.Size([96, 49, 288])
x.shape before view =  torch.Size([1, 50, 84, 288])
ShiftWindowMSA = before view B, H * W, C =  1 4200 288
------------ShiftWindowMSA -  <ViewBackward0 object at 0x7f5c195ac9a0>
------------SwinBlock -  <AddBackward0 object at 0x7f5c195ac9a0>
		block output:  torch.Size([1, 4200, 288])
------------SwinBlockSequence -  torch.Size([1, 4200, 288])
with_cp =  False
x.requires_grad =  True
ShiftWindowMSA = query view: B, L, C=  1 4200 288
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 288])
B=96, N=49, C=288
the qkv mod: 288 768 Linear(in_features=288, out_features=768, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 864])
NEW C= 288
qkv.shape =  torch.Size([96, 49, 864])
qkv.flatten.shape =  torch.Size([4064256])
self.embed_dims =  384
+++++ reshaping ....  96 49 3 self.num_heads= 12 24
96 49 3 12 24  =  4064256
self.num_heads =  12
C =  288
qkv after reshape  torch.Size([96, 49, 3, 12, 24])
qkv after permute  torch.Size([3, 96, 12, 49, 24])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f5c195ac9a0>
--proj input shape =  torch.Size([96, 49, 288])
--proj output shape =  torch.Size([96, 49, 288])
------------WindowMSA - x_shape Output torch.Size([96, 49, 288]) <UnsafeViewBackward0 object at 0x7f5c195ac9a0>
attn_windows.shape before merge =  torch.Size([96, 49, 288])
x.shape before view =  torch.Size([1, 50, 84, 288])
ShiftWindowMSA = before view B, H * W, C =  1 4200 288
------------ShiftWindowMSA -  <ViewBackward0 object at 0x7f5c195ac9a0>
------------SwinBlock -  <AddBackward0 object at 0x7f5c195ac9a0>
		block output:  torch.Size([1, 4200, 288])
------------SwinBlockSequence -  torch.Size([1, 4200, 288])
with_cp =  False
x.requires_grad =  True
ShiftWindowMSA = query view: B, L, C=  1 4200 288
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 288])
B=96, N=49, C=288
the qkv mod: 288 768 Linear(in_features=288, out_features=768, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 864])
NEW C= 288
qkv.shape =  torch.Size([96, 49, 864])
qkv.flatten.shape =  torch.Size([4064256])
self.embed_dims =  384
+++++ reshaping ....  96 49 3 self.num_heads= 12 24
96 49 3 12 24  =  4064256
self.num_heads =  12
C =  288
qkv after reshape  torch.Size([96, 49, 3, 12, 24])
qkv after permute  torch.Size([3, 96, 12, 49, 24])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f5c195ac9a0>
--proj input shape =  torch.Size([96, 49, 288])
--proj output shape =  torch.Size([96, 49, 288])
------------WindowMSA - x_shape Output torch.Size([96, 49, 288]) <UnsafeViewBackward0 object at 0x7f5c195ac9a0>
attn_windows.shape before merge =  torch.Size([96, 49, 288])
x.shape before view =  torch.Size([1, 50, 84, 288])
ShiftWindowMSA = before view B, H * W, C =  1 4200 288
------------ShiftWindowMSA -  <ViewBackward0 object at 0x7f5c195ac9a0>
------------SwinBlock -  <AddBackward0 object at 0x7f5c195ac9a0>
		block output:  torch.Size([1, 4200, 288])
------------SwinBlockSequence/downsample -  torch.Size([1, 1050, 576]) <UnsafeViewBackward0 object at 0x7f5c1972ff40>
down_hw_shape =  (25, 42)
x.shape =  torch.Size([1, 1050, 576])
hw_shape =  (25, 42)
stage out.shape =  torch.Size([1, 4200, 288])
out_hw_shape =  (50, 84)
stage =  2  / self.out_indices =  (1, 2, 3)
self.num_features =  [72, 144, 288, 576]
stage:  2  --- norm_layer out.shape =  torch.Size([1, 4200, 288])
torch.Size([1, 288, 50, 84])
------------SwinBlockSequence -  torch.Size([1, 1050, 576])
with_cp =  False
x.requires_grad =  True
ShiftWindowMSA = query view: B, L, C=  1 1050 576
	 hw_shape =  (25, 42)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([24, 49, 576])
B=24, N=49, C=576
the qkv mod: 576 1536 Linear(in_features=576, out_features=1536, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([24, 49, 1728])
NEW C= 576
qkv.shape =  torch.Size([24, 49, 1728])
qkv.flatten.shape =  torch.Size([2032128])
self.embed_dims =  768
+++++ reshaping ....  24 49 3 self.num_heads= 24 24
24 49 3 24 24  =  2032128
self.num_heads =  24
C =  576
qkv after reshape  torch.Size([24, 49, 3, 24, 24])
qkv after permute  torch.Size([3, 24, 24, 49, 24])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f5c195ac9a0>
--proj input shape =  torch.Size([24, 49, 576])
--proj output shape =  torch.Size([24, 49, 576])
------------WindowMSA - x_shape Output torch.Size([24, 49, 576]) <UnsafeViewBackward0 object at 0x7f5c195ac9a0>
attn_windows.shape before merge =  torch.Size([24, 49, 576])
x.shape before view =  torch.Size([1, 25, 42, 576])
ShiftWindowMSA = before view B, H * W, C =  1 1050 576
------------ShiftWindowMSA -  <ViewBackward0 object at 0x7f5c195ac9a0>
------------SwinBlock -  <AddBackward0 object at 0x7f5c195ac9a0>
		block output:  torch.Size([1, 1050, 576])
------------SwinBlockSequence -  torch.Size([1, 1050, 576])
with_cp =  False
x.requires_grad =  True
ShiftWindowMSA = query view: B, L, C=  1 1050 576
	 hw_shape =  (25, 42)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([24, 49, 576])
B=24, N=49, C=576
the qkv mod: 576 1536 Linear(in_features=576, out_features=1536, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([24, 49, 1728])
NEW C= 576
qkv.shape =  torch.Size([24, 49, 1728])
qkv.flatten.shape =  torch.Size([2032128])
self.embed_dims =  768
+++++ reshaping ....  24 49 3 self.num_heads= 24 24
24 49 3 24 24  =  2032128
self.num_heads =  24
C =  576
qkv after reshape  torch.Size([24, 49, 3, 24, 24])
qkv after permute  torch.Size([3, 24, 24, 49, 24])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f5c195ac9a0>
--proj input shape =  torch.Size([24, 49, 576])
--proj output shape =  torch.Size([24, 49, 576])
------------WindowMSA - x_shape Output torch.Size([24, 49, 576]) <UnsafeViewBackward0 object at 0x7f5c195ac9a0>
attn_windows.shape before merge =  torch.Size([24, 49, 576])
x.shape before view =  torch.Size([1, 25, 42, 576])
ShiftWindowMSA = before view B, H * W, C =  1 1050 576
------------ShiftWindowMSA -  <ViewBackward0 object at 0x7f5c195ac9a0>
------------SwinBlock -  <AddBackward0 object at 0x7f5c195ac9a0>
		block output:  torch.Size([1, 1050, 576])
x.shape =  torch.Size([1, 1050, 576])
hw_shape =  (25, 42)
stage out.shape =  torch.Size([1, 1050, 576])
out_hw_shape =  (25, 42)
stage =  3  / self.out_indices =  (1, 2, 3)
self.num_features =  [72, 144, 288, 576]
stage:  3  --- norm_layer out.shape =  torch.Size([1, 1050, 576])
torch.Size([1, 576, 25, 42])
------------SwinTransformer -  [<CloneBackward0 object at 0x7f5c195ac940>, <CloneBackward0 object at 0x7f5c195ac9a0>, <CloneBackward0 object at 0x7f5c195ac910>]



Pruned forward_time =  0.5302395820617676
Pruned fps =  1.8859399294779733
>>>>>>>>>>>>>>>>_freeze_stages =  -1
-----NEW SwinTransformer FORWARD PASS 



		SwinTransformer input shape:  torch.Size([1, 3, 800, 1333])


------------SwinBlockSequence -  torch.Size([1, 66800, 72])
with_cp =  False
x.requires_grad =  False
ShiftWindowMSA = query view: B, L, C=  1 66800 72
	 hw_shape =  (200, 334)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([1392, 49, 72])
B=1392, N=49, C=72
the qkv mod: 72 192 Linear(in_features=72, out_features=192, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([1392, 49, 216])
NEW C= 72
qkv.shape =  torch.Size([1392, 49, 216])
qkv.flatten.shape =  torch.Size([14732928])
self.embed_dims =  96
+++++ reshaping ....  1392 49 3 self.num_heads= 3 24
1392 49 3 3 24  =  14732928
self.num_heads =  3
C =  72
qkv after reshape  torch.Size([1392, 49, 3, 3, 24])
qkv after permute  torch.Size([3, 1392, 3, 49, 24])
		qkv.grad_fn >>>>>>  None
--proj input shape =  torch.Size([1392, 49, 72])
--proj output shape =  torch.Size([1392, 49, 72])
------------WindowMSA - x_shape Output torch.Size([1392, 49, 72]) None
attn_windows.shape before merge =  torch.Size([1392, 49, 72])
x.shape before view =  torch.Size([1, 200, 334, 72])
ShiftWindowMSA = before view B, H * W, C =  1 66800 72
------------ShiftWindowMSA -  None
------------SwinBlock -  None
		block output:  torch.Size([1, 66800, 72])
------------SwinBlockSequence -  torch.Size([1, 66800, 72])
with_cp =  False
x.requires_grad =  False
ShiftWindowMSA = query view: B, L, C=  1 66800 72
	 hw_shape =  (200, 334)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([1392, 49, 72])
B=1392, N=49, C=72
the qkv mod: 72 192 Linear(in_features=72, out_features=192, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([1392, 49, 216])
NEW C= 72
qkv.shape =  torch.Size([1392, 49, 216])
qkv.flatten.shape =  torch.Size([14732928])
self.embed_dims =  96
+++++ reshaping ....  1392 49 3 self.num_heads= 3 24
1392 49 3 3 24  =  14732928
self.num_heads =  3
C =  72
qkv after reshape  torch.Size([1392, 49, 3, 3, 24])
qkv after permute  torch.Size([3, 1392, 3, 49, 24])
		qkv.grad_fn >>>>>>  None
--proj input shape =  torch.Size([1392, 49, 72])
--proj output shape =  torch.Size([1392, 49, 72])
------------WindowMSA - x_shape Output torch.Size([1392, 49, 72]) None
attn_windows.shape before merge =  torch.Size([1392, 49, 72])
x.shape before view =  torch.Size([1, 200, 334, 72])
ShiftWindowMSA = before view B, H * W, C =  1 66800 72
------------ShiftWindowMSA -  None
------------SwinBlock -  None
		block output:  torch.Size([1, 66800, 72])
------------SwinBlockSequence/downsample -  torch.Size([1, 16700, 144]) None
down_hw_shape =  (100, 167)
x.shape =  torch.Size([1, 16700, 144])
hw_shape =  (100, 167)
stage out.shape =  torch.Size([1, 66800, 72])
out_hw_shape =  (200, 334)
stage =  0  / self.out_indices =  (1, 2, 3)
self.num_features =  [72, 144, 288, 576]
------------SwinBlockSequence -  torch.Size([1, 16700, 144])
with_cp =  False
x.requires_grad =  False
ShiftWindowMSA = query view: B, L, C=  1 16700 144
	 hw_shape =  (100, 167)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([360, 49, 144])
B=360, N=49, C=144
the qkv mod: 144 384 Linear(in_features=144, out_features=384, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([360, 49, 432])
NEW C= 144
qkv.shape =  torch.Size([360, 49, 432])
qkv.flatten.shape =  torch.Size([7620480])
self.embed_dims =  192
+++++ reshaping ....  360 49 3 self.num_heads= 6 24
360 49 3 6 24  =  7620480
self.num_heads =  6
C =  144
qkv after reshape  torch.Size([360, 49, 3, 6, 24])
qkv after permute  torch.Size([3, 360, 6, 49, 24])
		qkv.grad_fn >>>>>>  None
--proj input shape =  torch.Size([360, 49, 144])
--proj output shape =  torch.Size([360, 49, 144])
------------WindowMSA - x_shape Output torch.Size([360, 49, 144]) None
attn_windows.shape before merge =  torch.Size([360, 49, 144])
x.shape before view =  torch.Size([1, 100, 167, 144])
ShiftWindowMSA = before view B, H * W, C =  1 16700 144
------------ShiftWindowMSA -  None
------------SwinBlock -  None
		block output:  torch.Size([1, 16700, 144])
------------SwinBlockSequence -  torch.Size([1, 16700, 144])
with_cp =  False
x.requires_grad =  False
ShiftWindowMSA = query view: B, L, C=  1 16700 144
	 hw_shape =  (100, 167)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([360, 49, 144])
B=360, N=49, C=144
the qkv mod: 144 384 Linear(in_features=144, out_features=384, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([360, 49, 432])
NEW C= 144
qkv.shape =  torch.Size([360, 49, 432])
qkv.flatten.shape =  torch.Size([7620480])
self.embed_dims =  192
+++++ reshaping ....  360 49 3 self.num_heads= 6 24
360 49 3 6 24  =  7620480
self.num_heads =  6
C =  144
qkv after reshape  torch.Size([360, 49, 3, 6, 24])
qkv after permute  torch.Size([3, 360, 6, 49, 24])
		qkv.grad_fn >>>>>>  None
--proj input shape =  torch.Size([360, 49, 144])
--proj output shape =  torch.Size([360, 49, 144])
------------WindowMSA - x_shape Output torch.Size([360, 49, 144]) None
attn_windows.shape before merge =  torch.Size([360, 49, 144])
x.shape before view =  torch.Size([1, 100, 167, 144])
ShiftWindowMSA = before view B, H * W, C =  1 16700 144
------------ShiftWindowMSA -  None
------------SwinBlock -  None
		block output:  torch.Size([1, 16700, 144])
------------SwinBlockSequence/downsample -  torch.Size([1, 4200, 288]) None
down_hw_shape =  (50, 84)
x.shape =  torch.Size([1, 4200, 288])
hw_shape =  (50, 84)
stage out.shape =  torch.Size([1, 16700, 144])
out_hw_shape =  (100, 167)
stage =  1  / self.out_indices =  (1, 2, 3)
self.num_features =  [72, 144, 288, 576]
stage:  1  --- norm_layer out.shape =  torch.Size([1, 16700, 144])
torch.Size([1, 144, 100, 167])
------------SwinBlockSequence -  torch.Size([1, 4200, 288])
with_cp =  False
x.requires_grad =  False
ShiftWindowMSA = query view: B, L, C=  1 4200 288
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 288])
B=96, N=49, C=288
the qkv mod: 288 768 Linear(in_features=288, out_features=768, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 864])
NEW C= 288
qkv.shape =  torch.Size([96, 49, 864])
qkv.flatten.shape =  torch.Size([4064256])
self.embed_dims =  384
+++++ reshaping ....  96 49 3 self.num_heads= 12 24
96 49 3 12 24  =  4064256
self.num_heads =  12
C =  288
qkv after reshape  torch.Size([96, 49, 3, 12, 24])
qkv after permute  torch.Size([3, 96, 12, 49, 24])
		qkv.grad_fn >>>>>>  None
--proj input shape =  torch.Size([96, 49, 288])
--proj output shape =  torch.Size([96, 49, 288])
------------WindowMSA - x_shape Output torch.Size([96, 49, 288]) None
attn_windows.shape before merge =  torch.Size([96, 49, 288])
x.shape before view =  torch.Size([1, 50, 84, 288])
ShiftWindowMSA = before view B, H * W, C =  1 4200 288
------------ShiftWindowMSA -  None
------------SwinBlock -  None
		block output:  torch.Size([1, 4200, 288])
------------SwinBlockSequence -  torch.Size([1, 4200, 288])
with_cp =  False
x.requires_grad =  False
ShiftWindowMSA = query view: B, L, C=  1 4200 288
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 288])
B=96, N=49, C=288
the qkv mod: 288 768 Linear(in_features=288, out_features=768, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 864])
NEW C= 288
qkv.shape =  torch.Size([96, 49, 864])
qkv.flatten.shape =  torch.Size([4064256])
self.embed_dims =  384
+++++ reshaping ....  96 49 3 self.num_heads= 12 24
96 49 3 12 24  =  4064256
self.num_heads =  12
C =  288
qkv after reshape  torch.Size([96, 49, 3, 12, 24])
qkv after permute  torch.Size([3, 96, 12, 49, 24])
		qkv.grad_fn >>>>>>  None
--proj input shape =  torch.Size([96, 49, 288])
--proj output shape =  torch.Size([96, 49, 288])
------------WindowMSA - x_shape Output torch.Size([96, 49, 288]) None
attn_windows.shape before merge =  torch.Size([96, 49, 288])
x.shape before view =  torch.Size([1, 50, 84, 288])
ShiftWindowMSA = before view B, H * W, C =  1 4200 288
------------ShiftWindowMSA -  None
------------SwinBlock -  None
		block output:  torch.Size([1, 4200, 288])
------------SwinBlockSequence -  torch.Size([1, 4200, 288])
with_cp =  False
x.requires_grad =  False
ShiftWindowMSA = query view: B, L, C=  1 4200 288
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 288])
B=96, N=49, C=288
the qkv mod: 288 768 Linear(in_features=288, out_features=768, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 864])
NEW C= 288
qkv.shape =  torch.Size([96, 49, 864])
qkv.flatten.shape =  torch.Size([4064256])
self.embed_dims =  384
+++++ reshaping ....  96 49 3 self.num_heads= 12 24
96 49 3 12 24  =  4064256
self.num_heads =  12
C =  288
qkv after reshape  torch.Size([96, 49, 3, 12, 24])
qkv after permute  torch.Size([3, 96, 12, 49, 24])
		qkv.grad_fn >>>>>>  None
--proj input shape =  torch.Size([96, 49, 288])
--proj output shape =  torch.Size([96, 49, 288])
------------WindowMSA - x_shape Output torch.Size([96, 49, 288]) None
attn_windows.shape before merge =  torch.Size([96, 49, 288])
x.shape before view =  torch.Size([1, 50, 84, 288])
ShiftWindowMSA = before view B, H * W, C =  1 4200 288
------------ShiftWindowMSA -  None
------------SwinBlock -  None
		block output:  torch.Size([1, 4200, 288])
------------SwinBlockSequence -  torch.Size([1, 4200, 288])
with_cp =  False
x.requires_grad =  False
ShiftWindowMSA = query view: B, L, C=  1 4200 288
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 288])
B=96, N=49, C=288
the qkv mod: 288 768 Linear(in_features=288, out_features=768, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 864])
NEW C= 288
qkv.shape =  torch.Size([96, 49, 864])
qkv.flatten.shape =  torch.Size([4064256])
self.embed_dims =  384
+++++ reshaping ....  96 49 3 self.num_heads= 12 24
96 49 3 12 24  =  4064256
self.num_heads =  12
C =  288
qkv after reshape  torch.Size([96, 49, 3, 12, 24])
qkv after permute  torch.Size([3, 96, 12, 49, 24])
		qkv.grad_fn >>>>>>  None
--proj input shape =  torch.Size([96, 49, 288])
--proj output shape =  torch.Size([96, 49, 288])
------------WindowMSA - x_shape Output torch.Size([96, 49, 288]) None
attn_windows.shape before merge =  torch.Size([96, 49, 288])
x.shape before view =  torch.Size([1, 50, 84, 288])
ShiftWindowMSA = before view B, H * W, C =  1 4200 288
------------ShiftWindowMSA -  None
------------SwinBlock -  None
		block output:  torch.Size([1, 4200, 288])
------------SwinBlockSequence -  torch.Size([1, 4200, 288])
with_cp =  False
x.requires_grad =  False
ShiftWindowMSA = query view: B, L, C=  1 4200 288
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 288])
B=96, N=49, C=288
the qkv mod: 288 768 Linear(in_features=288, out_features=768, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 864])
NEW C= 288
qkv.shape =  torch.Size([96, 49, 864])
qkv.flatten.shape =  torch.Size([4064256])
self.embed_dims =  384
+++++ reshaping ....  96 49 3 self.num_heads= 12 24
96 49 3 12 24  =  4064256
self.num_heads =  12
C =  288
qkv after reshape  torch.Size([96, 49, 3, 12, 24])
qkv after permute  torch.Size([3, 96, 12, 49, 24])
		qkv.grad_fn >>>>>>  None
--proj input shape =  torch.Size([96, 49, 288])
--proj output shape =  torch.Size([96, 49, 288])
------------WindowMSA - x_shape Output torch.Size([96, 49, 288]) None
attn_windows.shape before merge =  torch.Size([96, 49, 288])
x.shape before view =  torch.Size([1, 50, 84, 288])
ShiftWindowMSA = before view B, H * W, C =  1 4200 288
------------ShiftWindowMSA -  None
------------SwinBlock -  None
		block output:  torch.Size([1, 4200, 288])
------------SwinBlockSequence -  torch.Size([1, 4200, 288])
with_cp =  False
x.requires_grad =  False
ShiftWindowMSA = query view: B, L, C=  1 4200 288
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 288])
B=96, N=49, C=288
the qkv mod: 288 768 Linear(in_features=288, out_features=768, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 864])
NEW C= 288
qkv.shape =  torch.Size([96, 49, 864])
qkv.flatten.shape =  torch.Size([4064256])
self.embed_dims =  384
+++++ reshaping ....  96 49 3 self.num_heads= 12 24
96 49 3 12 24  =  4064256
self.num_heads =  12
C =  288
qkv after reshape  torch.Size([96, 49, 3, 12, 24])
qkv after permute  torch.Size([3, 96, 12, 49, 24])
		qkv.grad_fn >>>>>>  None
--proj input shape =  torch.Size([96, 49, 288])
--proj output shape =  torch.Size([96, 49, 288])
------------WindowMSA - x_shape Output torch.Size([96, 49, 288]) None
attn_windows.shape before merge =  torch.Size([96, 49, 288])
x.shape before view =  torch.Size([1, 50, 84, 288])
ShiftWindowMSA = before view B, H * W, C =  1 4200 288
------------ShiftWindowMSA -  None
------------SwinBlock -  None
		block output:  torch.Size([1, 4200, 288])
------------SwinBlockSequence/downsample -  torch.Size([1, 1050, 576]) None
down_hw_shape =  (25, 42)
x.shape =  torch.Size([1, 1050, 576])
hw_shape =  (25, 42)
stage out.shape =  torch.Size([1, 4200, 288])
out_hw_shape =  (50, 84)
stage =  2  / self.out_indices =  (1, 2, 3)
self.num_features =  [72, 144, 288, 576]
stage:  2  --- norm_layer out.shape =  torch.Size([1, 4200, 288])
torch.Size([1, 288, 50, 84])
------------SwinBlockSequence -  torch.Size([1, 1050, 576])
with_cp =  False
x.requires_grad =  False
ShiftWindowMSA = query view: B, L, C=  1 1050 576
	 hw_shape =  (25, 42)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([24, 49, 576])
B=24, N=49, C=576
the qkv mod: 576 1536 Linear(in_features=576, out_features=1536, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([24, 49, 1728])
NEW C= 576
qkv.shape =  torch.Size([24, 49, 1728])
qkv.flatten.shape =  torch.Size([2032128])
self.embed_dims =  768
+++++ reshaping ....  24 49 3 self.num_heads= 24 24
24 49 3 24 24  =  2032128
self.num_heads =  24
C =  576
qkv after reshape  torch.Size([24, 49, 3, 24, 24])
qkv after permute  torch.Size([3, 24, 24, 49, 24])
		qkv.grad_fn >>>>>>  None
--proj input shape =  torch.Size([24, 49, 576])
--proj output shape =  torch.Size([24, 49, 576])
------------WindowMSA - x_shape Output torch.Size([24, 49, 576]) None
attn_windows.shape before merge =  torch.Size([24, 49, 576])
x.shape before view =  torch.Size([1, 25, 42, 576])
ShiftWindowMSA = before view B, H * W, C =  1 1050 576
------------ShiftWindowMSA -  None
------------SwinBlock -  None
		block output:  torch.Size([1, 1050, 576])
------------SwinBlockSequence -  torch.Size([1, 1050, 576])
with_cp =  False
x.requires_grad =  False
ShiftWindowMSA = query view: B, L, C=  1 1050 576
	 hw_shape =  (25, 42)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([24, 49, 576])
B=24, N=49, C=576
the qkv mod: 576 1536 Linear(in_features=576, out_features=1536, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([24, 49, 1728])
NEW C= 576
qkv.shape =  torch.Size([24, 49, 1728])
qkv.flatten.shape =  torch.Size([2032128])
self.embed_dims =  768
+++++ reshaping ....  24 49 3 self.num_heads= 24 24
24 49 3 24 24  =  2032128
self.num_heads =  24
C =  576
qkv after reshape  torch.Size([24, 49, 3, 24, 24])
qkv after permute  torch.Size([3, 24, 24, 49, 24])
		qkv.grad_fn >>>>>>  None
--proj input shape =  torch.Size([24, 49, 576])
--proj output shape =  torch.Size([24, 49, 576])
------------WindowMSA - x_shape Output torch.Size([24, 49, 576]) None
attn_windows.shape before merge =  torch.Size([24, 49, 576])
x.shape before view =  torch.Size([1, 25, 42, 576])
ShiftWindowMSA = before view B, H * W, C =  1 1050 576
------------ShiftWindowMSA -  None
------------SwinBlock -  None
		block output:  torch.Size([1, 1050, 576])
x.shape =  torch.Size([1, 1050, 576])
hw_shape =  (25, 42)
stage out.shape =  torch.Size([1, 1050, 576])
out_hw_shape =  (25, 42)
stage =  3  / self.out_indices =  (1, 2, 3)
self.num_features =  [72, 144, 288, 576]
stage:  3  --- norm_layer out.shape =  torch.Size([1, 1050, 576])
torch.Size([1, 576, 25, 42])
------------SwinTransformer -  [None, None, None]



Base MACs: 52.954591 G, Pruned MACs: 29.900406 G
Base Params: 27.520506 M, Pruned Params: 15.503682 M
