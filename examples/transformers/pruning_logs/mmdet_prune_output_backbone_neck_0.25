backbone ConfigType  =  {'type': 'SwinTransformer', 'embed_dims': 96, 'depths': [2, 2, 6, 2], 'num_heads': [3, 6, 12, 24], 'window_size': 7, 'mlp_ratio': 4, 'qkv_bias': True, 'qk_scale': None, 'drop_rate': 0.0, 'attn_drop_rate': 0.0, 'drop_path_rate': 0.2, 'patch_norm': True, 'out_indices': (1, 2, 3), 'with_cp': True, 'convert_weights': False}
Loads checkpoint by local backend from path: /mnt/disks/ext/exps/mini_coco/grounding_dino_swin-t_finetune_custom_dataset/E1/best_coco_bbox_mAP_epoch_2.pth
>>>>>>>>>>>>>>>>_freeze_stages =  -1
>>>>>>>>>>>>>>>>_freeze_stages =  -1
>>>>>>>>>>>>>>>>_freeze_stages =  -1
-----NEW SwinTransformer FORWARD PASS 



		SwinTransformer input shape:  torch.Size([1, 3, 800, 1333])


------------SwinBlockSequence -  torch.Size([1, 66800, 96])
with_cp =  False
x.requires_grad =  True
ShiftWindowMSA = query view: B, L, C=  1 66800 96
	 hw_shape =  (200, 334)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([1392, 49, 96])
B=1392, N=49, C=96
the qkv mod: 96 288 Linear(in_features=96, out_features=288, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([1392, 49, 288])
NEW C= 96
qkv.shape =  torch.Size([1392, 49, 288])
qkv.flatten.shape =  torch.Size([19643904])
self.embed_dims =  96
+++++ reshaping ....  1392 49 3 self.num_heads= 3 32
1392 49 3 3 32  =  19643904
self.num_heads =  3
C =  96
qkv after reshape  torch.Size([1392, 49, 3, 3, 32])
qkv after permute  torch.Size([3, 1392, 3, 49, 32])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f109575eb20>
--proj input shape =  torch.Size([1392, 49, 96])
--proj output shape =  torch.Size([1392, 49, 96])
------------WindowMSA - x_shape Output torch.Size([1392, 49, 96]) <UnsafeViewBackward0 object at 0x7f109575eb20>
attn_windows.shape before merge =  torch.Size([1392, 49, 96])
x.shape before view =  torch.Size([1, 200, 334, 96])
ShiftWindowMSA = before view B, H * W, C =  1 66800 96
------------ShiftWindowMSA -  <ViewBackward0 object at 0x7f109575eb20>
------------SwinBlock -  <AddBackward0 object at 0x7f109575eb20>
		block output:  torch.Size([1, 66800, 96])
------------SwinBlockSequence -  torch.Size([1, 66800, 96])
with_cp =  False
x.requires_grad =  True
ShiftWindowMSA = query view: B, L, C=  1 66800 96
	 hw_shape =  (200, 334)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([1392, 49, 96])
B=1392, N=49, C=96
the qkv mod: 96 288 Linear(in_features=96, out_features=288, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([1392, 49, 288])
NEW C= 96
qkv.shape =  torch.Size([1392, 49, 288])
qkv.flatten.shape =  torch.Size([19643904])
self.embed_dims =  96
+++++ reshaping ....  1392 49 3 self.num_heads= 3 32
1392 49 3 3 32  =  19643904
self.num_heads =  3
C =  96
qkv after reshape  torch.Size([1392, 49, 3, 3, 32])
qkv after permute  torch.Size([3, 1392, 3, 49, 32])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f109575eb20>
--proj input shape =  torch.Size([1392, 49, 96])
--proj output shape =  torch.Size([1392, 49, 96])
------------WindowMSA - x_shape Output torch.Size([1392, 49, 96]) <UnsafeViewBackward0 object at 0x7f109575eb20>
attn_windows.shape before merge =  torch.Size([1392, 49, 96])
x.shape before view =  torch.Size([1, 200, 334, 96])
ShiftWindowMSA = before view B, H * W, C =  1 66800 96
------------ShiftWindowMSA -  <MulBackward0 object at 0x7f109575eb20>
------------SwinBlock -  <AddBackward0 object at 0x7f109575eb20>
		block output:  torch.Size([1, 66800, 96])
------------SwinBlockSequence/downsample -  torch.Size([1, 16700, 192]) <UnsafeViewBackward0 object at 0x7f1094a34850>
down_hw_shape =  (100, 167)
x.shape =  torch.Size([1, 16700, 192])
hw_shape =  (100, 167)
stage out.shape =  torch.Size([1, 66800, 96])
out_hw_shape =  (200, 334)
stage =  0  / self.out_indices =  (1, 2, 3)
self.num_features =  [96, 192, 384, 768]
------------SwinBlockSequence -  torch.Size([1, 16700, 192])
with_cp =  False
x.requires_grad =  True
ShiftWindowMSA = query view: B, L, C=  1 16700 192
	 hw_shape =  (100, 167)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([360, 49, 192])
B=360, N=49, C=192
the qkv mod: 192 576 Linear(in_features=192, out_features=576, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([360, 49, 576])
NEW C= 192
qkv.shape =  torch.Size([360, 49, 576])
qkv.flatten.shape =  torch.Size([10160640])
self.embed_dims =  192
+++++ reshaping ....  360 49 3 self.num_heads= 6 32
360 49 3 6 32  =  10160640
self.num_heads =  6
C =  192
qkv after reshape  torch.Size([360, 49, 3, 6, 32])
qkv after permute  torch.Size([3, 360, 6, 49, 32])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f109575eb20>
--proj input shape =  torch.Size([360, 49, 192])
--proj output shape =  torch.Size([360, 49, 192])
------------WindowMSA - x_shape Output torch.Size([360, 49, 192]) <UnsafeViewBackward0 object at 0x7f109575eb20>
attn_windows.shape before merge =  torch.Size([360, 49, 192])
x.shape before view =  torch.Size([1, 100, 167, 192])
ShiftWindowMSA = before view B, H * W, C =  1 16700 192
------------ShiftWindowMSA -  <MulBackward0 object at 0x7f109575eb20>
------------SwinBlock -  <AddBackward0 object at 0x7f109575eb20>
		block output:  torch.Size([1, 16700, 192])
------------SwinBlockSequence -  torch.Size([1, 16700, 192])
with_cp =  False
x.requires_grad =  True
ShiftWindowMSA = query view: B, L, C=  1 16700 192
	 hw_shape =  (100, 167)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([360, 49, 192])
B=360, N=49, C=192
the qkv mod: 192 576 Linear(in_features=192, out_features=576, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([360, 49, 576])
NEW C= 192
qkv.shape =  torch.Size([360, 49, 576])
qkv.flatten.shape =  torch.Size([10160640])
self.embed_dims =  192
+++++ reshaping ....  360 49 3 self.num_heads= 6 32
360 49 3 6 32  =  10160640
self.num_heads =  6
C =  192
qkv after reshape  torch.Size([360, 49, 3, 6, 32])
qkv after permute  torch.Size([3, 360, 6, 49, 32])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f109575eb20>
--proj input shape =  torch.Size([360, 49, 192])
--proj output shape =  torch.Size([360, 49, 192])
------------WindowMSA - x_shape Output torch.Size([360, 49, 192]) <UnsafeViewBackward0 object at 0x7f109575eb20>
attn_windows.shape before merge =  torch.Size([360, 49, 192])
x.shape before view =  torch.Size([1, 100, 167, 192])
ShiftWindowMSA = before view B, H * W, C =  1 16700 192
------------ShiftWindowMSA -  <MulBackward0 object at 0x7f109575eb20>
------------SwinBlock -  <AddBackward0 object at 0x7f109575eb20>
		block output:  torch.Size([1, 16700, 192])
------------SwinBlockSequence/downsample -  torch.Size([1, 4200, 384]) <UnsafeViewBackward0 object at 0x7f1094a34850>
down_hw_shape =  (50, 84)
x.shape =  torch.Size([1, 4200, 384])
hw_shape =  (50, 84)
stage out.shape =  torch.Size([1, 16700, 192])
out_hw_shape =  (100, 167)
stage =  1  / self.out_indices =  (1, 2, 3)
self.num_features =  [96, 192, 384, 768]
stage:  1  --- norm_layer out.shape =  torch.Size([1, 16700, 192])
torch.Size([1, 192, 100, 167])
------------SwinBlockSequence -  torch.Size([1, 4200, 384])
with_cp =  False
x.requires_grad =  True
ShiftWindowMSA = query view: B, L, C=  1 4200 384
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 384])
B=96, N=49, C=384
the qkv mod: 384 1152 Linear(in_features=384, out_features=1152, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 1152])
NEW C= 384
qkv.shape =  torch.Size([96, 49, 1152])
qkv.flatten.shape =  torch.Size([5419008])
self.embed_dims =  384
+++++ reshaping ....  96 49 3 self.num_heads= 12 32
96 49 3 12 32  =  5419008
self.num_heads =  12
C =  384
qkv after reshape  torch.Size([96, 49, 3, 12, 32])
qkv after permute  torch.Size([3, 96, 12, 49, 32])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f109575eb20>
--proj input shape =  torch.Size([96, 49, 384])
--proj output shape =  torch.Size([96, 49, 384])
------------WindowMSA - x_shape Output torch.Size([96, 49, 384]) <UnsafeViewBackward0 object at 0x7f109575eb20>
attn_windows.shape before merge =  torch.Size([96, 49, 384])
x.shape before view =  torch.Size([1, 50, 84, 384])
ShiftWindowMSA = before view B, H * W, C =  1 4200 384
------------ShiftWindowMSA -  <MulBackward0 object at 0x7f109575eb20>
------------SwinBlock -  <AddBackward0 object at 0x7f109575eb20>
		block output:  torch.Size([1, 4200, 384])
------------SwinBlockSequence -  torch.Size([1, 4200, 384])
with_cp =  False
x.requires_grad =  True
ShiftWindowMSA = query view: B, L, C=  1 4200 384
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 384])
B=96, N=49, C=384
the qkv mod: 384 1152 Linear(in_features=384, out_features=1152, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 1152])
NEW C= 384
qkv.shape =  torch.Size([96, 49, 1152])
qkv.flatten.shape =  torch.Size([5419008])
self.embed_dims =  384
+++++ reshaping ....  96 49 3 self.num_heads= 12 32
96 49 3 12 32  =  5419008
self.num_heads =  12
C =  384
qkv after reshape  torch.Size([96, 49, 3, 12, 32])
qkv after permute  torch.Size([3, 96, 12, 49, 32])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f109575eb20>
--proj input shape =  torch.Size([96, 49, 384])
--proj output shape =  torch.Size([96, 49, 384])
------------WindowMSA - x_shape Output torch.Size([96, 49, 384]) <UnsafeViewBackward0 object at 0x7f109575eb20>
attn_windows.shape before merge =  torch.Size([96, 49, 384])
x.shape before view =  torch.Size([1, 50, 84, 384])
ShiftWindowMSA = before view B, H * W, C =  1 4200 384
------------ShiftWindowMSA -  <MulBackward0 object at 0x7f109575eb20>
------------SwinBlock -  <AddBackward0 object at 0x7f109575eb20>
		block output:  torch.Size([1, 4200, 384])
------------SwinBlockSequence -  torch.Size([1, 4200, 384])
with_cp =  False
x.requires_grad =  True
ShiftWindowMSA = query view: B, L, C=  1 4200 384
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 384])
B=96, N=49, C=384
the qkv mod: 384 1152 Linear(in_features=384, out_features=1152, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 1152])
NEW C= 384
qkv.shape =  torch.Size([96, 49, 1152])
qkv.flatten.shape =  torch.Size([5419008])
self.embed_dims =  384
+++++ reshaping ....  96 49 3 self.num_heads= 12 32
96 49 3 12 32  =  5419008
self.num_heads =  12
C =  384
qkv after reshape  torch.Size([96, 49, 3, 12, 32])
qkv after permute  torch.Size([3, 96, 12, 49, 32])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f109575eb20>
--proj input shape =  torch.Size([96, 49, 384])
--proj output shape =  torch.Size([96, 49, 384])
------------WindowMSA - x_shape Output torch.Size([96, 49, 384]) <UnsafeViewBackward0 object at 0x7f109575eb20>
attn_windows.shape before merge =  torch.Size([96, 49, 384])
x.shape before view =  torch.Size([1, 50, 84, 384])
ShiftWindowMSA = before view B, H * W, C =  1 4200 384
------------ShiftWindowMSA -  <MulBackward0 object at 0x7f109575eb20>
------------SwinBlock -  <AddBackward0 object at 0x7f109575eb20>
		block output:  torch.Size([1, 4200, 384])
------------SwinBlockSequence -  torch.Size([1, 4200, 384])
with_cp =  False
x.requires_grad =  True
ShiftWindowMSA = query view: B, L, C=  1 4200 384
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 384])
B=96, N=49, C=384
the qkv mod: 384 1152 Linear(in_features=384, out_features=1152, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 1152])
NEW C= 384
qkv.shape =  torch.Size([96, 49, 1152])
qkv.flatten.shape =  torch.Size([5419008])
self.embed_dims =  384
+++++ reshaping ....  96 49 3 self.num_heads= 12 32
96 49 3 12 32  =  5419008
self.num_heads =  12
C =  384
qkv after reshape  torch.Size([96, 49, 3, 12, 32])
qkv after permute  torch.Size([3, 96, 12, 49, 32])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f109575eb20>
--proj input shape =  torch.Size([96, 49, 384])
--proj output shape =  torch.Size([96, 49, 384])
------------WindowMSA - x_shape Output torch.Size([96, 49, 384]) <UnsafeViewBackward0 object at 0x7f109575eb20>
attn_windows.shape before merge =  torch.Size([96, 49, 384])
x.shape before view =  torch.Size([1, 50, 84, 384])
ShiftWindowMSA = before view B, H * W, C =  1 4200 384
------------ShiftWindowMSA -  <MulBackward0 object at 0x7f109575eb20>
------------SwinBlock -  <AddBackward0 object at 0x7f109575eb20>
		block output:  torch.Size([1, 4200, 384])
------------SwinBlockSequence -  torch.Size([1, 4200, 384])
with_cp =  False
x.requires_grad =  True
ShiftWindowMSA = query view: B, L, C=  1 4200 384
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 384])
B=96, N=49, C=384
the qkv mod: 384 1152 Linear(in_features=384, out_features=1152, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 1152])
NEW C= 384
qkv.shape =  torch.Size([96, 49, 1152])
qkv.flatten.shape =  torch.Size([5419008])
self.embed_dims =  384
+++++ reshaping ....  96 49 3 self.num_heads= 12 32
96 49 3 12 32  =  5419008
self.num_heads =  12
C =  384
qkv after reshape  torch.Size([96, 49, 3, 12, 32])
qkv after permute  torch.Size([3, 96, 12, 49, 32])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f109575eb20>
--proj input shape =  torch.Size([96, 49, 384])
--proj output shape =  torch.Size([96, 49, 384])
------------WindowMSA - x_shape Output torch.Size([96, 49, 384]) <UnsafeViewBackward0 object at 0x7f109575eb20>
attn_windows.shape before merge =  torch.Size([96, 49, 384])
x.shape before view =  torch.Size([1, 50, 84, 384])
ShiftWindowMSA = before view B, H * W, C =  1 4200 384
------------ShiftWindowMSA -  <MulBackward0 object at 0x7f109575eb20>
------------SwinBlock -  <AddBackward0 object at 0x7f109575eb20>
		block output:  torch.Size([1, 4200, 384])
------------SwinBlockSequence -  torch.Size([1, 4200, 384])
with_cp =  False
x.requires_grad =  True
ShiftWindowMSA = query view: B, L, C=  1 4200 384
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 384])
B=96, N=49, C=384
the qkv mod: 384 1152 Linear(in_features=384, out_features=1152, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 1152])
NEW C= 384
qkv.shape =  torch.Size([96, 49, 1152])
qkv.flatten.shape =  torch.Size([5419008])
self.embed_dims =  384
+++++ reshaping ....  96 49 3 self.num_heads= 12 32
96 49 3 12 32  =  5419008
self.num_heads =  12
C =  384
qkv after reshape  torch.Size([96, 49, 3, 12, 32])
qkv after permute  torch.Size([3, 96, 12, 49, 32])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f109575eb20>
--proj input shape =  torch.Size([96, 49, 384])
--proj output shape =  torch.Size([96, 49, 384])
------------WindowMSA - x_shape Output torch.Size([96, 49, 384]) <UnsafeViewBackward0 object at 0x7f109575eb20>
attn_windows.shape before merge =  torch.Size([96, 49, 384])
x.shape before view =  torch.Size([1, 50, 84, 384])
ShiftWindowMSA = before view B, H * W, C =  1 4200 384
------------ShiftWindowMSA -  <MulBackward0 object at 0x7f109575eb20>
------------SwinBlock -  <AddBackward0 object at 0x7f109575eb20>
		block output:  torch.Size([1, 4200, 384])
------------SwinBlockSequence/downsample -  torch.Size([1, 1050, 768]) <UnsafeViewBackward0 object at 0x7f1094a34850>
down_hw_shape =  (25, 42)
x.shape =  torch.Size([1, 1050, 768])
hw_shape =  (25, 42)
stage out.shape =  torch.Size([1, 4200, 384])
out_hw_shape =  (50, 84)
stage =  2  / self.out_indices =  (1, 2, 3)
self.num_features =  [96, 192, 384, 768]
stage:  2  --- norm_layer out.shape =  torch.Size([1, 4200, 384])
torch.Size([1, 384, 50, 84])
------------SwinBlockSequence -  torch.Size([1, 1050, 768])
with_cp =  False
x.requires_grad =  True
ShiftWindowMSA = query view: B, L, C=  1 1050 768
	 hw_shape =  (25, 42)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([24, 49, 768])
B=24, N=49, C=768
the qkv mod: 768 2304 Linear(in_features=768, out_features=2304, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([24, 49, 2304])
NEW C= 768
qkv.shape =  torch.Size([24, 49, 2304])
qkv.flatten.shape =  torch.Size([2709504])
self.embed_dims =  768
+++++ reshaping ....  24 49 3 self.num_heads= 24 32
24 49 3 24 32  =  2709504
self.num_heads =  24
C =  768
qkv after reshape  torch.Size([24, 49, 3, 24, 32])
qkv after permute  torch.Size([3, 24, 24, 49, 32])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f109575eb20>
--proj input shape =  torch.Size([24, 49, 768])
--proj output shape =  torch.Size([24, 49, 768])
------------WindowMSA - x_shape Output torch.Size([24, 49, 768]) <UnsafeViewBackward0 object at 0x7f109575eb20>
attn_windows.shape before merge =  torch.Size([24, 49, 768])
x.shape before view =  torch.Size([1, 25, 42, 768])
ShiftWindowMSA = before view B, H * W, C =  1 1050 768
------------ShiftWindowMSA -  <MulBackward0 object at 0x7f109575eb20>
------------SwinBlock -  <AddBackward0 object at 0x7f109575eb20>
		block output:  torch.Size([1, 1050, 768])
------------SwinBlockSequence -  torch.Size([1, 1050, 768])
with_cp =  False
x.requires_grad =  True
ShiftWindowMSA = query view: B, L, C=  1 1050 768
	 hw_shape =  (25, 42)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([24, 49, 768])
B=24, N=49, C=768
the qkv mod: 768 2304 Linear(in_features=768, out_features=2304, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([24, 49, 2304])
NEW C= 768
qkv.shape =  torch.Size([24, 49, 2304])
qkv.flatten.shape =  torch.Size([2709504])
self.embed_dims =  768
+++++ reshaping ....  24 49 3 self.num_heads= 24 32
24 49 3 24 32  =  2709504
self.num_heads =  24
C =  768
qkv after reshape  torch.Size([24, 49, 3, 24, 32])
qkv after permute  torch.Size([3, 24, 24, 49, 32])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f109575eb20>
--proj input shape =  torch.Size([24, 49, 768])
--proj output shape =  torch.Size([24, 49, 768])
------------WindowMSA - x_shape Output torch.Size([24, 49, 768]) <UnsafeViewBackward0 object at 0x7f109575eb20>
attn_windows.shape before merge =  torch.Size([24, 49, 768])
x.shape before view =  torch.Size([1, 25, 42, 768])
ShiftWindowMSA = before view B, H * W, C =  1 1050 768
------------ShiftWindowMSA -  <MulBackward0 object at 0x7f109575eb20>
------------SwinBlock -  <AddBackward0 object at 0x7f109575eb20>
		block output:  torch.Size([1, 1050, 768])
x.shape =  torch.Size([1, 1050, 768])
hw_shape =  (25, 42)
stage out.shape =  torch.Size([1, 1050, 768])
out_hw_shape =  (25, 42)
stage =  3  / self.out_indices =  (1, 2, 3)
self.num_features =  [96, 192, 384, 768]
stage:  3  --- norm_layer out.shape =  torch.Size([1, 1050, 768])
torch.Size([1, 768, 25, 42])
------------SwinTransformer -  [<CloneBackward0 object at 0x7f109575ec40>, <CloneBackward0 object at 0x7f109575eb20>, <CloneBackward0 object at 0x7f1094980400>]



[torch.Size([1, 192, 100, 167]), torch.Size([1, 384, 50, 84]), torch.Size([1, 768, 25, 42])]
Base forward_time =  0.7322895526885986
Base fps =  1.3655800445718547
Sequential(
  (backbone): SwinTransformer(
    (patch_embed): PatchEmbed(
      (adap_padding): AdaptivePadding()
      (projection): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
      (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
    )
    (drop_after_pos): Dropout(p=0.0, inplace=False)
    (stages): ModuleList(
      (0): SwinBlockSequence(
        (blocks): ModuleList(
          (0-1): 2 x SwinBlock(
            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=96, out_features=288, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=96, out_features=96, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=96, out_features=384, bias=True)
                  (1): GELU(approximate='none')
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=384, out_features=96, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
              (gamma2): Identity()
            )
          )
        )
        (downsample): PatchMerging(
          (adap_padding): AdaptivePadding()
          (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
          (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (reduction): Linear(in_features=384, out_features=192, bias=False)
        )
      )
      (1): SwinBlockSequence(
        (blocks): ModuleList(
          (0-1): 2 x SwinBlock(
            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=192, out_features=576, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=192, out_features=192, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=192, out_features=768, bias=True)
                  (1): GELU(approximate='none')
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=768, out_features=192, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
              (gamma2): Identity()
            )
          )
        )
        (downsample): PatchMerging(
          (adap_padding): AdaptivePadding()
          (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (reduction): Linear(in_features=768, out_features=384, bias=False)
        )
      )
      (2): SwinBlockSequence(
        (blocks): ModuleList(
          (0-5): 6 x SwinBlock(
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=384, out_features=1536, bias=True)
                  (1): GELU(approximate='none')
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=1536, out_features=384, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
              (gamma2): Identity()
            )
          )
        )
        (downsample): PatchMerging(
          (adap_padding): AdaptivePadding()
          (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
          (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
          (reduction): Linear(in_features=1536, out_features=768, bias=False)
        )
      )
      (3): SwinBlockSequence(
        (blocks): ModuleList(
          (0-1): 2 x SwinBlock(
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=768, out_features=3072, bias=True)
                  (1): GELU(approximate='none')
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=3072, out_features=768, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
              (gamma2): Identity()
            )
          )
        )
      )
    )
    (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (neck): ChannelMapper(
    (convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1))
        (gn): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (1): ConvModule(
        (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))
        (gn): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (2): ConvModule(
        (conv): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))
        (gn): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (extra_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(768, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (gn): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
  )
  init_cfg={'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
)
>>>>>>>>>>>>>>>>_freeze_stages =  -1
-----NEW SwinTransformer FORWARD PASS 



		SwinTransformer input shape:  torch.Size([1, 3, 800, 1333])


------------SwinBlockSequence -  torch.Size([1, 66800, 96])
with_cp =  False
x.requires_grad =  False
ShiftWindowMSA = query view: B, L, C=  1 66800 96
	 hw_shape =  (200, 334)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([1392, 49, 96])
B=1392, N=49, C=96
the qkv mod: 96 288 Linear(in_features=96, out_features=288, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([1392, 49, 288])
NEW C= 96
qkv.shape =  torch.Size([1392, 49, 288])
qkv.flatten.shape =  torch.Size([19643904])
self.embed_dims =  96
+++++ reshaping ....  1392 49 3 self.num_heads= 3 32
1392 49 3 3 32  =  19643904
self.num_heads =  3
C =  96
qkv after reshape  torch.Size([1392, 49, 3, 3, 32])
qkv after permute  torch.Size([3, 1392, 3, 49, 32])
		qkv.grad_fn >>>>>>  None
--proj input shape =  torch.Size([1392, 49, 96])
--proj output shape =  torch.Size([1392, 49, 96])
------------WindowMSA - x_shape Output torch.Size([1392, 49, 96]) None
attn_windows.shape before merge =  torch.Size([1392, 49, 96])
x.shape before view =  torch.Size([1, 200, 334, 96])
ShiftWindowMSA = before view B, H * W, C =  1 66800 96
------------ShiftWindowMSA -  None
------------SwinBlock -  None
		block output:  torch.Size([1, 66800, 96])
------------SwinBlockSequence -  torch.Size([1, 66800, 96])
with_cp =  False
x.requires_grad =  False
ShiftWindowMSA = query view: B, L, C=  1 66800 96
	 hw_shape =  (200, 334)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([1392, 49, 96])
B=1392, N=49, C=96
the qkv mod: 96 288 Linear(in_features=96, out_features=288, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([1392, 49, 288])
NEW C= 96
qkv.shape =  torch.Size([1392, 49, 288])
qkv.flatten.shape =  torch.Size([19643904])
self.embed_dims =  96
+++++ reshaping ....  1392 49 3 self.num_heads= 3 32
1392 49 3 3 32  =  19643904
self.num_heads =  3
C =  96
qkv after reshape  torch.Size([1392, 49, 3, 3, 32])
qkv after permute  torch.Size([3, 1392, 3, 49, 32])
		qkv.grad_fn >>>>>>  None
--proj input shape =  torch.Size([1392, 49, 96])
--proj output shape =  torch.Size([1392, 49, 96])
------------WindowMSA - x_shape Output torch.Size([1392, 49, 96]) None
attn_windows.shape before merge =  torch.Size([1392, 49, 96])
x.shape before view =  torch.Size([1, 200, 334, 96])
ShiftWindowMSA = before view B, H * W, C =  1 66800 96
------------ShiftWindowMSA -  None
------------SwinBlock -  None
		block output:  torch.Size([1, 66800, 96])
------------SwinBlockSequence/downsample -  torch.Size([1, 16700, 192]) None
down_hw_shape =  (100, 167)
x.shape =  torch.Size([1, 16700, 192])
hw_shape =  (100, 167)
stage out.shape =  torch.Size([1, 66800, 96])
out_hw_shape =  (200, 334)
stage =  0  / self.out_indices =  (1, 2, 3)
self.num_features =  [96, 192, 384, 768]
------------SwinBlockSequence -  torch.Size([1, 16700, 192])
with_cp =  False
x.requires_grad =  False
ShiftWindowMSA = query view: B, L, C=  1 16700 192
	 hw_shape =  (100, 167)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([360, 49, 192])
B=360, N=49, C=192
the qkv mod: 192 576 Linear(in_features=192, out_features=576, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([360, 49, 576])
NEW C= 192
qkv.shape =  torch.Size([360, 49, 576])
qkv.flatten.shape =  torch.Size([10160640])
self.embed_dims =  192
+++++ reshaping ....  360 49 3 self.num_heads= 6 32
360 49 3 6 32  =  10160640
self.num_heads =  6
C =  192
qkv after reshape  torch.Size([360, 49, 3, 6, 32])
qkv after permute  torch.Size([3, 360, 6, 49, 32])
		qkv.grad_fn >>>>>>  None
--proj input shape =  torch.Size([360, 49, 192])
--proj output shape =  torch.Size([360, 49, 192])
------------WindowMSA - x_shape Output torch.Size([360, 49, 192]) None
attn_windows.shape before merge =  torch.Size([360, 49, 192])
x.shape before view =  torch.Size([1, 100, 167, 192])
ShiftWindowMSA = before view B, H * W, C =  1 16700 192
------------ShiftWindowMSA -  None
------------SwinBlock -  None
		block output:  torch.Size([1, 16700, 192])
------------SwinBlockSequence -  torch.Size([1, 16700, 192])
with_cp =  False
x.requires_grad =  False
ShiftWindowMSA = query view: B, L, C=  1 16700 192
	 hw_shape =  (100, 167)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([360, 49, 192])
B=360, N=49, C=192
the qkv mod: 192 576 Linear(in_features=192, out_features=576, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([360, 49, 576])
NEW C= 192
qkv.shape =  torch.Size([360, 49, 576])
qkv.flatten.shape =  torch.Size([10160640])
self.embed_dims =  192
+++++ reshaping ....  360 49 3 self.num_heads= 6 32
360 49 3 6 32  =  10160640
self.num_heads =  6
C =  192
qkv after reshape  torch.Size([360, 49, 3, 6, 32])
qkv after permute  torch.Size([3, 360, 6, 49, 32])
		qkv.grad_fn >>>>>>  None
--proj input shape =  torch.Size([360, 49, 192])
--proj output shape =  torch.Size([360, 49, 192])
------------WindowMSA - x_shape Output torch.Size([360, 49, 192]) None
attn_windows.shape before merge =  torch.Size([360, 49, 192])
x.shape before view =  torch.Size([1, 100, 167, 192])
ShiftWindowMSA = before view B, H * W, C =  1 16700 192
------------ShiftWindowMSA -  None
------------SwinBlock -  None
		block output:  torch.Size([1, 16700, 192])
------------SwinBlockSequence/downsample -  torch.Size([1, 4200, 384]) None
down_hw_shape =  (50, 84)
x.shape =  torch.Size([1, 4200, 384])
hw_shape =  (50, 84)
stage out.shape =  torch.Size([1, 16700, 192])
out_hw_shape =  (100, 167)
stage =  1  / self.out_indices =  (1, 2, 3)
self.num_features =  [96, 192, 384, 768]
stage:  1  --- norm_layer out.shape =  torch.Size([1, 16700, 192])
torch.Size([1, 192, 100, 167])
------------SwinBlockSequence -  torch.Size([1, 4200, 384])
with_cp =  False
x.requires_grad =  False
ShiftWindowMSA = query view: B, L, C=  1 4200 384
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 384])
B=96, N=49, C=384
the qkv mod: 384 1152 Linear(in_features=384, out_features=1152, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 1152])
NEW C= 384
qkv.shape =  torch.Size([96, 49, 1152])
qkv.flatten.shape =  torch.Size([5419008])
self.embed_dims =  384
+++++ reshaping ....  96 49 3 self.num_heads= 12 32
96 49 3 12 32  =  5419008
self.num_heads =  12
C =  384
qkv after reshape  torch.Size([96, 49, 3, 12, 32])
qkv after permute  torch.Size([3, 96, 12, 49, 32])
		qkv.grad_fn >>>>>>  None
--proj input shape =  torch.Size([96, 49, 384])
--proj output shape =  torch.Size([96, 49, 384])
------------WindowMSA - x_shape Output torch.Size([96, 49, 384]) None
attn_windows.shape before merge =  torch.Size([96, 49, 384])
x.shape before view =  torch.Size([1, 50, 84, 384])
ShiftWindowMSA = before view B, H * W, C =  1 4200 384
------------ShiftWindowMSA -  None
------------SwinBlock -  None
		block output:  torch.Size([1, 4200, 384])
------------SwinBlockSequence -  torch.Size([1, 4200, 384])
with_cp =  False
x.requires_grad =  False
ShiftWindowMSA = query view: B, L, C=  1 4200 384
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 384])
B=96, N=49, C=384
the qkv mod: 384 1152 Linear(in_features=384, out_features=1152, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 1152])
NEW C= 384
qkv.shape =  torch.Size([96, 49, 1152])
qkv.flatten.shape =  torch.Size([5419008])
self.embed_dims =  384
+++++ reshaping ....  96 49 3 self.num_heads= 12 32
96 49 3 12 32  =  5419008
self.num_heads =  12
C =  384
qkv after reshape  torch.Size([96, 49, 3, 12, 32])
qkv after permute  torch.Size([3, 96, 12, 49, 32])
		qkv.grad_fn >>>>>>  None
--proj input shape =  torch.Size([96, 49, 384])
--proj output shape =  torch.Size([96, 49, 384])
------------WindowMSA - x_shape Output torch.Size([96, 49, 384]) None
attn_windows.shape before merge =  torch.Size([96, 49, 384])
x.shape before view =  torch.Size([1, 50, 84, 384])
ShiftWindowMSA = before view B, H * W, C =  1 4200 384
------------ShiftWindowMSA -  None
------------SwinBlock -  None
		block output:  torch.Size([1, 4200, 384])
------------SwinBlockSequence -  torch.Size([1, 4200, 384])
with_cp =  False
x.requires_grad =  False
ShiftWindowMSA = query view: B, L, C=  1 4200 384
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 384])
B=96, N=49, C=384
the qkv mod: 384 1152 Linear(in_features=384, out_features=1152, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 1152])
NEW C= 384
qkv.shape =  torch.Size([96, 49, 1152])
qkv.flatten.shape =  torch.Size([5419008])
self.embed_dims =  384
+++++ reshaping ....  96 49 3 self.num_heads= 12 32
96 49 3 12 32  =  5419008
self.num_heads =  12
C =  384
qkv after reshape  torch.Size([96, 49, 3, 12, 32])
qkv after permute  torch.Size([3, 96, 12, 49, 32])
		qkv.grad_fn >>>>>>  None
--proj input shape =  torch.Size([96, 49, 384])
--proj output shape =  torch.Size([96, 49, 384])
------------WindowMSA - x_shape Output torch.Size([96, 49, 384]) None
attn_windows.shape before merge =  torch.Size([96, 49, 384])
x.shape before view =  torch.Size([1, 50, 84, 384])
ShiftWindowMSA = before view B, H * W, C =  1 4200 384
------------ShiftWindowMSA -  None
------------SwinBlock -  None
		block output:  torch.Size([1, 4200, 384])
------------SwinBlockSequence -  torch.Size([1, 4200, 384])
with_cp =  False
x.requires_grad =  False
ShiftWindowMSA = query view: B, L, C=  1 4200 384
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 384])
B=96, N=49, C=384
the qkv mod: 384 1152 Linear(in_features=384, out_features=1152, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 1152])
NEW C= 384
qkv.shape =  torch.Size([96, 49, 1152])
qkv.flatten.shape =  torch.Size([5419008])
self.embed_dims =  384
+++++ reshaping ....  96 49 3 self.num_heads= 12 32
96 49 3 12 32  =  5419008
self.num_heads =  12
C =  384
qkv after reshape  torch.Size([96, 49, 3, 12, 32])
qkv after permute  torch.Size([3, 96, 12, 49, 32])
		qkv.grad_fn >>>>>>  None
--proj input shape =  torch.Size([96, 49, 384])
--proj output shape =  torch.Size([96, 49, 384])
------------WindowMSA - x_shape Output torch.Size([96, 49, 384]) None
attn_windows.shape before merge =  torch.Size([96, 49, 384])
x.shape before view =  torch.Size([1, 50, 84, 384])
ShiftWindowMSA = before view B, H * W, C =  1 4200 384
------------ShiftWindowMSA -  None
------------SwinBlock -  None
		block output:  torch.Size([1, 4200, 384])
------------SwinBlockSequence -  torch.Size([1, 4200, 384])
with_cp =  False
x.requires_grad =  False
ShiftWindowMSA = query view: B, L, C=  1 4200 384
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 384])
B=96, N=49, C=384
the qkv mod: 384 1152 Linear(in_features=384, out_features=1152, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 1152])
NEW C= 384
qkv.shape =  torch.Size([96, 49, 1152])
qkv.flatten.shape =  torch.Size([5419008])
self.embed_dims =  384
+++++ reshaping ....  96 49 3 self.num_heads= 12 32
96 49 3 12 32  =  5419008
self.num_heads =  12
C =  384
qkv after reshape  torch.Size([96, 49, 3, 12, 32])
qkv after permute  torch.Size([3, 96, 12, 49, 32])
		qkv.grad_fn >>>>>>  None
--proj input shape =  torch.Size([96, 49, 384])
--proj output shape =  torch.Size([96, 49, 384])
------------WindowMSA - x_shape Output torch.Size([96, 49, 384]) None
attn_windows.shape before merge =  torch.Size([96, 49, 384])
x.shape before view =  torch.Size([1, 50, 84, 384])
ShiftWindowMSA = before view B, H * W, C =  1 4200 384
------------ShiftWindowMSA -  None
------------SwinBlock -  None
		block output:  torch.Size([1, 4200, 384])
------------SwinBlockSequence -  torch.Size([1, 4200, 384])
with_cp =  False
x.requires_grad =  False
ShiftWindowMSA = query view: B, L, C=  1 4200 384
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 384])
B=96, N=49, C=384
the qkv mod: 384 1152 Linear(in_features=384, out_features=1152, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 1152])
NEW C= 384
qkv.shape =  torch.Size([96, 49, 1152])
qkv.flatten.shape =  torch.Size([5419008])
self.embed_dims =  384
+++++ reshaping ....  96 49 3 self.num_heads= 12 32
96 49 3 12 32  =  5419008
self.num_heads =  12
C =  384
qkv after reshape  torch.Size([96, 49, 3, 12, 32])
qkv after permute  torch.Size([3, 96, 12, 49, 32])
		qkv.grad_fn >>>>>>  None
--proj input shape =  torch.Size([96, 49, 384])
--proj output shape =  torch.Size([96, 49, 384])
------------WindowMSA - x_shape Output torch.Size([96, 49, 384]) None
attn_windows.shape before merge =  torch.Size([96, 49, 384])
x.shape before view =  torch.Size([1, 50, 84, 384])
ShiftWindowMSA = before view B, H * W, C =  1 4200 384
------------ShiftWindowMSA -  None
------------SwinBlock -  None
		block output:  torch.Size([1, 4200, 384])
------------SwinBlockSequence/downsample -  torch.Size([1, 1050, 768]) None
down_hw_shape =  (25, 42)
x.shape =  torch.Size([1, 1050, 768])
hw_shape =  (25, 42)
stage out.shape =  torch.Size([1, 4200, 384])
out_hw_shape =  (50, 84)
stage =  2  / self.out_indices =  (1, 2, 3)
self.num_features =  [96, 192, 384, 768]
stage:  2  --- norm_layer out.shape =  torch.Size([1, 4200, 384])
torch.Size([1, 384, 50, 84])
------------SwinBlockSequence -  torch.Size([1, 1050, 768])
with_cp =  False
x.requires_grad =  False
ShiftWindowMSA = query view: B, L, C=  1 1050 768
	 hw_shape =  (25, 42)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([24, 49, 768])
B=24, N=49, C=768
the qkv mod: 768 2304 Linear(in_features=768, out_features=2304, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([24, 49, 2304])
NEW C= 768
qkv.shape =  torch.Size([24, 49, 2304])
qkv.flatten.shape =  torch.Size([2709504])
self.embed_dims =  768
+++++ reshaping ....  24 49 3 self.num_heads= 24 32
24 49 3 24 32  =  2709504
self.num_heads =  24
C =  768
qkv after reshape  torch.Size([24, 49, 3, 24, 32])
qkv after permute  torch.Size([3, 24, 24, 49, 32])
		qkv.grad_fn >>>>>>  None
--proj input shape =  torch.Size([24, 49, 768])
--proj output shape =  torch.Size([24, 49, 768])
------------WindowMSA - x_shape Output torch.Size([24, 49, 768]) None
attn_windows.shape before merge =  torch.Size([24, 49, 768])
x.shape before view =  torch.Size([1, 25, 42, 768])
ShiftWindowMSA = before view B, H * W, C =  1 1050 768
------------ShiftWindowMSA -  None
------------SwinBlock -  None
		block output:  torch.Size([1, 1050, 768])
------------SwinBlockSequence -  torch.Size([1, 1050, 768])
with_cp =  False
x.requires_grad =  False
ShiftWindowMSA = query view: B, L, C=  1 1050 768
	 hw_shape =  (25, 42)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([24, 49, 768])
B=24, N=49, C=768
the qkv mod: 768 2304 Linear(in_features=768, out_features=2304, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([24, 49, 2304])
NEW C= 768
qkv.shape =  torch.Size([24, 49, 2304])
qkv.flatten.shape =  torch.Size([2709504])
self.embed_dims =  768
+++++ reshaping ....  24 49 3 self.num_heads= 24 32
24 49 3 24 32  =  2709504
self.num_heads =  24
C =  768
qkv after reshape  torch.Size([24, 49, 3, 24, 32])
qkv after permute  torch.Size([3, 24, 24, 49, 32])
		qkv.grad_fn >>>>>>  None
--proj input shape =  torch.Size([24, 49, 768])
--proj output shape =  torch.Size([24, 49, 768])
------------WindowMSA - x_shape Output torch.Size([24, 49, 768]) None
attn_windows.shape before merge =  torch.Size([24, 49, 768])
x.shape before view =  torch.Size([1, 25, 42, 768])
ShiftWindowMSA = before view B, H * W, C =  1 1050 768
------------ShiftWindowMSA -  None
------------SwinBlock -  None
		block output:  torch.Size([1, 1050, 768])
x.shape =  torch.Size([1, 1050, 768])
hw_shape =  (25, 42)
stage out.shape =  torch.Size([1, 1050, 768])
out_hw_shape =  (25, 42)
stage =  3  / self.out_indices =  (1, 2, 3)
self.num_features =  [96, 192, 384, 768]
stage:  3  --- norm_layer out.shape =  torch.Size([1, 1050, 768])
torch.Size([1, 768, 25, 42])
------------SwinTransformer -  [None, None, None]



[torch.Size([1, 192, 100, 167]), torch.Size([1, 384, 50, 84]), torch.Size([1, 768, 25, 42])]
Base Macs: 54.894878 M, Base Params: 29.637114 M
m.num_attention_heads ===  3
m.num_attention_heads ===  3
m.num_attention_heads ===  6
m.num_attention_heads ===  6
m.num_attention_heads ===  12
m.num_attention_heads ===  12
m.num_attention_heads ===  12
m.num_attention_heads ===  12
m.num_attention_heads ===  12
m.num_attention_heads ===  12
m.num_attention_heads ===  24
m.num_attention_heads ===  24
			IGNORING:  AdaptivePadding()
			IGNORING:  Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
			IGNORING:  LayerNorm((384,), eps=1e-05, elementwise_affine=True)
			IGNORING:  Linear(in_features=384, out_features=192, bias=False)
			IGNORING:  AdaptivePadding()
			IGNORING:  Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
			IGNORING:  LayerNorm((768,), eps=1e-05, elementwise_affine=True)
			IGNORING:  Linear(in_features=768, out_features=384, bias=False)
			IGNORING:  AdaptivePadding()
			IGNORING:  Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
			IGNORING:  LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
			IGNORING:  Linear(in_features=1536, out_features=768, bias=False)
			IGNORING:  Linear(in_features=96, out_features=288, bias=True)
			IGNORING:  Dropout(p=0.0, inplace=False)
			IGNORING:  Linear(in_features=96, out_features=96, bias=True)
			IGNORING:  Dropout(p=0.0, inplace=False)
			IGNORING:  Softmax(dim=-1)
			IGNORING:  Linear(in_features=96, out_features=288, bias=True)
			IGNORING:  Dropout(p=0.0, inplace=False)
			IGNORING:  Linear(in_features=96, out_features=96, bias=True)
			IGNORING:  Dropout(p=0.0, inplace=False)
			IGNORING:  Softmax(dim=-1)
			IGNORING:  Linear(in_features=192, out_features=576, bias=True)
			IGNORING:  Dropout(p=0.0, inplace=False)
			IGNORING:  Linear(in_features=192, out_features=192, bias=True)
			IGNORING:  Dropout(p=0.0, inplace=False)
			IGNORING:  Softmax(dim=-1)
			IGNORING:  Linear(in_features=192, out_features=576, bias=True)
			IGNORING:  Dropout(p=0.0, inplace=False)
			IGNORING:  Linear(in_features=192, out_features=192, bias=True)
			IGNORING:  Dropout(p=0.0, inplace=False)
			IGNORING:  Softmax(dim=-1)
			IGNORING:  Linear(in_features=384, out_features=1152, bias=True)
			IGNORING:  Dropout(p=0.0, inplace=False)
			IGNORING:  Linear(in_features=384, out_features=384, bias=True)
			IGNORING:  Dropout(p=0.0, inplace=False)
			IGNORING:  Softmax(dim=-1)
			IGNORING:  Linear(in_features=384, out_features=1152, bias=True)
			IGNORING:  Dropout(p=0.0, inplace=False)
			IGNORING:  Linear(in_features=384, out_features=384, bias=True)
			IGNORING:  Dropout(p=0.0, inplace=False)
			IGNORING:  Softmax(dim=-1)
			IGNORING:  Linear(in_features=384, out_features=1152, bias=True)
			IGNORING:  Dropout(p=0.0, inplace=False)
			IGNORING:  Linear(in_features=384, out_features=384, bias=True)
			IGNORING:  Dropout(p=0.0, inplace=False)
			IGNORING:  Softmax(dim=-1)
			IGNORING:  Linear(in_features=384, out_features=1152, bias=True)
			IGNORING:  Dropout(p=0.0, inplace=False)
			IGNORING:  Linear(in_features=384, out_features=384, bias=True)
			IGNORING:  Dropout(p=0.0, inplace=False)
			IGNORING:  Softmax(dim=-1)
			IGNORING:  Linear(in_features=384, out_features=1152, bias=True)
			IGNORING:  Dropout(p=0.0, inplace=False)
			IGNORING:  Linear(in_features=384, out_features=384, bias=True)
			IGNORING:  Dropout(p=0.0, inplace=False)
			IGNORING:  Softmax(dim=-1)
			IGNORING:  Linear(in_features=384, out_features=1152, bias=True)
			IGNORING:  Dropout(p=0.0, inplace=False)
			IGNORING:  Linear(in_features=384, out_features=384, bias=True)
			IGNORING:  Dropout(p=0.0, inplace=False)
			IGNORING:  Softmax(dim=-1)
			IGNORING:  Linear(in_features=768, out_features=2304, bias=True)
			IGNORING:  Dropout(p=0.0, inplace=False)
			IGNORING:  Linear(in_features=768, out_features=768, bias=True)
			IGNORING:  Dropout(p=0.0, inplace=False)
			IGNORING:  Softmax(dim=-1)
			IGNORING:  Linear(in_features=768, out_features=2304, bias=True)
			IGNORING:  Dropout(p=0.0, inplace=False)
			IGNORING:  Linear(in_features=768, out_features=768, bias=True)
			IGNORING:  Dropout(p=0.0, inplace=False)
			IGNORING:  Softmax(dim=-1)
self.IGNORED_LAYERS_IN_TRACING =  72 [AdaptivePadding(), Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2)), LayerNorm((384,), eps=1e-05, elementwise_affine=True), Linear(in_features=384, out_features=192, bias=False), AdaptivePadding(), Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2)), LayerNorm((768,), eps=1e-05, elementwise_affine=True), Linear(in_features=768, out_features=384, bias=False), AdaptivePadding(), Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2)), LayerNorm((1536,), eps=1e-05, elementwise_affine=True), Linear(in_features=1536, out_features=768, bias=False), Linear(in_features=96, out_features=288, bias=True), Dropout(p=0.0, inplace=False), Linear(in_features=96, out_features=96, bias=True), Dropout(p=0.0, inplace=False), Softmax(dim=-1), Linear(in_features=96, out_features=288, bias=True), Dropout(p=0.0, inplace=False), Linear(in_features=96, out_features=96, bias=True), Dropout(p=0.0, inplace=False), Softmax(dim=-1), Linear(in_features=192, out_features=576, bias=True), Dropout(p=0.0, inplace=False), Linear(in_features=192, out_features=192, bias=True), Dropout(p=0.0, inplace=False), Softmax(dim=-1), Linear(in_features=192, out_features=576, bias=True), Dropout(p=0.0, inplace=False), Linear(in_features=192, out_features=192, bias=True), Dropout(p=0.0, inplace=False), Softmax(dim=-1), Linear(in_features=384, out_features=1152, bias=True), Dropout(p=0.0, inplace=False), Linear(in_features=384, out_features=384, bias=True), Dropout(p=0.0, inplace=False), Softmax(dim=-1), Linear(in_features=384, out_features=1152, bias=True), Dropout(p=0.0, inplace=False), Linear(in_features=384, out_features=384, bias=True), Dropout(p=0.0, inplace=False), Softmax(dim=-1), Linear(in_features=384, out_features=1152, bias=True), Dropout(p=0.0, inplace=False), Linear(in_features=384, out_features=384, bias=True), Dropout(p=0.0, inplace=False), Softmax(dim=-1), Linear(in_features=384, out_features=1152, bias=True), Dropout(p=0.0, inplace=False), Linear(in_features=384, out_features=384, bias=True), Dropout(p=0.0, inplace=False), Softmax(dim=-1), Linear(in_features=384, out_features=1152, bias=True), Dropout(p=0.0, inplace=False), Linear(in_features=384, out_features=384, bias=True), Dropout(p=0.0, inplace=False), Softmax(dim=-1), Linear(in_features=384, out_features=1152, bias=True), Dropout(p=0.0, inplace=False), Linear(in_features=384, out_features=384, bias=True), Dropout(p=0.0, inplace=False), Softmax(dim=-1), Linear(in_features=768, out_features=2304, bias=True), Dropout(p=0.0, inplace=False), Linear(in_features=768, out_features=768, bias=True), Dropout(p=0.0, inplace=False), Softmax(dim=-1), Linear(in_features=768, out_features=2304, bias=True), Dropout(p=0.0, inplace=False), Linear(in_features=768, out_features=768, bias=True), Dropout(p=0.0, inplace=False), Softmax(dim=-1)]
backbone.patch_embed.projection.weight True
backbone.patch_embed.projection.bias True
backbone.patch_embed.norm.weight True
backbone.patch_embed.norm.bias True
backbone.stages.0.blocks.0.norm1.weight True
backbone.stages.0.blocks.0.norm1.bias True
backbone.stages.0.blocks.0.attn.w_msa.relative_position_bias_table True
backbone.stages.0.blocks.0.attn.w_msa.qkv.weight True
backbone.stages.0.blocks.0.attn.w_msa.qkv.bias True
backbone.stages.0.blocks.0.attn.w_msa.proj.weight True
backbone.stages.0.blocks.0.attn.w_msa.proj.bias True
backbone.stages.0.blocks.0.norm2.weight True
backbone.stages.0.blocks.0.norm2.bias True
backbone.stages.0.blocks.0.ffn.layers.0.0.weight True
backbone.stages.0.blocks.0.ffn.layers.0.0.bias True
backbone.stages.0.blocks.0.ffn.layers.1.weight True
backbone.stages.0.blocks.0.ffn.layers.1.bias True
backbone.stages.0.blocks.1.norm1.weight True
backbone.stages.0.blocks.1.norm1.bias True
backbone.stages.0.blocks.1.attn.w_msa.relative_position_bias_table True
backbone.stages.0.blocks.1.attn.w_msa.qkv.weight True
backbone.stages.0.blocks.1.attn.w_msa.qkv.bias True
backbone.stages.0.blocks.1.attn.w_msa.proj.weight True
backbone.stages.0.blocks.1.attn.w_msa.proj.bias True
backbone.stages.0.blocks.1.norm2.weight True
backbone.stages.0.blocks.1.norm2.bias True
backbone.stages.0.blocks.1.ffn.layers.0.0.weight True
backbone.stages.0.blocks.1.ffn.layers.0.0.bias True
backbone.stages.0.blocks.1.ffn.layers.1.weight True
backbone.stages.0.blocks.1.ffn.layers.1.bias True
backbone.stages.0.downsample.norm.weight True
backbone.stages.0.downsample.norm.bias True
backbone.stages.0.downsample.reduction.weight True
backbone.stages.1.blocks.0.norm1.weight True
backbone.stages.1.blocks.0.norm1.bias True
backbone.stages.1.blocks.0.attn.w_msa.relative_position_bias_table True
backbone.stages.1.blocks.0.attn.w_msa.qkv.weight True
backbone.stages.1.blocks.0.attn.w_msa.qkv.bias True
backbone.stages.1.blocks.0.attn.w_msa.proj.weight True
backbone.stages.1.blocks.0.attn.w_msa.proj.bias True
backbone.stages.1.blocks.0.norm2.weight True
backbone.stages.1.blocks.0.norm2.bias True
backbone.stages.1.blocks.0.ffn.layers.0.0.weight True
backbone.stages.1.blocks.0.ffn.layers.0.0.bias True
backbone.stages.1.blocks.0.ffn.layers.1.weight True
backbone.stages.1.blocks.0.ffn.layers.1.bias True
backbone.stages.1.blocks.1.norm1.weight True
backbone.stages.1.blocks.1.norm1.bias True
backbone.stages.1.blocks.1.attn.w_msa.relative_position_bias_table True
backbone.stages.1.blocks.1.attn.w_msa.qkv.weight True
backbone.stages.1.blocks.1.attn.w_msa.qkv.bias True
backbone.stages.1.blocks.1.attn.w_msa.proj.weight True
backbone.stages.1.blocks.1.attn.w_msa.proj.bias True
backbone.stages.1.blocks.1.norm2.weight True
backbone.stages.1.blocks.1.norm2.bias True
backbone.stages.1.blocks.1.ffn.layers.0.0.weight True
backbone.stages.1.blocks.1.ffn.layers.0.0.bias True
backbone.stages.1.blocks.1.ffn.layers.1.weight True
backbone.stages.1.blocks.1.ffn.layers.1.bias True
backbone.stages.1.downsample.norm.weight True
backbone.stages.1.downsample.norm.bias True
backbone.stages.1.downsample.reduction.weight True
backbone.stages.2.blocks.0.norm1.weight True
backbone.stages.2.blocks.0.norm1.bias True
backbone.stages.2.blocks.0.attn.w_msa.relative_position_bias_table True
backbone.stages.2.blocks.0.attn.w_msa.qkv.weight True
backbone.stages.2.blocks.0.attn.w_msa.qkv.bias True
backbone.stages.2.blocks.0.attn.w_msa.proj.weight True
backbone.stages.2.blocks.0.attn.w_msa.proj.bias True
backbone.stages.2.blocks.0.norm2.weight True
backbone.stages.2.blocks.0.norm2.bias True
backbone.stages.2.blocks.0.ffn.layers.0.0.weight True
backbone.stages.2.blocks.0.ffn.layers.0.0.bias True
backbone.stages.2.blocks.0.ffn.layers.1.weight True
backbone.stages.2.blocks.0.ffn.layers.1.bias True
backbone.stages.2.blocks.1.norm1.weight True
backbone.stages.2.blocks.1.norm1.bias True
backbone.stages.2.blocks.1.attn.w_msa.relative_position_bias_table True
backbone.stages.2.blocks.1.attn.w_msa.qkv.weight True
backbone.stages.2.blocks.1.attn.w_msa.qkv.bias True
backbone.stages.2.blocks.1.attn.w_msa.proj.weight True
backbone.stages.2.blocks.1.attn.w_msa.proj.bias True
backbone.stages.2.blocks.1.norm2.weight True
backbone.stages.2.blocks.1.norm2.bias True
backbone.stages.2.blocks.1.ffn.layers.0.0.weight True
backbone.stages.2.blocks.1.ffn.layers.0.0.bias True
backbone.stages.2.blocks.1.ffn.layers.1.weight True
backbone.stages.2.blocks.1.ffn.layers.1.bias True
backbone.stages.2.blocks.2.norm1.weight True
backbone.stages.2.blocks.2.norm1.bias True
backbone.stages.2.blocks.2.attn.w_msa.relative_position_bias_table True
backbone.stages.2.blocks.2.attn.w_msa.qkv.weight True
backbone.stages.2.blocks.2.attn.w_msa.qkv.bias True
backbone.stages.2.blocks.2.attn.w_msa.proj.weight True
backbone.stages.2.blocks.2.attn.w_msa.proj.bias True
backbone.stages.2.blocks.2.norm2.weight True
backbone.stages.2.blocks.2.norm2.bias True
backbone.stages.2.blocks.2.ffn.layers.0.0.weight True
backbone.stages.2.blocks.2.ffn.layers.0.0.bias True
backbone.stages.2.blocks.2.ffn.layers.1.weight True
backbone.stages.2.blocks.2.ffn.layers.1.bias True
backbone.stages.2.blocks.3.norm1.weight True
backbone.stages.2.blocks.3.norm1.bias True
backbone.stages.2.blocks.3.attn.w_msa.relative_position_bias_table True
backbone.stages.2.blocks.3.attn.w_msa.qkv.weight True
backbone.stages.2.blocks.3.attn.w_msa.qkv.bias True
backbone.stages.2.blocks.3.attn.w_msa.proj.weight True
backbone.stages.2.blocks.3.attn.w_msa.proj.bias True
backbone.stages.2.blocks.3.norm2.weight True
backbone.stages.2.blocks.3.norm2.bias True
backbone.stages.2.blocks.3.ffn.layers.0.0.weight True
backbone.stages.2.blocks.3.ffn.layers.0.0.bias True
backbone.stages.2.blocks.3.ffn.layers.1.weight True
backbone.stages.2.blocks.3.ffn.layers.1.bias True
backbone.stages.2.blocks.4.norm1.weight True
backbone.stages.2.blocks.4.norm1.bias True
backbone.stages.2.blocks.4.attn.w_msa.relative_position_bias_table True
backbone.stages.2.blocks.4.attn.w_msa.qkv.weight True
backbone.stages.2.blocks.4.attn.w_msa.qkv.bias True
backbone.stages.2.blocks.4.attn.w_msa.proj.weight True
backbone.stages.2.blocks.4.attn.w_msa.proj.bias True
backbone.stages.2.blocks.4.norm2.weight True
backbone.stages.2.blocks.4.norm2.bias True
backbone.stages.2.blocks.4.ffn.layers.0.0.weight True
backbone.stages.2.blocks.4.ffn.layers.0.0.bias True
backbone.stages.2.blocks.4.ffn.layers.1.weight True
backbone.stages.2.blocks.4.ffn.layers.1.bias True
backbone.stages.2.blocks.5.norm1.weight True
backbone.stages.2.blocks.5.norm1.bias True
backbone.stages.2.blocks.5.attn.w_msa.relative_position_bias_table True
backbone.stages.2.blocks.5.attn.w_msa.qkv.weight True
backbone.stages.2.blocks.5.attn.w_msa.qkv.bias True
backbone.stages.2.blocks.5.attn.w_msa.proj.weight True
backbone.stages.2.blocks.5.attn.w_msa.proj.bias True
backbone.stages.2.blocks.5.norm2.weight True
backbone.stages.2.blocks.5.norm2.bias True
backbone.stages.2.blocks.5.ffn.layers.0.0.weight True
backbone.stages.2.blocks.5.ffn.layers.0.0.bias True
backbone.stages.2.blocks.5.ffn.layers.1.weight True
backbone.stages.2.blocks.5.ffn.layers.1.bias True
backbone.stages.2.downsample.norm.weight True
backbone.stages.2.downsample.norm.bias True
backbone.stages.2.downsample.reduction.weight True
backbone.stages.3.blocks.0.norm1.weight True
backbone.stages.3.blocks.0.norm1.bias True
backbone.stages.3.blocks.0.attn.w_msa.relative_position_bias_table True
backbone.stages.3.blocks.0.attn.w_msa.qkv.weight True
backbone.stages.3.blocks.0.attn.w_msa.qkv.bias True
backbone.stages.3.blocks.0.attn.w_msa.proj.weight True
backbone.stages.3.blocks.0.attn.w_msa.proj.bias True
backbone.stages.3.blocks.0.norm2.weight True
backbone.stages.3.blocks.0.norm2.bias True
backbone.stages.3.blocks.0.ffn.layers.0.0.weight True
backbone.stages.3.blocks.0.ffn.layers.0.0.bias True
backbone.stages.3.blocks.0.ffn.layers.1.weight True
backbone.stages.3.blocks.0.ffn.layers.1.bias True
backbone.stages.3.blocks.1.norm1.weight True
backbone.stages.3.blocks.1.norm1.bias True
backbone.stages.3.blocks.1.attn.w_msa.relative_position_bias_table True
backbone.stages.3.blocks.1.attn.w_msa.qkv.weight True
backbone.stages.3.blocks.1.attn.w_msa.qkv.bias True
backbone.stages.3.blocks.1.attn.w_msa.proj.weight True
backbone.stages.3.blocks.1.attn.w_msa.proj.bias True
backbone.stages.3.blocks.1.norm2.weight True
backbone.stages.3.blocks.1.norm2.bias True
backbone.stages.3.blocks.1.ffn.layers.0.0.weight True
backbone.stages.3.blocks.1.ffn.layers.0.0.bias True
backbone.stages.3.blocks.1.ffn.layers.1.weight True
backbone.stages.3.blocks.1.ffn.layers.1.bias True
backbone.norm1.weight True
backbone.norm1.bias True
backbone.norm2.weight True
backbone.norm2.bias True
backbone.norm3.weight True
backbone.norm3.bias True
neck.convs.0.conv.weight True
neck.convs.0.conv.bias True
neck.convs.0.gn.weight True
neck.convs.0.gn.bias True
neck.convs.1.conv.weight True
neck.convs.1.conv.bias True
neck.convs.1.gn.weight True
neck.convs.1.gn.bias True
neck.convs.2.conv.weight True
neck.convs.2.conv.bias True
neck.convs.2.gn.weight True
neck.convs.2.gn.bias True
neck.extra_convs.0.conv.weight True
neck.extra_convs.0.conv.bias True
neck.extra_convs.0.gn.weight True
neck.extra_convs.0.gn.bias True
self._param_to_name =  dict_values([])
>>>>>>>>>>>>>>>>_freeze_stages =  -1
[Passed] {<class 'mmdet.models.layers.transformer.utils.PatchMerging'>, <class 'mmcv.cnn.bricks.wrappers.Linear'>, <class 'torch.nn.modules.conv.Conv2d'>, <class 'torch.nn.modules.normalization.GroupNorm'>, <class 'mmdet.models.backbones.swin.WindowMSA'>, <class 'torch.nn.modules.normalization.LayerNorm'>}
[Failed] {<class 'mmengine.model.base_module.Sequential'>, <class 'torch.nn.modules.container.ModuleList'>, <class 'mmcv.cnn.bricks.conv_module.ConvModule'>, <class 'torch.nn.modules.fold.Unfold'>, <class 'torch.nn.modules.linear.Identity'>, <class 'torch.nn.modules.dropout.Dropout'>, <class 'mmcv.cnn.bricks.transformer.FFN'>, <class 'torch.nn.modules.activation.Softmax'>, <class 'mmengine.model.base_module.ModuleList'>, <class 'mmcv.cnn.bricks.drop.DropPath'>, <class 'torch.nn.modules.linear.Linear'>, <class 'torch.nn.modules.container.Sequential'>, <class 'torch.nn.modules.normalization.LayerNorm'>, <class 'torch.nn.modules.activation.GELU'>, <class 'mmdet.models.backbones.swin.SwinBlock'>, <class 'mmdet.models.backbones.swin.ShiftWindowMSA'>, <class 'mmdet.models.backbones.swin.SwinBlockSequence'>, <class 'mmdet.models.layers.transformer.utils.AdaptivePadding'>, <class 'mmdet.models.necks.channel_mapper.ChannelMapper'>, <class 'mmdet.models.backbones.swin.SwinTransformer'>, <class 'mmdet.models.layers.transformer.utils.PatchEmbed'>}
self.IGNORED_LAYERS_IN_TRACING =  [AdaptivePadding(), Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2)), LayerNorm((384,), eps=1e-05, elementwise_affine=True), Linear(in_features=384, out_features=192, bias=False), AdaptivePadding(), Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2)), LayerNorm((768,), eps=1e-05, elementwise_affine=True), Linear(in_features=768, out_features=384, bias=False), AdaptivePadding(), Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2)), LayerNorm((1536,), eps=1e-05, elementwise_affine=True), Linear(in_features=1536, out_features=768, bias=False), Linear(in_features=96, out_features=288, bias=True), Dropout(p=0.0, inplace=False), Linear(in_features=96, out_features=96, bias=True), Dropout(p=0.0, inplace=False), Softmax(dim=-1), Linear(in_features=96, out_features=288, bias=True), Dropout(p=0.0, inplace=False), Linear(in_features=96, out_features=96, bias=True), Dropout(p=0.0, inplace=False), Softmax(dim=-1), Linear(in_features=192, out_features=576, bias=True), Dropout(p=0.0, inplace=False), Linear(in_features=192, out_features=192, bias=True), Dropout(p=0.0, inplace=False), Softmax(dim=-1), Linear(in_features=192, out_features=576, bias=True), Dropout(p=0.0, inplace=False), Linear(in_features=192, out_features=192, bias=True), Dropout(p=0.0, inplace=False), Softmax(dim=-1), Linear(in_features=384, out_features=1152, bias=True), Dropout(p=0.0, inplace=False), Linear(in_features=384, out_features=384, bias=True), Dropout(p=0.0, inplace=False), Softmax(dim=-1), Linear(in_features=384, out_features=1152, bias=True), Dropout(p=0.0, inplace=False), Linear(in_features=384, out_features=384, bias=True), Dropout(p=0.0, inplace=False), Softmax(dim=-1), Linear(in_features=384, out_features=1152, bias=True), Dropout(p=0.0, inplace=False), Linear(in_features=384, out_features=384, bias=True), Dropout(p=0.0, inplace=False), Softmax(dim=-1), Linear(in_features=384, out_features=1152, bias=True), Dropout(p=0.0, inplace=False), Linear(in_features=384, out_features=384, bias=True), Dropout(p=0.0, inplace=False), Softmax(dim=-1), Linear(in_features=384, out_features=1152, bias=True), Dropout(p=0.0, inplace=False), Linear(in_features=384, out_features=384, bias=True), Dropout(p=0.0, inplace=False), Softmax(dim=-1), Linear(in_features=384, out_features=1152, bias=True), Dropout(p=0.0, inplace=False), Linear(in_features=384, out_features=384, bias=True), Dropout(p=0.0, inplace=False), Softmax(dim=-1), Linear(in_features=768, out_features=2304, bias=True), Dropout(p=0.0, inplace=False), Linear(in_features=768, out_features=768, bias=True), Dropout(p=0.0, inplace=False), Softmax(dim=-1), Linear(in_features=768, out_features=2304, bias=True), Dropout(p=0.0, inplace=False), Linear(in_features=768, out_features=768, bias=True), Dropout(p=0.0, inplace=False), Softmax(dim=-1)]
registered_types =  (<class 'torch.nn.modules.conv._ConvNd'>, <class 'torch.nn.modules.linear.Linear'>, <class 'torch.nn.modules.batchnorm._BatchNorm'>, <class 'torch.nn.modules.conv._ConvNd'>, <class 'torch.nn.modules.activation.PReLU'>, <class 'torch.nn.modules.normalization.LayerNorm'>, <class 'torch.nn.modules.sparse.Embedding'>, <class 'torch.nn.parameter.Parameter'>, <class 'torch.nn.modules.activation.MultiheadAttention'>, <class 'torch.nn.modules.rnn.LSTM'>, <class 'torch.nn.modules.normalization.GroupNorm'>, <class 'torch.nn.modules.instancenorm._InstanceNorm'>, <class 'torch_pruning.ops._ConcatOp'>, <class 'torch_pruning.ops._SplitOp'>, <class 'torch_pruning.ops._ElementWiseOp'>, <class 'torch_pruning.ops._ElementWiseOp'>, <class 'torch_pruning.ops._ElementWiseOp'>, <class 'torch_pruning.ops._CustomizedOp'>, <class 'mmdet.models.layers.transformer.utils.PatchMerging'>, <class 'mmdet.models.backbones.swin.WindowMSA'>)
len hooks =  76
pre forward - gradfn2module =  0 dict_keys([])
-----NEW SwinTransformer FORWARD PASS 



		SwinTransformer input shape:  torch.Size([3, 800, 1333])


_record_grad_fn <class 'torch.nn.modules.conv.Conv2d'>, True, grad_fn: <SqueezeBackward1 object at 0x7f1083e84dc0>
-----NEW SwinTransformer FORWARD PASS 



		SwinTransformer input shape:  torch.Size([1, 3, 800, 1333])


_record_grad_fn <class 'torch.nn.modules.conv.Conv2d'>, True, grad_fn: <ConvolutionBackward0 object at 0x7f1083e84fd0>
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f1083efb5b0>
------------SwinBlockSequence -  torch.Size([1, 66800, 96])
with_cp =  False
x.requires_grad =  True
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f1083e8b100>
ShiftWindowMSA = query view: B, L, C=  1 66800 96
	 hw_shape =  (200, 334)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([1392, 49, 96])
B=1392, N=49, C=96
the qkv mod: 96 288 Linear(in_features=96, out_features=288, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([1392, 49, 288])
NEW C= 96
qkv.shape =  torch.Size([1392, 49, 288])
qkv.flatten.shape =  torch.Size([19643904])
self.embed_dims =  96
+++++ reshaping ....  1392 49 3 self.num_heads= 3 32
1392 49 3 3 32  =  19643904
self.num_heads =  3
C =  96
qkv after reshape  torch.Size([1392, 49, 3, 3, 32])
qkv after permute  torch.Size([3, 1392, 3, 49, 32])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f10949fd100>
--proj input shape =  torch.Size([1392, 49, 96])
--proj output shape =  torch.Size([1392, 49, 96])
------------WindowMSA - x_shape Output torch.Size([1392, 49, 96]) <UnsafeViewBackward0 object at 0x7f10949fd100>
_record_grad_fn <class 'mmdet.models.backbones.swin.WindowMSA'>, True, grad_fn: <UnsafeViewBackward0 object at 0x7f1094980400>
attn_windows.shape before merge =  torch.Size([1392, 49, 96])
x.shape before view =  torch.Size([1, 200, 334, 96])
ShiftWindowMSA = before view B, H * W, C =  1 66800 96
------------ShiftWindowMSA -  <ViewBackward0 object at 0x7f10949fd100>
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f1094980460>
_record_grad_fn <class 'mmcv.cnn.bricks.wrappers.Linear'>, True, grad_fn: <ViewBackward0 object at 0x7f1094980640>
_record_grad_fn <class 'mmcv.cnn.bricks.wrappers.Linear'>, True, grad_fn: <ViewBackward0 object at 0x7f10949804c0>
------------SwinBlock -  <AddBackward0 object at 0x7f10949fd100>
		block output:  torch.Size([1, 66800, 96])
------------SwinBlockSequence -  torch.Size([1, 66800, 96])
with_cp =  False
x.requires_grad =  True
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f1094980160>
ShiftWindowMSA = query view: B, L, C=  1 66800 96
	 hw_shape =  (200, 334)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([1392, 49, 96])
B=1392, N=49, C=96
the qkv mod: 96 288 Linear(in_features=96, out_features=288, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([1392, 49, 288])
NEW C= 96
qkv.shape =  torch.Size([1392, 49, 288])
qkv.flatten.shape =  torch.Size([19643904])
self.embed_dims =  96
+++++ reshaping ....  1392 49 3 self.num_heads= 3 32
1392 49 3 3 32  =  19643904
self.num_heads =  3
C =  96
qkv after reshape  torch.Size([1392, 49, 3, 3, 32])
qkv after permute  torch.Size([3, 1392, 3, 49, 32])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f10949fd100>
--proj input shape =  torch.Size([1392, 49, 96])
--proj output shape =  torch.Size([1392, 49, 96])
------------WindowMSA - x_shape Output torch.Size([1392, 49, 96]) <UnsafeViewBackward0 object at 0x7f10949fd100>
_record_grad_fn <class 'mmdet.models.backbones.swin.WindowMSA'>, True, grad_fn: <UnsafeViewBackward0 object at 0x7f10949805b0>
attn_windows.shape before merge =  torch.Size([1392, 49, 96])
x.shape before view =  torch.Size([1, 200, 334, 96])
ShiftWindowMSA = before view B, H * W, C =  1 66800 96
------------ShiftWindowMSA -  <ViewBackward0 object at 0x7f10949fd100>
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f1094980700>
_record_grad_fn <class 'mmcv.cnn.bricks.wrappers.Linear'>, True, grad_fn: <ViewBackward0 object at 0x7f10949805e0>
_record_grad_fn <class 'mmcv.cnn.bricks.wrappers.Linear'>, True, grad_fn: <ViewBackward0 object at 0x7f10949806d0>
------------SwinBlock -  <AddBackward0 object at 0x7f10949fd100>
		block output:  torch.Size([1, 66800, 96])
_record_grad_fn <class 'mmdet.models.layers.transformer.utils.PatchMerging'>, True, grad_fn: <UnsafeViewBackward0 object at 0x7f1083ea1b20>
------------SwinBlockSequence/downsample -  torch.Size([1, 16700, 192]) <UnsafeViewBackward0 object at 0x7f1083ea1b20>
down_hw_shape =  (100, 167)
x.shape =  torch.Size([1, 16700, 192])
hw_shape =  (100, 167)
stage out.shape =  torch.Size([1, 66800, 96])
out_hw_shape =  (200, 334)
stage =  0  / self.out_indices =  (1, 2, 3)
self.num_features =  [96, 192, 384, 768]
------------SwinBlockSequence -  torch.Size([1, 16700, 192])
with_cp =  False
x.requires_grad =  True
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f1083edb2e0>
ShiftWindowMSA = query view: B, L, C=  1 16700 192
	 hw_shape =  (100, 167)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([360, 49, 192])
B=360, N=49, C=192
the qkv mod: 192 576 Linear(in_features=192, out_features=576, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([360, 49, 576])
NEW C= 192
qkv.shape =  torch.Size([360, 49, 576])
qkv.flatten.shape =  torch.Size([10160640])
self.embed_dims =  192
+++++ reshaping ....  360 49 3 self.num_heads= 6 32
360 49 3 6 32  =  10160640
self.num_heads =  6
C =  192
qkv after reshape  torch.Size([360, 49, 3, 6, 32])
qkv after permute  torch.Size([3, 360, 6, 49, 32])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f10949803d0>
--proj input shape =  torch.Size([360, 49, 192])
--proj output shape =  torch.Size([360, 49, 192])
------------WindowMSA - x_shape Output torch.Size([360, 49, 192]) <UnsafeViewBackward0 object at 0x7f10949803d0>
_record_grad_fn <class 'mmdet.models.backbones.swin.WindowMSA'>, True, grad_fn: <UnsafeViewBackward0 object at 0x7f1083edb220>
attn_windows.shape before merge =  torch.Size([360, 49, 192])
x.shape before view =  torch.Size([1, 100, 167, 192])
ShiftWindowMSA = before view B, H * W, C =  1 16700 192
------------ShiftWindowMSA -  <ViewBackward0 object at 0x7f10949803d0>
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f1083edb4c0>
_record_grad_fn <class 'mmcv.cnn.bricks.wrappers.Linear'>, True, grad_fn: <ViewBackward0 object at 0x7f1083edb400>
_record_grad_fn <class 'mmcv.cnn.bricks.wrappers.Linear'>, True, grad_fn: <ViewBackward0 object at 0x7f1083edb430>
------------SwinBlock -  <AddBackward0 object at 0x7f10949803d0>
		block output:  torch.Size([1, 16700, 192])
------------SwinBlockSequence -  torch.Size([1, 16700, 192])
with_cp =  False
x.requires_grad =  True
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f1083edbeb0>
ShiftWindowMSA = query view: B, L, C=  1 16700 192
	 hw_shape =  (100, 167)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([360, 49, 192])
B=360, N=49, C=192
the qkv mod: 192 576 Linear(in_features=192, out_features=576, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([360, 49, 576])
NEW C= 192
qkv.shape =  torch.Size([360, 49, 576])
qkv.flatten.shape =  torch.Size([10160640])
self.embed_dims =  192
+++++ reshaping ....  360 49 3 self.num_heads= 6 32
360 49 3 6 32  =  10160640
self.num_heads =  6
C =  192
qkv after reshape  torch.Size([360, 49, 3, 6, 32])
qkv after permute  torch.Size([3, 360, 6, 49, 32])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f10949803d0>
--proj input shape =  torch.Size([360, 49, 192])
--proj output shape =  torch.Size([360, 49, 192])
------------WindowMSA - x_shape Output torch.Size([360, 49, 192]) <UnsafeViewBackward0 object at 0x7f10949803d0>
_record_grad_fn <class 'mmdet.models.backbones.swin.WindowMSA'>, True, grad_fn: <UnsafeViewBackward0 object at 0x7f1083edb580>
attn_windows.shape before merge =  torch.Size([360, 49, 192])
x.shape before view =  torch.Size([1, 100, 167, 192])
ShiftWindowMSA = before view B, H * W, C =  1 16700 192
------------ShiftWindowMSA -  <ViewBackward0 object at 0x7f10949803d0>
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f1083edb5b0>
_record_grad_fn <class 'mmcv.cnn.bricks.wrappers.Linear'>, True, grad_fn: <ViewBackward0 object at 0x7f1083edb370>
_record_grad_fn <class 'mmcv.cnn.bricks.wrappers.Linear'>, True, grad_fn: <ViewBackward0 object at 0x7f1083edb4f0>
------------SwinBlock -  <AddBackward0 object at 0x7f10949803d0>
		block output:  torch.Size([1, 16700, 192])
_record_grad_fn <class 'mmdet.models.layers.transformer.utils.PatchMerging'>, True, grad_fn: <UnsafeViewBackward0 object at 0x7f10949fd100>
------------SwinBlockSequence/downsample -  torch.Size([1, 4200, 384]) <UnsafeViewBackward0 object at 0x7f10949fd100>
down_hw_shape =  (50, 84)
x.shape =  torch.Size([1, 4200, 384])
hw_shape =  (50, 84)
stage out.shape =  torch.Size([1, 16700, 192])
out_hw_shape =  (100, 167)
stage =  1  / self.out_indices =  (1, 2, 3)
self.num_features =  [96, 192, 384, 768]
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f10949803d0>
stage:  1  --- norm_layer out.shape =  torch.Size([1, 16700, 192])
torch.Size([1, 192, 100, 167])
------------SwinBlockSequence -  torch.Size([1, 4200, 384])
with_cp =  False
x.requires_grad =  True
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f1083edb730>
ShiftWindowMSA = query view: B, L, C=  1 4200 384
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 384])
B=96, N=49, C=384
the qkv mod: 384 1152 Linear(in_features=384, out_features=1152, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 1152])
NEW C= 384
qkv.shape =  torch.Size([96, 49, 1152])
qkv.flatten.shape =  torch.Size([5419008])
self.embed_dims =  384
+++++ reshaping ....  96 49 3 self.num_heads= 12 32
96 49 3 12 32  =  5419008
self.num_heads =  12
C =  384
qkv after reshape  torch.Size([96, 49, 3, 12, 32])
qkv after permute  torch.Size([3, 96, 12, 49, 32])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f1083edb940>
--proj input shape =  torch.Size([96, 49, 384])
--proj output shape =  torch.Size([96, 49, 384])
------------WindowMSA - x_shape Output torch.Size([96, 49, 384]) <UnsafeViewBackward0 object at 0x7f1083edb940>
_record_grad_fn <class 'mmdet.models.backbones.swin.WindowMSA'>, True, grad_fn: <UnsafeViewBackward0 object at 0x7f1083edb7c0>
attn_windows.shape before merge =  torch.Size([96, 49, 384])
x.shape before view =  torch.Size([1, 50, 84, 384])
ShiftWindowMSA = before view B, H * W, C =  1 4200 384
------------ShiftWindowMSA -  <ViewBackward0 object at 0x7f1083edb940>
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f1083edba30>
_record_grad_fn <class 'mmcv.cnn.bricks.wrappers.Linear'>, True, grad_fn: <ViewBackward0 object at 0x7f1083edb880>
_record_grad_fn <class 'mmcv.cnn.bricks.wrappers.Linear'>, True, grad_fn: <ViewBackward0 object at 0x7f1083edbb20>
------------SwinBlock -  <AddBackward0 object at 0x7f1083edb940>
		block output:  torch.Size([1, 4200, 384])
------------SwinBlockSequence -  torch.Size([1, 4200, 384])
with_cp =  False
x.requires_grad =  True
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f1083edb880>
ShiftWindowMSA = query view: B, L, C=  1 4200 384
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 384])
B=96, N=49, C=384
the qkv mod: 384 1152 Linear(in_features=384, out_features=1152, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 1152])
NEW C= 384
qkv.shape =  torch.Size([96, 49, 1152])
qkv.flatten.shape =  torch.Size([5419008])
self.embed_dims =  384
+++++ reshaping ....  96 49 3 self.num_heads= 12 32
96 49 3 12 32  =  5419008
self.num_heads =  12
C =  384
qkv after reshape  torch.Size([96, 49, 3, 12, 32])
qkv after permute  torch.Size([3, 96, 12, 49, 32])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f1083edb940>
--proj input shape =  torch.Size([96, 49, 384])
--proj output shape =  torch.Size([96, 49, 384])
------------WindowMSA - x_shape Output torch.Size([96, 49, 384]) <UnsafeViewBackward0 object at 0x7f1083edb940>
_record_grad_fn <class 'mmdet.models.backbones.swin.WindowMSA'>, True, grad_fn: <UnsafeViewBackward0 object at 0x7f1083edb970>
attn_windows.shape before merge =  torch.Size([96, 49, 384])
x.shape before view =  torch.Size([1, 50, 84, 384])
ShiftWindowMSA = before view B, H * W, C =  1 4200 384
------------ShiftWindowMSA -  <ViewBackward0 object at 0x7f1083edb940>
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f1083edba00>
_record_grad_fn <class 'mmcv.cnn.bricks.wrappers.Linear'>, True, grad_fn: <ViewBackward0 object at 0x7f1083edbc40>
_record_grad_fn <class 'mmcv.cnn.bricks.wrappers.Linear'>, True, grad_fn: <ViewBackward0 object at 0x7f1083edba60>
------------SwinBlock -  <AddBackward0 object at 0x7f1083edb940>
		block output:  torch.Size([1, 4200, 384])
------------SwinBlockSequence -  torch.Size([1, 4200, 384])
with_cp =  False
x.requires_grad =  True
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f1083edba90>
ShiftWindowMSA = query view: B, L, C=  1 4200 384
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 384])
B=96, N=49, C=384
the qkv mod: 384 1152 Linear(in_features=384, out_features=1152, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 1152])
NEW C= 384
qkv.shape =  torch.Size([96, 49, 1152])
qkv.flatten.shape =  torch.Size([5419008])
self.embed_dims =  384
+++++ reshaping ....  96 49 3 self.num_heads= 12 32
96 49 3 12 32  =  5419008
self.num_heads =  12
C =  384
qkv after reshape  torch.Size([96, 49, 3, 12, 32])
qkv after permute  torch.Size([3, 96, 12, 49, 32])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f1083edb940>
--proj input shape =  torch.Size([96, 49, 384])
--proj output shape =  torch.Size([96, 49, 384])
------------WindowMSA - x_shape Output torch.Size([96, 49, 384]) <UnsafeViewBackward0 object at 0x7f1083edb940>
_record_grad_fn <class 'mmdet.models.backbones.swin.WindowMSA'>, True, grad_fn: <UnsafeViewBackward0 object at 0x7f1083edbbb0>
attn_windows.shape before merge =  torch.Size([96, 49, 384])
x.shape before view =  torch.Size([1, 50, 84, 384])
ShiftWindowMSA = before view B, H * W, C =  1 4200 384
------------ShiftWindowMSA -  <ViewBackward0 object at 0x7f1083edb940>
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f1083edbb50>
_record_grad_fn <class 'mmcv.cnn.bricks.wrappers.Linear'>, True, grad_fn: <ViewBackward0 object at 0x7f1083edbb80>
_record_grad_fn <class 'mmcv.cnn.bricks.wrappers.Linear'>, True, grad_fn: <ViewBackward0 object at 0x7f1083edbc70>
------------SwinBlock -  <AddBackward0 object at 0x7f1083edb940>
		block output:  torch.Size([1, 4200, 384])
------------SwinBlockSequence -  torch.Size([1, 4200, 384])
with_cp =  False
x.requires_grad =  True
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f1083edbca0>
ShiftWindowMSA = query view: B, L, C=  1 4200 384
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 384])
B=96, N=49, C=384
the qkv mod: 384 1152 Linear(in_features=384, out_features=1152, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 1152])
NEW C= 384
qkv.shape =  torch.Size([96, 49, 1152])
qkv.flatten.shape =  torch.Size([5419008])
self.embed_dims =  384
+++++ reshaping ....  96 49 3 self.num_heads= 12 32
96 49 3 12 32  =  5419008
self.num_heads =  12
C =  384
qkv after reshape  torch.Size([96, 49, 3, 12, 32])
qkv after permute  torch.Size([3, 96, 12, 49, 32])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f1083edb940>
--proj input shape =  torch.Size([96, 49, 384])
--proj output shape =  torch.Size([96, 49, 384])
------------WindowMSA - x_shape Output torch.Size([96, 49, 384]) <UnsafeViewBackward0 object at 0x7f1083edb940>
_record_grad_fn <class 'mmdet.models.backbones.swin.WindowMSA'>, True, grad_fn: <UnsafeViewBackward0 object at 0x7f1083edb700>
attn_windows.shape before merge =  torch.Size([96, 49, 384])
x.shape before view =  torch.Size([1, 50, 84, 384])
ShiftWindowMSA = before view B, H * W, C =  1 4200 384
------------ShiftWindowMSA -  <ViewBackward0 object at 0x7f1083edb940>
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f1083edb640>
_record_grad_fn <class 'mmcv.cnn.bricks.wrappers.Linear'>, True, grad_fn: <ViewBackward0 object at 0x7f1083edbd60>
_record_grad_fn <class 'mmcv.cnn.bricks.wrappers.Linear'>, True, grad_fn: <ViewBackward0 object at 0x7f1083edbe80>
------------SwinBlock -  <AddBackward0 object at 0x7f1083edb940>
		block output:  torch.Size([1, 4200, 384])
------------SwinBlockSequence -  torch.Size([1, 4200, 384])
with_cp =  False
x.requires_grad =  True
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f1083edbdc0>
ShiftWindowMSA = query view: B, L, C=  1 4200 384
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 384])
B=96, N=49, C=384
the qkv mod: 384 1152 Linear(in_features=384, out_features=1152, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 1152])
NEW C= 384
qkv.shape =  torch.Size([96, 49, 1152])
qkv.flatten.shape =  torch.Size([5419008])
self.embed_dims =  384
+++++ reshaping ....  96 49 3 self.num_heads= 12 32
96 49 3 12 32  =  5419008
self.num_heads =  12
C =  384
qkv after reshape  torch.Size([96, 49, 3, 12, 32])
qkv after permute  torch.Size([3, 96, 12, 49, 32])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f1083edb940>
--proj input shape =  torch.Size([96, 49, 384])
--proj output shape =  torch.Size([96, 49, 384])
------------WindowMSA - x_shape Output torch.Size([96, 49, 384]) <UnsafeViewBackward0 object at 0x7f1083edb940>
_record_grad_fn <class 'mmdet.models.backbones.swin.WindowMSA'>, True, grad_fn: <UnsafeViewBackward0 object at 0x7f1083edbdf0>
attn_windows.shape before merge =  torch.Size([96, 49, 384])
x.shape before view =  torch.Size([1, 50, 84, 384])
ShiftWindowMSA = before view B, H * W, C =  1 4200 384
------------ShiftWindowMSA -  <ViewBackward0 object at 0x7f1083edb940>
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f1083edbfa0>
_record_grad_fn <class 'mmcv.cnn.bricks.wrappers.Linear'>, True, grad_fn: <ViewBackward0 object at 0x7f1083edbee0>
_record_grad_fn <class 'mmcv.cnn.bricks.wrappers.Linear'>, True, grad_fn: <ViewBackward0 object at 0x7f1083edbf10>
------------SwinBlock -  <AddBackward0 object at 0x7f1083edb940>
		block output:  torch.Size([1, 4200, 384])
------------SwinBlockSequence -  torch.Size([1, 4200, 384])
with_cp =  False
x.requires_grad =  True
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f1083edbf10>
ShiftWindowMSA = query view: B, L, C=  1 4200 384
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 384])
B=96, N=49, C=384
the qkv mod: 384 1152 Linear(in_features=384, out_features=1152, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 1152])
NEW C= 384
qkv.shape =  torch.Size([96, 49, 1152])
qkv.flatten.shape =  torch.Size([5419008])
self.embed_dims =  384
+++++ reshaping ....  96 49 3 self.num_heads= 12 32
96 49 3 12 32  =  5419008
self.num_heads =  12
C =  384
qkv after reshape  torch.Size([96, 49, 3, 12, 32])
qkv after permute  torch.Size([3, 96, 12, 49, 32])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f1083edb940>
--proj input shape =  torch.Size([96, 49, 384])
--proj output shape =  torch.Size([96, 49, 384])
------------WindowMSA - x_shape Output torch.Size([96, 49, 384]) <UnsafeViewBackward0 object at 0x7f1083edb940>
_record_grad_fn <class 'mmdet.models.backbones.swin.WindowMSA'>, True, grad_fn: <UnsafeViewBackward0 object at 0x7f1083edbcd0>
attn_windows.shape before merge =  torch.Size([96, 49, 384])
x.shape before view =  torch.Size([1, 50, 84, 384])
ShiftWindowMSA = before view B, H * W, C =  1 4200 384
------------ShiftWindowMSA -  <ViewBackward0 object at 0x7f1083edb940>
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f1083edbfd0>
_record_grad_fn <class 'mmcv.cnn.bricks.wrappers.Linear'>, True, grad_fn: <ViewBackward0 object at 0x7f1083edb340>
_record_grad_fn <class 'mmcv.cnn.bricks.wrappers.Linear'>, True, grad_fn: <ViewBackward0 object at 0x7f1083edb310>
------------SwinBlock -  <AddBackward0 object at 0x7f1083edb940>
		block output:  torch.Size([1, 4200, 384])
_record_grad_fn <class 'mmdet.models.layers.transformer.utils.PatchMerging'>, True, grad_fn: <UnsafeViewBackward0 object at 0x7f1083edb670>
------------SwinBlockSequence/downsample -  torch.Size([1, 1050, 768]) <UnsafeViewBackward0 object at 0x7f1083edb670>
down_hw_shape =  (25, 42)
x.shape =  torch.Size([1, 1050, 768])
hw_shape =  (25, 42)
stage out.shape =  torch.Size([1, 4200, 384])
out_hw_shape =  (50, 84)
stage =  2  / self.out_indices =  (1, 2, 3)
self.num_features =  [96, 192, 384, 768]
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f1083edb940>
stage:  2  --- norm_layer out.shape =  torch.Size([1, 4200, 384])
torch.Size([1, 384, 50, 84])
------------SwinBlockSequence -  torch.Size([1, 1050, 768])
with_cp =  False
x.requires_grad =  True
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f1083edb7f0>
ShiftWindowMSA = query view: B, L, C=  1 1050 768
	 hw_shape =  (25, 42)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([24, 49, 768])
B=24, N=49, C=768
the qkv mod: 768 2304 Linear(in_features=768, out_features=2304, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([24, 49, 2304])
NEW C= 768
qkv.shape =  torch.Size([24, 49, 2304])
qkv.flatten.shape =  torch.Size([2709504])
self.embed_dims =  768
+++++ reshaping ....  24 49 3 self.num_heads= 24 32
24 49 3 24 32  =  2709504
self.num_heads =  24
C =  768
qkv after reshape  torch.Size([24, 49, 3, 24, 32])
qkv after permute  torch.Size([3, 24, 24, 49, 32])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f1083edb1f0>
--proj input shape =  torch.Size([24, 49, 768])
--proj output shape =  torch.Size([24, 49, 768])
------------WindowMSA - x_shape Output torch.Size([24, 49, 768]) <UnsafeViewBackward0 object at 0x7f1083edb1f0>
_record_grad_fn <class 'mmdet.models.backbones.swin.WindowMSA'>, True, grad_fn: <UnsafeViewBackward0 object at 0x7f1083edb1c0>
attn_windows.shape before merge =  torch.Size([24, 49, 768])
x.shape before view =  torch.Size([1, 25, 42, 768])
ShiftWindowMSA = before view B, H * W, C =  1 1050 768
------------ShiftWindowMSA -  <ViewBackward0 object at 0x7f1083edb1f0>
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f1083edb130>
_record_grad_fn <class 'mmcv.cnn.bricks.wrappers.Linear'>, True, grad_fn: <ViewBackward0 object at 0x7f1083edb100>
_record_grad_fn <class 'mmcv.cnn.bricks.wrappers.Linear'>, True, grad_fn: <ViewBackward0 object at 0x7f1083edb0d0>
------------SwinBlock -  <AddBackward0 object at 0x7f1083edb1f0>
		block output:  torch.Size([1, 1050, 768])
------------SwinBlockSequence -  torch.Size([1, 1050, 768])
with_cp =  False
x.requires_grad =  True
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f1083edb0a0>
ShiftWindowMSA = query view: B, L, C=  1 1050 768
	 hw_shape =  (25, 42)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([24, 49, 768])
B=24, N=49, C=768
the qkv mod: 768 2304 Linear(in_features=768, out_features=2304, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([24, 49, 2304])
NEW C= 768
qkv.shape =  torch.Size([24, 49, 2304])
qkv.flatten.shape =  torch.Size([2709504])
self.embed_dims =  768
+++++ reshaping ....  24 49 3 self.num_heads= 24 32
24 49 3 24 32  =  2709504
self.num_heads =  24
C =  768
qkv after reshape  torch.Size([24, 49, 3, 24, 32])
qkv after permute  torch.Size([3, 24, 24, 49, 32])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f1083edb1f0>
--proj input shape =  torch.Size([24, 49, 768])
--proj output shape =  torch.Size([24, 49, 768])
------------WindowMSA - x_shape Output torch.Size([24, 49, 768]) <UnsafeViewBackward0 object at 0x7f1083edb1f0>
_record_grad_fn <class 'mmdet.models.backbones.swin.WindowMSA'>, True, grad_fn: <UnsafeViewBackward0 object at 0x7f1083ed2fd0>
attn_windows.shape before merge =  torch.Size([24, 49, 768])
x.shape before view =  torch.Size([1, 25, 42, 768])
ShiftWindowMSA = before view B, H * W, C =  1 1050 768
------------ShiftWindowMSA -  <ViewBackward0 object at 0x7f1083edb1f0>
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f1083ed2fa0>
_record_grad_fn <class 'mmcv.cnn.bricks.wrappers.Linear'>, True, grad_fn: <ViewBackward0 object at 0x7f1083ed2f70>
_record_grad_fn <class 'mmcv.cnn.bricks.wrappers.Linear'>, True, grad_fn: <ViewBackward0 object at 0x7f1083ed2f40>
------------SwinBlock -  <AddBackward0 object at 0x7f1083edb1f0>
		block output:  torch.Size([1, 1050, 768])
x.shape =  torch.Size([1, 1050, 768])
hw_shape =  (25, 42)
stage out.shape =  torch.Size([1, 1050, 768])
out_hw_shape =  (25, 42)
stage =  3  / self.out_indices =  (1, 2, 3)
self.num_features =  [96, 192, 384, 768]
_record_grad_fn <class 'torch.nn.modules.normalization.LayerNorm'>, True, grad_fn: <NativeLayerNormBackward0 object at 0x7f1083edb250>
stage:  3  --- norm_layer out.shape =  torch.Size([1, 1050, 768])
torch.Size([1, 768, 25, 42])
------------SwinTransformer -  [<CloneBackward0 object at 0x7f1083edb1f0>, <CloneBackward0 object at 0x7f1083ed2eb0>, <CloneBackward0 object at 0x7f1083ed2e80>]



[torch.Size([1, 192, 100, 167]), torch.Size([1, 384, 50, 84]), torch.Size([1, 768, 25, 42])]
_record_grad_fn <class 'torch.nn.modules.conv.Conv2d'>, True, grad_fn: <ConvolutionBackward0 object at 0x7f1083ed2e50>
_record_grad_fn <class 'torch.nn.modules.normalization.GroupNorm'>, True, grad_fn: <NativeGroupNormBackward0 object at 0x7f1083ed2eb0>
_record_grad_fn <class 'torch.nn.modules.conv.Conv2d'>, True, grad_fn: <ConvolutionBackward0 object at 0x7f1083ed2d90>
_record_grad_fn <class 'torch.nn.modules.normalization.GroupNorm'>, True, grad_fn: <NativeGroupNormBackward0 object at 0x7f1083ed2e20>
_record_grad_fn <class 'torch.nn.modules.conv.Conv2d'>, True, grad_fn: <ConvolutionBackward0 object at 0x7f1083ed2d30>
_record_grad_fn <class 'torch.nn.modules.normalization.GroupNorm'>, True, grad_fn: <NativeGroupNormBackward0 object at 0x7f1083ed2d60>
_record_grad_fn <class 'torch.nn.modules.conv.Conv2d'>, True, grad_fn: <ConvolutionBackward0 object at 0x7f1083ed2c40>
_record_grad_fn <class 'torch.nn.modules.normalization.GroupNorm'>, True, grad_fn: <NativeGroupNormBackward0 object at 0x7f1083ed2e80>

visited =  {Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4)): 2, LayerNorm((96,), eps=1e-05, elementwise_affine=True): 1, LayerNorm((96,), eps=1e-05, elementwise_affine=True): 1, WindowMSA(
  (qkv): Linear(in_features=96, out_features=288, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=96, out_features=96, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
): 1, LayerNorm((96,), eps=1e-05, elementwise_affine=True): 1, Linear(in_features=96, out_features=384, bias=True): 1, Linear(in_features=384, out_features=96, bias=True): 1, LayerNorm((96,), eps=1e-05, elementwise_affine=True): 1, WindowMSA(
  (qkv): Linear(in_features=96, out_features=288, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=96, out_features=96, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
): 1, LayerNorm((96,), eps=1e-05, elementwise_affine=True): 1, Linear(in_features=96, out_features=384, bias=True): 1, Linear(in_features=384, out_features=96, bias=True): 1, PatchMerging(
  (adap_padding): AdaptivePadding()
  (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
  (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (reduction): Linear(in_features=384, out_features=192, bias=False)
): 1, LayerNorm((192,), eps=1e-05, elementwise_affine=True): 1, WindowMSA(
  (qkv): Linear(in_features=192, out_features=576, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=192, out_features=192, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
): 1, LayerNorm((192,), eps=1e-05, elementwise_affine=True): 1, Linear(in_features=192, out_features=768, bias=True): 1, Linear(in_features=768, out_features=192, bias=True): 1, LayerNorm((192,), eps=1e-05, elementwise_affine=True): 1, WindowMSA(
  (qkv): Linear(in_features=192, out_features=576, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=192, out_features=192, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
): 1, LayerNorm((192,), eps=1e-05, elementwise_affine=True): 1, Linear(in_features=192, out_features=768, bias=True): 1, Linear(in_features=768, out_features=192, bias=True): 1, PatchMerging(
  (adap_padding): AdaptivePadding()
  (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (reduction): Linear(in_features=768, out_features=384, bias=False)
): 1, LayerNorm((192,), eps=1e-05, elementwise_affine=True): 1, LayerNorm((384,), eps=1e-05, elementwise_affine=True): 1, WindowMSA(
  (qkv): Linear(in_features=384, out_features=1152, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=384, out_features=384, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
): 1, LayerNorm((384,), eps=1e-05, elementwise_affine=True): 1, Linear(in_features=384, out_features=1536, bias=True): 1, Linear(in_features=1536, out_features=384, bias=True): 1, LayerNorm((384,), eps=1e-05, elementwise_affine=True): 1, WindowMSA(
  (qkv): Linear(in_features=384, out_features=1152, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=384, out_features=384, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
): 1, LayerNorm((384,), eps=1e-05, elementwise_affine=True): 1, Linear(in_features=384, out_features=1536, bias=True): 1, Linear(in_features=1536, out_features=384, bias=True): 1, LayerNorm((384,), eps=1e-05, elementwise_affine=True): 1, WindowMSA(
  (qkv): Linear(in_features=384, out_features=1152, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=384, out_features=384, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
): 1, LayerNorm((384,), eps=1e-05, elementwise_affine=True): 1, Linear(in_features=384, out_features=1536, bias=True): 1, Linear(in_features=1536, out_features=384, bias=True): 1, LayerNorm((384,), eps=1e-05, elementwise_affine=True): 1, WindowMSA(
  (qkv): Linear(in_features=384, out_features=1152, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=384, out_features=384, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
): 1, LayerNorm((384,), eps=1e-05, elementwise_affine=True): 1, Linear(in_features=384, out_features=1536, bias=True): 1, Linear(in_features=1536, out_features=384, bias=True): 1, LayerNorm((384,), eps=1e-05, elementwise_affine=True): 1, WindowMSA(
  (qkv): Linear(in_features=384, out_features=1152, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=384, out_features=384, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
): 1, LayerNorm((384,), eps=1e-05, elementwise_affine=True): 1, Linear(in_features=384, out_features=1536, bias=True): 1, Linear(in_features=1536, out_features=384, bias=True): 1, LayerNorm((384,), eps=1e-05, elementwise_affine=True): 1, WindowMSA(
  (qkv): Linear(in_features=384, out_features=1152, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=384, out_features=384, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
): 1, LayerNorm((384,), eps=1e-05, elementwise_affine=True): 1, Linear(in_features=384, out_features=1536, bias=True): 1, Linear(in_features=1536, out_features=384, bias=True): 1, PatchMerging(
  (adap_padding): AdaptivePadding()
  (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
  (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (reduction): Linear(in_features=1536, out_features=768, bias=False)
): 1, LayerNorm((384,), eps=1e-05, elementwise_affine=True): 1, LayerNorm((768,), eps=1e-05, elementwise_affine=True): 1, WindowMSA(
  (qkv): Linear(in_features=768, out_features=2304, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=768, out_features=768, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
): 1, LayerNorm((768,), eps=1e-05, elementwise_affine=True): 1, Linear(in_features=768, out_features=3072, bias=True): 1, Linear(in_features=3072, out_features=768, bias=True): 1, LayerNorm((768,), eps=1e-05, elementwise_affine=True): 1, WindowMSA(
  (qkv): Linear(in_features=768, out_features=2304, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=768, out_features=768, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
): 1, LayerNorm((768,), eps=1e-05, elementwise_affine=True): 1, Linear(in_features=768, out_features=3072, bias=True): 1, Linear(in_features=3072, out_features=768, bias=True): 1, LayerNorm((768,), eps=1e-05, elementwise_affine=True): 1, Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1)): 1, GroupNorm(32, 256, eps=1e-05, affine=True): 1, Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1)): 1, GroupNorm(32, 256, eps=1e-05, affine=True): 1, Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1)): 1, GroupNorm(32, 256, eps=1e-05, affine=True): 1, Conv2d(768, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)): 1, GroupNorm(32, 256, eps=1e-05, affine=True): 1}


reused =  [Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))]

flattened_output =  <class 'torch.Tensor'>
flattened_output len =  4
flattened_output shapes =  [torch.Size([1, 256, 100, 167]), torch.Size([1, 256, 50, 84]), torch.Size([1, 256, 25, 42]), torch.Size([1, 256, 13, 21])]
grad_fn =  [<NativeGroupNormBackward0 object at 0x7f1083ed2eb0>, <NativeGroupNormBackward0 object at 0x7f1083ed2e20>, <NativeGroupNormBackward0 object at 0x7f1083ed2d60>, <NativeGroupNormBackward0 object at 0x7f1083ed2e80>]
post forward - gradfn2module =  77 dict_keys([<SqueezeBackward1 object at 0x7f1083e84dc0>, <ConvolutionBackward0 object at 0x7f1083e84fd0>, <NativeLayerNormBackward0 object at 0x7f1083efb5b0>, <NativeLayerNormBackward0 object at 0x7f1083e8b100>, <UnsafeViewBackward0 object at 0x7f1094980400>, <NativeLayerNormBackward0 object at 0x7f1094980460>, <ViewBackward0 object at 0x7f1094980640>, <ViewBackward0 object at 0x7f10949804c0>, <NativeLayerNormBackward0 object at 0x7f1094980160>, <UnsafeViewBackward0 object at 0x7f10949805b0>, <NativeLayerNormBackward0 object at 0x7f1094980700>, <ViewBackward0 object at 0x7f10949805e0>, <ViewBackward0 object at 0x7f10949806d0>, <UnsafeViewBackward0 object at 0x7f1083ea1b20>, <NativeLayerNormBackward0 object at 0x7f1083edb2e0>, <UnsafeViewBackward0 object at 0x7f1083edb220>, <NativeLayerNormBackward0 object at 0x7f1083edb4c0>, <ViewBackward0 object at 0x7f1083edb400>, <ViewBackward0 object at 0x7f1083edb430>, <NativeLayerNormBackward0 object at 0x7f1083edbeb0>, <UnsafeViewBackward0 object at 0x7f1083edb580>, <NativeLayerNormBackward0 object at 0x7f1083edb5b0>, <ViewBackward0 object at 0x7f1083edb370>, <ViewBackward0 object at 0x7f1083edb4f0>, <UnsafeViewBackward0 object at 0x7f10949fd100>, <NativeLayerNormBackward0 object at 0x7f10949803d0>, <NativeLayerNormBackward0 object at 0x7f1083edb730>, <UnsafeViewBackward0 object at 0x7f1083edb7c0>, <NativeLayerNormBackward0 object at 0x7f1083edba30>, <ViewBackward0 object at 0x7f1083edb8b0>, <ViewBackward0 object at 0x7f1083edbb20>, <NativeLayerNormBackward0 object at 0x7f1083edb880>, <UnsafeViewBackward0 object at 0x7f1083edb970>, <NativeLayerNormBackward0 object at 0x7f1083edba00>, <ViewBackward0 object at 0x7f1083edbc40>, <ViewBackward0 object at 0x7f1083edba60>, <NativeLayerNormBackward0 object at 0x7f1083edba90>, <UnsafeViewBackward0 object at 0x7f1083edbbb0>, <NativeLayerNormBackward0 object at 0x7f1083edbb50>, <ViewBackward0 object at 0x7f1083edbb80>, <ViewBackward0 object at 0x7f1083edbc70>, <NativeLayerNormBackward0 object at 0x7f1083edbca0>, <UnsafeViewBackward0 object at 0x7f1083edb700>, <NativeLayerNormBackward0 object at 0x7f1083edb640>, <ViewBackward0 object at 0x7f1083edbd60>, <ViewBackward0 object at 0x7f1083edbe80>, <NativeLayerNormBackward0 object at 0x7f1083edbdc0>, <UnsafeViewBackward0 object at 0x7f1083edbdf0>, <NativeLayerNormBackward0 object at 0x7f1083edbfa0>, <ViewBackward0 object at 0x7f1083edbee0>, <ViewBackward0 object at 0x7f1083edbd90>, <NativeLayerNormBackward0 object at 0x7f1083edbf10>, <UnsafeViewBackward0 object at 0x7f1083edbcd0>, <NativeLayerNormBackward0 object at 0x7f1083edbfd0>, <ViewBackward0 object at 0x7f1083edb340>, <ViewBackward0 object at 0x7f1083edb310>, <UnsafeViewBackward0 object at 0x7f1083edb670>, <NativeLayerNormBackward0 object at 0x7f1083edb940>, <NativeLayerNormBackward0 object at 0x7f1083edb7f0>, <UnsafeViewBackward0 object at 0x7f1083edb1c0>, <NativeLayerNormBackward0 object at 0x7f1083edb130>, <ViewBackward0 object at 0x7f1083edb100>, <ViewBackward0 object at 0x7f1083edb0d0>, <NativeLayerNormBackward0 object at 0x7f1083edb0a0>, <UnsafeViewBackward0 object at 0x7f1083ed2fd0>, <NativeLayerNormBackward0 object at 0x7f1083ed2fa0>, <ViewBackward0 object at 0x7f1083ed2f70>, <ViewBackward0 object at 0x7f1083ed2f40>, <NativeLayerNormBackward0 object at 0x7f1083edb250>, <ConvolutionBackward0 object at 0x7f1083ed2e50>, <NativeGroupNormBackward0 object at 0x7f1083ed2eb0>, <ConvolutionBackward0 object at 0x7f1083ed2d90>, <NativeGroupNormBackward0 object at 0x7f1083ed2e20>, <ConvolutionBackward0 object at 0x7f1083ed2d00>, <NativeGroupNormBackward0 object at 0x7f1083ed2d60>, <ConvolutionBackward0 object at 0x7f1083ed2d30>, <NativeGroupNormBackward0 object at 0x7f1083ed2e80>])

 module2node ==  dict_values([<Node: (neck.convs.0.gn (GroupNorm(32, 256, eps=1e-05, affine=True)))>, <Node: (neck.convs.0.conv (Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1))))>, <Node: (_ElementWiseOp_0(CloneBackward0))>, <Node: (_ElementWiseOp_1(PermuteBackward0))>, <Node: (_Reshape_2())>, <Node: (backbone.norm1 (LayerNorm((192,), eps=1e-05, elementwise_affine=True)))>, <Node: (_ElementWiseOp_3(AddBackward0))>, <Node: (_ElementWiseOp_4(AddBackward0))>, <Node: (backbone.stages.1.blocks.1.ffn.layers.1 (Linear(in_features=768, out_features=192, bias=True)))>, <Node: (_ElementWiseOp_5(AddmmBackward0))>, <Node: (_Reshape_6())>, <Node: (_ElementWiseOp_7(TBackward0))>, <Node: (_ElementWiseOp_8(GeluBackward0))>, <Node: (backbone.stages.1.blocks.1.ffn.layers.0.0 (Linear(in_features=192, out_features=768, bias=True)))>, <Node: (_ElementWiseOp_9(AddmmBackward0))>, <Node: (_Reshape_10())>, <Node: (_ElementWiseOp_11(TBackward0))>, <Node: (backbone.stages.1.blocks.1.norm2 (LayerNorm((192,), eps=1e-05, elementwise_affine=True)))>, <Node: (_Reshape_12())>, <Node: (_ElementWiseOp_13(AddBackward0))>, <Node: (_ElementWiseOp_14(AddBackward0))>, <Node: (backbone.stages.1.blocks.0.ffn.layers.1 (Linear(in_features=768, out_features=192, bias=True)))>, <Node: (_ElementWiseOp_15(AddmmBackward0))>, <Node: (_Reshape_16())>, <Node: (_ElementWiseOp_17(TBackward0))>, <Node: (_ElementWiseOp_18(GeluBackward0))>, <Node: (backbone.stages.1.blocks.0.ffn.layers.0.0 (Linear(in_features=192, out_features=768, bias=True)))>, <Node: (_ElementWiseOp_19(AddmmBackward0))>, <Node: (_Reshape_20())>, <Node: (_ElementWiseOp_21(TBackward0))>, <Node: (backbone.stages.1.blocks.0.norm2 (LayerNorm((192,), eps=1e-05, elementwise_affine=True)))>, <Node: (_Reshape_22())>, <Node: (backbone.stages.0.downsample (PatchMerging(
  (adap_padding): AdaptivePadding()
  (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
  (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (reduction): Linear(in_features=384, out_features=192, bias=False)
)))>, <Node: (_ElementWiseOp_23(MmBackward0))>, <Node: (_Reshape_24())>, <Node: (_ElementWiseOp_25(TBackward0))>, <Node: (_ElementWiseOp_26(NativeLayerNormBackward0))>, <Node: (_ElementWiseOp_27(TransposeBackward0))>, <Node: (_ElementWiseOp_28(Im2ColBackward0))>, <Node: (_ElementWiseOp_29(PermuteBackward0))>, <Node: (_Reshape_30())>, <Node: (_ElementWiseOp_31(AddBackward0))>, <Node: (_ElementWiseOp_32(AddBackward0))>, <Node: (backbone.stages.0.blocks.1.ffn.layers.1 (Linear(in_features=384, out_features=96, bias=True)))>, <Node: (_ElementWiseOp_33(AddmmBackward0))>, <Node: (_Reshape_34())>, <Node: (_ElementWiseOp_35(TBackward0))>, <Node: (_ElementWiseOp_36(GeluBackward0))>, <Node: (backbone.stages.0.blocks.1.ffn.layers.0.0 (Linear(in_features=96, out_features=384, bias=True)))>, <Node: (_ElementWiseOp_37(AddmmBackward0))>, <Node: (_Reshape_38())>, <Node: (_ElementWiseOp_39(TBackward0))>, <Node: (backbone.stages.0.blocks.1.norm2 (LayerNorm((96,), eps=1e-05, elementwise_affine=True)))>, <Node: (_Reshape_40())>, <Node: (_ElementWiseOp_41(AddBackward0))>, <Node: (_ElementWiseOp_42(AddBackward0))>, <Node: (backbone.stages.0.blocks.0.ffn.layers.1 (Linear(in_features=384, out_features=96, bias=True)))>, <Node: (_ElementWiseOp_43(AddmmBackward0))>, <Node: (_Reshape_44())>, <Node: (_ElementWiseOp_45(TBackward0))>, <Node: (_ElementWiseOp_46(GeluBackward0))>, <Node: (backbone.stages.0.blocks.0.ffn.layers.0.0 (Linear(in_features=96, out_features=384, bias=True)))>, <Node: (_ElementWiseOp_47(AddmmBackward0))>, <Node: (_Reshape_48())>, <Node: (_ElementWiseOp_49(TBackward0))>, <Node: (backbone.stages.0.blocks.0.norm2 (LayerNorm((96,), eps=1e-05, elementwise_affine=True)))>, <Node: (_Reshape_50())>, <Node: (backbone.patch_embed.norm (LayerNorm((96,), eps=1e-05, elementwise_affine=True)))>, <Node: (_ElementWiseOp_51(TransposeBackward0))>, <Node: (_Reshape_52())>, <Node: (backbone.patch_embed.projection (Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))))>, <Node: (_ElementWiseOp_53(CloneBackward0))>, <Node: (_ElementWiseOp_54(SliceBackward0))>, <Node: (_ElementWiseOp_55(SliceBackward0))>, <Node: (_ElementWiseOp_56(SliceBackward0))>, <Node: (_ElementWiseOp_57(SliceBackward0))>, <Node: (_Reshape_58())>, <Node: (_ElementWiseOp_59(CloneBackward0))>, <Node: (_ElementWiseOp_60(PermuteBackward0))>, <Node: (_Reshape_61())>, <Node: (_Reshape_62())>, <Node: (backbone.stages.0.blocks.0.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=96, out_features=288, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=96, out_features=96, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)))>, <Node: (_ElementWiseOp_63(CloneBackward0))>, <Node: (_ElementWiseOp_64(TransposeBackward0))>, <Node: (_Reshape_65())>, <Node: (_ElementWiseOp_66(BmmBackward0))>, <Node: (_Reshape_67())>, <Node: (_Reshape_68())>, <Node: (_ElementWiseOp_69(CloneBackward0))>, <Node: (_ElementWiseOp_70(ExpandBackward0))>, <Node: (_ElementWiseOp_71(SelectBackward0))>, <Node: (_ElementWiseOp_72(PermuteBackward0))>, <Node: (_Reshape_73())>, <Node: (_Reshape_74())>, <Node: (_ElementWiseOp_75(AddmmBackward0))>, <Node: (_Reshape_76())>, <Node: (_ElementWiseOp_77(TBackward0))>, <Node: (_Reshape_78())>, <Node: (_Reshape_79())>, <Node: (_ElementWiseOp_80(CloneBackward0))>, <Node: (_ElementWiseOp_81(PermuteBackward0))>, <Node: (_Reshape_82())>, <Node: (_ElementWiseOp_83(ConstantPadNdBackward0))>, <Node: (_Reshape_84())>, <Node: (backbone.stages.0.blocks.0.norm1 (LayerNorm((96,), eps=1e-05, elementwise_affine=True)))>, <Node: (_ElementWiseOp_85(ExpandBackward0))>, <Node: (_ElementWiseOp_86(SoftmaxBackward0))>, <Node: (_ElementWiseOp_87(AddBackward0))>, <Node: (_Reshape_88())>, <Node: (_ElementWiseOp_89(UnsqueezeBackward0))>, <Node: (_ElementWiseOp_90(CloneBackward0))>, <Node: (_ElementWiseOp_91(PermuteBackward0))>, <Node: (_Reshape_92())>, <Node: (_ElementWiseOp_93(IndexBackward0))>, <Node: (_ElementWiseOp_94(BmmBackward0))>, <Node: (_Reshape_95())>, <Node: (_Reshape_96())>, <Node: (_ElementWiseOp_97(CloneBackward0))>, <Node: (_ElementWiseOp_98(ExpandBackward0))>, <Node: (_ElementWiseOp_99(TransposeBackward0))>, <Node: (_ElementWiseOp_100(SelectBackward0))>, <Node: (_ElementWiseOp_101(CloneBackward0))>, <Node: (_ElementWiseOp_102(ExpandBackward0))>, <Node: (_ElementWiseOp_103(MulBackward0))>, <Node: (_ElementWiseOp_104(SelectBackward0))>, <Node: (_ElementWiseOp_105(CloneBackward0))>, <Node: (_ElementWiseOp_106(SliceBackward0))>, <Node: (_ElementWiseOp_107(SliceBackward0))>, <Node: (_ElementWiseOp_108(SliceBackward0))>, <Node: (_ElementWiseOp_109(SliceBackward0))>, <Node: (_ElementWiseOp_110(RollBackward0))>, <Node: (_Reshape_111())>, <Node: (_ElementWiseOp_112(CloneBackward0))>, <Node: (_ElementWiseOp_113(PermuteBackward0))>, <Node: (_Reshape_114())>, <Node: (_Reshape_115())>, <Node: (backbone.stages.0.blocks.1.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=96, out_features=288, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=96, out_features=96, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)))>, <Node: (_ElementWiseOp_116(CloneBackward0))>, <Node: (_ElementWiseOp_117(TransposeBackward0))>, <Node: (_Reshape_118())>, <Node: (_ElementWiseOp_119(BmmBackward0))>, <Node: (_Reshape_120())>, <Node: (_Reshape_121())>, <Node: (_ElementWiseOp_122(CloneBackward0))>, <Node: (_ElementWiseOp_123(ExpandBackward0))>, <Node: (_ElementWiseOp_124(SelectBackward0))>, <Node: (_ElementWiseOp_125(PermuteBackward0))>, <Node: (_Reshape_126())>, <Node: (_Reshape_127())>, <Node: (_ElementWiseOp_128(AddmmBackward0))>, <Node: (_Reshape_129())>, <Node: (_ElementWiseOp_130(TBackward0))>, <Node: (_Reshape_131())>, <Node: (_Reshape_132())>, <Node: (_ElementWiseOp_133(CloneBackward0))>, <Node: (_ElementWiseOp_134(PermuteBackward0))>, <Node: (_Reshape_135())>, <Node: (_ElementWiseOp_136(RollBackward0))>, <Node: (_ElementWiseOp_137(ConstantPadNdBackward0))>, <Node: (_Reshape_138())>, <Node: (backbone.stages.0.blocks.1.norm1 (LayerNorm((96,), eps=1e-05, elementwise_affine=True)))>, <Node: (_ElementWiseOp_139(ExpandBackward0))>, <Node: (_ElementWiseOp_140(SoftmaxBackward0))>, <Node: (_Reshape_141())>, <Node: (_ElementWiseOp_142(AddBackward0))>, <Node: (_Reshape_143())>, <Node: (_ElementWiseOp_144(AddBackward0))>, <Node: (_Reshape_145())>, <Node: (_ElementWiseOp_146(UnsqueezeBackward0))>, <Node: (_ElementWiseOp_147(CloneBackward0))>, <Node: (_ElementWiseOp_148(PermuteBackward0))>, <Node: (_Reshape_149())>, <Node: (_ElementWiseOp_150(IndexBackward0))>, <Node: (_ElementWiseOp_151(BmmBackward0))>, <Node: (_Reshape_152())>, <Node: (_Reshape_153())>, <Node: (_ElementWiseOp_154(CloneBackward0))>, <Node: (_ElementWiseOp_155(ExpandBackward0))>, <Node: (_ElementWiseOp_156(TransposeBackward0))>, <Node: (_ElementWiseOp_157(SelectBackward0))>, <Node: (_ElementWiseOp_158(CloneBackward0))>, <Node: (_ElementWiseOp_159(ExpandBackward0))>, <Node: (_ElementWiseOp_160(MulBackward0))>, <Node: (_ElementWiseOp_161(SelectBackward0))>, <Node: (_ElementWiseOp_162(CloneBackward0))>, <Node: (_ElementWiseOp_163(SliceBackward0))>, <Node: (_ElementWiseOp_164(SliceBackward0))>, <Node: (_ElementWiseOp_165(SliceBackward0))>, <Node: (_ElementWiseOp_166(SliceBackward0))>, <Node: (_Reshape_167())>, <Node: (_ElementWiseOp_168(CloneBackward0))>, <Node: (_ElementWiseOp_169(PermuteBackward0))>, <Node: (_Reshape_170())>, <Node: (_Reshape_171())>, <Node: (backbone.stages.1.blocks.0.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=192, out_features=576, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=192, out_features=192, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)))>, <Node: (_ElementWiseOp_172(CloneBackward0))>, <Node: (_ElementWiseOp_173(TransposeBackward0))>, <Node: (_Reshape_174())>, <Node: (_ElementWiseOp_175(BmmBackward0))>, <Node: (_Reshape_176())>, <Node: (_Reshape_177())>, <Node: (_ElementWiseOp_178(CloneBackward0))>, <Node: (_ElementWiseOp_179(ExpandBackward0))>, <Node: (_ElementWiseOp_180(SelectBackward0))>, <Node: (_ElementWiseOp_181(PermuteBackward0))>, <Node: (_Reshape_182())>, <Node: (_Reshape_183())>, <Node: (_ElementWiseOp_184(AddmmBackward0))>, <Node: (_Reshape_185())>, <Node: (_ElementWiseOp_186(TBackward0))>, <Node: (_Reshape_187())>, <Node: (_Reshape_188())>, <Node: (_ElementWiseOp_189(CloneBackward0))>, <Node: (_ElementWiseOp_190(PermuteBackward0))>, <Node: (_Reshape_191())>, <Node: (_ElementWiseOp_192(ConstantPadNdBackward0))>, <Node: (_Reshape_193())>, <Node: (backbone.stages.1.blocks.0.norm1 (LayerNorm((192,), eps=1e-05, elementwise_affine=True)))>, <Node: (_ElementWiseOp_194(ExpandBackward0))>, <Node: (_ElementWiseOp_195(SoftmaxBackward0))>, <Node: (_ElementWiseOp_196(AddBackward0))>, <Node: (_Reshape_197())>, <Node: (_ElementWiseOp_198(UnsqueezeBackward0))>, <Node: (_ElementWiseOp_199(CloneBackward0))>, <Node: (_ElementWiseOp_200(PermuteBackward0))>, <Node: (_Reshape_201())>, <Node: (_ElementWiseOp_202(IndexBackward0))>, <Node: (_ElementWiseOp_203(BmmBackward0))>, <Node: (_Reshape_204())>, <Node: (_Reshape_205())>, <Node: (_ElementWiseOp_206(CloneBackward0))>, <Node: (_ElementWiseOp_207(ExpandBackward0))>, <Node: (_ElementWiseOp_208(TransposeBackward0))>, <Node: (_ElementWiseOp_209(SelectBackward0))>, <Node: (_ElementWiseOp_210(CloneBackward0))>, <Node: (_ElementWiseOp_211(ExpandBackward0))>, <Node: (_ElementWiseOp_212(MulBackward0))>, <Node: (_ElementWiseOp_213(SelectBackward0))>, <Node: (_ElementWiseOp_214(CloneBackward0))>, <Node: (_ElementWiseOp_215(SliceBackward0))>, <Node: (_ElementWiseOp_216(SliceBackward0))>, <Node: (_ElementWiseOp_217(SliceBackward0))>, <Node: (_ElementWiseOp_218(SliceBackward0))>, <Node: (_ElementWiseOp_219(RollBackward0))>, <Node: (_Reshape_220())>, <Node: (_ElementWiseOp_221(CloneBackward0))>, <Node: (_ElementWiseOp_222(PermuteBackward0))>, <Node: (_Reshape_223())>, <Node: (_Reshape_224())>, <Node: (backbone.stages.1.blocks.1.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=192, out_features=576, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=192, out_features=192, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)))>, <Node: (_ElementWiseOp_225(CloneBackward0))>, <Node: (_ElementWiseOp_226(TransposeBackward0))>, <Node: (_Reshape_227())>, <Node: (_ElementWiseOp_228(BmmBackward0))>, <Node: (_Reshape_229())>, <Node: (_Reshape_230())>, <Node: (_ElementWiseOp_231(CloneBackward0))>, <Node: (_ElementWiseOp_232(ExpandBackward0))>, <Node: (_ElementWiseOp_233(SelectBackward0))>, <Node: (_ElementWiseOp_234(PermuteBackward0))>, <Node: (_Reshape_235())>, <Node: (_Reshape_236())>, <Node: (_ElementWiseOp_237(AddmmBackward0))>, <Node: (_Reshape_238())>, <Node: (_ElementWiseOp_239(TBackward0))>, <Node: (_Reshape_240())>, <Node: (_Reshape_241())>, <Node: (_ElementWiseOp_242(CloneBackward0))>, <Node: (_ElementWiseOp_243(PermuteBackward0))>, <Node: (_Reshape_244())>, <Node: (_ElementWiseOp_245(RollBackward0))>, <Node: (_ElementWiseOp_246(ConstantPadNdBackward0))>, <Node: (_Reshape_247())>, <Node: (backbone.stages.1.blocks.1.norm1 (LayerNorm((192,), eps=1e-05, elementwise_affine=True)))>, <Node: (_ElementWiseOp_248(ExpandBackward0))>, <Node: (_ElementWiseOp_249(SoftmaxBackward0))>, <Node: (_Reshape_250())>, <Node: (_ElementWiseOp_251(AddBackward0))>, <Node: (_Reshape_252())>, <Node: (_ElementWiseOp_253(AddBackward0))>, <Node: (_Reshape_254())>, <Node: (_ElementWiseOp_255(UnsqueezeBackward0))>, <Node: (_ElementWiseOp_256(CloneBackward0))>, <Node: (_ElementWiseOp_257(PermuteBackward0))>, <Node: (_Reshape_258())>, <Node: (_ElementWiseOp_259(IndexBackward0))>, <Node: (_ElementWiseOp_260(BmmBackward0))>, <Node: (_Reshape_261())>, <Node: (_Reshape_262())>, <Node: (_ElementWiseOp_263(CloneBackward0))>, <Node: (_ElementWiseOp_264(ExpandBackward0))>, <Node: (_ElementWiseOp_265(TransposeBackward0))>, <Node: (_ElementWiseOp_266(SelectBackward0))>, <Node: (_ElementWiseOp_267(CloneBackward0))>, <Node: (_ElementWiseOp_268(ExpandBackward0))>, <Node: (_ElementWiseOp_269(MulBackward0))>, <Node: (_ElementWiseOp_270(SelectBackward0))>, <Node: (neck.convs.1.gn (GroupNorm(32, 256, eps=1e-05, affine=True)))>, <Node: (neck.convs.1.conv (Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))))>, <Node: (_ElementWiseOp_271(CloneBackward0))>, <Node: (_ElementWiseOp_272(PermuteBackward0))>, <Node: (_Reshape_273())>, <Node: (backbone.norm2 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)))>, <Node: (_ElementWiseOp_274(AddBackward0))>, <Node: (_ElementWiseOp_275(AddBackward0))>, <Node: (backbone.stages.2.blocks.5.ffn.layers.1 (Linear(in_features=1536, out_features=384, bias=True)))>, <Node: (_ElementWiseOp_276(AddmmBackward0))>, <Node: (_Reshape_277())>, <Node: (_ElementWiseOp_278(TBackward0))>, <Node: (_ElementWiseOp_279(GeluBackward0))>, <Node: (backbone.stages.2.blocks.5.ffn.layers.0.0 (Linear(in_features=384, out_features=1536, bias=True)))>, <Node: (_ElementWiseOp_280(AddmmBackward0))>, <Node: (_Reshape_281())>, <Node: (_ElementWiseOp_282(TBackward0))>, <Node: (backbone.stages.2.blocks.5.norm2 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)))>, <Node: (_Reshape_283())>, <Node: (_ElementWiseOp_284(AddBackward0))>, <Node: (_ElementWiseOp_285(AddBackward0))>, <Node: (backbone.stages.2.blocks.4.ffn.layers.1 (Linear(in_features=1536, out_features=384, bias=True)))>, <Node: (_ElementWiseOp_286(AddmmBackward0))>, <Node: (_Reshape_287())>, <Node: (_ElementWiseOp_288(TBackward0))>, <Node: (_ElementWiseOp_289(GeluBackward0))>, <Node: (backbone.stages.2.blocks.4.ffn.layers.0.0 (Linear(in_features=384, out_features=1536, bias=True)))>, <Node: (_ElementWiseOp_290(AddmmBackward0))>, <Node: (_Reshape_291())>, <Node: (_ElementWiseOp_292(TBackward0))>, <Node: (backbone.stages.2.blocks.4.norm2 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)))>, <Node: (_Reshape_293())>, <Node: (_ElementWiseOp_294(AddBackward0))>, <Node: (_ElementWiseOp_295(AddBackward0))>, <Node: (backbone.stages.2.blocks.3.ffn.layers.1 (Linear(in_features=1536, out_features=384, bias=True)))>, <Node: (_ElementWiseOp_296(AddmmBackward0))>, <Node: (_Reshape_297())>, <Node: (_ElementWiseOp_298(TBackward0))>, <Node: (_ElementWiseOp_299(GeluBackward0))>, <Node: (backbone.stages.2.blocks.3.ffn.layers.0.0 (Linear(in_features=384, out_features=1536, bias=True)))>, <Node: (_ElementWiseOp_300(AddmmBackward0))>, <Node: (_Reshape_301())>, <Node: (_ElementWiseOp_302(TBackward0))>, <Node: (backbone.stages.2.blocks.3.norm2 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)))>, <Node: (_Reshape_303())>, <Node: (_ElementWiseOp_304(AddBackward0))>, <Node: (_ElementWiseOp_305(AddBackward0))>, <Node: (backbone.stages.2.blocks.2.ffn.layers.1 (Linear(in_features=1536, out_features=384, bias=True)))>, <Node: (_ElementWiseOp_306(AddmmBackward0))>, <Node: (_Reshape_307())>, <Node: (_ElementWiseOp_308(TBackward0))>, <Node: (_ElementWiseOp_309(GeluBackward0))>, <Node: (backbone.stages.2.blocks.2.ffn.layers.0.0 (Linear(in_features=384, out_features=1536, bias=True)))>, <Node: (_ElementWiseOp_310(AddmmBackward0))>, <Node: (_Reshape_311())>, <Node: (_ElementWiseOp_312(TBackward0))>, <Node: (backbone.stages.2.blocks.2.norm2 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)))>, <Node: (_Reshape_313())>, <Node: (_ElementWiseOp_314(AddBackward0))>, <Node: (_ElementWiseOp_315(AddBackward0))>, <Node: (backbone.stages.2.blocks.1.ffn.layers.1 (Linear(in_features=1536, out_features=384, bias=True)))>, <Node: (_ElementWiseOp_316(AddmmBackward0))>, <Node: (_Reshape_317())>, <Node: (_ElementWiseOp_318(TBackward0))>, <Node: (_ElementWiseOp_319(GeluBackward0))>, <Node: (backbone.stages.2.blocks.1.ffn.layers.0.0 (Linear(in_features=384, out_features=1536, bias=True)))>, <Node: (_ElementWiseOp_320(AddmmBackward0))>, <Node: (_Reshape_321())>, <Node: (_ElementWiseOp_322(TBackward0))>, <Node: (backbone.stages.2.blocks.1.norm2 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)))>, <Node: (_Reshape_323())>, <Node: (_ElementWiseOp_324(AddBackward0))>, <Node: (_ElementWiseOp_325(AddBackward0))>, <Node: (backbone.stages.2.blocks.0.ffn.layers.1 (Linear(in_features=1536, out_features=384, bias=True)))>, <Node: (_ElementWiseOp_326(AddmmBackward0))>, <Node: (_Reshape_327())>, <Node: (_ElementWiseOp_328(TBackward0))>, <Node: (_ElementWiseOp_329(GeluBackward0))>, <Node: (backbone.stages.2.blocks.0.ffn.layers.0.0 (Linear(in_features=384, out_features=1536, bias=True)))>, <Node: (_ElementWiseOp_330(AddmmBackward0))>, <Node: (_Reshape_331())>, <Node: (_ElementWiseOp_332(TBackward0))>, <Node: (backbone.stages.2.blocks.0.norm2 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)))>, <Node: (_Reshape_333())>, <Node: (backbone.stages.1.downsample (PatchMerging(
  (adap_padding): AdaptivePadding()
  (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (reduction): Linear(in_features=768, out_features=384, bias=False)
)))>, <Node: (_ElementWiseOp_334(MmBackward0))>, <Node: (_Reshape_335())>, <Node: (_ElementWiseOp_336(TBackward0))>, <Node: (_ElementWiseOp_337(NativeLayerNormBackward0))>, <Node: (_ElementWiseOp_338(TransposeBackward0))>, <Node: (_ElementWiseOp_339(Im2ColBackward0))>, <Node: (_ElementWiseOp_340(ConstantPadNdBackward0))>, <Node: (_ElementWiseOp_341(PermuteBackward0))>, <Node: (_Reshape_342())>, <Node: (_ElementWiseOp_343(SliceBackward0))>, <Node: (_ElementWiseOp_344(SliceBackward0))>, <Node: (_ElementWiseOp_345(SliceBackward0))>, <Node: (_Reshape_346())>, <Node: (_ElementWiseOp_347(CloneBackward0))>, <Node: (_ElementWiseOp_348(PermuteBackward0))>, <Node: (_Reshape_349())>, <Node: (_Reshape_350())>, <Node: (backbone.stages.2.blocks.0.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=384, out_features=1152, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=384, out_features=384, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)))>, <Node: (_ElementWiseOp_351(CloneBackward0))>, <Node: (_ElementWiseOp_352(TransposeBackward0))>, <Node: (_Reshape_353())>, <Node: (_ElementWiseOp_354(BmmBackward0))>, <Node: (_Reshape_355())>, <Node: (_Reshape_356())>, <Node: (_ElementWiseOp_357(CloneBackward0))>, <Node: (_ElementWiseOp_358(ExpandBackward0))>, <Node: (_ElementWiseOp_359(SelectBackward0))>, <Node: (_ElementWiseOp_360(PermuteBackward0))>, <Node: (_Reshape_361())>, <Node: (_Reshape_362())>, <Node: (_ElementWiseOp_363(AddmmBackward0))>, <Node: (_Reshape_364())>, <Node: (_ElementWiseOp_365(TBackward0))>, <Node: (_Reshape_366())>, <Node: (_Reshape_367())>, <Node: (_ElementWiseOp_368(CloneBackward0))>, <Node: (_ElementWiseOp_369(PermuteBackward0))>, <Node: (_Reshape_370())>, <Node: (_ElementWiseOp_371(ConstantPadNdBackward0))>, <Node: (_Reshape_372())>, <Node: (backbone.stages.2.blocks.0.norm1 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)))>, <Node: (_ElementWiseOp_373(ExpandBackward0))>, <Node: (_ElementWiseOp_374(SoftmaxBackward0))>, <Node: (_ElementWiseOp_375(AddBackward0))>, <Node: (_Reshape_376())>, <Node: (_ElementWiseOp_377(UnsqueezeBackward0))>, <Node: (_ElementWiseOp_378(CloneBackward0))>, <Node: (_ElementWiseOp_379(PermuteBackward0))>, <Node: (_Reshape_380())>, <Node: (_ElementWiseOp_381(IndexBackward0))>, <Node: (_ElementWiseOp_382(BmmBackward0))>, <Node: (_Reshape_383())>, <Node: (_Reshape_384())>, <Node: (_ElementWiseOp_385(CloneBackward0))>, <Node: (_ElementWiseOp_386(ExpandBackward0))>, <Node: (_ElementWiseOp_387(TransposeBackward0))>, <Node: (_ElementWiseOp_388(SelectBackward0))>, <Node: (_ElementWiseOp_389(CloneBackward0))>, <Node: (_ElementWiseOp_390(ExpandBackward0))>, <Node: (_ElementWiseOp_391(MulBackward0))>, <Node: (_ElementWiseOp_392(SelectBackward0))>, <Node: (_ElementWiseOp_393(SliceBackward0))>, <Node: (_ElementWiseOp_394(SliceBackward0))>, <Node: (_ElementWiseOp_395(SliceBackward0))>, <Node: (_ElementWiseOp_396(RollBackward0))>, <Node: (_Reshape_397())>, <Node: (_ElementWiseOp_398(CloneBackward0))>, <Node: (_ElementWiseOp_399(PermuteBackward0))>, <Node: (_Reshape_400())>, <Node: (_Reshape_401())>, <Node: (backbone.stages.2.blocks.1.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=384, out_features=1152, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=384, out_features=384, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)))>, <Node: (_ElementWiseOp_402(CloneBackward0))>, <Node: (_ElementWiseOp_403(TransposeBackward0))>, <Node: (_Reshape_404())>, <Node: (_ElementWiseOp_405(BmmBackward0))>, <Node: (_Reshape_406())>, <Node: (_Reshape_407())>, <Node: (_ElementWiseOp_408(CloneBackward0))>, <Node: (_ElementWiseOp_409(ExpandBackward0))>, <Node: (_ElementWiseOp_410(SelectBackward0))>, <Node: (_ElementWiseOp_411(PermuteBackward0))>, <Node: (_Reshape_412())>, <Node: (_Reshape_413())>, <Node: (_ElementWiseOp_414(AddmmBackward0))>, <Node: (_Reshape_415())>, <Node: (_ElementWiseOp_416(TBackward0))>, <Node: (_Reshape_417())>, <Node: (_Reshape_418())>, <Node: (_ElementWiseOp_419(CloneBackward0))>, <Node: (_ElementWiseOp_420(PermuteBackward0))>, <Node: (_Reshape_421())>, <Node: (_ElementWiseOp_422(RollBackward0))>, <Node: (_ElementWiseOp_423(ConstantPadNdBackward0))>, <Node: (_Reshape_424())>, <Node: (backbone.stages.2.blocks.1.norm1 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)))>, <Node: (_ElementWiseOp_425(ExpandBackward0))>, <Node: (_ElementWiseOp_426(SoftmaxBackward0))>, <Node: (_Reshape_427())>, <Node: (_ElementWiseOp_428(AddBackward0))>, <Node: (_Reshape_429())>, <Node: (_ElementWiseOp_430(AddBackward0))>, <Node: (_Reshape_431())>, <Node: (_ElementWiseOp_432(UnsqueezeBackward0))>, <Node: (_ElementWiseOp_433(CloneBackward0))>, <Node: (_ElementWiseOp_434(PermuteBackward0))>, <Node: (_Reshape_435())>, <Node: (_ElementWiseOp_436(IndexBackward0))>, <Node: (_ElementWiseOp_437(BmmBackward0))>, <Node: (_Reshape_438())>, <Node: (_Reshape_439())>, <Node: (_ElementWiseOp_440(CloneBackward0))>, <Node: (_ElementWiseOp_441(ExpandBackward0))>, <Node: (_ElementWiseOp_442(TransposeBackward0))>, <Node: (_ElementWiseOp_443(SelectBackward0))>, <Node: (_ElementWiseOp_444(CloneBackward0))>, <Node: (_ElementWiseOp_445(ExpandBackward0))>, <Node: (_ElementWiseOp_446(MulBackward0))>, <Node: (_ElementWiseOp_447(SelectBackward0))>, <Node: (_ElementWiseOp_448(SliceBackward0))>, <Node: (_ElementWiseOp_449(SliceBackward0))>, <Node: (_ElementWiseOp_450(SliceBackward0))>, <Node: (_Reshape_451())>, <Node: (_ElementWiseOp_452(CloneBackward0))>, <Node: (_ElementWiseOp_453(PermuteBackward0))>, <Node: (_Reshape_454())>, <Node: (_Reshape_455())>, <Node: (backbone.stages.2.blocks.2.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=384, out_features=1152, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=384, out_features=384, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)))>, <Node: (_ElementWiseOp_456(CloneBackward0))>, <Node: (_ElementWiseOp_457(TransposeBackward0))>, <Node: (_Reshape_458())>, <Node: (_ElementWiseOp_459(BmmBackward0))>, <Node: (_Reshape_460())>, <Node: (_Reshape_461())>, <Node: (_ElementWiseOp_462(CloneBackward0))>, <Node: (_ElementWiseOp_463(ExpandBackward0))>, <Node: (_ElementWiseOp_464(SelectBackward0))>, <Node: (_ElementWiseOp_465(PermuteBackward0))>, <Node: (_Reshape_466())>, <Node: (_Reshape_467())>, <Node: (_ElementWiseOp_468(AddmmBackward0))>, <Node: (_Reshape_469())>, <Node: (_ElementWiseOp_470(TBackward0))>, <Node: (_Reshape_471())>, <Node: (_Reshape_472())>, <Node: (_ElementWiseOp_473(CloneBackward0))>, <Node: (_ElementWiseOp_474(PermuteBackward0))>, <Node: (_Reshape_475())>, <Node: (_ElementWiseOp_476(ConstantPadNdBackward0))>, <Node: (_Reshape_477())>, <Node: (backbone.stages.2.blocks.2.norm1 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)))>, <Node: (_ElementWiseOp_478(ExpandBackward0))>, <Node: (_ElementWiseOp_479(SoftmaxBackward0))>, <Node: (_ElementWiseOp_480(AddBackward0))>, <Node: (_Reshape_481())>, <Node: (_ElementWiseOp_482(UnsqueezeBackward0))>, <Node: (_ElementWiseOp_483(CloneBackward0))>, <Node: (_ElementWiseOp_484(PermuteBackward0))>, <Node: (_Reshape_485())>, <Node: (_ElementWiseOp_486(IndexBackward0))>, <Node: (_ElementWiseOp_487(BmmBackward0))>, <Node: (_Reshape_488())>, <Node: (_Reshape_489())>, <Node: (_ElementWiseOp_490(CloneBackward0))>, <Node: (_ElementWiseOp_491(ExpandBackward0))>, <Node: (_ElementWiseOp_492(TransposeBackward0))>, <Node: (_ElementWiseOp_493(SelectBackward0))>, <Node: (_ElementWiseOp_494(CloneBackward0))>, <Node: (_ElementWiseOp_495(ExpandBackward0))>, <Node: (_ElementWiseOp_496(MulBackward0))>, <Node: (_ElementWiseOp_497(SelectBackward0))>, <Node: (_ElementWiseOp_498(SliceBackward0))>, <Node: (_ElementWiseOp_499(SliceBackward0))>, <Node: (_ElementWiseOp_500(SliceBackward0))>, <Node: (_ElementWiseOp_501(RollBackward0))>, <Node: (_Reshape_502())>, <Node: (_ElementWiseOp_503(CloneBackward0))>, <Node: (_ElementWiseOp_504(PermuteBackward0))>, <Node: (_Reshape_505())>, <Node: (_Reshape_506())>, <Node: (backbone.stages.2.blocks.3.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=384, out_features=1152, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=384, out_features=384, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)))>, <Node: (_ElementWiseOp_507(CloneBackward0))>, <Node: (_ElementWiseOp_508(TransposeBackward0))>, <Node: (_Reshape_509())>, <Node: (_ElementWiseOp_510(BmmBackward0))>, <Node: (_Reshape_511())>, <Node: (_Reshape_512())>, <Node: (_ElementWiseOp_513(CloneBackward0))>, <Node: (_ElementWiseOp_514(ExpandBackward0))>, <Node: (_ElementWiseOp_515(SelectBackward0))>, <Node: (_ElementWiseOp_516(PermuteBackward0))>, <Node: (_Reshape_517())>, <Node: (_Reshape_518())>, <Node: (_ElementWiseOp_519(AddmmBackward0))>, <Node: (_Reshape_520())>, <Node: (_ElementWiseOp_521(TBackward0))>, <Node: (_Reshape_522())>, <Node: (_Reshape_523())>, <Node: (_ElementWiseOp_524(CloneBackward0))>, <Node: (_ElementWiseOp_525(PermuteBackward0))>, <Node: (_Reshape_526())>, <Node: (_ElementWiseOp_527(RollBackward0))>, <Node: (_ElementWiseOp_528(ConstantPadNdBackward0))>, <Node: (_Reshape_529())>, <Node: (backbone.stages.2.blocks.3.norm1 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)))>, <Node: (_ElementWiseOp_530(ExpandBackward0))>, <Node: (_ElementWiseOp_531(SoftmaxBackward0))>, <Node: (_Reshape_532())>, <Node: (_ElementWiseOp_533(AddBackward0))>, <Node: (_Reshape_534())>, <Node: (_ElementWiseOp_535(AddBackward0))>, <Node: (_Reshape_536())>, <Node: (_ElementWiseOp_537(UnsqueezeBackward0))>, <Node: (_ElementWiseOp_538(CloneBackward0))>, <Node: (_ElementWiseOp_539(PermuteBackward0))>, <Node: (_Reshape_540())>, <Node: (_ElementWiseOp_541(IndexBackward0))>, <Node: (_ElementWiseOp_542(BmmBackward0))>, <Node: (_Reshape_543())>, <Node: (_Reshape_544())>, <Node: (_ElementWiseOp_545(CloneBackward0))>, <Node: (_ElementWiseOp_546(ExpandBackward0))>, <Node: (_ElementWiseOp_547(TransposeBackward0))>, <Node: (_ElementWiseOp_548(SelectBackward0))>, <Node: (_ElementWiseOp_549(CloneBackward0))>, <Node: (_ElementWiseOp_550(ExpandBackward0))>, <Node: (_ElementWiseOp_551(MulBackward0))>, <Node: (_ElementWiseOp_552(SelectBackward0))>, <Node: (_ElementWiseOp_553(SliceBackward0))>, <Node: (_ElementWiseOp_554(SliceBackward0))>, <Node: (_ElementWiseOp_555(SliceBackward0))>, <Node: (_Reshape_556())>, <Node: (_ElementWiseOp_557(CloneBackward0))>, <Node: (_ElementWiseOp_558(PermuteBackward0))>, <Node: (_Reshape_559())>, <Node: (_Reshape_560())>, <Node: (backbone.stages.2.blocks.4.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=384, out_features=1152, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=384, out_features=384, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)))>, <Node: (_ElementWiseOp_561(CloneBackward0))>, <Node: (_ElementWiseOp_562(TransposeBackward0))>, <Node: (_Reshape_563())>, <Node: (_ElementWiseOp_564(BmmBackward0))>, <Node: (_Reshape_565())>, <Node: (_Reshape_566())>, <Node: (_ElementWiseOp_567(CloneBackward0))>, <Node: (_ElementWiseOp_568(ExpandBackward0))>, <Node: (_ElementWiseOp_569(SelectBackward0))>, <Node: (_ElementWiseOp_570(PermuteBackward0))>, <Node: (_Reshape_571())>, <Node: (_Reshape_572())>, <Node: (_ElementWiseOp_573(AddmmBackward0))>, <Node: (_Reshape_574())>, <Node: (_ElementWiseOp_575(TBackward0))>, <Node: (_Reshape_576())>, <Node: (_Reshape_577())>, <Node: (_ElementWiseOp_578(CloneBackward0))>, <Node: (_ElementWiseOp_579(PermuteBackward0))>, <Node: (_Reshape_580())>, <Node: (_ElementWiseOp_581(ConstantPadNdBackward0))>, <Node: (_Reshape_582())>, <Node: (backbone.stages.2.blocks.4.norm1 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)))>, <Node: (_ElementWiseOp_583(ExpandBackward0))>, <Node: (_ElementWiseOp_584(SoftmaxBackward0))>, <Node: (_ElementWiseOp_585(AddBackward0))>, <Node: (_Reshape_586())>, <Node: (_ElementWiseOp_587(UnsqueezeBackward0))>, <Node: (_ElementWiseOp_588(CloneBackward0))>, <Node: (_ElementWiseOp_589(PermuteBackward0))>, <Node: (_Reshape_590())>, <Node: (_ElementWiseOp_591(IndexBackward0))>, <Node: (_ElementWiseOp_592(BmmBackward0))>, <Node: (_Reshape_593())>, <Node: (_Reshape_594())>, <Node: (_ElementWiseOp_595(CloneBackward0))>, <Node: (_ElementWiseOp_596(ExpandBackward0))>, <Node: (_ElementWiseOp_597(TransposeBackward0))>, <Node: (_ElementWiseOp_598(SelectBackward0))>, <Node: (_ElementWiseOp_599(CloneBackward0))>, <Node: (_ElementWiseOp_600(ExpandBackward0))>, <Node: (_ElementWiseOp_601(MulBackward0))>, <Node: (_ElementWiseOp_602(SelectBackward0))>, <Node: (_ElementWiseOp_603(SliceBackward0))>, <Node: (_ElementWiseOp_604(SliceBackward0))>, <Node: (_ElementWiseOp_605(SliceBackward0))>, <Node: (_ElementWiseOp_606(RollBackward0))>, <Node: (_Reshape_607())>, <Node: (_ElementWiseOp_608(CloneBackward0))>, <Node: (_ElementWiseOp_609(PermuteBackward0))>, <Node: (_Reshape_610())>, <Node: (_Reshape_611())>, <Node: (backbone.stages.2.blocks.5.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=384, out_features=1152, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=384, out_features=384, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)))>, <Node: (_ElementWiseOp_612(CloneBackward0))>, <Node: (_ElementWiseOp_613(TransposeBackward0))>, <Node: (_Reshape_614())>, <Node: (_ElementWiseOp_615(BmmBackward0))>, <Node: (_Reshape_616())>, <Node: (_Reshape_617())>, <Node: (_ElementWiseOp_618(CloneBackward0))>, <Node: (_ElementWiseOp_619(ExpandBackward0))>, <Node: (_ElementWiseOp_620(SelectBackward0))>, <Node: (_ElementWiseOp_621(PermuteBackward0))>, <Node: (_Reshape_622())>, <Node: (_Reshape_623())>, <Node: (_ElementWiseOp_624(AddmmBackward0))>, <Node: (_Reshape_625())>, <Node: (_ElementWiseOp_626(TBackward0))>, <Node: (_Reshape_627())>, <Node: (_Reshape_628())>, <Node: (_ElementWiseOp_629(CloneBackward0))>, <Node: (_ElementWiseOp_630(PermuteBackward0))>, <Node: (_Reshape_631())>, <Node: (_ElementWiseOp_632(RollBackward0))>, <Node: (_ElementWiseOp_633(ConstantPadNdBackward0))>, <Node: (_Reshape_634())>, <Node: (backbone.stages.2.blocks.5.norm1 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)))>, <Node: (_ElementWiseOp_635(ExpandBackward0))>, <Node: (_ElementWiseOp_636(SoftmaxBackward0))>, <Node: (_Reshape_637())>, <Node: (_ElementWiseOp_638(AddBackward0))>, <Node: (_Reshape_639())>, <Node: (_ElementWiseOp_640(AddBackward0))>, <Node: (_Reshape_641())>, <Node: (_ElementWiseOp_642(UnsqueezeBackward0))>, <Node: (_ElementWiseOp_643(CloneBackward0))>, <Node: (_ElementWiseOp_644(PermuteBackward0))>, <Node: (_Reshape_645())>, <Node: (_ElementWiseOp_646(IndexBackward0))>, <Node: (_ElementWiseOp_647(BmmBackward0))>, <Node: (_Reshape_648())>, <Node: (_Reshape_649())>, <Node: (_ElementWiseOp_650(CloneBackward0))>, <Node: (_ElementWiseOp_651(ExpandBackward0))>, <Node: (_ElementWiseOp_652(TransposeBackward0))>, <Node: (_ElementWiseOp_653(SelectBackward0))>, <Node: (_ElementWiseOp_654(CloneBackward0))>, <Node: (_ElementWiseOp_655(ExpandBackward0))>, <Node: (_ElementWiseOp_656(MulBackward0))>, <Node: (_ElementWiseOp_657(SelectBackward0))>, <Node: (neck.convs.2.gn (GroupNorm(32, 256, eps=1e-05, affine=True)))>, <Node: (neck.convs.2.conv (Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))))>, <Node: (_ElementWiseOp_658(CloneBackward0))>, <Node: (_ElementWiseOp_659(PermuteBackward0))>, <Node: (_Reshape_660())>, <Node: (backbone.norm3 (LayerNorm((768,), eps=1e-05, elementwise_affine=True)))>, <Node: (_ElementWiseOp_661(AddBackward0))>, <Node: (_ElementWiseOp_662(AddBackward0))>, <Node: (backbone.stages.3.blocks.1.ffn.layers.1 (Linear(in_features=3072, out_features=768, bias=True)))>, <Node: (_ElementWiseOp_663(AddmmBackward0))>, <Node: (_Reshape_664())>, <Node: (_ElementWiseOp_665(TBackward0))>, <Node: (_ElementWiseOp_666(GeluBackward0))>, <Node: (backbone.stages.3.blocks.1.ffn.layers.0.0 (Linear(in_features=768, out_features=3072, bias=True)))>, <Node: (_ElementWiseOp_667(AddmmBackward0))>, <Node: (_Reshape_668())>, <Node: (_ElementWiseOp_669(TBackward0))>, <Node: (backbone.stages.3.blocks.1.norm2 (LayerNorm((768,), eps=1e-05, elementwise_affine=True)))>, <Node: (_Reshape_670())>, <Node: (_ElementWiseOp_671(AddBackward0))>, <Node: (_ElementWiseOp_672(AddBackward0))>, <Node: (backbone.stages.3.blocks.0.ffn.layers.1 (Linear(in_features=3072, out_features=768, bias=True)))>, <Node: (_ElementWiseOp_673(AddmmBackward0))>, <Node: (_Reshape_674())>, <Node: (_ElementWiseOp_675(TBackward0))>, <Node: (_ElementWiseOp_676(GeluBackward0))>, <Node: (backbone.stages.3.blocks.0.ffn.layers.0.0 (Linear(in_features=768, out_features=3072, bias=True)))>, <Node: (_ElementWiseOp_677(AddmmBackward0))>, <Node: (_Reshape_678())>, <Node: (_ElementWiseOp_679(TBackward0))>, <Node: (backbone.stages.3.blocks.0.norm2 (LayerNorm((768,), eps=1e-05, elementwise_affine=True)))>, <Node: (_Reshape_680())>, <Node: (backbone.stages.2.downsample (PatchMerging(
  (adap_padding): AdaptivePadding()
  (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
  (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (reduction): Linear(in_features=1536, out_features=768, bias=False)
)))>, <Node: (_ElementWiseOp_681(MmBackward0))>, <Node: (_Reshape_682())>, <Node: (_ElementWiseOp_683(TBackward0))>, <Node: (_ElementWiseOp_684(NativeLayerNormBackward0))>, <Node: (_ElementWiseOp_685(TransposeBackward0))>, <Node: (_ElementWiseOp_686(Im2ColBackward0))>, <Node: (_ElementWiseOp_687(PermuteBackward0))>, <Node: (_Reshape_688())>, <Node: (_ElementWiseOp_689(SliceBackward0))>, <Node: (_ElementWiseOp_690(SliceBackward0))>, <Node: (_ElementWiseOp_691(SliceBackward0))>, <Node: (_Reshape_692())>, <Node: (_ElementWiseOp_693(CloneBackward0))>, <Node: (_ElementWiseOp_694(PermuteBackward0))>, <Node: (_Reshape_695())>, <Node: (_Reshape_696())>, <Node: (backbone.stages.3.blocks.0.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=768, out_features=2304, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=768, out_features=768, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)))>, <Node: (_ElementWiseOp_697(CloneBackward0))>, <Node: (_ElementWiseOp_698(TransposeBackward0))>, <Node: (_Reshape_699())>, <Node: (_ElementWiseOp_700(BmmBackward0))>, <Node: (_Reshape_701())>, <Node: (_Reshape_702())>, <Node: (_ElementWiseOp_703(CloneBackward0))>, <Node: (_ElementWiseOp_704(ExpandBackward0))>, <Node: (_ElementWiseOp_705(SelectBackward0))>, <Node: (_ElementWiseOp_706(PermuteBackward0))>, <Node: (_Reshape_707())>, <Node: (_Reshape_708())>, <Node: (_ElementWiseOp_709(AddmmBackward0))>, <Node: (_Reshape_710())>, <Node: (_ElementWiseOp_711(TBackward0))>, <Node: (_Reshape_712())>, <Node: (_Reshape_713())>, <Node: (_ElementWiseOp_714(CloneBackward0))>, <Node: (_ElementWiseOp_715(PermuteBackward0))>, <Node: (_Reshape_716())>, <Node: (_ElementWiseOp_717(ConstantPadNdBackward0))>, <Node: (_Reshape_718())>, <Node: (backbone.stages.3.blocks.0.norm1 (LayerNorm((768,), eps=1e-05, elementwise_affine=True)))>, <Node: (_ElementWiseOp_719(ExpandBackward0))>, <Node: (_ElementWiseOp_720(SoftmaxBackward0))>, <Node: (_ElementWiseOp_721(AddBackward0))>, <Node: (_Reshape_722())>, <Node: (_ElementWiseOp_723(UnsqueezeBackward0))>, <Node: (_ElementWiseOp_724(CloneBackward0))>, <Node: (_ElementWiseOp_725(PermuteBackward0))>, <Node: (_Reshape_726())>, <Node: (_ElementWiseOp_727(IndexBackward0))>, <Node: (_ElementWiseOp_728(BmmBackward0))>, <Node: (_Reshape_729())>, <Node: (_Reshape_730())>, <Node: (_ElementWiseOp_731(CloneBackward0))>, <Node: (_ElementWiseOp_732(ExpandBackward0))>, <Node: (_ElementWiseOp_733(TransposeBackward0))>, <Node: (_ElementWiseOp_734(SelectBackward0))>, <Node: (_ElementWiseOp_735(CloneBackward0))>, <Node: (_ElementWiseOp_736(ExpandBackward0))>, <Node: (_ElementWiseOp_737(MulBackward0))>, <Node: (_ElementWiseOp_738(SelectBackward0))>, <Node: (_ElementWiseOp_739(SliceBackward0))>, <Node: (_ElementWiseOp_740(SliceBackward0))>, <Node: (_ElementWiseOp_741(SliceBackward0))>, <Node: (_ElementWiseOp_742(RollBackward0))>, <Node: (_Reshape_743())>, <Node: (_ElementWiseOp_744(CloneBackward0))>, <Node: (_ElementWiseOp_745(PermuteBackward0))>, <Node: (_Reshape_746())>, <Node: (_Reshape_747())>, <Node: (backbone.stages.3.blocks.1.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=768, out_features=2304, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=768, out_features=768, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)))>, <Node: (_ElementWiseOp_748(CloneBackward0))>, <Node: (_ElementWiseOp_749(TransposeBackward0))>, <Node: (_Reshape_750())>, <Node: (_ElementWiseOp_751(BmmBackward0))>, <Node: (_Reshape_752())>, <Node: (_Reshape_753())>, <Node: (_ElementWiseOp_754(CloneBackward0))>, <Node: (_ElementWiseOp_755(ExpandBackward0))>, <Node: (_ElementWiseOp_756(SelectBackward0))>, <Node: (_ElementWiseOp_757(PermuteBackward0))>, <Node: (_Reshape_758())>, <Node: (_Reshape_759())>, <Node: (_ElementWiseOp_760(AddmmBackward0))>, <Node: (_Reshape_761())>, <Node: (_ElementWiseOp_762(TBackward0))>, <Node: (_Reshape_763())>, <Node: (_Reshape_764())>, <Node: (_ElementWiseOp_765(CloneBackward0))>, <Node: (_ElementWiseOp_766(PermuteBackward0))>, <Node: (_Reshape_767())>, <Node: (_ElementWiseOp_768(RollBackward0))>, <Node: (_ElementWiseOp_769(ConstantPadNdBackward0))>, <Node: (_Reshape_770())>, <Node: (backbone.stages.3.blocks.1.norm1 (LayerNorm((768,), eps=1e-05, elementwise_affine=True)))>, <Node: (_ElementWiseOp_771(ExpandBackward0))>, <Node: (_ElementWiseOp_772(SoftmaxBackward0))>, <Node: (_Reshape_773())>, <Node: (_ElementWiseOp_774(AddBackward0))>, <Node: (_Reshape_775())>, <Node: (_ElementWiseOp_776(AddBackward0))>, <Node: (_Reshape_777())>, <Node: (_ElementWiseOp_778(UnsqueezeBackward0))>, <Node: (_ElementWiseOp_779(CloneBackward0))>, <Node: (_ElementWiseOp_780(PermuteBackward0))>, <Node: (_Reshape_781())>, <Node: (_ElementWiseOp_782(IndexBackward0))>, <Node: (_ElementWiseOp_783(BmmBackward0))>, <Node: (_Reshape_784())>, <Node: (_Reshape_785())>, <Node: (_ElementWiseOp_786(CloneBackward0))>, <Node: (_ElementWiseOp_787(ExpandBackward0))>, <Node: (_ElementWiseOp_788(TransposeBackward0))>, <Node: (_ElementWiseOp_789(SelectBackward0))>, <Node: (_ElementWiseOp_790(CloneBackward0))>, <Node: (_ElementWiseOp_791(ExpandBackward0))>, <Node: (_ElementWiseOp_792(MulBackward0))>, <Node: (_ElementWiseOp_793(SelectBackward0))>, <Node: (neck.extra_convs.0.gn (GroupNorm(32, 256, eps=1e-05, affine=True)))>, <Node: (neck.extra_convs.0.conv (Conv2d(768, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))))>]) 

PatchMergingPruner () get_out_channels/  192 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
PatchMergingPruner () get_in_channels/  96 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
WindowMSAPruner layer.embed_dims =  96
WindowMSAPruner layer.qkv.in_features =  96
WindowMSAPruner layer.embed_dims =  96
WindowMSAPruner layer.qkv.in_features =  96
WindowMSAPruner layer.embed_dims =  192
WindowMSAPruner layer.qkv.in_features =  192
WindowMSAPruner layer.embed_dims =  192
WindowMSAPruner layer.qkv.in_features =  192
PatchMergingPruner () get_out_channels/  384 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
PatchMergingPruner () get_in_channels/  192 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
WindowMSAPruner layer.embed_dims =  384
WindowMSAPruner layer.qkv.in_features =  384
WindowMSAPruner layer.embed_dims =  384
WindowMSAPruner layer.qkv.in_features =  384
WindowMSAPruner layer.embed_dims =  384
WindowMSAPruner layer.qkv.in_features =  384
WindowMSAPruner layer.embed_dims =  384
WindowMSAPruner layer.qkv.in_features =  384
WindowMSAPruner layer.embed_dims =  384
WindowMSAPruner layer.qkv.in_features =  384
WindowMSAPruner layer.embed_dims =  384
WindowMSAPruner layer.qkv.in_features =  384
PatchMergingPruner () get_out_channels/  768 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
PatchMergingPruner () get_in_channels/  384 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
WindowMSAPruner layer.embed_dims =  768
WindowMSAPruner layer.qkv.in_features =  768
WindowMSAPruner layer.embed_dims =  768
WindowMSAPruner layer.qkv.in_features =  768
<class 'torch.nn.modules.normalization.GroupNorm'> continue 2
	get_all_groups -- Adding layer:  <class 'torch.nn.modules.conv.Conv2d'>
		prune_local()/  _check_pruning_ratio OK
Conv/Linear Output GroupNormImportance() / layer Type =  <class 'torch.nn.modules.conv.Conv2d'>
idxs =  64
prunable_chs =  256
idxs =  64
prunable_chs =  256
pruner.step 
--------------------------------
          Pruning Group
--------------------------------
[0] prune_out_channels on neck.convs.0.conv (Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1))) => prune_out_channels on neck.convs.0.conv (Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1))), #idxs=64
[1] prune_out_channels on neck.convs.0.conv (Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1))) => prune_out_channels on neck.convs.0.gn (GroupNorm(32, 256, eps=1e-05, affine=True)), #idxs=64
--------------------------------

<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
	get_all_groups -- Adding layer:  <class 'torch.nn.modules.normalization.LayerNorm'>
PatchMergingPruner () get_in_channels/  192 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
WindowMSAPruner layer.qkv.in_features =  192
PatchMergingPruner () get_out_channels/  192 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
WindowMSAPruner layer.qkv.in_features =  192
WindowMSAPruner layer.embed_dims =  192
WindowMSAPruner layer.embed_dims =  192
		prune_local()/  _check_pruning_ratio OK
local_imp len =  192 torch.Size([192])
i =  5
<class 'torch.nn.modules.conv.Conv2d'> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191] Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))

Conv/Linear Output GroupNormImportance() / layer Type =  <class 'mmcv.cnn.bricks.wrappers.Linear'>
local_imp len =  192 torch.Size([192])
i =  24
<class 'mmcv.cnn.bricks.wrappers.Linear'> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191] Linear(in_features=192, out_features=768, bias=True)

Conv/Linear Output GroupNormImportance() / layer Type =  <class 'mmcv.cnn.bricks.wrappers.Linear'>
local_imp len =  192 torch.Size([192])
i =  81
<class 'mmcv.cnn.bricks.wrappers.Linear'> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191] Linear(in_features=192, out_features=768, bias=True)

idxs =  48
prunable_chs =  192
idxs =  48
prunable_chs =  192
PatchMergingPruner () get_in_channels/  192 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
idxs =  48
prunable_chs =  192
idxs =  48
prunable_chs =  192
idxs =  48
prunable_chs =  192
WindowMSAPruner layer.qkv.in_features =  192
PatchMergingPruner () get_out_channels/  192 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
idxs =  48
prunable_chs =  192
idxs =  48
prunable_chs =  192
idxs =  48
prunable_chs =  192
WindowMSAPruner layer.qkv.in_features =  192
WindowMSAPruner layer.embed_dims =  192
idxs =  48
prunable_chs =  192
WindowMSAPruner layer.embed_dims =  192
idxs =  48
prunable_chs =  192
pruner.step 
--------------------------------
          Pruning Group
--------------------------------
[0] prune_out_channels on backbone.norm1 (LayerNorm((192,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on backbone.norm1 (LayerNorm((192,), eps=1e-05, elementwise_affine=True)), #idxs=48
[1] prune_out_channels on backbone.norm1 (LayerNorm((192,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _ElementWiseOp_3(AddBackward0), #idxs=48
[2] prune_out_channels on backbone.norm1 (LayerNorm((192,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_2(), #idxs=48
[3] prune_out_channels on _Reshape_2() => prune_out_channels on _ElementWiseOp_1(PermuteBackward0), #idxs=48
[4] prune_out_channels on _ElementWiseOp_1(PermuteBackward0) => prune_out_channels on _ElementWiseOp_0(CloneBackward0), #idxs=48
[5] prune_out_channels on _ElementWiseOp_0(CloneBackward0) => prune_in_channels on neck.convs.0.conv (Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))), #idxs=48
[6] prune_out_channels on _ElementWiseOp_3(AddBackward0) => prune_out_channels on _ElementWiseOp_4(AddBackward0), #idxs=48
[7] prune_out_channels on _ElementWiseOp_3(AddBackward0) => prune_out_channels on backbone.stages.1.blocks.1.ffn.layers.1 (Linear(in_features=768, out_features=192, bias=True)), #idxs=48
[8] prune_out_channels on _ElementWiseOp_3(AddBackward0) => prune_out_channels on _Reshape_342(), #idxs=48
[9] prune_out_channels on _Reshape_342() => prune_out_channels on _ElementWiseOp_341(PermuteBackward0), #idxs=48
[10] prune_out_channels on _ElementWiseOp_341(PermuteBackward0) => prune_out_channels on _ElementWiseOp_340(ConstantPadNdBackward0), #idxs=48
[11] prune_out_channels on _ElementWiseOp_340(ConstantPadNdBackward0) => prune_out_channels on _ElementWiseOp_339(Im2ColBackward0), #idxs=48
[12] prune_out_channels on _ElementWiseOp_339(Im2ColBackward0) => prune_out_channels on _ElementWiseOp_338(TransposeBackward0), #idxs=48
[13] prune_out_channels on _ElementWiseOp_338(TransposeBackward0) => prune_out_channels on _ElementWiseOp_337(NativeLayerNormBackward0), #idxs=48
[14] prune_out_channels on _ElementWiseOp_337(NativeLayerNormBackward0) => prune_out_channels on _Reshape_335(), #idxs=48
[15] prune_out_channels on _Reshape_335() => prune_out_channels on _ElementWiseOp_334(MmBackward0), #idxs=48
[16] prune_out_channels on _ElementWiseOp_334(MmBackward0) => prune_out_channels on _ElementWiseOp_336(TBackward0), #idxs=48
[17] prune_out_channels on _ElementWiseOp_334(MmBackward0) => prune_in_channels on backbone.stages.1.downsample (PatchMerging(
  (adap_padding): AdaptivePadding()
  (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (reduction): Linear(in_features=768, out_features=384, bias=False)
)), #idxs=48
[18] prune_out_channels on _ElementWiseOp_4(AddBackward0) => prune_out_channels on _Reshape_12(), #idxs=48
[19] prune_out_channels on _ElementWiseOp_4(AddBackward0) => prune_out_channels on _ElementWiseOp_13(AddBackward0), #idxs=48
[20] prune_out_channels on _ElementWiseOp_4(AddBackward0) => prune_out_channels on backbone.stages.1.blocks.1.norm2 (LayerNorm((192,), eps=1e-05, elementwise_affine=True)), #idxs=48
[21] prune_out_channels on backbone.stages.1.blocks.1.norm2 (LayerNorm((192,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_10(), #idxs=48
[22] prune_out_channels on _Reshape_10() => prune_out_channels on _ElementWiseOp_9(AddmmBackward0), #idxs=48
[23] prune_out_channels on _ElementWiseOp_9(AddmmBackward0) => prune_out_channels on _ElementWiseOp_11(TBackward0), #idxs=48
[24] prune_out_channels on _ElementWiseOp_9(AddmmBackward0) => prune_in_channels on backbone.stages.1.blocks.1.ffn.layers.0.0 (Linear(in_features=192, out_features=768, bias=True)), #idxs=48
[25] prune_out_channels on _ElementWiseOp_13(AddBackward0) => prune_out_channels on _ElementWiseOp_14(AddBackward0), #idxs=48
[26] prune_out_channels on _ElementWiseOp_13(AddBackward0) => prune_out_channels on backbone.stages.1.blocks.0.ffn.layers.1 (Linear(in_features=768, out_features=192, bias=True)), #idxs=48
[27] prune_out_channels on _ElementWiseOp_13(AddBackward0) => prune_out_channels on backbone.stages.1.blocks.1.norm1 (LayerNorm((192,), eps=1e-05, elementwise_affine=True)), #idxs=48
[28] prune_out_channels on backbone.stages.1.blocks.1.norm1 (LayerNorm((192,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_247(), #idxs=48
[29] prune_out_channels on _Reshape_247() => prune_out_channels on _ElementWiseOp_246(ConstantPadNdBackward0), #idxs=48
[30] prune_out_channels on _ElementWiseOp_246(ConstantPadNdBackward0) => prune_out_channels on _ElementWiseOp_245(RollBackward0), #idxs=48
[31] prune_out_channels on _ElementWiseOp_245(RollBackward0) => prune_out_channels on _Reshape_244(), #idxs=48
[32] prune_out_channels on _Reshape_244() => prune_out_channels on _ElementWiseOp_243(PermuteBackward0), #idxs=48
[33] prune_out_channels on _ElementWiseOp_243(PermuteBackward0) => prune_out_channels on _ElementWiseOp_242(CloneBackward0), #idxs=48
[34] prune_out_channels on _ElementWiseOp_242(CloneBackward0) => prune_out_channels on _Reshape_241(), #idxs=48
[35] prune_out_channels on _Reshape_241() => prune_out_channels on _Reshape_240(), #idxs=48
[36] prune_out_channels on _Reshape_240() => prune_out_channels on _Reshape_238(), #idxs=48
[37] prune_out_channels on _Reshape_238() => prune_out_channels on _ElementWiseOp_237(AddmmBackward0), #idxs=48
[38] prune_out_channels on _ElementWiseOp_237(AddmmBackward0) => prune_out_channels on _ElementWiseOp_239(TBackward0), #idxs=48
[39] prune_out_channels on _ElementWiseOp_237(AddmmBackward0) => prune_out_channels on _Reshape_236(), #idxs=48
[40] prune_out_channels on _Reshape_236() => prune_out_channels on _Reshape_235(), #idxs=48
[41] prune_out_channels on _Reshape_235() => prune_out_channels on _ElementWiseOp_234(PermuteBackward0), #idxs=48
[42] prune_out_channels on _ElementWiseOp_234(PermuteBackward0) => prune_out_channels on _ElementWiseOp_233(SelectBackward0), #idxs=48
[43] prune_out_channels on _ElementWiseOp_234(PermuteBackward0) => prune_out_channels on _ElementWiseOp_266(SelectBackward0), #idxs=48
[44] prune_out_channels on _ElementWiseOp_234(PermuteBackward0) => prune_out_channels on _ElementWiseOp_270(SelectBackward0), #idxs=48
[45] prune_out_channels on _ElementWiseOp_270(SelectBackward0) => prune_out_channels on _ElementWiseOp_269(MulBackward0), #idxs=48
[46] prune_out_channels on _ElementWiseOp_269(MulBackward0) => prune_out_channels on _ElementWiseOp_268(ExpandBackward0), #idxs=48
[47] prune_out_channels on _ElementWiseOp_268(ExpandBackward0) => prune_out_channels on _ElementWiseOp_267(CloneBackward0), #idxs=48
[48] prune_out_channels on _ElementWiseOp_267(CloneBackward0) => prune_out_channels on _Reshape_261(), #idxs=48
[49] prune_out_channels on _Reshape_261() => prune_out_channels on _ElementWiseOp_260(BmmBackward0), #idxs=48
[50] prune_out_channels on _ElementWiseOp_260(BmmBackward0) => prune_out_channels on _Reshape_262(), #idxs=48
[51] prune_out_channels on _ElementWiseOp_260(BmmBackward0) => prune_out_channels on _Reshape_254(), #idxs=48
[52] prune_out_channels on _Reshape_254() => prune_out_channels on _ElementWiseOp_253(AddBackward0), #idxs=48
[53] prune_out_channels on _ElementWiseOp_253(AddBackward0) => prune_out_channels on _ElementWiseOp_255(UnsqueezeBackward0), #idxs=48
[54] prune_out_channels on _ElementWiseOp_253(AddBackward0) => prune_out_channels on _Reshape_252(), #idxs=48
[55] prune_out_channels on _Reshape_252() => prune_out_channels on _ElementWiseOp_251(AddBackward0), #idxs=48
[56] prune_out_channels on _ElementWiseOp_251(AddBackward0) => prune_out_channels on _Reshape_250(), #idxs=48
[57] prune_out_channels on _Reshape_250() => prune_out_channels on _ElementWiseOp_249(SoftmaxBackward0), #idxs=48
[58] prune_out_channels on _ElementWiseOp_249(SoftmaxBackward0) => prune_out_channels on _ElementWiseOp_248(ExpandBackward0), #idxs=48
[59] prune_out_channels on _ElementWiseOp_248(ExpandBackward0) => prune_out_channels on _Reshape_229(), #idxs=48
[60] prune_out_channels on _Reshape_229() => prune_out_channels on _ElementWiseOp_228(BmmBackward0), #idxs=48
[61] prune_out_channels on _ElementWiseOp_228(BmmBackward0) => prune_out_channels on _Reshape_230(), #idxs=48
[62] prune_out_channels on _ElementWiseOp_228(BmmBackward0) => prune_out_channels on _Reshape_227(), #idxs=48
[63] prune_out_channels on _Reshape_227() => prune_out_channels on _ElementWiseOp_226(TransposeBackward0), #idxs=48
[64] prune_out_channels on _ElementWiseOp_226(TransposeBackward0) => prune_out_channels on _ElementWiseOp_225(CloneBackward0), #idxs=48
[65] prune_out_channels on _ElementWiseOp_225(CloneBackward0) => prune_in_channels on backbone.stages.1.blocks.1.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=192, out_features=576, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=192, out_features=192, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)), #idxs=48
[66] prune_out_channels on _Reshape_230() => prune_out_channels on _ElementWiseOp_231(CloneBackward0), #idxs=48
[67] prune_out_channels on _ElementWiseOp_231(CloneBackward0) => prune_out_channels on _ElementWiseOp_232(ExpandBackward0), #idxs=48
[68] prune_out_channels on _ElementWiseOp_255(UnsqueezeBackward0) => prune_out_channels on _ElementWiseOp_256(CloneBackward0), #idxs=48
[69] prune_out_channels on _ElementWiseOp_256(CloneBackward0) => prune_out_channels on _ElementWiseOp_257(PermuteBackward0), #idxs=48
[70] prune_out_channels on _ElementWiseOp_257(PermuteBackward0) => prune_out_channels on _Reshape_258(), #idxs=48
[71] prune_out_channels on _Reshape_258() => prune_out_channels on _ElementWiseOp_259(IndexBackward0), #idxs=48
[72] prune_out_channels on _Reshape_262() => prune_out_channels on _ElementWiseOp_263(CloneBackward0), #idxs=48
[73] prune_out_channels on _ElementWiseOp_263(CloneBackward0) => prune_out_channels on _ElementWiseOp_264(ExpandBackward0), #idxs=48
[74] prune_out_channels on _ElementWiseOp_264(ExpandBackward0) => prune_out_channels on _ElementWiseOp_265(TransposeBackward0), #idxs=48
[75] prune_out_channels on _ElementWiseOp_14(AddBackward0) => prune_out_channels on _Reshape_22(), #idxs=48
[76] prune_out_channels on _ElementWiseOp_14(AddBackward0) => prune_out_channels on backbone.stages.0.downsample (PatchMerging(
  (adap_padding): AdaptivePadding()
  (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
  (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (reduction): Linear(in_features=384, out_features=192, bias=False)
)), #idxs=48
[77] prune_out_channels on _ElementWiseOp_14(AddBackward0) => prune_out_channels on backbone.stages.1.blocks.0.norm2 (LayerNorm((192,), eps=1e-05, elementwise_affine=True)), #idxs=48
[78] prune_out_channels on backbone.stages.1.blocks.0.norm2 (LayerNorm((192,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_20(), #idxs=48
[79] prune_out_channels on _Reshape_20() => prune_out_channels on _ElementWiseOp_19(AddmmBackward0), #idxs=48
[80] prune_out_channels on _ElementWiseOp_19(AddmmBackward0) => prune_out_channels on _ElementWiseOp_21(TBackward0), #idxs=48
[81] prune_out_channels on _ElementWiseOp_19(AddmmBackward0) => prune_in_channels on backbone.stages.1.blocks.0.ffn.layers.0.0 (Linear(in_features=192, out_features=768, bias=True)), #idxs=48
[82] prune_out_channels on backbone.stages.0.downsample (PatchMerging(
  (adap_padding): AdaptivePadding()
  (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
  (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (reduction): Linear(in_features=384, out_features=192, bias=False)
)) => prune_out_channels on backbone.stages.1.blocks.0.norm1 (LayerNorm((192,), eps=1e-05, elementwise_affine=True)), #idxs=48
[83] prune_out_channels on backbone.stages.1.blocks.0.norm1 (LayerNorm((192,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_193(), #idxs=48
[84] prune_out_channels on _Reshape_193() => prune_out_channels on _ElementWiseOp_192(ConstantPadNdBackward0), #idxs=48
[85] prune_out_channels on _ElementWiseOp_192(ConstantPadNdBackward0) => prune_out_channels on _Reshape_191(), #idxs=48
[86] prune_out_channels on _Reshape_191() => prune_out_channels on _ElementWiseOp_190(PermuteBackward0), #idxs=48
[87] prune_out_channels on _ElementWiseOp_190(PermuteBackward0) => prune_out_channels on _ElementWiseOp_189(CloneBackward0), #idxs=48
[88] prune_out_channels on _ElementWiseOp_189(CloneBackward0) => prune_out_channels on _Reshape_188(), #idxs=48
[89] prune_out_channels on _Reshape_188() => prune_out_channels on _Reshape_187(), #idxs=48
[90] prune_out_channels on _Reshape_187() => prune_out_channels on _Reshape_185(), #idxs=48
[91] prune_out_channels on _Reshape_185() => prune_out_channels on _ElementWiseOp_184(AddmmBackward0), #idxs=48
[92] prune_out_channels on _ElementWiseOp_184(AddmmBackward0) => prune_out_channels on _ElementWiseOp_186(TBackward0), #idxs=48
[93] prune_out_channels on _ElementWiseOp_184(AddmmBackward0) => prune_out_channels on _Reshape_183(), #idxs=48
[94] prune_out_channels on _Reshape_183() => prune_out_channels on _Reshape_182(), #idxs=48
[95] prune_out_channels on _Reshape_182() => prune_out_channels on _ElementWiseOp_181(PermuteBackward0), #idxs=48
[96] prune_out_channels on _ElementWiseOp_181(PermuteBackward0) => prune_out_channels on _ElementWiseOp_180(SelectBackward0), #idxs=48
[97] prune_out_channels on _ElementWiseOp_181(PermuteBackward0) => prune_out_channels on _ElementWiseOp_209(SelectBackward0), #idxs=48
[98] prune_out_channels on _ElementWiseOp_181(PermuteBackward0) => prune_out_channels on _ElementWiseOp_213(SelectBackward0), #idxs=48
[99] prune_out_channels on _ElementWiseOp_213(SelectBackward0) => prune_out_channels on _ElementWiseOp_212(MulBackward0), #idxs=48
[100] prune_out_channels on _ElementWiseOp_212(MulBackward0) => prune_out_channels on _ElementWiseOp_211(ExpandBackward0), #idxs=48
[101] prune_out_channels on _ElementWiseOp_211(ExpandBackward0) => prune_out_channels on _ElementWiseOp_210(CloneBackward0), #idxs=48
[102] prune_out_channels on _ElementWiseOp_210(CloneBackward0) => prune_out_channels on _Reshape_204(), #idxs=48
[103] prune_out_channels on _Reshape_204() => prune_out_channels on _ElementWiseOp_203(BmmBackward0), #idxs=48
[104] prune_out_channels on _ElementWiseOp_203(BmmBackward0) => prune_out_channels on _Reshape_205(), #idxs=48
[105] prune_out_channels on _ElementWiseOp_203(BmmBackward0) => prune_out_channels on _Reshape_197(), #idxs=48
[106] prune_out_channels on _Reshape_197() => prune_out_channels on _ElementWiseOp_196(AddBackward0), #idxs=48
[107] prune_out_channels on _ElementWiseOp_196(AddBackward0) => prune_out_channels on _ElementWiseOp_198(UnsqueezeBackward0), #idxs=48
[108] prune_out_channels on _ElementWiseOp_196(AddBackward0) => prune_out_channels on _ElementWiseOp_195(SoftmaxBackward0), #idxs=48
[109] prune_out_channels on _ElementWiseOp_195(SoftmaxBackward0) => prune_out_channels on _ElementWiseOp_194(ExpandBackward0), #idxs=48
[110] prune_out_channels on _ElementWiseOp_194(ExpandBackward0) => prune_out_channels on _Reshape_176(), #idxs=48
[111] prune_out_channels on _Reshape_176() => prune_out_channels on _ElementWiseOp_175(BmmBackward0), #idxs=48
[112] prune_out_channels on _ElementWiseOp_175(BmmBackward0) => prune_out_channels on _Reshape_177(), #idxs=48
[113] prune_out_channels on _ElementWiseOp_175(BmmBackward0) => prune_out_channels on _Reshape_174(), #idxs=48
[114] prune_out_channels on _Reshape_174() => prune_out_channels on _ElementWiseOp_173(TransposeBackward0), #idxs=48
[115] prune_out_channels on _ElementWiseOp_173(TransposeBackward0) => prune_out_channels on _ElementWiseOp_172(CloneBackward0), #idxs=48
[116] prune_out_channels on _ElementWiseOp_172(CloneBackward0) => prune_in_channels on backbone.stages.1.blocks.0.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=192, out_features=576, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=192, out_features=192, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)), #idxs=48
[117] prune_out_channels on _Reshape_177() => prune_out_channels on _ElementWiseOp_178(CloneBackward0), #idxs=48
[118] prune_out_channels on _ElementWiseOp_178(CloneBackward0) => prune_out_channels on _ElementWiseOp_179(ExpandBackward0), #idxs=48
[119] prune_out_channels on _ElementWiseOp_198(UnsqueezeBackward0) => prune_out_channels on _ElementWiseOp_199(CloneBackward0), #idxs=48
[120] prune_out_channels on _ElementWiseOp_199(CloneBackward0) => prune_out_channels on _ElementWiseOp_200(PermuteBackward0), #idxs=48
[121] prune_out_channels on _ElementWiseOp_200(PermuteBackward0) => prune_out_channels on _Reshape_201(), #idxs=48
[122] prune_out_channels on _Reshape_201() => prune_out_channels on _ElementWiseOp_202(IndexBackward0), #idxs=48
[123] prune_out_channels on _Reshape_205() => prune_out_channels on _ElementWiseOp_206(CloneBackward0), #idxs=48
[124] prune_out_channels on _ElementWiseOp_206(CloneBackward0) => prune_out_channels on _ElementWiseOp_207(ExpandBackward0), #idxs=48
[125] prune_out_channels on _ElementWiseOp_207(ExpandBackward0) => prune_out_channels on _ElementWiseOp_208(TransposeBackward0), #idxs=48
[126] prune_out_channels on _Reshape_22() => prune_out_channels on _ElementWiseOp_162(CloneBackward0), #idxs=48
[127] prune_out_channels on _ElementWiseOp_162(CloneBackward0) => prune_out_channels on _ElementWiseOp_163(SliceBackward0), #idxs=48
[128] prune_out_channels on _ElementWiseOp_163(SliceBackward0) => prune_out_channels on _ElementWiseOp_164(SliceBackward0), #idxs=48
[129] prune_out_channels on _ElementWiseOp_164(SliceBackward0) => prune_out_channels on _ElementWiseOp_165(SliceBackward0), #idxs=48
[130] prune_out_channels on _ElementWiseOp_165(SliceBackward0) => prune_out_channels on _ElementWiseOp_166(SliceBackward0), #idxs=48
[131] prune_out_channels on _ElementWiseOp_166(SliceBackward0) => prune_out_channels on _Reshape_167(), #idxs=48
[132] prune_out_channels on _Reshape_167() => prune_out_channels on _ElementWiseOp_168(CloneBackward0), #idxs=48
[133] prune_out_channels on _ElementWiseOp_168(CloneBackward0) => prune_out_channels on _ElementWiseOp_169(PermuteBackward0), #idxs=48
[134] prune_out_channels on _ElementWiseOp_169(PermuteBackward0) => prune_out_channels on _Reshape_170(), #idxs=48
[135] prune_out_channels on _Reshape_170() => prune_out_channels on _Reshape_171(), #idxs=48
[136] prune_out_channels on _Reshape_171() => prune_out_channels on backbone.stages.1.blocks.0.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=192, out_features=576, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=192, out_features=192, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)), #idxs=48
[137] prune_out_channels on _Reshape_12() => prune_out_channels on _ElementWiseOp_214(CloneBackward0), #idxs=48
[138] prune_out_channels on _ElementWiseOp_214(CloneBackward0) => prune_out_channels on _ElementWiseOp_215(SliceBackward0), #idxs=48
[139] prune_out_channels on _ElementWiseOp_215(SliceBackward0) => prune_out_channels on _ElementWiseOp_216(SliceBackward0), #idxs=48
[140] prune_out_channels on _ElementWiseOp_216(SliceBackward0) => prune_out_channels on _ElementWiseOp_217(SliceBackward0), #idxs=48
[141] prune_out_channels on _ElementWiseOp_217(SliceBackward0) => prune_out_channels on _ElementWiseOp_218(SliceBackward0), #idxs=48
[142] prune_out_channels on _ElementWiseOp_218(SliceBackward0) => prune_out_channels on _ElementWiseOp_219(RollBackward0), #idxs=48
[143] prune_out_channels on _ElementWiseOp_219(RollBackward0) => prune_out_channels on _Reshape_220(), #idxs=48
[144] prune_out_channels on _Reshape_220() => prune_out_channels on _ElementWiseOp_221(CloneBackward0), #idxs=48
[145] prune_out_channels on _ElementWiseOp_221(CloneBackward0) => prune_out_channels on _ElementWiseOp_222(PermuteBackward0), #idxs=48
[146] prune_out_channels on _ElementWiseOp_222(PermuteBackward0) => prune_out_channels on _Reshape_223(), #idxs=48
[147] prune_out_channels on _Reshape_223() => prune_out_channels on _Reshape_224(), #idxs=48
[148] prune_out_channels on _Reshape_224() => prune_out_channels on backbone.stages.1.blocks.1.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=192, out_features=576, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=192, out_features=192, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)), #idxs=48
--------------------------------

PatchMergingPruner () prune_in_channels/  768 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
PatchMergingPruner () prune_in_channels/ dim =  192
len indx =  48
idxs_repeated =  192
WindowMSAPruner prune_in_channels() /  48
PatchMergingPruner () prune_out_channels/  192 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
WindowMSAPruner prune_in_channels() /  48
WindowMSAPruner prune_out_channels() /  48
WindowMSAPruner prune_out_channels/ dim =  192
	len indx =  48
WindowMSAPruner - idxs =  [5, 6, 9, 19, 23, 31, 39, 40, 42, 54, 55, 63, 64, 66, 68, 70, 77, 83, 85, 87, 89, 97, 98, 100, 102, 105, 107, 109, 115, 117, 121, 131, 137, 141, 142, 144, 145, 146, 148, 150, 153, 161, 164, 167, 169, 174, 187, 191]
	idxs_repeated =  192
WindowMSAPruner - idxs_repeated =  [5, 6, 9, 19, 23, 31, 39, 40, 42, 54, 55, 63, 64, 66, 68, 70, 77, 83, 85, 87, 89, 97, 98, 100, 102, 105, 107, 109, 115, 117, 121, 131, 137, 141, 142, 144, 145, 146, 148, 150, 153, 161, 164, 167, 169, 174, 187, 191, 197, 198, 201, 211, 215, 223, 231, 232, 234, 246, 247, 255, 256, 258, 260, 262, 269, 275, 277, 279, 281, 289, 290, 292, 294, 297, 299, 301, 307, 309, 313, 323, 329, 333, 334, 336, 337, 338, 340, 342, 345, 353, 356, 359, 361, 366, 379, 383, 389, 390, 393, 403, 407, 415, 423, 424, 426, 438, 439, 447, 448, 450, 452, 454, 461, 467, 469, 471, 473, 481, 482, 484, 486, 489, 491, 493, 499, 501, 505, 515, 521, 525, 526, 528, 529, 530, 532, 534, 537, 545, 548, 551, 553, 558, 571, 575, 581, 582, 585, 595, 599, 607, 615, 616, 618, 630, 631, 639, 640, 642, 644, 646, 653, 659, 661, 663, 665, 673, 674, 676, 678, 681, 683, 685, 691, 693, 697, 707, 713, 717, 718, 720, 721, 722, 724, 726, 729, 737, 740, 743, 745, 750, 763, 767]
WindowMSAPruner prune_out_channels idxs_repeated =  192
WindowMSAPruner prune_out_channels() /  48
WindowMSAPruner prune_out_channels/ dim =  192
	len indx =  48
WindowMSAPruner - idxs =  [5, 6, 9, 19, 23, 31, 39, 40, 42, 54, 55, 63, 64, 66, 68, 70, 77, 83, 85, 87, 89, 97, 98, 100, 102, 105, 107, 109, 115, 117, 121, 131, 137, 141, 142, 144, 145, 146, 148, 150, 153, 161, 164, 167, 169, 174, 187, 191]
	idxs_repeated =  192
WindowMSAPruner - idxs_repeated =  [5, 6, 9, 19, 23, 31, 39, 40, 42, 54, 55, 63, 64, 66, 68, 70, 77, 83, 85, 87, 89, 97, 98, 100, 102, 105, 107, 109, 115, 117, 121, 131, 137, 141, 142, 144, 145, 146, 148, 150, 153, 161, 164, 167, 169, 174, 187, 191, 197, 198, 201, 211, 215, 223, 231, 232, 234, 246, 247, 255, 256, 258, 260, 262, 269, 275, 277, 279, 281, 289, 290, 292, 294, 297, 299, 301, 307, 309, 313, 323, 329, 333, 334, 336, 337, 338, 340, 342, 345, 353, 356, 359, 361, 366, 379, 383, 389, 390, 393, 403, 407, 415, 423, 424, 426, 438, 439, 447, 448, 450, 452, 454, 461, 467, 469, 471, 473, 481, 482, 484, 486, 489, 491, 493, 499, 501, 505, 515, 521, 525, 526, 528, 529, 530, 532, 534, 537, 545, 548, 551, 553, 558, 571, 575, 581, 582, 585, 595, 599, 607, 615, 616, 618, 630, 631, 639, 640, 642, 644, 646, 653, 659, 661, 663, 665, 673, 674, 676, 678, 681, 683, 685, 691, 693, 697, 707, 713, 717, 718, 720, 721, 722, 724, 726, 729, 737, 740, 743, 745, 750, 763, 767]
WindowMSAPruner prune_out_channels idxs_repeated =  192
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
	get_all_groups -- Adding layer:  <class 'mmcv.cnn.bricks.wrappers.Linear'>
		prune_local()/  _check_pruning_ratio OK
Conv/Linear Output GroupNormImportance() / layer Type =  <class 'mmcv.cnn.bricks.wrappers.Linear'>
local_imp len =  768 torch.Size([768])
i =  5
<class 'mmcv.cnn.bricks.wrappers.Linear'> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767] Linear(in_features=768, out_features=144, bias=True)

idxs =  192
prunable_chs =  768
pruner.step 
--------------------------------
          Pruning Group
--------------------------------
[0] prune_out_channels on backbone.stages.1.blocks.1.ffn.layers.0.0 (Linear(in_features=144, out_features=768, bias=True)) => prune_out_channels on backbone.stages.1.blocks.1.ffn.layers.0.0 (Linear(in_features=144, out_features=768, bias=True)), #idxs=192
[1] prune_out_channels on backbone.stages.1.blocks.1.ffn.layers.0.0 (Linear(in_features=144, out_features=768, bias=True)) => prune_out_channels on _ElementWiseOp_8(GeluBackward0), #idxs=192
[2] prune_out_channels on _ElementWiseOp_8(GeluBackward0) => prune_out_channels on _Reshape_6(), #idxs=192
[3] prune_out_channels on _Reshape_6() => prune_out_channels on _ElementWiseOp_5(AddmmBackward0), #idxs=192
[4] prune_out_channels on _ElementWiseOp_5(AddmmBackward0) => prune_out_channels on _ElementWiseOp_7(TBackward0), #idxs=192
[5] prune_out_channels on _ElementWiseOp_5(AddmmBackward0) => prune_in_channels on backbone.stages.1.blocks.1.ffn.layers.1 (Linear(in_features=768, out_features=144, bias=True)), #idxs=192
--------------------------------

<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
continue 4
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
	get_all_groups -- Adding layer:  <class 'mmcv.cnn.bricks.wrappers.Linear'>
		prune_local()/  _check_pruning_ratio OK
Conv/Linear Output GroupNormImportance() / layer Type =  <class 'mmcv.cnn.bricks.wrappers.Linear'>
local_imp len =  768 torch.Size([768])
i =  5
<class 'mmcv.cnn.bricks.wrappers.Linear'> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767] Linear(in_features=768, out_features=144, bias=True)

idxs =  192
prunable_chs =  768
pruner.step 
--------------------------------
          Pruning Group
--------------------------------
[0] prune_out_channels on backbone.stages.1.blocks.0.ffn.layers.0.0 (Linear(in_features=144, out_features=768, bias=True)) => prune_out_channels on backbone.stages.1.blocks.0.ffn.layers.0.0 (Linear(in_features=144, out_features=768, bias=True)), #idxs=192
[1] prune_out_channels on backbone.stages.1.blocks.0.ffn.layers.0.0 (Linear(in_features=144, out_features=768, bias=True)) => prune_out_channels on _ElementWiseOp_18(GeluBackward0), #idxs=192
[2] prune_out_channels on _ElementWiseOp_18(GeluBackward0) => prune_out_channels on _Reshape_16(), #idxs=192
[3] prune_out_channels on _Reshape_16() => prune_out_channels on _ElementWiseOp_15(AddmmBackward0), #idxs=192
[4] prune_out_channels on _ElementWiseOp_15(AddmmBackward0) => prune_out_channels on _ElementWiseOp_17(TBackward0), #idxs=192
[5] prune_out_channels on _ElementWiseOp_15(AddmmBackward0) => prune_in_channels on backbone.stages.1.blocks.0.ffn.layers.1 (Linear(in_features=768, out_features=144, bias=True)), #idxs=192
--------------------------------

<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
continue 4
<class 'torch_pruning.ops._ReshapeOp'> continue 2
PatchMergingPruner () get_out_channels/  144 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
	get_all_groups -- Adding layer:  <class 'mmcv.cnn.bricks.wrappers.Linear'>
PatchMergingPruner () get_in_channels/  96 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
WindowMSAPruner layer.qkv.in_features =  96
WindowMSAPruner layer.qkv.in_features =  96
WindowMSAPruner layer.embed_dims =  96
WindowMSAPruner layer.embed_dims =  96
		prune_local()/  _check_pruning_ratio OK
Conv/Linear Output GroupNormImportance() / layer Type =  <class 'mmcv.cnn.bricks.wrappers.Linear'>
local_imp len =  96 torch.Size([96])
i =  18
<class 'mmcv.cnn.bricks.wrappers.Linear'> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95] Linear(in_features=96, out_features=384, bias=True)

Conv/Linear Output GroupNormImportance() / layer Type =  <class 'mmcv.cnn.bricks.wrappers.Linear'>
local_imp len =  96 torch.Size([96])
i =  75
<class 'mmcv.cnn.bricks.wrappers.Linear'> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95] Linear(in_features=96, out_features=384, bias=True)

Conv/Linear Output GroupNormImportance() / layer Type =  <class 'torch.nn.modules.conv.Conv2d'>
idxs =  24
prunable_chs =  96
PatchMergingPruner () get_in_channels/  96 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
idxs =  24
prunable_chs =  96
idxs =  24
prunable_chs =  96
idxs =  24
prunable_chs =  96
WindowMSAPruner layer.qkv.in_features =  96
idxs =  24
prunable_chs =  96
idxs =  24
prunable_chs =  96
idxs =  24
prunable_chs =  96
WindowMSAPruner layer.qkv.in_features =  96
idxs =  24
prunable_chs =  96
WindowMSAPruner layer.embed_dims =  96
idxs =  24
prunable_chs =  96
WindowMSAPruner layer.embed_dims =  96
idxs =  24
prunable_chs =  96
pruner.step 
--------------------------------
          Pruning Group
--------------------------------
[0] prune_out_channels on backbone.stages.0.blocks.1.ffn.layers.1 (Linear(in_features=384, out_features=96, bias=True)) => prune_out_channels on backbone.stages.0.blocks.1.ffn.layers.1 (Linear(in_features=384, out_features=96, bias=True)), #idxs=24
[1] prune_out_channels on backbone.stages.0.blocks.1.ffn.layers.1 (Linear(in_features=384, out_features=96, bias=True)) => prune_out_channels on _ElementWiseOp_31(AddBackward0), #idxs=24
[2] prune_out_channels on _ElementWiseOp_31(AddBackward0) => prune_out_channels on _ElementWiseOp_32(AddBackward0), #idxs=24
[3] prune_out_channels on _ElementWiseOp_31(AddBackward0) => prune_out_channels on _Reshape_30(), #idxs=24
[4] prune_out_channels on _Reshape_30() => prune_out_channels on _ElementWiseOp_29(PermuteBackward0), #idxs=24
[5] prune_out_channels on _ElementWiseOp_29(PermuteBackward0) => prune_out_channels on _ElementWiseOp_28(Im2ColBackward0), #idxs=24
[6] prune_out_channels on _ElementWiseOp_28(Im2ColBackward0) => prune_out_channels on _ElementWiseOp_27(TransposeBackward0), #idxs=24
[7] prune_out_channels on _ElementWiseOp_27(TransposeBackward0) => prune_out_channels on _ElementWiseOp_26(NativeLayerNormBackward0), #idxs=24
[8] prune_out_channels on _ElementWiseOp_26(NativeLayerNormBackward0) => prune_out_channels on _Reshape_24(), #idxs=24
[9] prune_out_channels on _Reshape_24() => prune_out_channels on _ElementWiseOp_23(MmBackward0), #idxs=24
[10] prune_out_channels on _ElementWiseOp_23(MmBackward0) => prune_out_channels on _ElementWiseOp_25(TBackward0), #idxs=24
[11] prune_out_channels on _ElementWiseOp_23(MmBackward0) => prune_in_channels on backbone.stages.0.downsample (PatchMerging(
  (adap_padding): AdaptivePadding()
  (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
  (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
  (reduction): Linear(in_features=384, out_features=144, bias=False)
)), #idxs=24
[12] prune_out_channels on _ElementWiseOp_32(AddBackward0) => prune_out_channels on _Reshape_40(), #idxs=24
[13] prune_out_channels on _ElementWiseOp_32(AddBackward0) => prune_out_channels on _ElementWiseOp_41(AddBackward0), #idxs=24
[14] prune_out_channels on _ElementWiseOp_32(AddBackward0) => prune_out_channels on backbone.stages.0.blocks.1.norm2 (LayerNorm((96,), eps=1e-05, elementwise_affine=True)), #idxs=24
[15] prune_out_channels on backbone.stages.0.blocks.1.norm2 (LayerNorm((96,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_38(), #idxs=24
[16] prune_out_channels on _Reshape_38() => prune_out_channels on _ElementWiseOp_37(AddmmBackward0), #idxs=24
[17] prune_out_channels on _ElementWiseOp_37(AddmmBackward0) => prune_out_channels on _ElementWiseOp_39(TBackward0), #idxs=24
[18] prune_out_channels on _ElementWiseOp_37(AddmmBackward0) => prune_in_channels on backbone.stages.0.blocks.1.ffn.layers.0.0 (Linear(in_features=96, out_features=384, bias=True)), #idxs=24
[19] prune_out_channels on _ElementWiseOp_41(AddBackward0) => prune_out_channels on _ElementWiseOp_42(AddBackward0), #idxs=24
[20] prune_out_channels on _ElementWiseOp_41(AddBackward0) => prune_out_channels on backbone.stages.0.blocks.0.ffn.layers.1 (Linear(in_features=384, out_features=96, bias=True)), #idxs=24
[21] prune_out_channels on _ElementWiseOp_41(AddBackward0) => prune_out_channels on backbone.stages.0.blocks.1.norm1 (LayerNorm((96,), eps=1e-05, elementwise_affine=True)), #idxs=24
[22] prune_out_channels on backbone.stages.0.blocks.1.norm1 (LayerNorm((96,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_138(), #idxs=24
[23] prune_out_channels on _Reshape_138() => prune_out_channels on _ElementWiseOp_137(ConstantPadNdBackward0), #idxs=24
[24] prune_out_channels on _ElementWiseOp_137(ConstantPadNdBackward0) => prune_out_channels on _ElementWiseOp_136(RollBackward0), #idxs=24
[25] prune_out_channels on _ElementWiseOp_136(RollBackward0) => prune_out_channels on _Reshape_135(), #idxs=24
[26] prune_out_channels on _Reshape_135() => prune_out_channels on _ElementWiseOp_134(PermuteBackward0), #idxs=24
[27] prune_out_channels on _ElementWiseOp_134(PermuteBackward0) => prune_out_channels on _ElementWiseOp_133(CloneBackward0), #idxs=24
[28] prune_out_channels on _ElementWiseOp_133(CloneBackward0) => prune_out_channels on _Reshape_132(), #idxs=24
[29] prune_out_channels on _Reshape_132() => prune_out_channels on _Reshape_131(), #idxs=24
[30] prune_out_channels on _Reshape_131() => prune_out_channels on _Reshape_129(), #idxs=24
[31] prune_out_channels on _Reshape_129() => prune_out_channels on _ElementWiseOp_128(AddmmBackward0), #idxs=24
[32] prune_out_channels on _ElementWiseOp_128(AddmmBackward0) => prune_out_channels on _ElementWiseOp_130(TBackward0), #idxs=24
[33] prune_out_channels on _ElementWiseOp_128(AddmmBackward0) => prune_out_channels on _Reshape_127(), #idxs=24
[34] prune_out_channels on _Reshape_127() => prune_out_channels on _Reshape_126(), #idxs=24
[35] prune_out_channels on _Reshape_126() => prune_out_channels on _ElementWiseOp_125(PermuteBackward0), #idxs=24
[36] prune_out_channels on _ElementWiseOp_125(PermuteBackward0) => prune_out_channels on _ElementWiseOp_124(SelectBackward0), #idxs=24
[37] prune_out_channels on _ElementWiseOp_125(PermuteBackward0) => prune_out_channels on _ElementWiseOp_157(SelectBackward0), #idxs=24
[38] prune_out_channels on _ElementWiseOp_125(PermuteBackward0) => prune_out_channels on _ElementWiseOp_161(SelectBackward0), #idxs=24
[39] prune_out_channels on _ElementWiseOp_161(SelectBackward0) => prune_out_channels on _ElementWiseOp_160(MulBackward0), #idxs=24
[40] prune_out_channels on _ElementWiseOp_160(MulBackward0) => prune_out_channels on _ElementWiseOp_159(ExpandBackward0), #idxs=24
[41] prune_out_channels on _ElementWiseOp_159(ExpandBackward0) => prune_out_channels on _ElementWiseOp_158(CloneBackward0), #idxs=24
[42] prune_out_channels on _ElementWiseOp_158(CloneBackward0) => prune_out_channels on _Reshape_152(), #idxs=24
[43] prune_out_channels on _Reshape_152() => prune_out_channels on _ElementWiseOp_151(BmmBackward0), #idxs=24
[44] prune_out_channels on _ElementWiseOp_151(BmmBackward0) => prune_out_channels on _Reshape_153(), #idxs=24
[45] prune_out_channels on _ElementWiseOp_151(BmmBackward0) => prune_out_channels on _Reshape_145(), #idxs=24
[46] prune_out_channels on _Reshape_145() => prune_out_channels on _ElementWiseOp_144(AddBackward0), #idxs=24
[47] prune_out_channels on _ElementWiseOp_144(AddBackward0) => prune_out_channels on _ElementWiseOp_146(UnsqueezeBackward0), #idxs=24
[48] prune_out_channels on _ElementWiseOp_144(AddBackward0) => prune_out_channels on _Reshape_143(), #idxs=24
[49] prune_out_channels on _Reshape_143() => prune_out_channels on _ElementWiseOp_142(AddBackward0), #idxs=24
[50] prune_out_channels on _ElementWiseOp_142(AddBackward0) => prune_out_channels on _Reshape_141(), #idxs=24
[51] prune_out_channels on _Reshape_141() => prune_out_channels on _ElementWiseOp_140(SoftmaxBackward0), #idxs=24
[52] prune_out_channels on _ElementWiseOp_140(SoftmaxBackward0) => prune_out_channels on _ElementWiseOp_139(ExpandBackward0), #idxs=24
[53] prune_out_channels on _ElementWiseOp_139(ExpandBackward0) => prune_out_channels on _Reshape_120(), #idxs=24
[54] prune_out_channels on _Reshape_120() => prune_out_channels on _ElementWiseOp_119(BmmBackward0), #idxs=24
[55] prune_out_channels on _ElementWiseOp_119(BmmBackward0) => prune_out_channels on _Reshape_121(), #idxs=24
[56] prune_out_channels on _ElementWiseOp_119(BmmBackward0) => prune_out_channels on _Reshape_118(), #idxs=24
[57] prune_out_channels on _Reshape_118() => prune_out_channels on _ElementWiseOp_117(TransposeBackward0), #idxs=24
[58] prune_out_channels on _ElementWiseOp_117(TransposeBackward0) => prune_out_channels on _ElementWiseOp_116(CloneBackward0), #idxs=24
[59] prune_out_channels on _ElementWiseOp_116(CloneBackward0) => prune_in_channels on backbone.stages.0.blocks.1.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=96, out_features=288, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=96, out_features=96, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)), #idxs=24
[60] prune_out_channels on _Reshape_121() => prune_out_channels on _ElementWiseOp_122(CloneBackward0), #idxs=24
[61] prune_out_channels on _ElementWiseOp_122(CloneBackward0) => prune_out_channels on _ElementWiseOp_123(ExpandBackward0), #idxs=24
[62] prune_out_channels on _ElementWiseOp_146(UnsqueezeBackward0) => prune_out_channels on _ElementWiseOp_147(CloneBackward0), #idxs=24
[63] prune_out_channels on _ElementWiseOp_147(CloneBackward0) => prune_out_channels on _ElementWiseOp_148(PermuteBackward0), #idxs=24
[64] prune_out_channels on _ElementWiseOp_148(PermuteBackward0) => prune_out_channels on _Reshape_149(), #idxs=24
[65] prune_out_channels on _Reshape_149() => prune_out_channels on _ElementWiseOp_150(IndexBackward0), #idxs=24
[66] prune_out_channels on _Reshape_153() => prune_out_channels on _ElementWiseOp_154(CloneBackward0), #idxs=24
[67] prune_out_channels on _ElementWiseOp_154(CloneBackward0) => prune_out_channels on _ElementWiseOp_155(ExpandBackward0), #idxs=24
[68] prune_out_channels on _ElementWiseOp_155(ExpandBackward0) => prune_out_channels on _ElementWiseOp_156(TransposeBackward0), #idxs=24
[69] prune_out_channels on _ElementWiseOp_42(AddBackward0) => prune_out_channels on _Reshape_50(), #idxs=24
[70] prune_out_channels on _ElementWiseOp_42(AddBackward0) => prune_out_channels on backbone.patch_embed.norm (LayerNorm((96,), eps=1e-05, elementwise_affine=True)), #idxs=24
[71] prune_out_channels on _ElementWiseOp_42(AddBackward0) => prune_out_channels on backbone.stages.0.blocks.0.norm2 (LayerNorm((96,), eps=1e-05, elementwise_affine=True)), #idxs=24
[72] prune_out_channels on backbone.stages.0.blocks.0.norm2 (LayerNorm((96,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_48(), #idxs=24
[73] prune_out_channels on _Reshape_48() => prune_out_channels on _ElementWiseOp_47(AddmmBackward0), #idxs=24
[74] prune_out_channels on _ElementWiseOp_47(AddmmBackward0) => prune_out_channels on _ElementWiseOp_49(TBackward0), #idxs=24
[75] prune_out_channels on _ElementWiseOp_47(AddmmBackward0) => prune_in_channels on backbone.stages.0.blocks.0.ffn.layers.0.0 (Linear(in_features=96, out_features=384, bias=True)), #idxs=24
[76] prune_out_channels on backbone.patch_embed.norm (LayerNorm((96,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _ElementWiseOp_51(TransposeBackward0), #idxs=24
[77] prune_out_channels on backbone.patch_embed.norm (LayerNorm((96,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on backbone.stages.0.blocks.0.norm1 (LayerNorm((96,), eps=1e-05, elementwise_affine=True)), #idxs=24
[78] prune_out_channels on backbone.stages.0.blocks.0.norm1 (LayerNorm((96,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_84(), #idxs=24
[79] prune_out_channels on _Reshape_84() => prune_out_channels on _ElementWiseOp_83(ConstantPadNdBackward0), #idxs=24
[80] prune_out_channels on _ElementWiseOp_83(ConstantPadNdBackward0) => prune_out_channels on _Reshape_82(), #idxs=24
[81] prune_out_channels on _Reshape_82() => prune_out_channels on _ElementWiseOp_81(PermuteBackward0), #idxs=24
[82] prune_out_channels on _ElementWiseOp_81(PermuteBackward0) => prune_out_channels on _ElementWiseOp_80(CloneBackward0), #idxs=24
[83] prune_out_channels on _ElementWiseOp_80(CloneBackward0) => prune_out_channels on _Reshape_79(), #idxs=24
[84] prune_out_channels on _Reshape_79() => prune_out_channels on _Reshape_78(), #idxs=24
[85] prune_out_channels on _Reshape_78() => prune_out_channels on _Reshape_76(), #idxs=24
[86] prune_out_channels on _Reshape_76() => prune_out_channels on _ElementWiseOp_75(AddmmBackward0), #idxs=24
[87] prune_out_channels on _ElementWiseOp_75(AddmmBackward0) => prune_out_channels on _ElementWiseOp_77(TBackward0), #idxs=24
[88] prune_out_channels on _ElementWiseOp_75(AddmmBackward0) => prune_out_channels on _Reshape_74(), #idxs=24
[89] prune_out_channels on _Reshape_74() => prune_out_channels on _Reshape_73(), #idxs=24
[90] prune_out_channels on _Reshape_73() => prune_out_channels on _ElementWiseOp_72(PermuteBackward0), #idxs=24
[91] prune_out_channels on _ElementWiseOp_72(PermuteBackward0) => prune_out_channels on _ElementWiseOp_71(SelectBackward0), #idxs=24
[92] prune_out_channels on _ElementWiseOp_72(PermuteBackward0) => prune_out_channels on _ElementWiseOp_100(SelectBackward0), #idxs=24
[93] prune_out_channels on _ElementWiseOp_72(PermuteBackward0) => prune_out_channels on _ElementWiseOp_104(SelectBackward0), #idxs=24
[94] prune_out_channels on _ElementWiseOp_104(SelectBackward0) => prune_out_channels on _ElementWiseOp_103(MulBackward0), #idxs=24
[95] prune_out_channels on _ElementWiseOp_103(MulBackward0) => prune_out_channels on _ElementWiseOp_102(ExpandBackward0), #idxs=24
[96] prune_out_channels on _ElementWiseOp_102(ExpandBackward0) => prune_out_channels on _ElementWiseOp_101(CloneBackward0), #idxs=24
[97] prune_out_channels on _ElementWiseOp_101(CloneBackward0) => prune_out_channels on _Reshape_95(), #idxs=24
[98] prune_out_channels on _Reshape_95() => prune_out_channels on _ElementWiseOp_94(BmmBackward0), #idxs=24
[99] prune_out_channels on _ElementWiseOp_94(BmmBackward0) => prune_out_channels on _Reshape_96(), #idxs=24
[100] prune_out_channels on _ElementWiseOp_94(BmmBackward0) => prune_out_channels on _Reshape_88(), #idxs=24
[101] prune_out_channels on _Reshape_88() => prune_out_channels on _ElementWiseOp_87(AddBackward0), #idxs=24
[102] prune_out_channels on _ElementWiseOp_87(AddBackward0) => prune_out_channels on _ElementWiseOp_89(UnsqueezeBackward0), #idxs=24
[103] prune_out_channels on _ElementWiseOp_87(AddBackward0) => prune_out_channels on _ElementWiseOp_86(SoftmaxBackward0), #idxs=24
[104] prune_out_channels on _ElementWiseOp_86(SoftmaxBackward0) => prune_out_channels on _ElementWiseOp_85(ExpandBackward0), #idxs=24
[105] prune_out_channels on _ElementWiseOp_85(ExpandBackward0) => prune_out_channels on _Reshape_67(), #idxs=24
[106] prune_out_channels on _Reshape_67() => prune_out_channels on _ElementWiseOp_66(BmmBackward0), #idxs=24
[107] prune_out_channels on _ElementWiseOp_66(BmmBackward0) => prune_out_channels on _Reshape_68(), #idxs=24
[108] prune_out_channels on _ElementWiseOp_66(BmmBackward0) => prune_out_channels on _Reshape_65(), #idxs=24
[109] prune_out_channels on _Reshape_65() => prune_out_channels on _ElementWiseOp_64(TransposeBackward0), #idxs=24
[110] prune_out_channels on _ElementWiseOp_64(TransposeBackward0) => prune_out_channels on _ElementWiseOp_63(CloneBackward0), #idxs=24
[111] prune_out_channels on _ElementWiseOp_63(CloneBackward0) => prune_in_channels on backbone.stages.0.blocks.0.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=96, out_features=288, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=96, out_features=96, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)), #idxs=24
[112] prune_out_channels on _Reshape_68() => prune_out_channels on _ElementWiseOp_69(CloneBackward0), #idxs=24
[113] prune_out_channels on _ElementWiseOp_69(CloneBackward0) => prune_out_channels on _ElementWiseOp_70(ExpandBackward0), #idxs=24
[114] prune_out_channels on _ElementWiseOp_89(UnsqueezeBackward0) => prune_out_channels on _ElementWiseOp_90(CloneBackward0), #idxs=24
[115] prune_out_channels on _ElementWiseOp_90(CloneBackward0) => prune_out_channels on _ElementWiseOp_91(PermuteBackward0), #idxs=24
[116] prune_out_channels on _ElementWiseOp_91(PermuteBackward0) => prune_out_channels on _Reshape_92(), #idxs=24
[117] prune_out_channels on _Reshape_92() => prune_out_channels on _ElementWiseOp_93(IndexBackward0), #idxs=24
[118] prune_out_channels on _Reshape_96() => prune_out_channels on _ElementWiseOp_97(CloneBackward0), #idxs=24
[119] prune_out_channels on _ElementWiseOp_97(CloneBackward0) => prune_out_channels on _ElementWiseOp_98(ExpandBackward0), #idxs=24
[120] prune_out_channels on _ElementWiseOp_98(ExpandBackward0) => prune_out_channels on _ElementWiseOp_99(TransposeBackward0), #idxs=24
[121] prune_out_channels on _ElementWiseOp_51(TransposeBackward0) => prune_out_channels on _Reshape_52(), #idxs=24
[122] prune_out_channels on _Reshape_52() => prune_out_channels on backbone.patch_embed.projection (Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))), #idxs=24
[123] prune_out_channels on _Reshape_50() => prune_out_channels on _ElementWiseOp_53(CloneBackward0), #idxs=24
[124] prune_out_channels on _ElementWiseOp_53(CloneBackward0) => prune_out_channels on _ElementWiseOp_54(SliceBackward0), #idxs=24
[125] prune_out_channels on _ElementWiseOp_54(SliceBackward0) => prune_out_channels on _ElementWiseOp_55(SliceBackward0), #idxs=24
[126] prune_out_channels on _ElementWiseOp_55(SliceBackward0) => prune_out_channels on _ElementWiseOp_56(SliceBackward0), #idxs=24
[127] prune_out_channels on _ElementWiseOp_56(SliceBackward0) => prune_out_channels on _ElementWiseOp_57(SliceBackward0), #idxs=24
[128] prune_out_channels on _ElementWiseOp_57(SliceBackward0) => prune_out_channels on _Reshape_58(), #idxs=24
[129] prune_out_channels on _Reshape_58() => prune_out_channels on _ElementWiseOp_59(CloneBackward0), #idxs=24
[130] prune_out_channels on _ElementWiseOp_59(CloneBackward0) => prune_out_channels on _ElementWiseOp_60(PermuteBackward0), #idxs=24
[131] prune_out_channels on _ElementWiseOp_60(PermuteBackward0) => prune_out_channels on _Reshape_61(), #idxs=24
[132] prune_out_channels on _Reshape_61() => prune_out_channels on _Reshape_62(), #idxs=24
[133] prune_out_channels on _Reshape_62() => prune_out_channels on backbone.stages.0.blocks.0.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=96, out_features=288, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=96, out_features=96, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)), #idxs=24
[134] prune_out_channels on _Reshape_40() => prune_out_channels on _ElementWiseOp_105(CloneBackward0), #idxs=24
[135] prune_out_channels on _ElementWiseOp_105(CloneBackward0) => prune_out_channels on _ElementWiseOp_106(SliceBackward0), #idxs=24
[136] prune_out_channels on _ElementWiseOp_106(SliceBackward0) => prune_out_channels on _ElementWiseOp_107(SliceBackward0), #idxs=24
[137] prune_out_channels on _ElementWiseOp_107(SliceBackward0) => prune_out_channels on _ElementWiseOp_108(SliceBackward0), #idxs=24
[138] prune_out_channels on _ElementWiseOp_108(SliceBackward0) => prune_out_channels on _ElementWiseOp_109(SliceBackward0), #idxs=24
[139] prune_out_channels on _ElementWiseOp_109(SliceBackward0) => prune_out_channels on _ElementWiseOp_110(RollBackward0), #idxs=24
[140] prune_out_channels on _ElementWiseOp_110(RollBackward0) => prune_out_channels on _Reshape_111(), #idxs=24
[141] prune_out_channels on _Reshape_111() => prune_out_channels on _ElementWiseOp_112(CloneBackward0), #idxs=24
[142] prune_out_channels on _ElementWiseOp_112(CloneBackward0) => prune_out_channels on _ElementWiseOp_113(PermuteBackward0), #idxs=24
[143] prune_out_channels on _ElementWiseOp_113(PermuteBackward0) => prune_out_channels on _Reshape_114(), #idxs=24
[144] prune_out_channels on _Reshape_114() => prune_out_channels on _Reshape_115(), #idxs=24
[145] prune_out_channels on _Reshape_115() => prune_out_channels on backbone.stages.0.blocks.1.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=96, out_features=288, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=96, out_features=96, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)), #idxs=24
--------------------------------

PatchMergingPruner () prune_in_channels/  384 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
PatchMergingPruner () prune_in_channels/ dim =  96
len indx =  24
idxs_repeated =  96
WindowMSAPruner prune_in_channels() /  24
WindowMSAPruner prune_in_channels() /  24
WindowMSAPruner prune_out_channels() /  24
WindowMSAPruner prune_out_channels/ dim =  96
	len indx =  24
WindowMSAPruner - idxs =  [1, 8, 10, 12, 23, 31, 32, 40, 41, 45, 47, 48, 53, 54, 56, 67, 68, 70, 75, 83, 88, 89, 93, 94]
	idxs_repeated =  96
WindowMSAPruner - idxs_repeated =  [1, 8, 10, 12, 23, 31, 32, 40, 41, 45, 47, 48, 53, 54, 56, 67, 68, 70, 75, 83, 88, 89, 93, 94, 97, 104, 106, 108, 119, 127, 128, 136, 137, 141, 143, 144, 149, 150, 152, 163, 164, 166, 171, 179, 184, 185, 189, 190, 193, 200, 202, 204, 215, 223, 224, 232, 233, 237, 239, 240, 245, 246, 248, 259, 260, 262, 267, 275, 280, 281, 285, 286, 289, 296, 298, 300, 311, 319, 320, 328, 329, 333, 335, 336, 341, 342, 344, 355, 356, 358, 363, 371, 376, 377, 381, 382]
WindowMSAPruner prune_out_channels idxs_repeated =  96
WindowMSAPruner prune_out_channels() /  24
WindowMSAPruner prune_out_channels/ dim =  96
	len indx =  24
WindowMSAPruner - idxs =  [1, 8, 10, 12, 23, 31, 32, 40, 41, 45, 47, 48, 53, 54, 56, 67, 68, 70, 75, 83, 88, 89, 93, 94]
	idxs_repeated =  96
WindowMSAPruner - idxs_repeated =  [1, 8, 10, 12, 23, 31, 32, 40, 41, 45, 47, 48, 53, 54, 56, 67, 68, 70, 75, 83, 88, 89, 93, 94, 97, 104, 106, 108, 119, 127, 128, 136, 137, 141, 143, 144, 149, 150, 152, 163, 164, 166, 171, 179, 184, 185, 189, 190, 193, 200, 202, 204, 215, 223, 224, 232, 233, 237, 239, 240, 245, 246, 248, 259, 260, 262, 267, 275, 280, 281, 285, 286, 289, 296, 298, 300, 311, 319, 320, 328, 329, 333, 335, 336, 341, 342, 344, 355, 356, 358, 363, 371, 376, 377, 381, 382]
WindowMSAPruner prune_out_channels idxs_repeated =  96
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
	get_all_groups -- Adding layer:  <class 'mmcv.cnn.bricks.wrappers.Linear'>
		prune_local()/  _check_pruning_ratio OK
Conv/Linear Output GroupNormImportance() / layer Type =  <class 'mmcv.cnn.bricks.wrappers.Linear'>
local_imp len =  384 torch.Size([384])
i =  5
<class 'mmcv.cnn.bricks.wrappers.Linear'> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383] Linear(in_features=384, out_features=72, bias=True)

idxs =  96
prunable_chs =  384
pruner.step 
--------------------------------
          Pruning Group
--------------------------------
[0] prune_out_channels on backbone.stages.0.blocks.1.ffn.layers.0.0 (Linear(in_features=72, out_features=384, bias=True)) => prune_out_channels on backbone.stages.0.blocks.1.ffn.layers.0.0 (Linear(in_features=72, out_features=384, bias=True)), #idxs=96
[1] prune_out_channels on backbone.stages.0.blocks.1.ffn.layers.0.0 (Linear(in_features=72, out_features=384, bias=True)) => prune_out_channels on _ElementWiseOp_36(GeluBackward0), #idxs=96
[2] prune_out_channels on _ElementWiseOp_36(GeluBackward0) => prune_out_channels on _Reshape_34(), #idxs=96
[3] prune_out_channels on _Reshape_34() => prune_out_channels on _ElementWiseOp_33(AddmmBackward0), #idxs=96
[4] prune_out_channels on _ElementWiseOp_33(AddmmBackward0) => prune_out_channels on _ElementWiseOp_35(TBackward0), #idxs=96
[5] prune_out_channels on _ElementWiseOp_33(AddmmBackward0) => prune_in_channels on backbone.stages.0.blocks.1.ffn.layers.1 (Linear(in_features=384, out_features=72, bias=True)), #idxs=96
--------------------------------

<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
continue 4
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
	get_all_groups -- Adding layer:  <class 'mmcv.cnn.bricks.wrappers.Linear'>
		prune_local()/  _check_pruning_ratio OK
Conv/Linear Output GroupNormImportance() / layer Type =  <class 'mmcv.cnn.bricks.wrappers.Linear'>
local_imp len =  384 torch.Size([384])
i =  5
<class 'mmcv.cnn.bricks.wrappers.Linear'> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383] Linear(in_features=384, out_features=72, bias=True)

idxs =  96
prunable_chs =  384
pruner.step 
--------------------------------
          Pruning Group
--------------------------------
[0] prune_out_channels on backbone.stages.0.blocks.0.ffn.layers.0.0 (Linear(in_features=72, out_features=384, bias=True)) => prune_out_channels on backbone.stages.0.blocks.0.ffn.layers.0.0 (Linear(in_features=72, out_features=384, bias=True)), #idxs=96
[1] prune_out_channels on backbone.stages.0.blocks.0.ffn.layers.0.0 (Linear(in_features=72, out_features=384, bias=True)) => prune_out_channels on _ElementWiseOp_46(GeluBackward0), #idxs=96
[2] prune_out_channels on _ElementWiseOp_46(GeluBackward0) => prune_out_channels on _Reshape_44(), #idxs=96
[3] prune_out_channels on _Reshape_44() => prune_out_channels on _ElementWiseOp_43(AddmmBackward0), #idxs=96
[4] prune_out_channels on _ElementWiseOp_43(AddmmBackward0) => prune_out_channels on _ElementWiseOp_45(TBackward0), #idxs=96
[5] prune_out_channels on _ElementWiseOp_43(AddmmBackward0) => prune_in_channels on backbone.stages.0.blocks.0.ffn.layers.1 (Linear(in_features=384, out_features=72, bias=True)), #idxs=96
--------------------------------

<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
continue 4
<class 'torch_pruning.ops._ReshapeOp'> continue 2
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
WindowMSAPruner layer.embed_dims =  72
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
WindowMSAPruner layer.embed_dims =  72
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
WindowMSAPruner layer.embed_dims =  144
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
WindowMSAPruner layer.embed_dims =  144
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch.nn.modules.normalization.GroupNorm'> continue 2
	get_all_groups -- Adding layer:  <class 'torch.nn.modules.conv.Conv2d'>
		prune_local()/  _check_pruning_ratio OK
Conv/Linear Output GroupNormImportance() / layer Type =  <class 'torch.nn.modules.conv.Conv2d'>
idxs =  64
prunable_chs =  256
idxs =  64
prunable_chs =  256
pruner.step 
--------------------------------
          Pruning Group
--------------------------------
[0] prune_out_channels on neck.convs.1.conv (Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))) => prune_out_channels on neck.convs.1.conv (Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))), #idxs=64
[1] prune_out_channels on neck.convs.1.conv (Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))) => prune_out_channels on neck.convs.1.gn (GroupNorm(32, 256, eps=1e-05, affine=True)), #idxs=64
--------------------------------

<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
	get_all_groups -- Adding layer:  <class 'torch.nn.modules.normalization.LayerNorm'>
PatchMergingPruner () get_in_channels/  384 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
WindowMSAPruner layer.qkv.in_features =  384
WindowMSAPruner layer.qkv.in_features =  384
WindowMSAPruner layer.qkv.in_features =  384
WindowMSAPruner layer.qkv.in_features =  384
WindowMSAPruner layer.qkv.in_features =  384
PatchMergingPruner () get_out_channels/  384 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
WindowMSAPruner layer.qkv.in_features =  384
WindowMSAPruner layer.embed_dims =  384
WindowMSAPruner layer.embed_dims =  384
WindowMSAPruner layer.embed_dims =  384
WindowMSAPruner layer.embed_dims =  384
WindowMSAPruner layer.embed_dims =  384
WindowMSAPruner layer.embed_dims =  384
		prune_local()/  _check_pruning_ratio OK
local_imp len =  384 torch.Size([384])
i =  5
<class 'torch.nn.modules.conv.Conv2d'> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383] Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))

Conv/Linear Output GroupNormImportance() / layer Type =  <class 'mmcv.cnn.bricks.wrappers.Linear'>
local_imp len =  384 torch.Size([384])
i =  23
<class 'mmcv.cnn.bricks.wrappers.Linear'> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383] Linear(in_features=384, out_features=1536, bias=True)

Conv/Linear Output GroupNormImportance() / layer Type =  <class 'mmcv.cnn.bricks.wrappers.Linear'>
local_imp len =  384 torch.Size([384])
i =  80
<class 'mmcv.cnn.bricks.wrappers.Linear'> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383] Linear(in_features=384, out_features=1536, bias=True)

Conv/Linear Output GroupNormImportance() / layer Type =  <class 'mmcv.cnn.bricks.wrappers.Linear'>
local_imp len =  384 torch.Size([384])
i =  133
<class 'mmcv.cnn.bricks.wrappers.Linear'> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383] Linear(in_features=384, out_features=1536, bias=True)

Conv/Linear Output GroupNormImportance() / layer Type =  <class 'mmcv.cnn.bricks.wrappers.Linear'>
local_imp len =  384 torch.Size([384])
i =  190
<class 'mmcv.cnn.bricks.wrappers.Linear'> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383] Linear(in_features=384, out_features=1536, bias=True)

Conv/Linear Output GroupNormImportance() / layer Type =  <class 'mmcv.cnn.bricks.wrappers.Linear'>
local_imp len =  384 torch.Size([384])
i =  243
<class 'mmcv.cnn.bricks.wrappers.Linear'> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383] Linear(in_features=384, out_features=1536, bias=True)

Conv/Linear Output GroupNormImportance() / layer Type =  <class 'mmcv.cnn.bricks.wrappers.Linear'>
local_imp len =  384 torch.Size([384])
i =  300
<class 'mmcv.cnn.bricks.wrappers.Linear'> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383] Linear(in_features=384, out_features=1536, bias=True)

idxs =  96
prunable_chs =  384
idxs =  96
prunable_chs =  384
PatchMergingPruner () get_in_channels/  384 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
idxs =  96
prunable_chs =  384
idxs =  96
prunable_chs =  384
idxs =  96
prunable_chs =  384
WindowMSAPruner layer.qkv.in_features =  384
idxs =  96
prunable_chs =  384
idxs =  96
prunable_chs =  384
idxs =  96
prunable_chs =  384
WindowMSAPruner layer.qkv.in_features =  384
idxs =  96
prunable_chs =  384
idxs =  96
prunable_chs =  384
idxs =  96
prunable_chs =  384
WindowMSAPruner layer.qkv.in_features =  384
idxs =  96
prunable_chs =  384
idxs =  96
prunable_chs =  384
idxs =  96
prunable_chs =  384
WindowMSAPruner layer.qkv.in_features =  384
idxs =  96
prunable_chs =  384
idxs =  96
prunable_chs =  384
idxs =  96
prunable_chs =  384
WindowMSAPruner layer.qkv.in_features =  384
PatchMergingPruner () get_out_channels/  384 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
idxs =  96
prunable_chs =  384
idxs =  96
prunable_chs =  384
idxs =  96
prunable_chs =  384
WindowMSAPruner layer.qkv.in_features =  384
WindowMSAPruner layer.embed_dims =  384
idxs =  96
prunable_chs =  384
WindowMSAPruner layer.embed_dims =  384
idxs =  96
prunable_chs =  384
WindowMSAPruner layer.embed_dims =  384
idxs =  96
prunable_chs =  384
WindowMSAPruner layer.embed_dims =  384
idxs =  96
prunable_chs =  384
WindowMSAPruner layer.embed_dims =  384
idxs =  96
prunable_chs =  384
WindowMSAPruner layer.embed_dims =  384
idxs =  96
prunable_chs =  384
pruner.step 
--------------------------------
          Pruning Group
--------------------------------
[0] prune_out_channels on backbone.norm2 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on backbone.norm2 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)), #idxs=96
[1] prune_out_channels on backbone.norm2 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _ElementWiseOp_274(AddBackward0), #idxs=96
[2] prune_out_channels on backbone.norm2 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_273(), #idxs=96
[3] prune_out_channels on _Reshape_273() => prune_out_channels on _ElementWiseOp_272(PermuteBackward0), #idxs=96
[4] prune_out_channels on _ElementWiseOp_272(PermuteBackward0) => prune_out_channels on _ElementWiseOp_271(CloneBackward0), #idxs=96
[5] prune_out_channels on _ElementWiseOp_271(CloneBackward0) => prune_in_channels on neck.convs.1.conv (Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))), #idxs=96
[6] prune_out_channels on _ElementWiseOp_274(AddBackward0) => prune_out_channels on _ElementWiseOp_275(AddBackward0), #idxs=96
[7] prune_out_channels on _ElementWiseOp_274(AddBackward0) => prune_out_channels on backbone.stages.2.blocks.5.ffn.layers.1 (Linear(in_features=1536, out_features=384, bias=True)), #idxs=96
[8] prune_out_channels on _ElementWiseOp_274(AddBackward0) => prune_out_channels on _Reshape_688(), #idxs=96
[9] prune_out_channels on _Reshape_688() => prune_out_channels on _ElementWiseOp_687(PermuteBackward0), #idxs=96
[10] prune_out_channels on _ElementWiseOp_687(PermuteBackward0) => prune_out_channels on _ElementWiseOp_686(Im2ColBackward0), #idxs=96
[11] prune_out_channels on _ElementWiseOp_686(Im2ColBackward0) => prune_out_channels on _ElementWiseOp_685(TransposeBackward0), #idxs=96
[12] prune_out_channels on _ElementWiseOp_685(TransposeBackward0) => prune_out_channels on _ElementWiseOp_684(NativeLayerNormBackward0), #idxs=96
[13] prune_out_channels on _ElementWiseOp_684(NativeLayerNormBackward0) => prune_out_channels on _Reshape_682(), #idxs=96
[14] prune_out_channels on _Reshape_682() => prune_out_channels on _ElementWiseOp_681(MmBackward0), #idxs=96
[15] prune_out_channels on _ElementWiseOp_681(MmBackward0) => prune_out_channels on _ElementWiseOp_683(TBackward0), #idxs=96
[16] prune_out_channels on _ElementWiseOp_681(MmBackward0) => prune_in_channels on backbone.stages.2.downsample (PatchMerging(
  (adap_padding): AdaptivePadding()
  (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
  (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
  (reduction): Linear(in_features=1536, out_features=768, bias=False)
)), #idxs=96
[17] prune_out_channels on _ElementWiseOp_275(AddBackward0) => prune_out_channels on _Reshape_283(), #idxs=96
[18] prune_out_channels on _ElementWiseOp_275(AddBackward0) => prune_out_channels on _ElementWiseOp_284(AddBackward0), #idxs=96
[19] prune_out_channels on _ElementWiseOp_275(AddBackward0) => prune_out_channels on backbone.stages.2.blocks.5.norm2 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)), #idxs=96
[20] prune_out_channels on backbone.stages.2.blocks.5.norm2 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_281(), #idxs=96
[21] prune_out_channels on _Reshape_281() => prune_out_channels on _ElementWiseOp_280(AddmmBackward0), #idxs=96
[22] prune_out_channels on _ElementWiseOp_280(AddmmBackward0) => prune_out_channels on _ElementWiseOp_282(TBackward0), #idxs=96
[23] prune_out_channels on _ElementWiseOp_280(AddmmBackward0) => prune_in_channels on backbone.stages.2.blocks.5.ffn.layers.0.0 (Linear(in_features=384, out_features=1536, bias=True)), #idxs=96
[24] prune_out_channels on _ElementWiseOp_284(AddBackward0) => prune_out_channels on _ElementWiseOp_285(AddBackward0), #idxs=96
[25] prune_out_channels on _ElementWiseOp_284(AddBackward0) => prune_out_channels on backbone.stages.2.blocks.4.ffn.layers.1 (Linear(in_features=1536, out_features=384, bias=True)), #idxs=96
[26] prune_out_channels on _ElementWiseOp_284(AddBackward0) => prune_out_channels on backbone.stages.2.blocks.5.norm1 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)), #idxs=96
[27] prune_out_channels on backbone.stages.2.blocks.5.norm1 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_634(), #idxs=96
[28] prune_out_channels on _Reshape_634() => prune_out_channels on _ElementWiseOp_633(ConstantPadNdBackward0), #idxs=96
[29] prune_out_channels on _ElementWiseOp_633(ConstantPadNdBackward0) => prune_out_channels on _ElementWiseOp_632(RollBackward0), #idxs=96
[30] prune_out_channels on _ElementWiseOp_632(RollBackward0) => prune_out_channels on _Reshape_631(), #idxs=96
[31] prune_out_channels on _Reshape_631() => prune_out_channels on _ElementWiseOp_630(PermuteBackward0), #idxs=96
[32] prune_out_channels on _ElementWiseOp_630(PermuteBackward0) => prune_out_channels on _ElementWiseOp_629(CloneBackward0), #idxs=96
[33] prune_out_channels on _ElementWiseOp_629(CloneBackward0) => prune_out_channels on _Reshape_628(), #idxs=96
[34] prune_out_channels on _Reshape_628() => prune_out_channels on _Reshape_627(), #idxs=96
[35] prune_out_channels on _Reshape_627() => prune_out_channels on _Reshape_625(), #idxs=96
[36] prune_out_channels on _Reshape_625() => prune_out_channels on _ElementWiseOp_624(AddmmBackward0), #idxs=96
[37] prune_out_channels on _ElementWiseOp_624(AddmmBackward0) => prune_out_channels on _ElementWiseOp_626(TBackward0), #idxs=96
[38] prune_out_channels on _ElementWiseOp_624(AddmmBackward0) => prune_out_channels on _Reshape_623(), #idxs=96
[39] prune_out_channels on _Reshape_623() => prune_out_channels on _Reshape_622(), #idxs=96
[40] prune_out_channels on _Reshape_622() => prune_out_channels on _ElementWiseOp_621(PermuteBackward0), #idxs=96
[41] prune_out_channels on _ElementWiseOp_621(PermuteBackward0) => prune_out_channels on _ElementWiseOp_620(SelectBackward0), #idxs=96
[42] prune_out_channels on _ElementWiseOp_621(PermuteBackward0) => prune_out_channels on _ElementWiseOp_653(SelectBackward0), #idxs=96
[43] prune_out_channels on _ElementWiseOp_621(PermuteBackward0) => prune_out_channels on _ElementWiseOp_657(SelectBackward0), #idxs=96
[44] prune_out_channels on _ElementWiseOp_657(SelectBackward0) => prune_out_channels on _ElementWiseOp_656(MulBackward0), #idxs=96
[45] prune_out_channels on _ElementWiseOp_656(MulBackward0) => prune_out_channels on _ElementWiseOp_655(ExpandBackward0), #idxs=96
[46] prune_out_channels on _ElementWiseOp_655(ExpandBackward0) => prune_out_channels on _ElementWiseOp_654(CloneBackward0), #idxs=96
[47] prune_out_channels on _ElementWiseOp_654(CloneBackward0) => prune_out_channels on _Reshape_648(), #idxs=96
[48] prune_out_channels on _Reshape_648() => prune_out_channels on _ElementWiseOp_647(BmmBackward0), #idxs=96
[49] prune_out_channels on _ElementWiseOp_647(BmmBackward0) => prune_out_channels on _Reshape_649(), #idxs=96
[50] prune_out_channels on _ElementWiseOp_647(BmmBackward0) => prune_out_channels on _Reshape_641(), #idxs=96
[51] prune_out_channels on _Reshape_641() => prune_out_channels on _ElementWiseOp_640(AddBackward0), #idxs=96
[52] prune_out_channels on _ElementWiseOp_640(AddBackward0) => prune_out_channels on _ElementWiseOp_642(UnsqueezeBackward0), #idxs=96
[53] prune_out_channels on _ElementWiseOp_640(AddBackward0) => prune_out_channels on _Reshape_639(), #idxs=96
[54] prune_out_channels on _Reshape_639() => prune_out_channels on _ElementWiseOp_638(AddBackward0), #idxs=96
[55] prune_out_channels on _ElementWiseOp_638(AddBackward0) => prune_out_channels on _Reshape_637(), #idxs=96
[56] prune_out_channels on _Reshape_637() => prune_out_channels on _ElementWiseOp_636(SoftmaxBackward0), #idxs=96
[57] prune_out_channels on _ElementWiseOp_636(SoftmaxBackward0) => prune_out_channels on _ElementWiseOp_635(ExpandBackward0), #idxs=96
[58] prune_out_channels on _ElementWiseOp_635(ExpandBackward0) => prune_out_channels on _Reshape_616(), #idxs=96
[59] prune_out_channels on _Reshape_616() => prune_out_channels on _ElementWiseOp_615(BmmBackward0), #idxs=96
[60] prune_out_channels on _ElementWiseOp_615(BmmBackward0) => prune_out_channels on _Reshape_617(), #idxs=96
[61] prune_out_channels on _ElementWiseOp_615(BmmBackward0) => prune_out_channels on _Reshape_614(), #idxs=96
[62] prune_out_channels on _Reshape_614() => prune_out_channels on _ElementWiseOp_613(TransposeBackward0), #idxs=96
[63] prune_out_channels on _ElementWiseOp_613(TransposeBackward0) => prune_out_channels on _ElementWiseOp_612(CloneBackward0), #idxs=96
[64] prune_out_channels on _ElementWiseOp_612(CloneBackward0) => prune_in_channels on backbone.stages.2.blocks.5.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=384, out_features=1152, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=384, out_features=384, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)), #idxs=96
[65] prune_out_channels on _Reshape_617() => prune_out_channels on _ElementWiseOp_618(CloneBackward0), #idxs=96
[66] prune_out_channels on _ElementWiseOp_618(CloneBackward0) => prune_out_channels on _ElementWiseOp_619(ExpandBackward0), #idxs=96
[67] prune_out_channels on _ElementWiseOp_642(UnsqueezeBackward0) => prune_out_channels on _ElementWiseOp_643(CloneBackward0), #idxs=96
[68] prune_out_channels on _ElementWiseOp_643(CloneBackward0) => prune_out_channels on _ElementWiseOp_644(PermuteBackward0), #idxs=96
[69] prune_out_channels on _ElementWiseOp_644(PermuteBackward0) => prune_out_channels on _Reshape_645(), #idxs=96
[70] prune_out_channels on _Reshape_645() => prune_out_channels on _ElementWiseOp_646(IndexBackward0), #idxs=96
[71] prune_out_channels on _Reshape_649() => prune_out_channels on _ElementWiseOp_650(CloneBackward0), #idxs=96
[72] prune_out_channels on _ElementWiseOp_650(CloneBackward0) => prune_out_channels on _ElementWiseOp_651(ExpandBackward0), #idxs=96
[73] prune_out_channels on _ElementWiseOp_651(ExpandBackward0) => prune_out_channels on _ElementWiseOp_652(TransposeBackward0), #idxs=96
[74] prune_out_channels on _ElementWiseOp_285(AddBackward0) => prune_out_channels on _Reshape_293(), #idxs=96
[75] prune_out_channels on _ElementWiseOp_285(AddBackward0) => prune_out_channels on _ElementWiseOp_294(AddBackward0), #idxs=96
[76] prune_out_channels on _ElementWiseOp_285(AddBackward0) => prune_out_channels on backbone.stages.2.blocks.4.norm2 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)), #idxs=96
[77] prune_out_channels on backbone.stages.2.blocks.4.norm2 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_291(), #idxs=96
[78] prune_out_channels on _Reshape_291() => prune_out_channels on _ElementWiseOp_290(AddmmBackward0), #idxs=96
[79] prune_out_channels on _ElementWiseOp_290(AddmmBackward0) => prune_out_channels on _ElementWiseOp_292(TBackward0), #idxs=96
[80] prune_out_channels on _ElementWiseOp_290(AddmmBackward0) => prune_in_channels on backbone.stages.2.blocks.4.ffn.layers.0.0 (Linear(in_features=384, out_features=1536, bias=True)), #idxs=96
[81] prune_out_channels on _ElementWiseOp_294(AddBackward0) => prune_out_channels on _ElementWiseOp_295(AddBackward0), #idxs=96
[82] prune_out_channels on _ElementWiseOp_294(AddBackward0) => prune_out_channels on backbone.stages.2.blocks.3.ffn.layers.1 (Linear(in_features=1536, out_features=384, bias=True)), #idxs=96
[83] prune_out_channels on _ElementWiseOp_294(AddBackward0) => prune_out_channels on backbone.stages.2.blocks.4.norm1 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)), #idxs=96
[84] prune_out_channels on backbone.stages.2.blocks.4.norm1 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_582(), #idxs=96
[85] prune_out_channels on _Reshape_582() => prune_out_channels on _ElementWiseOp_581(ConstantPadNdBackward0), #idxs=96
[86] prune_out_channels on _ElementWiseOp_581(ConstantPadNdBackward0) => prune_out_channels on _Reshape_580(), #idxs=96
[87] prune_out_channels on _Reshape_580() => prune_out_channels on _ElementWiseOp_579(PermuteBackward0), #idxs=96
[88] prune_out_channels on _ElementWiseOp_579(PermuteBackward0) => prune_out_channels on _ElementWiseOp_578(CloneBackward0), #idxs=96
[89] prune_out_channels on _ElementWiseOp_578(CloneBackward0) => prune_out_channels on _Reshape_577(), #idxs=96
[90] prune_out_channels on _Reshape_577() => prune_out_channels on _Reshape_576(), #idxs=96
[91] prune_out_channels on _Reshape_576() => prune_out_channels on _Reshape_574(), #idxs=96
[92] prune_out_channels on _Reshape_574() => prune_out_channels on _ElementWiseOp_573(AddmmBackward0), #idxs=96
[93] prune_out_channels on _ElementWiseOp_573(AddmmBackward0) => prune_out_channels on _ElementWiseOp_575(TBackward0), #idxs=96
[94] prune_out_channels on _ElementWiseOp_573(AddmmBackward0) => prune_out_channels on _Reshape_572(), #idxs=96
[95] prune_out_channels on _Reshape_572() => prune_out_channels on _Reshape_571(), #idxs=96
[96] prune_out_channels on _Reshape_571() => prune_out_channels on _ElementWiseOp_570(PermuteBackward0), #idxs=96
[97] prune_out_channels on _ElementWiseOp_570(PermuteBackward0) => prune_out_channels on _ElementWiseOp_569(SelectBackward0), #idxs=96
[98] prune_out_channels on _ElementWiseOp_570(PermuteBackward0) => prune_out_channels on _ElementWiseOp_598(SelectBackward0), #idxs=96
[99] prune_out_channels on _ElementWiseOp_570(PermuteBackward0) => prune_out_channels on _ElementWiseOp_602(SelectBackward0), #idxs=96
[100] prune_out_channels on _ElementWiseOp_602(SelectBackward0) => prune_out_channels on _ElementWiseOp_601(MulBackward0), #idxs=96
[101] prune_out_channels on _ElementWiseOp_601(MulBackward0) => prune_out_channels on _ElementWiseOp_600(ExpandBackward0), #idxs=96
[102] prune_out_channels on _ElementWiseOp_600(ExpandBackward0) => prune_out_channels on _ElementWiseOp_599(CloneBackward0), #idxs=96
[103] prune_out_channels on _ElementWiseOp_599(CloneBackward0) => prune_out_channels on _Reshape_593(), #idxs=96
[104] prune_out_channels on _Reshape_593() => prune_out_channels on _ElementWiseOp_592(BmmBackward0), #idxs=96
[105] prune_out_channels on _ElementWiseOp_592(BmmBackward0) => prune_out_channels on _Reshape_594(), #idxs=96
[106] prune_out_channels on _ElementWiseOp_592(BmmBackward0) => prune_out_channels on _Reshape_586(), #idxs=96
[107] prune_out_channels on _Reshape_586() => prune_out_channels on _ElementWiseOp_585(AddBackward0), #idxs=96
[108] prune_out_channels on _ElementWiseOp_585(AddBackward0) => prune_out_channels on _ElementWiseOp_587(UnsqueezeBackward0), #idxs=96
[109] prune_out_channels on _ElementWiseOp_585(AddBackward0) => prune_out_channels on _ElementWiseOp_584(SoftmaxBackward0), #idxs=96
[110] prune_out_channels on _ElementWiseOp_584(SoftmaxBackward0) => prune_out_channels on _ElementWiseOp_583(ExpandBackward0), #idxs=96
[111] prune_out_channels on _ElementWiseOp_583(ExpandBackward0) => prune_out_channels on _Reshape_565(), #idxs=96
[112] prune_out_channels on _Reshape_565() => prune_out_channels on _ElementWiseOp_564(BmmBackward0), #idxs=96
[113] prune_out_channels on _ElementWiseOp_564(BmmBackward0) => prune_out_channels on _Reshape_566(), #idxs=96
[114] prune_out_channels on _ElementWiseOp_564(BmmBackward0) => prune_out_channels on _Reshape_563(), #idxs=96
[115] prune_out_channels on _Reshape_563() => prune_out_channels on _ElementWiseOp_562(TransposeBackward0), #idxs=96
[116] prune_out_channels on _ElementWiseOp_562(TransposeBackward0) => prune_out_channels on _ElementWiseOp_561(CloneBackward0), #idxs=96
[117] prune_out_channels on _ElementWiseOp_561(CloneBackward0) => prune_in_channels on backbone.stages.2.blocks.4.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=384, out_features=1152, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=384, out_features=384, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)), #idxs=96
[118] prune_out_channels on _Reshape_566() => prune_out_channels on _ElementWiseOp_567(CloneBackward0), #idxs=96
[119] prune_out_channels on _ElementWiseOp_567(CloneBackward0) => prune_out_channels on _ElementWiseOp_568(ExpandBackward0), #idxs=96
[120] prune_out_channels on _ElementWiseOp_587(UnsqueezeBackward0) => prune_out_channels on _ElementWiseOp_588(CloneBackward0), #idxs=96
[121] prune_out_channels on _ElementWiseOp_588(CloneBackward0) => prune_out_channels on _ElementWiseOp_589(PermuteBackward0), #idxs=96
[122] prune_out_channels on _ElementWiseOp_589(PermuteBackward0) => prune_out_channels on _Reshape_590(), #idxs=96
[123] prune_out_channels on _Reshape_590() => prune_out_channels on _ElementWiseOp_591(IndexBackward0), #idxs=96
[124] prune_out_channels on _Reshape_594() => prune_out_channels on _ElementWiseOp_595(CloneBackward0), #idxs=96
[125] prune_out_channels on _ElementWiseOp_595(CloneBackward0) => prune_out_channels on _ElementWiseOp_596(ExpandBackward0), #idxs=96
[126] prune_out_channels on _ElementWiseOp_596(ExpandBackward0) => prune_out_channels on _ElementWiseOp_597(TransposeBackward0), #idxs=96
[127] prune_out_channels on _ElementWiseOp_295(AddBackward0) => prune_out_channels on _Reshape_303(), #idxs=96
[128] prune_out_channels on _ElementWiseOp_295(AddBackward0) => prune_out_channels on _ElementWiseOp_304(AddBackward0), #idxs=96
[129] prune_out_channels on _ElementWiseOp_295(AddBackward0) => prune_out_channels on backbone.stages.2.blocks.3.norm2 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)), #idxs=96
[130] prune_out_channels on backbone.stages.2.blocks.3.norm2 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_301(), #idxs=96
[131] prune_out_channels on _Reshape_301() => prune_out_channels on _ElementWiseOp_300(AddmmBackward0), #idxs=96
[132] prune_out_channels on _ElementWiseOp_300(AddmmBackward0) => prune_out_channels on _ElementWiseOp_302(TBackward0), #idxs=96
[133] prune_out_channels on _ElementWiseOp_300(AddmmBackward0) => prune_in_channels on backbone.stages.2.blocks.3.ffn.layers.0.0 (Linear(in_features=384, out_features=1536, bias=True)), #idxs=96
[134] prune_out_channels on _ElementWiseOp_304(AddBackward0) => prune_out_channels on _ElementWiseOp_305(AddBackward0), #idxs=96
[135] prune_out_channels on _ElementWiseOp_304(AddBackward0) => prune_out_channels on backbone.stages.2.blocks.2.ffn.layers.1 (Linear(in_features=1536, out_features=384, bias=True)), #idxs=96
[136] prune_out_channels on _ElementWiseOp_304(AddBackward0) => prune_out_channels on backbone.stages.2.blocks.3.norm1 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)), #idxs=96
[137] prune_out_channels on backbone.stages.2.blocks.3.norm1 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_529(), #idxs=96
[138] prune_out_channels on _Reshape_529() => prune_out_channels on _ElementWiseOp_528(ConstantPadNdBackward0), #idxs=96
[139] prune_out_channels on _ElementWiseOp_528(ConstantPadNdBackward0) => prune_out_channels on _ElementWiseOp_527(RollBackward0), #idxs=96
[140] prune_out_channels on _ElementWiseOp_527(RollBackward0) => prune_out_channels on _Reshape_526(), #idxs=96
[141] prune_out_channels on _Reshape_526() => prune_out_channels on _ElementWiseOp_525(PermuteBackward0), #idxs=96
[142] prune_out_channels on _ElementWiseOp_525(PermuteBackward0) => prune_out_channels on _ElementWiseOp_524(CloneBackward0), #idxs=96
[143] prune_out_channels on _ElementWiseOp_524(CloneBackward0) => prune_out_channels on _Reshape_523(), #idxs=96
[144] prune_out_channels on _Reshape_523() => prune_out_channels on _Reshape_522(), #idxs=96
[145] prune_out_channels on _Reshape_522() => prune_out_channels on _Reshape_520(), #idxs=96
[146] prune_out_channels on _Reshape_520() => prune_out_channels on _ElementWiseOp_519(AddmmBackward0), #idxs=96
[147] prune_out_channels on _ElementWiseOp_519(AddmmBackward0) => prune_out_channels on _ElementWiseOp_521(TBackward0), #idxs=96
[148] prune_out_channels on _ElementWiseOp_519(AddmmBackward0) => prune_out_channels on _Reshape_518(), #idxs=96
[149] prune_out_channels on _Reshape_518() => prune_out_channels on _Reshape_517(), #idxs=96
[150] prune_out_channels on _Reshape_517() => prune_out_channels on _ElementWiseOp_516(PermuteBackward0), #idxs=96
[151] prune_out_channels on _ElementWiseOp_516(PermuteBackward0) => prune_out_channels on _ElementWiseOp_515(SelectBackward0), #idxs=96
[152] prune_out_channels on _ElementWiseOp_516(PermuteBackward0) => prune_out_channels on _ElementWiseOp_548(SelectBackward0), #idxs=96
[153] prune_out_channels on _ElementWiseOp_516(PermuteBackward0) => prune_out_channels on _ElementWiseOp_552(SelectBackward0), #idxs=96
[154] prune_out_channels on _ElementWiseOp_552(SelectBackward0) => prune_out_channels on _ElementWiseOp_551(MulBackward0), #idxs=96
[155] prune_out_channels on _ElementWiseOp_551(MulBackward0) => prune_out_channels on _ElementWiseOp_550(ExpandBackward0), #idxs=96
[156] prune_out_channels on _ElementWiseOp_550(ExpandBackward0) => prune_out_channels on _ElementWiseOp_549(CloneBackward0), #idxs=96
[157] prune_out_channels on _ElementWiseOp_549(CloneBackward0) => prune_out_channels on _Reshape_543(), #idxs=96
[158] prune_out_channels on _Reshape_543() => prune_out_channels on _ElementWiseOp_542(BmmBackward0), #idxs=96
[159] prune_out_channels on _ElementWiseOp_542(BmmBackward0) => prune_out_channels on _Reshape_544(), #idxs=96
[160] prune_out_channels on _ElementWiseOp_542(BmmBackward0) => prune_out_channels on _Reshape_536(), #idxs=96
[161] prune_out_channels on _Reshape_536() => prune_out_channels on _ElementWiseOp_535(AddBackward0), #idxs=96
[162] prune_out_channels on _ElementWiseOp_535(AddBackward0) => prune_out_channels on _ElementWiseOp_537(UnsqueezeBackward0), #idxs=96
[163] prune_out_channels on _ElementWiseOp_535(AddBackward0) => prune_out_channels on _Reshape_534(), #idxs=96
[164] prune_out_channels on _Reshape_534() => prune_out_channels on _ElementWiseOp_533(AddBackward0), #idxs=96
[165] prune_out_channels on _ElementWiseOp_533(AddBackward0) => prune_out_channels on _Reshape_532(), #idxs=96
[166] prune_out_channels on _Reshape_532() => prune_out_channels on _ElementWiseOp_531(SoftmaxBackward0), #idxs=96
[167] prune_out_channels on _ElementWiseOp_531(SoftmaxBackward0) => prune_out_channels on _ElementWiseOp_530(ExpandBackward0), #idxs=96
[168] prune_out_channels on _ElementWiseOp_530(ExpandBackward0) => prune_out_channels on _Reshape_511(), #idxs=96
[169] prune_out_channels on _Reshape_511() => prune_out_channels on _ElementWiseOp_510(BmmBackward0), #idxs=96
[170] prune_out_channels on _ElementWiseOp_510(BmmBackward0) => prune_out_channels on _Reshape_512(), #idxs=96
[171] prune_out_channels on _ElementWiseOp_510(BmmBackward0) => prune_out_channels on _Reshape_509(), #idxs=96
[172] prune_out_channels on _Reshape_509() => prune_out_channels on _ElementWiseOp_508(TransposeBackward0), #idxs=96
[173] prune_out_channels on _ElementWiseOp_508(TransposeBackward0) => prune_out_channels on _ElementWiseOp_507(CloneBackward0), #idxs=96
[174] prune_out_channels on _ElementWiseOp_507(CloneBackward0) => prune_in_channels on backbone.stages.2.blocks.3.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=384, out_features=1152, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=384, out_features=384, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)), #idxs=96
[175] prune_out_channels on _Reshape_512() => prune_out_channels on _ElementWiseOp_513(CloneBackward0), #idxs=96
[176] prune_out_channels on _ElementWiseOp_513(CloneBackward0) => prune_out_channels on _ElementWiseOp_514(ExpandBackward0), #idxs=96
[177] prune_out_channels on _ElementWiseOp_537(UnsqueezeBackward0) => prune_out_channels on _ElementWiseOp_538(CloneBackward0), #idxs=96
[178] prune_out_channels on _ElementWiseOp_538(CloneBackward0) => prune_out_channels on _ElementWiseOp_539(PermuteBackward0), #idxs=96
[179] prune_out_channels on _ElementWiseOp_539(PermuteBackward0) => prune_out_channels on _Reshape_540(), #idxs=96
[180] prune_out_channels on _Reshape_540() => prune_out_channels on _ElementWiseOp_541(IndexBackward0), #idxs=96
[181] prune_out_channels on _Reshape_544() => prune_out_channels on _ElementWiseOp_545(CloneBackward0), #idxs=96
[182] prune_out_channels on _ElementWiseOp_545(CloneBackward0) => prune_out_channels on _ElementWiseOp_546(ExpandBackward0), #idxs=96
[183] prune_out_channels on _ElementWiseOp_546(ExpandBackward0) => prune_out_channels on _ElementWiseOp_547(TransposeBackward0), #idxs=96
[184] prune_out_channels on _ElementWiseOp_305(AddBackward0) => prune_out_channels on _Reshape_313(), #idxs=96
[185] prune_out_channels on _ElementWiseOp_305(AddBackward0) => prune_out_channels on _ElementWiseOp_314(AddBackward0), #idxs=96
[186] prune_out_channels on _ElementWiseOp_305(AddBackward0) => prune_out_channels on backbone.stages.2.blocks.2.norm2 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)), #idxs=96
[187] prune_out_channels on backbone.stages.2.blocks.2.norm2 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_311(), #idxs=96
[188] prune_out_channels on _Reshape_311() => prune_out_channels on _ElementWiseOp_310(AddmmBackward0), #idxs=96
[189] prune_out_channels on _ElementWiseOp_310(AddmmBackward0) => prune_out_channels on _ElementWiseOp_312(TBackward0), #idxs=96
[190] prune_out_channels on _ElementWiseOp_310(AddmmBackward0) => prune_in_channels on backbone.stages.2.blocks.2.ffn.layers.0.0 (Linear(in_features=384, out_features=1536, bias=True)), #idxs=96
[191] prune_out_channels on _ElementWiseOp_314(AddBackward0) => prune_out_channels on _ElementWiseOp_315(AddBackward0), #idxs=96
[192] prune_out_channels on _ElementWiseOp_314(AddBackward0) => prune_out_channels on backbone.stages.2.blocks.1.ffn.layers.1 (Linear(in_features=1536, out_features=384, bias=True)), #idxs=96
[193] prune_out_channels on _ElementWiseOp_314(AddBackward0) => prune_out_channels on backbone.stages.2.blocks.2.norm1 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)), #idxs=96
[194] prune_out_channels on backbone.stages.2.blocks.2.norm1 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_477(), #idxs=96
[195] prune_out_channels on _Reshape_477() => prune_out_channels on _ElementWiseOp_476(ConstantPadNdBackward0), #idxs=96
[196] prune_out_channels on _ElementWiseOp_476(ConstantPadNdBackward0) => prune_out_channels on _Reshape_475(), #idxs=96
[197] prune_out_channels on _Reshape_475() => prune_out_channels on _ElementWiseOp_474(PermuteBackward0), #idxs=96
[198] prune_out_channels on _ElementWiseOp_474(PermuteBackward0) => prune_out_channels on _ElementWiseOp_473(CloneBackward0), #idxs=96
[199] prune_out_channels on _ElementWiseOp_473(CloneBackward0) => prune_out_channels on _Reshape_472(), #idxs=96
[200] prune_out_channels on _Reshape_472() => prune_out_channels on _Reshape_471(), #idxs=96
[201] prune_out_channels on _Reshape_471() => prune_out_channels on _Reshape_469(), #idxs=96
[202] prune_out_channels on _Reshape_469() => prune_out_channels on _ElementWiseOp_468(AddmmBackward0), #idxs=96
[203] prune_out_channels on _ElementWiseOp_468(AddmmBackward0) => prune_out_channels on _ElementWiseOp_470(TBackward0), #idxs=96
[204] prune_out_channels on _ElementWiseOp_468(AddmmBackward0) => prune_out_channels on _Reshape_467(), #idxs=96
[205] prune_out_channels on _Reshape_467() => prune_out_channels on _Reshape_466(), #idxs=96
[206] prune_out_channels on _Reshape_466() => prune_out_channels on _ElementWiseOp_465(PermuteBackward0), #idxs=96
[207] prune_out_channels on _ElementWiseOp_465(PermuteBackward0) => prune_out_channels on _ElementWiseOp_464(SelectBackward0), #idxs=96
[208] prune_out_channels on _ElementWiseOp_465(PermuteBackward0) => prune_out_channels on _ElementWiseOp_493(SelectBackward0), #idxs=96
[209] prune_out_channels on _ElementWiseOp_465(PermuteBackward0) => prune_out_channels on _ElementWiseOp_497(SelectBackward0), #idxs=96
[210] prune_out_channels on _ElementWiseOp_497(SelectBackward0) => prune_out_channels on _ElementWiseOp_496(MulBackward0), #idxs=96
[211] prune_out_channels on _ElementWiseOp_496(MulBackward0) => prune_out_channels on _ElementWiseOp_495(ExpandBackward0), #idxs=96
[212] prune_out_channels on _ElementWiseOp_495(ExpandBackward0) => prune_out_channels on _ElementWiseOp_494(CloneBackward0), #idxs=96
[213] prune_out_channels on _ElementWiseOp_494(CloneBackward0) => prune_out_channels on _Reshape_488(), #idxs=96
[214] prune_out_channels on _Reshape_488() => prune_out_channels on _ElementWiseOp_487(BmmBackward0), #idxs=96
[215] prune_out_channels on _ElementWiseOp_487(BmmBackward0) => prune_out_channels on _Reshape_489(), #idxs=96
[216] prune_out_channels on _ElementWiseOp_487(BmmBackward0) => prune_out_channels on _Reshape_481(), #idxs=96
[217] prune_out_channels on _Reshape_481() => prune_out_channels on _ElementWiseOp_480(AddBackward0), #idxs=96
[218] prune_out_channels on _ElementWiseOp_480(AddBackward0) => prune_out_channels on _ElementWiseOp_482(UnsqueezeBackward0), #idxs=96
[219] prune_out_channels on _ElementWiseOp_480(AddBackward0) => prune_out_channels on _ElementWiseOp_479(SoftmaxBackward0), #idxs=96
[220] prune_out_channels on _ElementWiseOp_479(SoftmaxBackward0) => prune_out_channels on _ElementWiseOp_478(ExpandBackward0), #idxs=96
[221] prune_out_channels on _ElementWiseOp_478(ExpandBackward0) => prune_out_channels on _Reshape_460(), #idxs=96
[222] prune_out_channels on _Reshape_460() => prune_out_channels on _ElementWiseOp_459(BmmBackward0), #idxs=96
[223] prune_out_channels on _ElementWiseOp_459(BmmBackward0) => prune_out_channels on _Reshape_461(), #idxs=96
[224] prune_out_channels on _ElementWiseOp_459(BmmBackward0) => prune_out_channels on _Reshape_458(), #idxs=96
[225] prune_out_channels on _Reshape_458() => prune_out_channels on _ElementWiseOp_457(TransposeBackward0), #idxs=96
[226] prune_out_channels on _ElementWiseOp_457(TransposeBackward0) => prune_out_channels on _ElementWiseOp_456(CloneBackward0), #idxs=96
[227] prune_out_channels on _ElementWiseOp_456(CloneBackward0) => prune_in_channels on backbone.stages.2.blocks.2.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=384, out_features=1152, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=384, out_features=384, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)), #idxs=96
[228] prune_out_channels on _Reshape_461() => prune_out_channels on _ElementWiseOp_462(CloneBackward0), #idxs=96
[229] prune_out_channels on _ElementWiseOp_462(CloneBackward0) => prune_out_channels on _ElementWiseOp_463(ExpandBackward0), #idxs=96
[230] prune_out_channels on _ElementWiseOp_482(UnsqueezeBackward0) => prune_out_channels on _ElementWiseOp_483(CloneBackward0), #idxs=96
[231] prune_out_channels on _ElementWiseOp_483(CloneBackward0) => prune_out_channels on _ElementWiseOp_484(PermuteBackward0), #idxs=96
[232] prune_out_channels on _ElementWiseOp_484(PermuteBackward0) => prune_out_channels on _Reshape_485(), #idxs=96
[233] prune_out_channels on _Reshape_485() => prune_out_channels on _ElementWiseOp_486(IndexBackward0), #idxs=96
[234] prune_out_channels on _Reshape_489() => prune_out_channels on _ElementWiseOp_490(CloneBackward0), #idxs=96
[235] prune_out_channels on _ElementWiseOp_490(CloneBackward0) => prune_out_channels on _ElementWiseOp_491(ExpandBackward0), #idxs=96
[236] prune_out_channels on _ElementWiseOp_491(ExpandBackward0) => prune_out_channels on _ElementWiseOp_492(TransposeBackward0), #idxs=96
[237] prune_out_channels on _ElementWiseOp_315(AddBackward0) => prune_out_channels on _Reshape_323(), #idxs=96
[238] prune_out_channels on _ElementWiseOp_315(AddBackward0) => prune_out_channels on _ElementWiseOp_324(AddBackward0), #idxs=96
[239] prune_out_channels on _ElementWiseOp_315(AddBackward0) => prune_out_channels on backbone.stages.2.blocks.1.norm2 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)), #idxs=96
[240] prune_out_channels on backbone.stages.2.blocks.1.norm2 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_321(), #idxs=96
[241] prune_out_channels on _Reshape_321() => prune_out_channels on _ElementWiseOp_320(AddmmBackward0), #idxs=96
[242] prune_out_channels on _ElementWiseOp_320(AddmmBackward0) => prune_out_channels on _ElementWiseOp_322(TBackward0), #idxs=96
[243] prune_out_channels on _ElementWiseOp_320(AddmmBackward0) => prune_in_channels on backbone.stages.2.blocks.1.ffn.layers.0.0 (Linear(in_features=384, out_features=1536, bias=True)), #idxs=96
[244] prune_out_channels on _ElementWiseOp_324(AddBackward0) => prune_out_channels on _ElementWiseOp_325(AddBackward0), #idxs=96
[245] prune_out_channels on _ElementWiseOp_324(AddBackward0) => prune_out_channels on backbone.stages.2.blocks.0.ffn.layers.1 (Linear(in_features=1536, out_features=384, bias=True)), #idxs=96
[246] prune_out_channels on _ElementWiseOp_324(AddBackward0) => prune_out_channels on backbone.stages.2.blocks.1.norm1 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)), #idxs=96
[247] prune_out_channels on backbone.stages.2.blocks.1.norm1 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_424(), #idxs=96
[248] prune_out_channels on _Reshape_424() => prune_out_channels on _ElementWiseOp_423(ConstantPadNdBackward0), #idxs=96
[249] prune_out_channels on _ElementWiseOp_423(ConstantPadNdBackward0) => prune_out_channels on _ElementWiseOp_422(RollBackward0), #idxs=96
[250] prune_out_channels on _ElementWiseOp_422(RollBackward0) => prune_out_channels on _Reshape_421(), #idxs=96
[251] prune_out_channels on _Reshape_421() => prune_out_channels on _ElementWiseOp_420(PermuteBackward0), #idxs=96
[252] prune_out_channels on _ElementWiseOp_420(PermuteBackward0) => prune_out_channels on _ElementWiseOp_419(CloneBackward0), #idxs=96
[253] prune_out_channels on _ElementWiseOp_419(CloneBackward0) => prune_out_channels on _Reshape_418(), #idxs=96
[254] prune_out_channels on _Reshape_418() => prune_out_channels on _Reshape_417(), #idxs=96
[255] prune_out_channels on _Reshape_417() => prune_out_channels on _Reshape_415(), #idxs=96
[256] prune_out_channels on _Reshape_415() => prune_out_channels on _ElementWiseOp_414(AddmmBackward0), #idxs=96
[257] prune_out_channels on _ElementWiseOp_414(AddmmBackward0) => prune_out_channels on _ElementWiseOp_416(TBackward0), #idxs=96
[258] prune_out_channels on _ElementWiseOp_414(AddmmBackward0) => prune_out_channels on _Reshape_413(), #idxs=96
[259] prune_out_channels on _Reshape_413() => prune_out_channels on _Reshape_412(), #idxs=96
[260] prune_out_channels on _Reshape_412() => prune_out_channels on _ElementWiseOp_411(PermuteBackward0), #idxs=96
[261] prune_out_channels on _ElementWiseOp_411(PermuteBackward0) => prune_out_channels on _ElementWiseOp_410(SelectBackward0), #idxs=96
[262] prune_out_channels on _ElementWiseOp_411(PermuteBackward0) => prune_out_channels on _ElementWiseOp_443(SelectBackward0), #idxs=96
[263] prune_out_channels on _ElementWiseOp_411(PermuteBackward0) => prune_out_channels on _ElementWiseOp_447(SelectBackward0), #idxs=96
[264] prune_out_channels on _ElementWiseOp_447(SelectBackward0) => prune_out_channels on _ElementWiseOp_446(MulBackward0), #idxs=96
[265] prune_out_channels on _ElementWiseOp_446(MulBackward0) => prune_out_channels on _ElementWiseOp_445(ExpandBackward0), #idxs=96
[266] prune_out_channels on _ElementWiseOp_445(ExpandBackward0) => prune_out_channels on _ElementWiseOp_444(CloneBackward0), #idxs=96
[267] prune_out_channels on _ElementWiseOp_444(CloneBackward0) => prune_out_channels on _Reshape_438(), #idxs=96
[268] prune_out_channels on _Reshape_438() => prune_out_channels on _ElementWiseOp_437(BmmBackward0), #idxs=96
[269] prune_out_channels on _ElementWiseOp_437(BmmBackward0) => prune_out_channels on _Reshape_439(), #idxs=96
[270] prune_out_channels on _ElementWiseOp_437(BmmBackward0) => prune_out_channels on _Reshape_431(), #idxs=96
[271] prune_out_channels on _Reshape_431() => prune_out_channels on _ElementWiseOp_430(AddBackward0), #idxs=96
[272] prune_out_channels on _ElementWiseOp_430(AddBackward0) => prune_out_channels on _ElementWiseOp_432(UnsqueezeBackward0), #idxs=96
[273] prune_out_channels on _ElementWiseOp_430(AddBackward0) => prune_out_channels on _Reshape_429(), #idxs=96
[274] prune_out_channels on _Reshape_429() => prune_out_channels on _ElementWiseOp_428(AddBackward0), #idxs=96
[275] prune_out_channels on _ElementWiseOp_428(AddBackward0) => prune_out_channels on _Reshape_427(), #idxs=96
[276] prune_out_channels on _Reshape_427() => prune_out_channels on _ElementWiseOp_426(SoftmaxBackward0), #idxs=96
[277] prune_out_channels on _ElementWiseOp_426(SoftmaxBackward0) => prune_out_channels on _ElementWiseOp_425(ExpandBackward0), #idxs=96
[278] prune_out_channels on _ElementWiseOp_425(ExpandBackward0) => prune_out_channels on _Reshape_406(), #idxs=96
[279] prune_out_channels on _Reshape_406() => prune_out_channels on _ElementWiseOp_405(BmmBackward0), #idxs=96
[280] prune_out_channels on _ElementWiseOp_405(BmmBackward0) => prune_out_channels on _Reshape_407(), #idxs=96
[281] prune_out_channels on _ElementWiseOp_405(BmmBackward0) => prune_out_channels on _Reshape_404(), #idxs=96
[282] prune_out_channels on _Reshape_404() => prune_out_channels on _ElementWiseOp_403(TransposeBackward0), #idxs=96
[283] prune_out_channels on _ElementWiseOp_403(TransposeBackward0) => prune_out_channels on _ElementWiseOp_402(CloneBackward0), #idxs=96
[284] prune_out_channels on _ElementWiseOp_402(CloneBackward0) => prune_in_channels on backbone.stages.2.blocks.1.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=384, out_features=1152, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=384, out_features=384, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)), #idxs=96
[285] prune_out_channels on _Reshape_407() => prune_out_channels on _ElementWiseOp_408(CloneBackward0), #idxs=96
[286] prune_out_channels on _ElementWiseOp_408(CloneBackward0) => prune_out_channels on _ElementWiseOp_409(ExpandBackward0), #idxs=96
[287] prune_out_channels on _ElementWiseOp_432(UnsqueezeBackward0) => prune_out_channels on _ElementWiseOp_433(CloneBackward0), #idxs=96
[288] prune_out_channels on _ElementWiseOp_433(CloneBackward0) => prune_out_channels on _ElementWiseOp_434(PermuteBackward0), #idxs=96
[289] prune_out_channels on _ElementWiseOp_434(PermuteBackward0) => prune_out_channels on _Reshape_435(), #idxs=96
[290] prune_out_channels on _Reshape_435() => prune_out_channels on _ElementWiseOp_436(IndexBackward0), #idxs=96
[291] prune_out_channels on _Reshape_439() => prune_out_channels on _ElementWiseOp_440(CloneBackward0), #idxs=96
[292] prune_out_channels on _ElementWiseOp_440(CloneBackward0) => prune_out_channels on _ElementWiseOp_441(ExpandBackward0), #idxs=96
[293] prune_out_channels on _ElementWiseOp_441(ExpandBackward0) => prune_out_channels on _ElementWiseOp_442(TransposeBackward0), #idxs=96
[294] prune_out_channels on _ElementWiseOp_325(AddBackward0) => prune_out_channels on _Reshape_333(), #idxs=96
[295] prune_out_channels on _ElementWiseOp_325(AddBackward0) => prune_out_channels on backbone.stages.1.downsample (PatchMerging(
  (adap_padding): AdaptivePadding()
  (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
  (norm): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
  (reduction): Linear(in_features=576, out_features=384, bias=False)
)), #idxs=96
[296] prune_out_channels on _ElementWiseOp_325(AddBackward0) => prune_out_channels on backbone.stages.2.blocks.0.norm2 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)), #idxs=96
[297] prune_out_channels on backbone.stages.2.blocks.0.norm2 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_331(), #idxs=96
[298] prune_out_channels on _Reshape_331() => prune_out_channels on _ElementWiseOp_330(AddmmBackward0), #idxs=96
[299] prune_out_channels on _ElementWiseOp_330(AddmmBackward0) => prune_out_channels on _ElementWiseOp_332(TBackward0), #idxs=96
[300] prune_out_channels on _ElementWiseOp_330(AddmmBackward0) => prune_in_channels on backbone.stages.2.blocks.0.ffn.layers.0.0 (Linear(in_features=384, out_features=1536, bias=True)), #idxs=96
[301] prune_out_channels on backbone.stages.1.downsample (PatchMerging(
  (adap_padding): AdaptivePadding()
  (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
  (norm): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
  (reduction): Linear(in_features=576, out_features=384, bias=False)
)) => prune_out_channels on backbone.stages.2.blocks.0.norm1 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)), #idxs=96
[302] prune_out_channels on backbone.stages.2.blocks.0.norm1 (LayerNorm((384,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_372(), #idxs=96
[303] prune_out_channels on _Reshape_372() => prune_out_channels on _ElementWiseOp_371(ConstantPadNdBackward0), #idxs=96
[304] prune_out_channels on _ElementWiseOp_371(ConstantPadNdBackward0) => prune_out_channels on _Reshape_370(), #idxs=96
[305] prune_out_channels on _Reshape_370() => prune_out_channels on _ElementWiseOp_369(PermuteBackward0), #idxs=96
[306] prune_out_channels on _ElementWiseOp_369(PermuteBackward0) => prune_out_channels on _ElementWiseOp_368(CloneBackward0), #idxs=96
[307] prune_out_channels on _ElementWiseOp_368(CloneBackward0) => prune_out_channels on _Reshape_367(), #idxs=96
[308] prune_out_channels on _Reshape_367() => prune_out_channels on _Reshape_366(), #idxs=96
[309] prune_out_channels on _Reshape_366() => prune_out_channels on _Reshape_364(), #idxs=96
[310] prune_out_channels on _Reshape_364() => prune_out_channels on _ElementWiseOp_363(AddmmBackward0), #idxs=96
[311] prune_out_channels on _ElementWiseOp_363(AddmmBackward0) => prune_out_channels on _ElementWiseOp_365(TBackward0), #idxs=96
[312] prune_out_channels on _ElementWiseOp_363(AddmmBackward0) => prune_out_channels on _Reshape_362(), #idxs=96
[313] prune_out_channels on _Reshape_362() => prune_out_channels on _Reshape_361(), #idxs=96
[314] prune_out_channels on _Reshape_361() => prune_out_channels on _ElementWiseOp_360(PermuteBackward0), #idxs=96
[315] prune_out_channels on _ElementWiseOp_360(PermuteBackward0) => prune_out_channels on _ElementWiseOp_359(SelectBackward0), #idxs=96
[316] prune_out_channels on _ElementWiseOp_360(PermuteBackward0) => prune_out_channels on _ElementWiseOp_388(SelectBackward0), #idxs=96
[317] prune_out_channels on _ElementWiseOp_360(PermuteBackward0) => prune_out_channels on _ElementWiseOp_392(SelectBackward0), #idxs=96
[318] prune_out_channels on _ElementWiseOp_392(SelectBackward0) => prune_out_channels on _ElementWiseOp_391(MulBackward0), #idxs=96
[319] prune_out_channels on _ElementWiseOp_391(MulBackward0) => prune_out_channels on _ElementWiseOp_390(ExpandBackward0), #idxs=96
[320] prune_out_channels on _ElementWiseOp_390(ExpandBackward0) => prune_out_channels on _ElementWiseOp_389(CloneBackward0), #idxs=96
[321] prune_out_channels on _ElementWiseOp_389(CloneBackward0) => prune_out_channels on _Reshape_383(), #idxs=96
[322] prune_out_channels on _Reshape_383() => prune_out_channels on _ElementWiseOp_382(BmmBackward0), #idxs=96
[323] prune_out_channels on _ElementWiseOp_382(BmmBackward0) => prune_out_channels on _Reshape_384(), #idxs=96
[324] prune_out_channels on _ElementWiseOp_382(BmmBackward0) => prune_out_channels on _Reshape_376(), #idxs=96
[325] prune_out_channels on _Reshape_376() => prune_out_channels on _ElementWiseOp_375(AddBackward0), #idxs=96
[326] prune_out_channels on _ElementWiseOp_375(AddBackward0) => prune_out_channels on _ElementWiseOp_377(UnsqueezeBackward0), #idxs=96
[327] prune_out_channels on _ElementWiseOp_375(AddBackward0) => prune_out_channels on _ElementWiseOp_374(SoftmaxBackward0), #idxs=96
[328] prune_out_channels on _ElementWiseOp_374(SoftmaxBackward0) => prune_out_channels on _ElementWiseOp_373(ExpandBackward0), #idxs=96
[329] prune_out_channels on _ElementWiseOp_373(ExpandBackward0) => prune_out_channels on _Reshape_355(), #idxs=96
[330] prune_out_channels on _Reshape_355() => prune_out_channels on _ElementWiseOp_354(BmmBackward0), #idxs=96
[331] prune_out_channels on _ElementWiseOp_354(BmmBackward0) => prune_out_channels on _Reshape_356(), #idxs=96
[332] prune_out_channels on _ElementWiseOp_354(BmmBackward0) => prune_out_channels on _Reshape_353(), #idxs=96
[333] prune_out_channels on _Reshape_353() => prune_out_channels on _ElementWiseOp_352(TransposeBackward0), #idxs=96
[334] prune_out_channels on _ElementWiseOp_352(TransposeBackward0) => prune_out_channels on _ElementWiseOp_351(CloneBackward0), #idxs=96
[335] prune_out_channels on _ElementWiseOp_351(CloneBackward0) => prune_in_channels on backbone.stages.2.blocks.0.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=384, out_features=1152, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=384, out_features=384, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)), #idxs=96
[336] prune_out_channels on _Reshape_356() => prune_out_channels on _ElementWiseOp_357(CloneBackward0), #idxs=96
[337] prune_out_channels on _ElementWiseOp_357(CloneBackward0) => prune_out_channels on _ElementWiseOp_358(ExpandBackward0), #idxs=96
[338] prune_out_channels on _ElementWiseOp_377(UnsqueezeBackward0) => prune_out_channels on _ElementWiseOp_378(CloneBackward0), #idxs=96
[339] prune_out_channels on _ElementWiseOp_378(CloneBackward0) => prune_out_channels on _ElementWiseOp_379(PermuteBackward0), #idxs=96
[340] prune_out_channels on _ElementWiseOp_379(PermuteBackward0) => prune_out_channels on _Reshape_380(), #idxs=96
[341] prune_out_channels on _Reshape_380() => prune_out_channels on _ElementWiseOp_381(IndexBackward0), #idxs=96
[342] prune_out_channels on _Reshape_384() => prune_out_channels on _ElementWiseOp_385(CloneBackward0), #idxs=96
[343] prune_out_channels on _ElementWiseOp_385(CloneBackward0) => prune_out_channels on _ElementWiseOp_386(ExpandBackward0), #idxs=96
[344] prune_out_channels on _ElementWiseOp_386(ExpandBackward0) => prune_out_channels on _ElementWiseOp_387(TransposeBackward0), #idxs=96
[345] prune_out_channels on _Reshape_333() => prune_out_channels on _ElementWiseOp_343(SliceBackward0), #idxs=96
[346] prune_out_channels on _ElementWiseOp_343(SliceBackward0) => prune_out_channels on _ElementWiseOp_344(SliceBackward0), #idxs=96
[347] prune_out_channels on _ElementWiseOp_344(SliceBackward0) => prune_out_channels on _ElementWiseOp_345(SliceBackward0), #idxs=96
[348] prune_out_channels on _ElementWiseOp_345(SliceBackward0) => prune_out_channels on _Reshape_346(), #idxs=96
[349] prune_out_channels on _Reshape_346() => prune_out_channels on _ElementWiseOp_347(CloneBackward0), #idxs=96
[350] prune_out_channels on _ElementWiseOp_347(CloneBackward0) => prune_out_channels on _ElementWiseOp_348(PermuteBackward0), #idxs=96
[351] prune_out_channels on _ElementWiseOp_348(PermuteBackward0) => prune_out_channels on _Reshape_349(), #idxs=96
[352] prune_out_channels on _Reshape_349() => prune_out_channels on _Reshape_350(), #idxs=96
[353] prune_out_channels on _Reshape_350() => prune_out_channels on backbone.stages.2.blocks.0.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=384, out_features=1152, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=384, out_features=384, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)), #idxs=96
[354] prune_out_channels on _Reshape_323() => prune_out_channels on _ElementWiseOp_393(SliceBackward0), #idxs=96
[355] prune_out_channels on _ElementWiseOp_393(SliceBackward0) => prune_out_channels on _ElementWiseOp_394(SliceBackward0), #idxs=96
[356] prune_out_channels on _ElementWiseOp_394(SliceBackward0) => prune_out_channels on _ElementWiseOp_395(SliceBackward0), #idxs=96
[357] prune_out_channels on _ElementWiseOp_395(SliceBackward0) => prune_out_channels on _ElementWiseOp_396(RollBackward0), #idxs=96
[358] prune_out_channels on _ElementWiseOp_396(RollBackward0) => prune_out_channels on _Reshape_397(), #idxs=96
[359] prune_out_channels on _Reshape_397() => prune_out_channels on _ElementWiseOp_398(CloneBackward0), #idxs=96
[360] prune_out_channels on _ElementWiseOp_398(CloneBackward0) => prune_out_channels on _ElementWiseOp_399(PermuteBackward0), #idxs=96
[361] prune_out_channels on _ElementWiseOp_399(PermuteBackward0) => prune_out_channels on _Reshape_400(), #idxs=96
[362] prune_out_channels on _Reshape_400() => prune_out_channels on _Reshape_401(), #idxs=96
[363] prune_out_channels on _Reshape_401() => prune_out_channels on backbone.stages.2.blocks.1.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=384, out_features=1152, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=384, out_features=384, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)), #idxs=96
[364] prune_out_channels on _Reshape_313() => prune_out_channels on _ElementWiseOp_448(SliceBackward0), #idxs=96
[365] prune_out_channels on _ElementWiseOp_448(SliceBackward0) => prune_out_channels on _ElementWiseOp_449(SliceBackward0), #idxs=96
[366] prune_out_channels on _ElementWiseOp_449(SliceBackward0) => prune_out_channels on _ElementWiseOp_450(SliceBackward0), #idxs=96
[367] prune_out_channels on _ElementWiseOp_450(SliceBackward0) => prune_out_channels on _Reshape_451(), #idxs=96
[368] prune_out_channels on _Reshape_451() => prune_out_channels on _ElementWiseOp_452(CloneBackward0), #idxs=96
[369] prune_out_channels on _ElementWiseOp_452(CloneBackward0) => prune_out_channels on _ElementWiseOp_453(PermuteBackward0), #idxs=96
[370] prune_out_channels on _ElementWiseOp_453(PermuteBackward0) => prune_out_channels on _Reshape_454(), #idxs=96
[371] prune_out_channels on _Reshape_454() => prune_out_channels on _Reshape_455(), #idxs=96
[372] prune_out_channels on _Reshape_455() => prune_out_channels on backbone.stages.2.blocks.2.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=384, out_features=1152, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=384, out_features=384, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)), #idxs=96
[373] prune_out_channels on _Reshape_303() => prune_out_channels on _ElementWiseOp_498(SliceBackward0), #idxs=96
[374] prune_out_channels on _ElementWiseOp_498(SliceBackward0) => prune_out_channels on _ElementWiseOp_499(SliceBackward0), #idxs=96
[375] prune_out_channels on _ElementWiseOp_499(SliceBackward0) => prune_out_channels on _ElementWiseOp_500(SliceBackward0), #idxs=96
[376] prune_out_channels on _ElementWiseOp_500(SliceBackward0) => prune_out_channels on _ElementWiseOp_501(RollBackward0), #idxs=96
[377] prune_out_channels on _ElementWiseOp_501(RollBackward0) => prune_out_channels on _Reshape_502(), #idxs=96
[378] prune_out_channels on _Reshape_502() => prune_out_channels on _ElementWiseOp_503(CloneBackward0), #idxs=96
[379] prune_out_channels on _ElementWiseOp_503(CloneBackward0) => prune_out_channels on _ElementWiseOp_504(PermuteBackward0), #idxs=96
[380] prune_out_channels on _ElementWiseOp_504(PermuteBackward0) => prune_out_channels on _Reshape_505(), #idxs=96
[381] prune_out_channels on _Reshape_505() => prune_out_channels on _Reshape_506(), #idxs=96
[382] prune_out_channels on _Reshape_506() => prune_out_channels on backbone.stages.2.blocks.3.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=384, out_features=1152, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=384, out_features=384, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)), #idxs=96
[383] prune_out_channels on _Reshape_293() => prune_out_channels on _ElementWiseOp_553(SliceBackward0), #idxs=96
[384] prune_out_channels on _ElementWiseOp_553(SliceBackward0) => prune_out_channels on _ElementWiseOp_554(SliceBackward0), #idxs=96
[385] prune_out_channels on _ElementWiseOp_554(SliceBackward0) => prune_out_channels on _ElementWiseOp_555(SliceBackward0), #idxs=96
[386] prune_out_channels on _ElementWiseOp_555(SliceBackward0) => prune_out_channels on _Reshape_556(), #idxs=96
[387] prune_out_channels on _Reshape_556() => prune_out_channels on _ElementWiseOp_557(CloneBackward0), #idxs=96
[388] prune_out_channels on _ElementWiseOp_557(CloneBackward0) => prune_out_channels on _ElementWiseOp_558(PermuteBackward0), #idxs=96
[389] prune_out_channels on _ElementWiseOp_558(PermuteBackward0) => prune_out_channels on _Reshape_559(), #idxs=96
[390] prune_out_channels on _Reshape_559() => prune_out_channels on _Reshape_560(), #idxs=96
[391] prune_out_channels on _Reshape_560() => prune_out_channels on backbone.stages.2.blocks.4.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=384, out_features=1152, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=384, out_features=384, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)), #idxs=96
[392] prune_out_channels on _Reshape_283() => prune_out_channels on _ElementWiseOp_603(SliceBackward0), #idxs=96
[393] prune_out_channels on _ElementWiseOp_603(SliceBackward0) => prune_out_channels on _ElementWiseOp_604(SliceBackward0), #idxs=96
[394] prune_out_channels on _ElementWiseOp_604(SliceBackward0) => prune_out_channels on _ElementWiseOp_605(SliceBackward0), #idxs=96
[395] prune_out_channels on _ElementWiseOp_605(SliceBackward0) => prune_out_channels on _ElementWiseOp_606(RollBackward0), #idxs=96
[396] prune_out_channels on _ElementWiseOp_606(RollBackward0) => prune_out_channels on _Reshape_607(), #idxs=96
[397] prune_out_channels on _Reshape_607() => prune_out_channels on _ElementWiseOp_608(CloneBackward0), #idxs=96
[398] prune_out_channels on _ElementWiseOp_608(CloneBackward0) => prune_out_channels on _ElementWiseOp_609(PermuteBackward0), #idxs=96
[399] prune_out_channels on _ElementWiseOp_609(PermuteBackward0) => prune_out_channels on _Reshape_610(), #idxs=96
[400] prune_out_channels on _Reshape_610() => prune_out_channels on _Reshape_611(), #idxs=96
[401] prune_out_channels on _Reshape_611() => prune_out_channels on backbone.stages.2.blocks.5.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=384, out_features=1152, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=384, out_features=384, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)), #idxs=96
--------------------------------

PatchMergingPruner () prune_in_channels/  1536 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
PatchMergingPruner () prune_in_channels/ dim =  384
len indx =  96
idxs_repeated =  384
WindowMSAPruner prune_in_channels() /  96
WindowMSAPruner prune_in_channels() /  96
WindowMSAPruner prune_in_channels() /  96
WindowMSAPruner prune_in_channels() /  96
WindowMSAPruner prune_in_channels() /  96
PatchMergingPruner () prune_out_channels/  384 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
WindowMSAPruner prune_in_channels() /  96
WindowMSAPruner prune_out_channels() /  96
WindowMSAPruner prune_out_channels/ dim =  384
	len indx =  96
WindowMSAPruner - idxs =  [0, 2, 7, 10, 11, 12, 15, 17, 18, 19, 25, 26, 29, 31, 33, 44, 46, 55, 61, 65, 68, 79, 81, 83, 86, 87, 89, 91, 96, 111, 114, 115, 130, 137, 140, 143, 145, 146, 147, 156, 158, 175, 189, 191, 199, 202, 203, 209, 212, 214, 220, 223, 224, 229, 231, 235, 237, 240, 243, 244, 248, 252, 259, 260, 261, 270, 274, 293, 303, 304, 305, 311, 313, 314, 315, 317, 318, 323, 326, 330, 332, 333, 335, 336, 338, 339, 341, 345, 346, 359, 363, 370, 371, 374, 382, 383]
	idxs_repeated =  384
WindowMSAPruner - idxs_repeated =  [0, 2, 7, 10, 11, 12, 15, 17, 18, 19, 25, 26, 29, 31, 33, 44, 46, 55, 61, 65, 68, 79, 81, 83, 86, 87, 89, 91, 96, 111, 114, 115, 130, 137, 140, 143, 145, 146, 147, 156, 158, 175, 189, 191, 199, 202, 203, 209, 212, 214, 220, 223, 224, 229, 231, 235, 237, 240, 243, 244, 248, 252, 259, 260, 261, 270, 274, 293, 303, 304, 305, 311, 313, 314, 315, 317, 318, 323, 326, 330, 332, 333, 335, 336, 338, 339, 341, 345, 346, 359, 363, 370, 371, 374, 382, 383, 384, 386, 391, 394, 395, 396, 399, 401, 402, 403, 409, 410, 413, 415, 417, 428, 430, 439, 445, 449, 452, 463, 465, 467, 470, 471, 473, 475, 480, 495, 498, 499, 514, 521, 524, 527, 529, 530, 531, 540, 542, 559, 573, 575, 583, 586, 587, 593, 596, 598, 604, 607, 608, 613, 615, 619, 621, 624, 627, 628, 632, 636, 643, 644, 645, 654, 658, 677, 687, 688, 689, 695, 697, 698, 699, 701, 702, 707, 710, 714, 716, 717, 719, 720, 722, 723, 725, 729, 730, 743, 747, 754, 755, 758, 766, 767, 768, 770, 775, 778, 779, 780, 783, 785, 786, 787, 793, 794, 797, 799, 801, 812, 814, 823, 829, 833, 836, 847, 849, 851, 854, 855, 857, 859, 864, 879, 882, 883, 898, 905, 908, 911, 913, 914, 915, 924, 926, 943, 957, 959, 967, 970, 971, 977, 980, 982, 988, 991, 992, 997, 999, 1003, 1005, 1008, 1011, 1012, 1016, 1020, 1027, 1028, 1029, 1038, 1042, 1061, 1071, 1072, 1073, 1079, 1081, 1082, 1083, 1085, 1086, 1091, 1094, 1098, 1100, 1101, 1103, 1104, 1106, 1107, 1109, 1113, 1114, 1127, 1131, 1138, 1139, 1142, 1150, 1151, 1152, 1154, 1159, 1162, 1163, 1164, 1167, 1169, 1170, 1171, 1177, 1178, 1181, 1183, 1185, 1196, 1198, 1207, 1213, 1217, 1220, 1231, 1233, 1235, 1238, 1239, 1241, 1243, 1248, 1263, 1266, 1267, 1282, 1289, 1292, 1295, 1297, 1298, 1299, 1308, 1310, 1327, 1341, 1343, 1351, 1354, 1355, 1361, 1364, 1366, 1372, 1375, 1376, 1381, 1383, 1387, 1389, 1392, 1395, 1396, 1400, 1404, 1411, 1412, 1413, 1422, 1426, 1445, 1455, 1456, 1457, 1463, 1465, 1466, 1467, 1469, 1470, 1475, 1478, 1482, 1484, 1485, 1487, 1488, 1490, 1491, 1493, 1497, 1498, 1511, 1515, 1522, 1523, 1526, 1534, 1535]
WindowMSAPruner prune_out_channels idxs_repeated =  384
WindowMSAPruner prune_out_channels() /  96
WindowMSAPruner prune_out_channels/ dim =  384
	len indx =  96
WindowMSAPruner - idxs =  [0, 2, 7, 10, 11, 12, 15, 17, 18, 19, 25, 26, 29, 31, 33, 44, 46, 55, 61, 65, 68, 79, 81, 83, 86, 87, 89, 91, 96, 111, 114, 115, 130, 137, 140, 143, 145, 146, 147, 156, 158, 175, 189, 191, 199, 202, 203, 209, 212, 214, 220, 223, 224, 229, 231, 235, 237, 240, 243, 244, 248, 252, 259, 260, 261, 270, 274, 293, 303, 304, 305, 311, 313, 314, 315, 317, 318, 323, 326, 330, 332, 333, 335, 336, 338, 339, 341, 345, 346, 359, 363, 370, 371, 374, 382, 383]
	idxs_repeated =  384
WindowMSAPruner - idxs_repeated =  [0, 2, 7, 10, 11, 12, 15, 17, 18, 19, 25, 26, 29, 31, 33, 44, 46, 55, 61, 65, 68, 79, 81, 83, 86, 87, 89, 91, 96, 111, 114, 115, 130, 137, 140, 143, 145, 146, 147, 156, 158, 175, 189, 191, 199, 202, 203, 209, 212, 214, 220, 223, 224, 229, 231, 235, 237, 240, 243, 244, 248, 252, 259, 260, 261, 270, 274, 293, 303, 304, 305, 311, 313, 314, 315, 317, 318, 323, 326, 330, 332, 333, 335, 336, 338, 339, 341, 345, 346, 359, 363, 370, 371, 374, 382, 383, 384, 386, 391, 394, 395, 396, 399, 401, 402, 403, 409, 410, 413, 415, 417, 428, 430, 439, 445, 449, 452, 463, 465, 467, 470, 471, 473, 475, 480, 495, 498, 499, 514, 521, 524, 527, 529, 530, 531, 540, 542, 559, 573, 575, 583, 586, 587, 593, 596, 598, 604, 607, 608, 613, 615, 619, 621, 624, 627, 628, 632, 636, 643, 644, 645, 654, 658, 677, 687, 688, 689, 695, 697, 698, 699, 701, 702, 707, 710, 714, 716, 717, 719, 720, 722, 723, 725, 729, 730, 743, 747, 754, 755, 758, 766, 767, 768, 770, 775, 778, 779, 780, 783, 785, 786, 787, 793, 794, 797, 799, 801, 812, 814, 823, 829, 833, 836, 847, 849, 851, 854, 855, 857, 859, 864, 879, 882, 883, 898, 905, 908, 911, 913, 914, 915, 924, 926, 943, 957, 959, 967, 970, 971, 977, 980, 982, 988, 991, 992, 997, 999, 1003, 1005, 1008, 1011, 1012, 1016, 1020, 1027, 1028, 1029, 1038, 1042, 1061, 1071, 1072, 1073, 1079, 1081, 1082, 1083, 1085, 1086, 1091, 1094, 1098, 1100, 1101, 1103, 1104, 1106, 1107, 1109, 1113, 1114, 1127, 1131, 1138, 1139, 1142, 1150, 1151, 1152, 1154, 1159, 1162, 1163, 1164, 1167, 1169, 1170, 1171, 1177, 1178, 1181, 1183, 1185, 1196, 1198, 1207, 1213, 1217, 1220, 1231, 1233, 1235, 1238, 1239, 1241, 1243, 1248, 1263, 1266, 1267, 1282, 1289, 1292, 1295, 1297, 1298, 1299, 1308, 1310, 1327, 1341, 1343, 1351, 1354, 1355, 1361, 1364, 1366, 1372, 1375, 1376, 1381, 1383, 1387, 1389, 1392, 1395, 1396, 1400, 1404, 1411, 1412, 1413, 1422, 1426, 1445, 1455, 1456, 1457, 1463, 1465, 1466, 1467, 1469, 1470, 1475, 1478, 1482, 1484, 1485, 1487, 1488, 1490, 1491, 1493, 1497, 1498, 1511, 1515, 1522, 1523, 1526, 1534, 1535]
WindowMSAPruner prune_out_channels idxs_repeated =  384
WindowMSAPruner prune_out_channels() /  96
WindowMSAPruner prune_out_channels/ dim =  384
	len indx =  96
WindowMSAPruner - idxs =  [0, 2, 7, 10, 11, 12, 15, 17, 18, 19, 25, 26, 29, 31, 33, 44, 46, 55, 61, 65, 68, 79, 81, 83, 86, 87, 89, 91, 96, 111, 114, 115, 130, 137, 140, 143, 145, 146, 147, 156, 158, 175, 189, 191, 199, 202, 203, 209, 212, 214, 220, 223, 224, 229, 231, 235, 237, 240, 243, 244, 248, 252, 259, 260, 261, 270, 274, 293, 303, 304, 305, 311, 313, 314, 315, 317, 318, 323, 326, 330, 332, 333, 335, 336, 338, 339, 341, 345, 346, 359, 363, 370, 371, 374, 382, 383]
	idxs_repeated =  384
WindowMSAPruner - idxs_repeated =  [0, 2, 7, 10, 11, 12, 15, 17, 18, 19, 25, 26, 29, 31, 33, 44, 46, 55, 61, 65, 68, 79, 81, 83, 86, 87, 89, 91, 96, 111, 114, 115, 130, 137, 140, 143, 145, 146, 147, 156, 158, 175, 189, 191, 199, 202, 203, 209, 212, 214, 220, 223, 224, 229, 231, 235, 237, 240, 243, 244, 248, 252, 259, 260, 261, 270, 274, 293, 303, 304, 305, 311, 313, 314, 315, 317, 318, 323, 326, 330, 332, 333, 335, 336, 338, 339, 341, 345, 346, 359, 363, 370, 371, 374, 382, 383, 384, 386, 391, 394, 395, 396, 399, 401, 402, 403, 409, 410, 413, 415, 417, 428, 430, 439, 445, 449, 452, 463, 465, 467, 470, 471, 473, 475, 480, 495, 498, 499, 514, 521, 524, 527, 529, 530, 531, 540, 542, 559, 573, 575, 583, 586, 587, 593, 596, 598, 604, 607, 608, 613, 615, 619, 621, 624, 627, 628, 632, 636, 643, 644, 645, 654, 658, 677, 687, 688, 689, 695, 697, 698, 699, 701, 702, 707, 710, 714, 716, 717, 719, 720, 722, 723, 725, 729, 730, 743, 747, 754, 755, 758, 766, 767, 768, 770, 775, 778, 779, 780, 783, 785, 786, 787, 793, 794, 797, 799, 801, 812, 814, 823, 829, 833, 836, 847, 849, 851, 854, 855, 857, 859, 864, 879, 882, 883, 898, 905, 908, 911, 913, 914, 915, 924, 926, 943, 957, 959, 967, 970, 971, 977, 980, 982, 988, 991, 992, 997, 999, 1003, 1005, 1008, 1011, 1012, 1016, 1020, 1027, 1028, 1029, 1038, 1042, 1061, 1071, 1072, 1073, 1079, 1081, 1082, 1083, 1085, 1086, 1091, 1094, 1098, 1100, 1101, 1103, 1104, 1106, 1107, 1109, 1113, 1114, 1127, 1131, 1138, 1139, 1142, 1150, 1151, 1152, 1154, 1159, 1162, 1163, 1164, 1167, 1169, 1170, 1171, 1177, 1178, 1181, 1183, 1185, 1196, 1198, 1207, 1213, 1217, 1220, 1231, 1233, 1235, 1238, 1239, 1241, 1243, 1248, 1263, 1266, 1267, 1282, 1289, 1292, 1295, 1297, 1298, 1299, 1308, 1310, 1327, 1341, 1343, 1351, 1354, 1355, 1361, 1364, 1366, 1372, 1375, 1376, 1381, 1383, 1387, 1389, 1392, 1395, 1396, 1400, 1404, 1411, 1412, 1413, 1422, 1426, 1445, 1455, 1456, 1457, 1463, 1465, 1466, 1467, 1469, 1470, 1475, 1478, 1482, 1484, 1485, 1487, 1488, 1490, 1491, 1493, 1497, 1498, 1511, 1515, 1522, 1523, 1526, 1534, 1535]
WindowMSAPruner prune_out_channels idxs_repeated =  384
WindowMSAPruner prune_out_channels() /  96
WindowMSAPruner prune_out_channels/ dim =  384
	len indx =  96
WindowMSAPruner - idxs =  [0, 2, 7, 10, 11, 12, 15, 17, 18, 19, 25, 26, 29, 31, 33, 44, 46, 55, 61, 65, 68, 79, 81, 83, 86, 87, 89, 91, 96, 111, 114, 115, 130, 137, 140, 143, 145, 146, 147, 156, 158, 175, 189, 191, 199, 202, 203, 209, 212, 214, 220, 223, 224, 229, 231, 235, 237, 240, 243, 244, 248, 252, 259, 260, 261, 270, 274, 293, 303, 304, 305, 311, 313, 314, 315, 317, 318, 323, 326, 330, 332, 333, 335, 336, 338, 339, 341, 345, 346, 359, 363, 370, 371, 374, 382, 383]
	idxs_repeated =  384
WindowMSAPruner - idxs_repeated =  [0, 2, 7, 10, 11, 12, 15, 17, 18, 19, 25, 26, 29, 31, 33, 44, 46, 55, 61, 65, 68, 79, 81, 83, 86, 87, 89, 91, 96, 111, 114, 115, 130, 137, 140, 143, 145, 146, 147, 156, 158, 175, 189, 191, 199, 202, 203, 209, 212, 214, 220, 223, 224, 229, 231, 235, 237, 240, 243, 244, 248, 252, 259, 260, 261, 270, 274, 293, 303, 304, 305, 311, 313, 314, 315, 317, 318, 323, 326, 330, 332, 333, 335, 336, 338, 339, 341, 345, 346, 359, 363, 370, 371, 374, 382, 383, 384, 386, 391, 394, 395, 396, 399, 401, 402, 403, 409, 410, 413, 415, 417, 428, 430, 439, 445, 449, 452, 463, 465, 467, 470, 471, 473, 475, 480, 495, 498, 499, 514, 521, 524, 527, 529, 530, 531, 540, 542, 559, 573, 575, 583, 586, 587, 593, 596, 598, 604, 607, 608, 613, 615, 619, 621, 624, 627, 628, 632, 636, 643, 644, 645, 654, 658, 677, 687, 688, 689, 695, 697, 698, 699, 701, 702, 707, 710, 714, 716, 717, 719, 720, 722, 723, 725, 729, 730, 743, 747, 754, 755, 758, 766, 767, 768, 770, 775, 778, 779, 780, 783, 785, 786, 787, 793, 794, 797, 799, 801, 812, 814, 823, 829, 833, 836, 847, 849, 851, 854, 855, 857, 859, 864, 879, 882, 883, 898, 905, 908, 911, 913, 914, 915, 924, 926, 943, 957, 959, 967, 970, 971, 977, 980, 982, 988, 991, 992, 997, 999, 1003, 1005, 1008, 1011, 1012, 1016, 1020, 1027, 1028, 1029, 1038, 1042, 1061, 1071, 1072, 1073, 1079, 1081, 1082, 1083, 1085, 1086, 1091, 1094, 1098, 1100, 1101, 1103, 1104, 1106, 1107, 1109, 1113, 1114, 1127, 1131, 1138, 1139, 1142, 1150, 1151, 1152, 1154, 1159, 1162, 1163, 1164, 1167, 1169, 1170, 1171, 1177, 1178, 1181, 1183, 1185, 1196, 1198, 1207, 1213, 1217, 1220, 1231, 1233, 1235, 1238, 1239, 1241, 1243, 1248, 1263, 1266, 1267, 1282, 1289, 1292, 1295, 1297, 1298, 1299, 1308, 1310, 1327, 1341, 1343, 1351, 1354, 1355, 1361, 1364, 1366, 1372, 1375, 1376, 1381, 1383, 1387, 1389, 1392, 1395, 1396, 1400, 1404, 1411, 1412, 1413, 1422, 1426, 1445, 1455, 1456, 1457, 1463, 1465, 1466, 1467, 1469, 1470, 1475, 1478, 1482, 1484, 1485, 1487, 1488, 1490, 1491, 1493, 1497, 1498, 1511, 1515, 1522, 1523, 1526, 1534, 1535]
WindowMSAPruner prune_out_channels idxs_repeated =  384
WindowMSAPruner prune_out_channels() /  96
WindowMSAPruner prune_out_channels/ dim =  384
	len indx =  96
WindowMSAPruner - idxs =  [0, 2, 7, 10, 11, 12, 15, 17, 18, 19, 25, 26, 29, 31, 33, 44, 46, 55, 61, 65, 68, 79, 81, 83, 86, 87, 89, 91, 96, 111, 114, 115, 130, 137, 140, 143, 145, 146, 147, 156, 158, 175, 189, 191, 199, 202, 203, 209, 212, 214, 220, 223, 224, 229, 231, 235, 237, 240, 243, 244, 248, 252, 259, 260, 261, 270, 274, 293, 303, 304, 305, 311, 313, 314, 315, 317, 318, 323, 326, 330, 332, 333, 335, 336, 338, 339, 341, 345, 346, 359, 363, 370, 371, 374, 382, 383]
	idxs_repeated =  384
WindowMSAPruner - idxs_repeated =  [0, 2, 7, 10, 11, 12, 15, 17, 18, 19, 25, 26, 29, 31, 33, 44, 46, 55, 61, 65, 68, 79, 81, 83, 86, 87, 89, 91, 96, 111, 114, 115, 130, 137, 140, 143, 145, 146, 147, 156, 158, 175, 189, 191, 199, 202, 203, 209, 212, 214, 220, 223, 224, 229, 231, 235, 237, 240, 243, 244, 248, 252, 259, 260, 261, 270, 274, 293, 303, 304, 305, 311, 313, 314, 315, 317, 318, 323, 326, 330, 332, 333, 335, 336, 338, 339, 341, 345, 346, 359, 363, 370, 371, 374, 382, 383, 384, 386, 391, 394, 395, 396, 399, 401, 402, 403, 409, 410, 413, 415, 417, 428, 430, 439, 445, 449, 452, 463, 465, 467, 470, 471, 473, 475, 480, 495, 498, 499, 514, 521, 524, 527, 529, 530, 531, 540, 542, 559, 573, 575, 583, 586, 587, 593, 596, 598, 604, 607, 608, 613, 615, 619, 621, 624, 627, 628, 632, 636, 643, 644, 645, 654, 658, 677, 687, 688, 689, 695, 697, 698, 699, 701, 702, 707, 710, 714, 716, 717, 719, 720, 722, 723, 725, 729, 730, 743, 747, 754, 755, 758, 766, 767, 768, 770, 775, 778, 779, 780, 783, 785, 786, 787, 793, 794, 797, 799, 801, 812, 814, 823, 829, 833, 836, 847, 849, 851, 854, 855, 857, 859, 864, 879, 882, 883, 898, 905, 908, 911, 913, 914, 915, 924, 926, 943, 957, 959, 967, 970, 971, 977, 980, 982, 988, 991, 992, 997, 999, 1003, 1005, 1008, 1011, 1012, 1016, 1020, 1027, 1028, 1029, 1038, 1042, 1061, 1071, 1072, 1073, 1079, 1081, 1082, 1083, 1085, 1086, 1091, 1094, 1098, 1100, 1101, 1103, 1104, 1106, 1107, 1109, 1113, 1114, 1127, 1131, 1138, 1139, 1142, 1150, 1151, 1152, 1154, 1159, 1162, 1163, 1164, 1167, 1169, 1170, 1171, 1177, 1178, 1181, 1183, 1185, 1196, 1198, 1207, 1213, 1217, 1220, 1231, 1233, 1235, 1238, 1239, 1241, 1243, 1248, 1263, 1266, 1267, 1282, 1289, 1292, 1295, 1297, 1298, 1299, 1308, 1310, 1327, 1341, 1343, 1351, 1354, 1355, 1361, 1364, 1366, 1372, 1375, 1376, 1381, 1383, 1387, 1389, 1392, 1395, 1396, 1400, 1404, 1411, 1412, 1413, 1422, 1426, 1445, 1455, 1456, 1457, 1463, 1465, 1466, 1467, 1469, 1470, 1475, 1478, 1482, 1484, 1485, 1487, 1488, 1490, 1491, 1493, 1497, 1498, 1511, 1515, 1522, 1523, 1526, 1534, 1535]
WindowMSAPruner prune_out_channels idxs_repeated =  384
WindowMSAPruner prune_out_channels() /  96
WindowMSAPruner prune_out_channels/ dim =  384
	len indx =  96
WindowMSAPruner - idxs =  [0, 2, 7, 10, 11, 12, 15, 17, 18, 19, 25, 26, 29, 31, 33, 44, 46, 55, 61, 65, 68, 79, 81, 83, 86, 87, 89, 91, 96, 111, 114, 115, 130, 137, 140, 143, 145, 146, 147, 156, 158, 175, 189, 191, 199, 202, 203, 209, 212, 214, 220, 223, 224, 229, 231, 235, 237, 240, 243, 244, 248, 252, 259, 260, 261, 270, 274, 293, 303, 304, 305, 311, 313, 314, 315, 317, 318, 323, 326, 330, 332, 333, 335, 336, 338, 339, 341, 345, 346, 359, 363, 370, 371, 374, 382, 383]
	idxs_repeated =  384
WindowMSAPruner - idxs_repeated =  [0, 2, 7, 10, 11, 12, 15, 17, 18, 19, 25, 26, 29, 31, 33, 44, 46, 55, 61, 65, 68, 79, 81, 83, 86, 87, 89, 91, 96, 111, 114, 115, 130, 137, 140, 143, 145, 146, 147, 156, 158, 175, 189, 191, 199, 202, 203, 209, 212, 214, 220, 223, 224, 229, 231, 235, 237, 240, 243, 244, 248, 252, 259, 260, 261, 270, 274, 293, 303, 304, 305, 311, 313, 314, 315, 317, 318, 323, 326, 330, 332, 333, 335, 336, 338, 339, 341, 345, 346, 359, 363, 370, 371, 374, 382, 383, 384, 386, 391, 394, 395, 396, 399, 401, 402, 403, 409, 410, 413, 415, 417, 428, 430, 439, 445, 449, 452, 463, 465, 467, 470, 471, 473, 475, 480, 495, 498, 499, 514, 521, 524, 527, 529, 530, 531, 540, 542, 559, 573, 575, 583, 586, 587, 593, 596, 598, 604, 607, 608, 613, 615, 619, 621, 624, 627, 628, 632, 636, 643, 644, 645, 654, 658, 677, 687, 688, 689, 695, 697, 698, 699, 701, 702, 707, 710, 714, 716, 717, 719, 720, 722, 723, 725, 729, 730, 743, 747, 754, 755, 758, 766, 767, 768, 770, 775, 778, 779, 780, 783, 785, 786, 787, 793, 794, 797, 799, 801, 812, 814, 823, 829, 833, 836, 847, 849, 851, 854, 855, 857, 859, 864, 879, 882, 883, 898, 905, 908, 911, 913, 914, 915, 924, 926, 943, 957, 959, 967, 970, 971, 977, 980, 982, 988, 991, 992, 997, 999, 1003, 1005, 1008, 1011, 1012, 1016, 1020, 1027, 1028, 1029, 1038, 1042, 1061, 1071, 1072, 1073, 1079, 1081, 1082, 1083, 1085, 1086, 1091, 1094, 1098, 1100, 1101, 1103, 1104, 1106, 1107, 1109, 1113, 1114, 1127, 1131, 1138, 1139, 1142, 1150, 1151, 1152, 1154, 1159, 1162, 1163, 1164, 1167, 1169, 1170, 1171, 1177, 1178, 1181, 1183, 1185, 1196, 1198, 1207, 1213, 1217, 1220, 1231, 1233, 1235, 1238, 1239, 1241, 1243, 1248, 1263, 1266, 1267, 1282, 1289, 1292, 1295, 1297, 1298, 1299, 1308, 1310, 1327, 1341, 1343, 1351, 1354, 1355, 1361, 1364, 1366, 1372, 1375, 1376, 1381, 1383, 1387, 1389, 1392, 1395, 1396, 1400, 1404, 1411, 1412, 1413, 1422, 1426, 1445, 1455, 1456, 1457, 1463, 1465, 1466, 1467, 1469, 1470, 1475, 1478, 1482, 1484, 1485, 1487, 1488, 1490, 1491, 1493, 1497, 1498, 1511, 1515, 1522, 1523, 1526, 1534, 1535]
WindowMSAPruner prune_out_channels idxs_repeated =  384
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
	get_all_groups -- Adding layer:  <class 'mmcv.cnn.bricks.wrappers.Linear'>
		prune_local()/  _check_pruning_ratio OK
Conv/Linear Output GroupNormImportance() / layer Type =  <class 'mmcv.cnn.bricks.wrappers.Linear'>
local_imp len =  1536 torch.Size([1536])
i =  5
<class 'mmcv.cnn.bricks.wrappers.Linear'> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501, 1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509, 1510, 1511, 1512, 1513, 1514, 1515, 1516, 1517, 1518, 1519, 1520, 1521, 1522, 1523, 1524, 1525, 1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533, 1534, 1535] Linear(in_features=1536, out_features=288, bias=True)

idxs =  384
prunable_chs =  1536
pruner.step 
--------------------------------
          Pruning Group
--------------------------------
[0] prune_out_channels on backbone.stages.2.blocks.5.ffn.layers.0.0 (Linear(in_features=288, out_features=1536, bias=True)) => prune_out_channels on backbone.stages.2.blocks.5.ffn.layers.0.0 (Linear(in_features=288, out_features=1536, bias=True)), #idxs=384
[1] prune_out_channels on backbone.stages.2.blocks.5.ffn.layers.0.0 (Linear(in_features=288, out_features=1536, bias=True)) => prune_out_channels on _ElementWiseOp_279(GeluBackward0), #idxs=384
[2] prune_out_channels on _ElementWiseOp_279(GeluBackward0) => prune_out_channels on _Reshape_277(), #idxs=384
[3] prune_out_channels on _Reshape_277() => prune_out_channels on _ElementWiseOp_276(AddmmBackward0), #idxs=384
[4] prune_out_channels on _ElementWiseOp_276(AddmmBackward0) => prune_out_channels on _ElementWiseOp_278(TBackward0), #idxs=384
[5] prune_out_channels on _ElementWiseOp_276(AddmmBackward0) => prune_in_channels on backbone.stages.2.blocks.5.ffn.layers.1 (Linear(in_features=1536, out_features=288, bias=True)), #idxs=384
--------------------------------

<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
continue 4
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
	get_all_groups -- Adding layer:  <class 'mmcv.cnn.bricks.wrappers.Linear'>
		prune_local()/  _check_pruning_ratio OK
Conv/Linear Output GroupNormImportance() / layer Type =  <class 'mmcv.cnn.bricks.wrappers.Linear'>
local_imp len =  1536 torch.Size([1536])
i =  5
<class 'mmcv.cnn.bricks.wrappers.Linear'> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501, 1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509, 1510, 1511, 1512, 1513, 1514, 1515, 1516, 1517, 1518, 1519, 1520, 1521, 1522, 1523, 1524, 1525, 1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533, 1534, 1535] Linear(in_features=1536, out_features=288, bias=True)

idxs =  384
prunable_chs =  1536
pruner.step 
--------------------------------
          Pruning Group
--------------------------------
[0] prune_out_channels on backbone.stages.2.blocks.4.ffn.layers.0.0 (Linear(in_features=288, out_features=1536, bias=True)) => prune_out_channels on backbone.stages.2.blocks.4.ffn.layers.0.0 (Linear(in_features=288, out_features=1536, bias=True)), #idxs=384
[1] prune_out_channels on backbone.stages.2.blocks.4.ffn.layers.0.0 (Linear(in_features=288, out_features=1536, bias=True)) => prune_out_channels on _ElementWiseOp_289(GeluBackward0), #idxs=384
[2] prune_out_channels on _ElementWiseOp_289(GeluBackward0) => prune_out_channels on _Reshape_287(), #idxs=384
[3] prune_out_channels on _Reshape_287() => prune_out_channels on _ElementWiseOp_286(AddmmBackward0), #idxs=384
[4] prune_out_channels on _ElementWiseOp_286(AddmmBackward0) => prune_out_channels on _ElementWiseOp_288(TBackward0), #idxs=384
[5] prune_out_channels on _ElementWiseOp_286(AddmmBackward0) => prune_in_channels on backbone.stages.2.blocks.4.ffn.layers.1 (Linear(in_features=1536, out_features=288, bias=True)), #idxs=384
--------------------------------

<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
continue 4
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
	get_all_groups -- Adding layer:  <class 'mmcv.cnn.bricks.wrappers.Linear'>
		prune_local()/  _check_pruning_ratio OK
Conv/Linear Output GroupNormImportance() / layer Type =  <class 'mmcv.cnn.bricks.wrappers.Linear'>
local_imp len =  1536 torch.Size([1536])
i =  5
<class 'mmcv.cnn.bricks.wrappers.Linear'> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501, 1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509, 1510, 1511, 1512, 1513, 1514, 1515, 1516, 1517, 1518, 1519, 1520, 1521, 1522, 1523, 1524, 1525, 1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533, 1534, 1535] Linear(in_features=1536, out_features=288, bias=True)

idxs =  384
prunable_chs =  1536
pruner.step 
--------------------------------
          Pruning Group
--------------------------------
[0] prune_out_channels on backbone.stages.2.blocks.3.ffn.layers.0.0 (Linear(in_features=288, out_features=1536, bias=True)) => prune_out_channels on backbone.stages.2.blocks.3.ffn.layers.0.0 (Linear(in_features=288, out_features=1536, bias=True)), #idxs=384
[1] prune_out_channels on backbone.stages.2.blocks.3.ffn.layers.0.0 (Linear(in_features=288, out_features=1536, bias=True)) => prune_out_channels on _ElementWiseOp_299(GeluBackward0), #idxs=384
[2] prune_out_channels on _ElementWiseOp_299(GeluBackward0) => prune_out_channels on _Reshape_297(), #idxs=384
[3] prune_out_channels on _Reshape_297() => prune_out_channels on _ElementWiseOp_296(AddmmBackward0), #idxs=384
[4] prune_out_channels on _ElementWiseOp_296(AddmmBackward0) => prune_out_channels on _ElementWiseOp_298(TBackward0), #idxs=384
[5] prune_out_channels on _ElementWiseOp_296(AddmmBackward0) => prune_in_channels on backbone.stages.2.blocks.3.ffn.layers.1 (Linear(in_features=1536, out_features=288, bias=True)), #idxs=384
--------------------------------

<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
continue 4
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
	get_all_groups -- Adding layer:  <class 'mmcv.cnn.bricks.wrappers.Linear'>
		prune_local()/  _check_pruning_ratio OK
Conv/Linear Output GroupNormImportance() / layer Type =  <class 'mmcv.cnn.bricks.wrappers.Linear'>
local_imp len =  1536 torch.Size([1536])
i =  5
<class 'mmcv.cnn.bricks.wrappers.Linear'> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501, 1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509, 1510, 1511, 1512, 1513, 1514, 1515, 1516, 1517, 1518, 1519, 1520, 1521, 1522, 1523, 1524, 1525, 1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533, 1534, 1535] Linear(in_features=1536, out_features=288, bias=True)

idxs =  384
prunable_chs =  1536
pruner.step 
--------------------------------
          Pruning Group
--------------------------------
[0] prune_out_channels on backbone.stages.2.blocks.2.ffn.layers.0.0 (Linear(in_features=288, out_features=1536, bias=True)) => prune_out_channels on backbone.stages.2.blocks.2.ffn.layers.0.0 (Linear(in_features=288, out_features=1536, bias=True)), #idxs=384
[1] prune_out_channels on backbone.stages.2.blocks.2.ffn.layers.0.0 (Linear(in_features=288, out_features=1536, bias=True)) => prune_out_channels on _ElementWiseOp_309(GeluBackward0), #idxs=384
[2] prune_out_channels on _ElementWiseOp_309(GeluBackward0) => prune_out_channels on _Reshape_307(), #idxs=384
[3] prune_out_channels on _Reshape_307() => prune_out_channels on _ElementWiseOp_306(AddmmBackward0), #idxs=384
[4] prune_out_channels on _ElementWiseOp_306(AddmmBackward0) => prune_out_channels on _ElementWiseOp_308(TBackward0), #idxs=384
[5] prune_out_channels on _ElementWiseOp_306(AddmmBackward0) => prune_in_channels on backbone.stages.2.blocks.2.ffn.layers.1 (Linear(in_features=1536, out_features=288, bias=True)), #idxs=384
--------------------------------

<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
continue 4
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
	get_all_groups -- Adding layer:  <class 'mmcv.cnn.bricks.wrappers.Linear'>
		prune_local()/  _check_pruning_ratio OK
Conv/Linear Output GroupNormImportance() / layer Type =  <class 'mmcv.cnn.bricks.wrappers.Linear'>
local_imp len =  1536 torch.Size([1536])
i =  5
<class 'mmcv.cnn.bricks.wrappers.Linear'> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501, 1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509, 1510, 1511, 1512, 1513, 1514, 1515, 1516, 1517, 1518, 1519, 1520, 1521, 1522, 1523, 1524, 1525, 1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533, 1534, 1535] Linear(in_features=1536, out_features=288, bias=True)

idxs =  384
prunable_chs =  1536
pruner.step 
--------------------------------
          Pruning Group
--------------------------------
[0] prune_out_channels on backbone.stages.2.blocks.1.ffn.layers.0.0 (Linear(in_features=288, out_features=1536, bias=True)) => prune_out_channels on backbone.stages.2.blocks.1.ffn.layers.0.0 (Linear(in_features=288, out_features=1536, bias=True)), #idxs=384
[1] prune_out_channels on backbone.stages.2.blocks.1.ffn.layers.0.0 (Linear(in_features=288, out_features=1536, bias=True)) => prune_out_channels on _ElementWiseOp_319(GeluBackward0), #idxs=384
[2] prune_out_channels on _ElementWiseOp_319(GeluBackward0) => prune_out_channels on _Reshape_317(), #idxs=384
[3] prune_out_channels on _Reshape_317() => prune_out_channels on _ElementWiseOp_316(AddmmBackward0), #idxs=384
[4] prune_out_channels on _ElementWiseOp_316(AddmmBackward0) => prune_out_channels on _ElementWiseOp_318(TBackward0), #idxs=384
[5] prune_out_channels on _ElementWiseOp_316(AddmmBackward0) => prune_in_channels on backbone.stages.2.blocks.1.ffn.layers.1 (Linear(in_features=1536, out_features=288, bias=True)), #idxs=384
--------------------------------

<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
continue 4
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
	get_all_groups -- Adding layer:  <class 'mmcv.cnn.bricks.wrappers.Linear'>
		prune_local()/  _check_pruning_ratio OK
Conv/Linear Output GroupNormImportance() / layer Type =  <class 'mmcv.cnn.bricks.wrappers.Linear'>
local_imp len =  1536 torch.Size([1536])
i =  5
<class 'mmcv.cnn.bricks.wrappers.Linear'> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501, 1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509, 1510, 1511, 1512, 1513, 1514, 1515, 1516, 1517, 1518, 1519, 1520, 1521, 1522, 1523, 1524, 1525, 1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533, 1534, 1535] Linear(in_features=1536, out_features=288, bias=True)

idxs =  384
prunable_chs =  1536
pruner.step 
--------------------------------
          Pruning Group
--------------------------------
[0] prune_out_channels on backbone.stages.2.blocks.0.ffn.layers.0.0 (Linear(in_features=288, out_features=1536, bias=True)) => prune_out_channels on backbone.stages.2.blocks.0.ffn.layers.0.0 (Linear(in_features=288, out_features=1536, bias=True)), #idxs=384
[1] prune_out_channels on backbone.stages.2.blocks.0.ffn.layers.0.0 (Linear(in_features=288, out_features=1536, bias=True)) => prune_out_channels on _ElementWiseOp_329(GeluBackward0), #idxs=384
[2] prune_out_channels on _ElementWiseOp_329(GeluBackward0) => prune_out_channels on _Reshape_327(), #idxs=384
[3] prune_out_channels on _Reshape_327() => prune_out_channels on _ElementWiseOp_326(AddmmBackward0), #idxs=384
[4] prune_out_channels on _ElementWiseOp_326(AddmmBackward0) => prune_out_channels on _ElementWiseOp_328(TBackward0), #idxs=384
[5] prune_out_channels on _ElementWiseOp_326(AddmmBackward0) => prune_in_channels on backbone.stages.2.blocks.0.ffn.layers.1 (Linear(in_features=1536, out_features=288, bias=True)), #idxs=384
--------------------------------

<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
continue 4
<class 'torch_pruning.ops._ReshapeOp'> continue 2
PatchMergingPruner () get_out_channels/  288 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
WindowMSAPruner layer.embed_dims =  288
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
WindowMSAPruner layer.embed_dims =  288
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
WindowMSAPruner layer.embed_dims =  288
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
WindowMSAPruner layer.embed_dims =  288
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
WindowMSAPruner layer.embed_dims =  288
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
WindowMSAPruner layer.embed_dims =  288
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch.nn.modules.normalization.GroupNorm'> continue 2
	get_all_groups -- Adding layer:  <class 'torch.nn.modules.conv.Conv2d'>
		prune_local()/  _check_pruning_ratio OK
Conv/Linear Output GroupNormImportance() / layer Type =  <class 'torch.nn.modules.conv.Conv2d'>
idxs =  64
prunable_chs =  256
idxs =  64
prunable_chs =  256
pruner.step 
--------------------------------
          Pruning Group
--------------------------------
[0] prune_out_channels on neck.convs.2.conv (Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))) => prune_out_channels on neck.convs.2.conv (Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))), #idxs=64
[1] prune_out_channels on neck.convs.2.conv (Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))) => prune_out_channels on neck.convs.2.gn (GroupNorm(32, 256, eps=1e-05, affine=True)), #idxs=64
--------------------------------

<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
	get_all_groups -- Adding layer:  <class 'torch.nn.modules.normalization.LayerNorm'>
WindowMSAPruner layer.qkv.in_features =  768
PatchMergingPruner () get_out_channels/  768 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
WindowMSAPruner layer.qkv.in_features =  768
WindowMSAPruner layer.embed_dims =  768
WindowMSAPruner layer.embed_dims =  768
		prune_local()/  _check_pruning_ratio OK
local_imp len =  768 torch.Size([768])
i =  5
<class 'torch.nn.modules.conv.Conv2d'> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767] Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))

local_imp len =  768 torch.Size([768])
i =  6
<class 'torch.nn.modules.conv.Conv2d'> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767] Conv2d(768, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))

Conv/Linear Output GroupNormImportance() / layer Type =  <class 'mmcv.cnn.bricks.wrappers.Linear'>
local_imp len =  768 torch.Size([768])
i =  15
<class 'mmcv.cnn.bricks.wrappers.Linear'> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767] Linear(in_features=768, out_features=3072, bias=True)

Conv/Linear Output GroupNormImportance() / layer Type =  <class 'mmcv.cnn.bricks.wrappers.Linear'>
local_imp len =  768 torch.Size([768])
i =  72
<class 'mmcv.cnn.bricks.wrappers.Linear'> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767] Linear(in_features=768, out_features=3072, bias=True)

idxs =  192
prunable_chs =  768
idxs =  192
prunable_chs =  768
idxs =  192
prunable_chs =  768
idxs =  192
prunable_chs =  768
idxs =  192
prunable_chs =  768
WindowMSAPruner layer.qkv.in_features =  768
PatchMergingPruner () get_out_channels/  768 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
idxs =  192
prunable_chs =  768
idxs =  192
prunable_chs =  768
idxs =  192
prunable_chs =  768
WindowMSAPruner layer.qkv.in_features =  768
WindowMSAPruner layer.embed_dims =  768
idxs =  192
prunable_chs =  768
WindowMSAPruner layer.embed_dims =  768
idxs =  192
prunable_chs =  768
pruner.step 
--------------------------------
          Pruning Group
--------------------------------
[0] prune_out_channels on backbone.norm3 (LayerNorm((768,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on backbone.norm3 (LayerNorm((768,), eps=1e-05, elementwise_affine=True)), #idxs=192
[1] prune_out_channels on backbone.norm3 (LayerNorm((768,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _ElementWiseOp_661(AddBackward0), #idxs=192
[2] prune_out_channels on backbone.norm3 (LayerNorm((768,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_660(), #idxs=192
[3] prune_out_channels on _Reshape_660() => prune_out_channels on _ElementWiseOp_659(PermuteBackward0), #idxs=192
[4] prune_out_channels on _ElementWiseOp_659(PermuteBackward0) => prune_out_channels on _ElementWiseOp_658(CloneBackward0), #idxs=192
[5] prune_out_channels on _ElementWiseOp_658(CloneBackward0) => prune_in_channels on neck.convs.2.conv (Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1))), #idxs=192
[6] prune_out_channels on _ElementWiseOp_658(CloneBackward0) => prune_in_channels on neck.extra_convs.0.conv (Conv2d(768, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))), #idxs=192
[7] prune_out_channels on _ElementWiseOp_661(AddBackward0) => prune_out_channels on _ElementWiseOp_662(AddBackward0), #idxs=192
[8] prune_out_channels on _ElementWiseOp_661(AddBackward0) => prune_out_channels on backbone.stages.3.blocks.1.ffn.layers.1 (Linear(in_features=3072, out_features=768, bias=True)), #idxs=192
[9] prune_out_channels on _ElementWiseOp_662(AddBackward0) => prune_out_channels on _Reshape_670(), #idxs=192
[10] prune_out_channels on _ElementWiseOp_662(AddBackward0) => prune_out_channels on _ElementWiseOp_671(AddBackward0), #idxs=192
[11] prune_out_channels on _ElementWiseOp_662(AddBackward0) => prune_out_channels on backbone.stages.3.blocks.1.norm2 (LayerNorm((768,), eps=1e-05, elementwise_affine=True)), #idxs=192
[12] prune_out_channels on backbone.stages.3.blocks.1.norm2 (LayerNorm((768,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_668(), #idxs=192
[13] prune_out_channels on _Reshape_668() => prune_out_channels on _ElementWiseOp_667(AddmmBackward0), #idxs=192
[14] prune_out_channels on _ElementWiseOp_667(AddmmBackward0) => prune_out_channels on _ElementWiseOp_669(TBackward0), #idxs=192
[15] prune_out_channels on _ElementWiseOp_667(AddmmBackward0) => prune_in_channels on backbone.stages.3.blocks.1.ffn.layers.0.0 (Linear(in_features=768, out_features=3072, bias=True)), #idxs=192
[16] prune_out_channels on _ElementWiseOp_671(AddBackward0) => prune_out_channels on _ElementWiseOp_672(AddBackward0), #idxs=192
[17] prune_out_channels on _ElementWiseOp_671(AddBackward0) => prune_out_channels on backbone.stages.3.blocks.0.ffn.layers.1 (Linear(in_features=3072, out_features=768, bias=True)), #idxs=192
[18] prune_out_channels on _ElementWiseOp_671(AddBackward0) => prune_out_channels on backbone.stages.3.blocks.1.norm1 (LayerNorm((768,), eps=1e-05, elementwise_affine=True)), #idxs=192
[19] prune_out_channels on backbone.stages.3.blocks.1.norm1 (LayerNorm((768,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_770(), #idxs=192
[20] prune_out_channels on _Reshape_770() => prune_out_channels on _ElementWiseOp_769(ConstantPadNdBackward0), #idxs=192
[21] prune_out_channels on _ElementWiseOp_769(ConstantPadNdBackward0) => prune_out_channels on _ElementWiseOp_768(RollBackward0), #idxs=192
[22] prune_out_channels on _ElementWiseOp_768(RollBackward0) => prune_out_channels on _Reshape_767(), #idxs=192
[23] prune_out_channels on _Reshape_767() => prune_out_channels on _ElementWiseOp_766(PermuteBackward0), #idxs=192
[24] prune_out_channels on _ElementWiseOp_766(PermuteBackward0) => prune_out_channels on _ElementWiseOp_765(CloneBackward0), #idxs=192
[25] prune_out_channels on _ElementWiseOp_765(CloneBackward0) => prune_out_channels on _Reshape_764(), #idxs=192
[26] prune_out_channels on _Reshape_764() => prune_out_channels on _Reshape_763(), #idxs=192
[27] prune_out_channels on _Reshape_763() => prune_out_channels on _Reshape_761(), #idxs=192
[28] prune_out_channels on _Reshape_761() => prune_out_channels on _ElementWiseOp_760(AddmmBackward0), #idxs=192
[29] prune_out_channels on _ElementWiseOp_760(AddmmBackward0) => prune_out_channels on _ElementWiseOp_762(TBackward0), #idxs=192
[30] prune_out_channels on _ElementWiseOp_760(AddmmBackward0) => prune_out_channels on _Reshape_759(), #idxs=192
[31] prune_out_channels on _Reshape_759() => prune_out_channels on _Reshape_758(), #idxs=192
[32] prune_out_channels on _Reshape_758() => prune_out_channels on _ElementWiseOp_757(PermuteBackward0), #idxs=192
[33] prune_out_channels on _ElementWiseOp_757(PermuteBackward0) => prune_out_channels on _ElementWiseOp_756(SelectBackward0), #idxs=192
[34] prune_out_channels on _ElementWiseOp_757(PermuteBackward0) => prune_out_channels on _ElementWiseOp_789(SelectBackward0), #idxs=192
[35] prune_out_channels on _ElementWiseOp_757(PermuteBackward0) => prune_out_channels on _ElementWiseOp_793(SelectBackward0), #idxs=192
[36] prune_out_channels on _ElementWiseOp_793(SelectBackward0) => prune_out_channels on _ElementWiseOp_792(MulBackward0), #idxs=192
[37] prune_out_channels on _ElementWiseOp_792(MulBackward0) => prune_out_channels on _ElementWiseOp_791(ExpandBackward0), #idxs=192
[38] prune_out_channels on _ElementWiseOp_791(ExpandBackward0) => prune_out_channels on _ElementWiseOp_790(CloneBackward0), #idxs=192
[39] prune_out_channels on _ElementWiseOp_790(CloneBackward0) => prune_out_channels on _Reshape_784(), #idxs=192
[40] prune_out_channels on _Reshape_784() => prune_out_channels on _ElementWiseOp_783(BmmBackward0), #idxs=192
[41] prune_out_channels on _ElementWiseOp_783(BmmBackward0) => prune_out_channels on _Reshape_785(), #idxs=192
[42] prune_out_channels on _ElementWiseOp_783(BmmBackward0) => prune_out_channels on _Reshape_777(), #idxs=192
[43] prune_out_channels on _Reshape_777() => prune_out_channels on _ElementWiseOp_776(AddBackward0), #idxs=192
[44] prune_out_channels on _ElementWiseOp_776(AddBackward0) => prune_out_channels on _ElementWiseOp_778(UnsqueezeBackward0), #idxs=192
[45] prune_out_channels on _ElementWiseOp_776(AddBackward0) => prune_out_channels on _Reshape_775(), #idxs=192
[46] prune_out_channels on _Reshape_775() => prune_out_channels on _ElementWiseOp_774(AddBackward0), #idxs=192
[47] prune_out_channels on _ElementWiseOp_774(AddBackward0) => prune_out_channels on _Reshape_773(), #idxs=192
[48] prune_out_channels on _Reshape_773() => prune_out_channels on _ElementWiseOp_772(SoftmaxBackward0), #idxs=192
[49] prune_out_channels on _ElementWiseOp_772(SoftmaxBackward0) => prune_out_channels on _ElementWiseOp_771(ExpandBackward0), #idxs=192
[50] prune_out_channels on _ElementWiseOp_771(ExpandBackward0) => prune_out_channels on _Reshape_752(), #idxs=192
[51] prune_out_channels on _Reshape_752() => prune_out_channels on _ElementWiseOp_751(BmmBackward0), #idxs=192
[52] prune_out_channels on _ElementWiseOp_751(BmmBackward0) => prune_out_channels on _Reshape_753(), #idxs=192
[53] prune_out_channels on _ElementWiseOp_751(BmmBackward0) => prune_out_channels on _Reshape_750(), #idxs=192
[54] prune_out_channels on _Reshape_750() => prune_out_channels on _ElementWiseOp_749(TransposeBackward0), #idxs=192
[55] prune_out_channels on _ElementWiseOp_749(TransposeBackward0) => prune_out_channels on _ElementWiseOp_748(CloneBackward0), #idxs=192
[56] prune_out_channels on _ElementWiseOp_748(CloneBackward0) => prune_in_channels on backbone.stages.3.blocks.1.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=768, out_features=2304, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=768, out_features=768, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)), #idxs=192
[57] prune_out_channels on _Reshape_753() => prune_out_channels on _ElementWiseOp_754(CloneBackward0), #idxs=192
[58] prune_out_channels on _ElementWiseOp_754(CloneBackward0) => prune_out_channels on _ElementWiseOp_755(ExpandBackward0), #idxs=192
[59] prune_out_channels on _ElementWiseOp_778(UnsqueezeBackward0) => prune_out_channels on _ElementWiseOp_779(CloneBackward0), #idxs=192
[60] prune_out_channels on _ElementWiseOp_779(CloneBackward0) => prune_out_channels on _ElementWiseOp_780(PermuteBackward0), #idxs=192
[61] prune_out_channels on _ElementWiseOp_780(PermuteBackward0) => prune_out_channels on _Reshape_781(), #idxs=192
[62] prune_out_channels on _Reshape_781() => prune_out_channels on _ElementWiseOp_782(IndexBackward0), #idxs=192
[63] prune_out_channels on _Reshape_785() => prune_out_channels on _ElementWiseOp_786(CloneBackward0), #idxs=192
[64] prune_out_channels on _ElementWiseOp_786(CloneBackward0) => prune_out_channels on _ElementWiseOp_787(ExpandBackward0), #idxs=192
[65] prune_out_channels on _ElementWiseOp_787(ExpandBackward0) => prune_out_channels on _ElementWiseOp_788(TransposeBackward0), #idxs=192
[66] prune_out_channels on _ElementWiseOp_672(AddBackward0) => prune_out_channels on _Reshape_680(), #idxs=192
[67] prune_out_channels on _ElementWiseOp_672(AddBackward0) => prune_out_channels on backbone.stages.2.downsample (PatchMerging(
  (adap_padding): AdaptivePadding()
  (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
  (norm): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)
  (reduction): Linear(in_features=1152, out_features=768, bias=False)
)), #idxs=192
[68] prune_out_channels on _ElementWiseOp_672(AddBackward0) => prune_out_channels on backbone.stages.3.blocks.0.norm2 (LayerNorm((768,), eps=1e-05, elementwise_affine=True)), #idxs=192
[69] prune_out_channels on backbone.stages.3.blocks.0.norm2 (LayerNorm((768,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_678(), #idxs=192
[70] prune_out_channels on _Reshape_678() => prune_out_channels on _ElementWiseOp_677(AddmmBackward0), #idxs=192
[71] prune_out_channels on _ElementWiseOp_677(AddmmBackward0) => prune_out_channels on _ElementWiseOp_679(TBackward0), #idxs=192
[72] prune_out_channels on _ElementWiseOp_677(AddmmBackward0) => prune_in_channels on backbone.stages.3.blocks.0.ffn.layers.0.0 (Linear(in_features=768, out_features=3072, bias=True)), #idxs=192
[73] prune_out_channels on backbone.stages.2.downsample (PatchMerging(
  (adap_padding): AdaptivePadding()
  (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
  (norm): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)
  (reduction): Linear(in_features=1152, out_features=768, bias=False)
)) => prune_out_channels on backbone.stages.3.blocks.0.norm1 (LayerNorm((768,), eps=1e-05, elementwise_affine=True)), #idxs=192
[74] prune_out_channels on backbone.stages.3.blocks.0.norm1 (LayerNorm((768,), eps=1e-05, elementwise_affine=True)) => prune_out_channels on _Reshape_718(), #idxs=192
[75] prune_out_channels on _Reshape_718() => prune_out_channels on _ElementWiseOp_717(ConstantPadNdBackward0), #idxs=192
[76] prune_out_channels on _ElementWiseOp_717(ConstantPadNdBackward0) => prune_out_channels on _Reshape_716(), #idxs=192
[77] prune_out_channels on _Reshape_716() => prune_out_channels on _ElementWiseOp_715(PermuteBackward0), #idxs=192
[78] prune_out_channels on _ElementWiseOp_715(PermuteBackward0) => prune_out_channels on _ElementWiseOp_714(CloneBackward0), #idxs=192
[79] prune_out_channels on _ElementWiseOp_714(CloneBackward0) => prune_out_channels on _Reshape_713(), #idxs=192
[80] prune_out_channels on _Reshape_713() => prune_out_channels on _Reshape_712(), #idxs=192
[81] prune_out_channels on _Reshape_712() => prune_out_channels on _Reshape_710(), #idxs=192
[82] prune_out_channels on _Reshape_710() => prune_out_channels on _ElementWiseOp_709(AddmmBackward0), #idxs=192
[83] prune_out_channels on _ElementWiseOp_709(AddmmBackward0) => prune_out_channels on _ElementWiseOp_711(TBackward0), #idxs=192
[84] prune_out_channels on _ElementWiseOp_709(AddmmBackward0) => prune_out_channels on _Reshape_708(), #idxs=192
[85] prune_out_channels on _Reshape_708() => prune_out_channels on _Reshape_707(), #idxs=192
[86] prune_out_channels on _Reshape_707() => prune_out_channels on _ElementWiseOp_706(PermuteBackward0), #idxs=192
[87] prune_out_channels on _ElementWiseOp_706(PermuteBackward0) => prune_out_channels on _ElementWiseOp_705(SelectBackward0), #idxs=192
[88] prune_out_channels on _ElementWiseOp_706(PermuteBackward0) => prune_out_channels on _ElementWiseOp_734(SelectBackward0), #idxs=192
[89] prune_out_channels on _ElementWiseOp_706(PermuteBackward0) => prune_out_channels on _ElementWiseOp_738(SelectBackward0), #idxs=192
[90] prune_out_channels on _ElementWiseOp_738(SelectBackward0) => prune_out_channels on _ElementWiseOp_737(MulBackward0), #idxs=192
[91] prune_out_channels on _ElementWiseOp_737(MulBackward0) => prune_out_channels on _ElementWiseOp_736(ExpandBackward0), #idxs=192
[92] prune_out_channels on _ElementWiseOp_736(ExpandBackward0) => prune_out_channels on _ElementWiseOp_735(CloneBackward0), #idxs=192
[93] prune_out_channels on _ElementWiseOp_735(CloneBackward0) => prune_out_channels on _Reshape_729(), #idxs=192
[94] prune_out_channels on _Reshape_729() => prune_out_channels on _ElementWiseOp_728(BmmBackward0), #idxs=192
[95] prune_out_channels on _ElementWiseOp_728(BmmBackward0) => prune_out_channels on _Reshape_730(), #idxs=192
[96] prune_out_channels on _ElementWiseOp_728(BmmBackward0) => prune_out_channels on _Reshape_722(), #idxs=192
[97] prune_out_channels on _Reshape_722() => prune_out_channels on _ElementWiseOp_721(AddBackward0), #idxs=192
[98] prune_out_channels on _ElementWiseOp_721(AddBackward0) => prune_out_channels on _ElementWiseOp_723(UnsqueezeBackward0), #idxs=192
[99] prune_out_channels on _ElementWiseOp_721(AddBackward0) => prune_out_channels on _ElementWiseOp_720(SoftmaxBackward0), #idxs=192
[100] prune_out_channels on _ElementWiseOp_720(SoftmaxBackward0) => prune_out_channels on _ElementWiseOp_719(ExpandBackward0), #idxs=192
[101] prune_out_channels on _ElementWiseOp_719(ExpandBackward0) => prune_out_channels on _Reshape_701(), #idxs=192
[102] prune_out_channels on _Reshape_701() => prune_out_channels on _ElementWiseOp_700(BmmBackward0), #idxs=192
[103] prune_out_channels on _ElementWiseOp_700(BmmBackward0) => prune_out_channels on _Reshape_702(), #idxs=192
[104] prune_out_channels on _ElementWiseOp_700(BmmBackward0) => prune_out_channels on _Reshape_699(), #idxs=192
[105] prune_out_channels on _Reshape_699() => prune_out_channels on _ElementWiseOp_698(TransposeBackward0), #idxs=192
[106] prune_out_channels on _ElementWiseOp_698(TransposeBackward0) => prune_out_channels on _ElementWiseOp_697(CloneBackward0), #idxs=192
[107] prune_out_channels on _ElementWiseOp_697(CloneBackward0) => prune_in_channels on backbone.stages.3.blocks.0.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=768, out_features=2304, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=768, out_features=768, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)), #idxs=192
[108] prune_out_channels on _Reshape_702() => prune_out_channels on _ElementWiseOp_703(CloneBackward0), #idxs=192
[109] prune_out_channels on _ElementWiseOp_703(CloneBackward0) => prune_out_channels on _ElementWiseOp_704(ExpandBackward0), #idxs=192
[110] prune_out_channels on _ElementWiseOp_723(UnsqueezeBackward0) => prune_out_channels on _ElementWiseOp_724(CloneBackward0), #idxs=192
[111] prune_out_channels on _ElementWiseOp_724(CloneBackward0) => prune_out_channels on _ElementWiseOp_725(PermuteBackward0), #idxs=192
[112] prune_out_channels on _ElementWiseOp_725(PermuteBackward0) => prune_out_channels on _Reshape_726(), #idxs=192
[113] prune_out_channels on _Reshape_726() => prune_out_channels on _ElementWiseOp_727(IndexBackward0), #idxs=192
[114] prune_out_channels on _Reshape_730() => prune_out_channels on _ElementWiseOp_731(CloneBackward0), #idxs=192
[115] prune_out_channels on _ElementWiseOp_731(CloneBackward0) => prune_out_channels on _ElementWiseOp_732(ExpandBackward0), #idxs=192
[116] prune_out_channels on _ElementWiseOp_732(ExpandBackward0) => prune_out_channels on _ElementWiseOp_733(TransposeBackward0), #idxs=192
[117] prune_out_channels on _Reshape_680() => prune_out_channels on _ElementWiseOp_689(SliceBackward0), #idxs=192
[118] prune_out_channels on _ElementWiseOp_689(SliceBackward0) => prune_out_channels on _ElementWiseOp_690(SliceBackward0), #idxs=192
[119] prune_out_channels on _ElementWiseOp_690(SliceBackward0) => prune_out_channels on _ElementWiseOp_691(SliceBackward0), #idxs=192
[120] prune_out_channels on _ElementWiseOp_691(SliceBackward0) => prune_out_channels on _Reshape_692(), #idxs=192
[121] prune_out_channels on _Reshape_692() => prune_out_channels on _ElementWiseOp_693(CloneBackward0), #idxs=192
[122] prune_out_channels on _ElementWiseOp_693(CloneBackward0) => prune_out_channels on _ElementWiseOp_694(PermuteBackward0), #idxs=192
[123] prune_out_channels on _ElementWiseOp_694(PermuteBackward0) => prune_out_channels on _Reshape_695(), #idxs=192
[124] prune_out_channels on _Reshape_695() => prune_out_channels on _Reshape_696(), #idxs=192
[125] prune_out_channels on _Reshape_696() => prune_out_channels on backbone.stages.3.blocks.0.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=768, out_features=2304, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=768, out_features=768, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)), #idxs=192
[126] prune_out_channels on _Reshape_670() => prune_out_channels on _ElementWiseOp_739(SliceBackward0), #idxs=192
[127] prune_out_channels on _ElementWiseOp_739(SliceBackward0) => prune_out_channels on _ElementWiseOp_740(SliceBackward0), #idxs=192
[128] prune_out_channels on _ElementWiseOp_740(SliceBackward0) => prune_out_channels on _ElementWiseOp_741(SliceBackward0), #idxs=192
[129] prune_out_channels on _ElementWiseOp_741(SliceBackward0) => prune_out_channels on _ElementWiseOp_742(RollBackward0), #idxs=192
[130] prune_out_channels on _ElementWiseOp_742(RollBackward0) => prune_out_channels on _Reshape_743(), #idxs=192
[131] prune_out_channels on _Reshape_743() => prune_out_channels on _ElementWiseOp_744(CloneBackward0), #idxs=192
[132] prune_out_channels on _ElementWiseOp_744(CloneBackward0) => prune_out_channels on _ElementWiseOp_745(PermuteBackward0), #idxs=192
[133] prune_out_channels on _ElementWiseOp_745(PermuteBackward0) => prune_out_channels on _Reshape_746(), #idxs=192
[134] prune_out_channels on _Reshape_746() => prune_out_channels on _Reshape_747(), #idxs=192
[135] prune_out_channels on _Reshape_747() => prune_out_channels on backbone.stages.3.blocks.1.attn.w_msa (WindowMSA(
  (qkv): Linear(in_features=768, out_features=2304, bias=True)
  (attn_drop): Dropout(p=0.0, inplace=False)
  (proj): Linear(in_features=768, out_features=768, bias=True)
  (proj_drop): Dropout(p=0.0, inplace=False)
  (softmax): Softmax(dim=-1)
)), #idxs=192
--------------------------------

WindowMSAPruner prune_in_channels() /  192
PatchMergingPruner () prune_out_channels/  768 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
WindowMSAPruner prune_in_channels() /  192
WindowMSAPruner prune_out_channels() /  192
WindowMSAPruner prune_out_channels/ dim =  768
	len indx =  192
WindowMSAPruner - idxs =  [0, 5, 15, 18, 19, 27, 29, 41, 43, 48, 49, 54, 56, 58, 60, 62, 64, 66, 69, 78, 99, 102, 105, 106, 107, 109, 110, 116, 120, 123, 131, 133, 134, 135, 136, 137, 143, 144, 150, 152, 154, 159, 165, 166, 171, 181, 183, 192, 201, 202, 203, 208, 212, 213, 214, 215, 219, 222, 226, 227, 230, 232, 234, 235, 236, 243, 253, 261, 262, 263, 267, 269, 272, 279, 283, 285, 286, 289, 290, 296, 299, 304, 307, 309, 311, 320, 321, 331, 340, 343, 346, 349, 355, 367, 369, 370, 376, 378, 380, 381, 385, 387, 391, 392, 402, 405, 407, 410, 416, 419, 420, 431, 432, 433, 434, 438, 443, 444, 448, 453, 454, 461, 463, 465, 466, 467, 473, 475, 476, 480, 482, 483, 486, 488, 492, 501, 508, 509, 515, 529, 531, 534, 543, 545, 549, 557, 558, 560, 565, 569, 595, 596, 600, 614, 619, 623, 633, 634, 637, 641, 642, 649, 650, 652, 654, 663, 665, 670, 671, 675, 677, 678, 686, 691, 693, 697, 701, 710, 713, 715, 716, 718, 725, 726, 731, 733, 734, 741, 745, 751, 753, 762]
	idxs_repeated =  768
WindowMSAPruner - idxs_repeated =  [0, 5, 15, 18, 19, 27, 29, 41, 43, 48, 49, 54, 56, 58, 60, 62, 64, 66, 69, 78, 99, 102, 105, 106, 107, 109, 110, 116, 120, 123, 131, 133, 134, 135, 136, 137, 143, 144, 150, 152, 154, 159, 165, 166, 171, 181, 183, 192, 201, 202, 203, 208, 212, 213, 214, 215, 219, 222, 226, 227, 230, 232, 234, 235, 236, 243, 253, 261, 262, 263, 267, 269, 272, 279, 283, 285, 286, 289, 290, 296, 299, 304, 307, 309, 311, 320, 321, 331, 340, 343, 346, 349, 355, 367, 369, 370, 376, 378, 380, 381, 385, 387, 391, 392, 402, 405, 407, 410, 416, 419, 420, 431, 432, 433, 434, 438, 443, 444, 448, 453, 454, 461, 463, 465, 466, 467, 473, 475, 476, 480, 482, 483, 486, 488, 492, 501, 508, 509, 515, 529, 531, 534, 543, 545, 549, 557, 558, 560, 565, 569, 595, 596, 600, 614, 619, 623, 633, 634, 637, 641, 642, 649, 650, 652, 654, 663, 665, 670, 671, 675, 677, 678, 686, 691, 693, 697, 701, 710, 713, 715, 716, 718, 725, 726, 731, 733, 734, 741, 745, 751, 753, 762, 768, 773, 783, 786, 787, 795, 797, 809, 811, 816, 817, 822, 824, 826, 828, 830, 832, 834, 837, 846, 867, 870, 873, 874, 875, 877, 878, 884, 888, 891, 899, 901, 902, 903, 904, 905, 911, 912, 918, 920, 922, 927, 933, 934, 939, 949, 951, 960, 969, 970, 971, 976, 980, 981, 982, 983, 987, 990, 994, 995, 998, 1000, 1002, 1003, 1004, 1011, 1021, 1029, 1030, 1031, 1035, 1037, 1040, 1047, 1051, 1053, 1054, 1057, 1058, 1064, 1067, 1072, 1075, 1077, 1079, 1088, 1089, 1099, 1108, 1111, 1114, 1117, 1123, 1135, 1137, 1138, 1144, 1146, 1148, 1149, 1153, 1155, 1159, 1160, 1170, 1173, 1175, 1178, 1184, 1187, 1188, 1199, 1200, 1201, 1202, 1206, 1211, 1212, 1216, 1221, 1222, 1229, 1231, 1233, 1234, 1235, 1241, 1243, 1244, 1248, 1250, 1251, 1254, 1256, 1260, 1269, 1276, 1277, 1283, 1297, 1299, 1302, 1311, 1313, 1317, 1325, 1326, 1328, 1333, 1337, 1363, 1364, 1368, 1382, 1387, 1391, 1401, 1402, 1405, 1409, 1410, 1417, 1418, 1420, 1422, 1431, 1433, 1438, 1439, 1443, 1445, 1446, 1454, 1459, 1461, 1465, 1469, 1478, 1481, 1483, 1484, 1486, 1493, 1494, 1499, 1501, 1502, 1509, 1513, 1519, 1521, 1530, 1536, 1541, 1551, 1554, 1555, 1563, 1565, 1577, 1579, 1584, 1585, 1590, 1592, 1594, 1596, 1598, 1600, 1602, 1605, 1614, 1635, 1638, 1641, 1642, 1643, 1645, 1646, 1652, 1656, 1659, 1667, 1669, 1670, 1671, 1672, 1673, 1679, 1680, 1686, 1688, 1690, 1695, 1701, 1702, 1707, 1717, 1719, 1728, 1737, 1738, 1739, 1744, 1748, 1749, 1750, 1751, 1755, 1758, 1762, 1763, 1766, 1768, 1770, 1771, 1772, 1779, 1789, 1797, 1798, 1799, 1803, 1805, 1808, 1815, 1819, 1821, 1822, 1825, 1826, 1832, 1835, 1840, 1843, 1845, 1847, 1856, 1857, 1867, 1876, 1879, 1882, 1885, 1891, 1903, 1905, 1906, 1912, 1914, 1916, 1917, 1921, 1923, 1927, 1928, 1938, 1941, 1943, 1946, 1952, 1955, 1956, 1967, 1968, 1969, 1970, 1974, 1979, 1980, 1984, 1989, 1990, 1997, 1999, 2001, 2002, 2003, 2009, 2011, 2012, 2016, 2018, 2019, 2022, 2024, 2028, 2037, 2044, 2045, 2051, 2065, 2067, 2070, 2079, 2081, 2085, 2093, 2094, 2096, 2101, 2105, 2131, 2132, 2136, 2150, 2155, 2159, 2169, 2170, 2173, 2177, 2178, 2185, 2186, 2188, 2190, 2199, 2201, 2206, 2207, 2211, 2213, 2214, 2222, 2227, 2229, 2233, 2237, 2246, 2249, 2251, 2252, 2254, 2261, 2262, 2267, 2269, 2270, 2277, 2281, 2287, 2289, 2298, 2304, 2309, 2319, 2322, 2323, 2331, 2333, 2345, 2347, 2352, 2353, 2358, 2360, 2362, 2364, 2366, 2368, 2370, 2373, 2382, 2403, 2406, 2409, 2410, 2411, 2413, 2414, 2420, 2424, 2427, 2435, 2437, 2438, 2439, 2440, 2441, 2447, 2448, 2454, 2456, 2458, 2463, 2469, 2470, 2475, 2485, 2487, 2496, 2505, 2506, 2507, 2512, 2516, 2517, 2518, 2519, 2523, 2526, 2530, 2531, 2534, 2536, 2538, 2539, 2540, 2547, 2557, 2565, 2566, 2567, 2571, 2573, 2576, 2583, 2587, 2589, 2590, 2593, 2594, 2600, 2603, 2608, 2611, 2613, 2615, 2624, 2625, 2635, 2644, 2647, 2650, 2653, 2659, 2671, 2673, 2674, 2680, 2682, 2684, 2685, 2689, 2691, 2695, 2696, 2706, 2709, 2711, 2714, 2720, 2723, 2724, 2735, 2736, 2737, 2738, 2742, 2747, 2748, 2752, 2757, 2758, 2765, 2767, 2769, 2770, 2771, 2777, 2779, 2780, 2784, 2786, 2787, 2790, 2792, 2796, 2805, 2812, 2813, 2819, 2833, 2835, 2838, 2847, 2849, 2853, 2861, 2862, 2864, 2869, 2873, 2899, 2900, 2904, 2918, 2923, 2927, 2937, 2938, 2941, 2945, 2946, 2953, 2954, 2956, 2958, 2967, 2969, 2974, 2975, 2979, 2981, 2982, 2990, 2995, 2997, 3001, 3005, 3014, 3017, 3019, 3020, 3022, 3029, 3030, 3035, 3037, 3038, 3045, 3049, 3055, 3057, 3066]
WindowMSAPruner prune_out_channels idxs_repeated =  768
WindowMSAPruner prune_out_channels() /  192
WindowMSAPruner prune_out_channels/ dim =  768
	len indx =  192
WindowMSAPruner - idxs =  [0, 5, 15, 18, 19, 27, 29, 41, 43, 48, 49, 54, 56, 58, 60, 62, 64, 66, 69, 78, 99, 102, 105, 106, 107, 109, 110, 116, 120, 123, 131, 133, 134, 135, 136, 137, 143, 144, 150, 152, 154, 159, 165, 166, 171, 181, 183, 192, 201, 202, 203, 208, 212, 213, 214, 215, 219, 222, 226, 227, 230, 232, 234, 235, 236, 243, 253, 261, 262, 263, 267, 269, 272, 279, 283, 285, 286, 289, 290, 296, 299, 304, 307, 309, 311, 320, 321, 331, 340, 343, 346, 349, 355, 367, 369, 370, 376, 378, 380, 381, 385, 387, 391, 392, 402, 405, 407, 410, 416, 419, 420, 431, 432, 433, 434, 438, 443, 444, 448, 453, 454, 461, 463, 465, 466, 467, 473, 475, 476, 480, 482, 483, 486, 488, 492, 501, 508, 509, 515, 529, 531, 534, 543, 545, 549, 557, 558, 560, 565, 569, 595, 596, 600, 614, 619, 623, 633, 634, 637, 641, 642, 649, 650, 652, 654, 663, 665, 670, 671, 675, 677, 678, 686, 691, 693, 697, 701, 710, 713, 715, 716, 718, 725, 726, 731, 733, 734, 741, 745, 751, 753, 762]
	idxs_repeated =  768
WindowMSAPruner - idxs_repeated =  [0, 5, 15, 18, 19, 27, 29, 41, 43, 48, 49, 54, 56, 58, 60, 62, 64, 66, 69, 78, 99, 102, 105, 106, 107, 109, 110, 116, 120, 123, 131, 133, 134, 135, 136, 137, 143, 144, 150, 152, 154, 159, 165, 166, 171, 181, 183, 192, 201, 202, 203, 208, 212, 213, 214, 215, 219, 222, 226, 227, 230, 232, 234, 235, 236, 243, 253, 261, 262, 263, 267, 269, 272, 279, 283, 285, 286, 289, 290, 296, 299, 304, 307, 309, 311, 320, 321, 331, 340, 343, 346, 349, 355, 367, 369, 370, 376, 378, 380, 381, 385, 387, 391, 392, 402, 405, 407, 410, 416, 419, 420, 431, 432, 433, 434, 438, 443, 444, 448, 453, 454, 461, 463, 465, 466, 467, 473, 475, 476, 480, 482, 483, 486, 488, 492, 501, 508, 509, 515, 529, 531, 534, 543, 545, 549, 557, 558, 560, 565, 569, 595, 596, 600, 614, 619, 623, 633, 634, 637, 641, 642, 649, 650, 652, 654, 663, 665, 670, 671, 675, 677, 678, 686, 691, 693, 697, 701, 710, 713, 715, 716, 718, 725, 726, 731, 733, 734, 741, 745, 751, 753, 762, 768, 773, 783, 786, 787, 795, 797, 809, 811, 816, 817, 822, 824, 826, 828, 830, 832, 834, 837, 846, 867, 870, 873, 874, 875, 877, 878, 884, 888, 891, 899, 901, 902, 903, 904, 905, 911, 912, 918, 920, 922, 927, 933, 934, 939, 949, 951, 960, 969, 970, 971, 976, 980, 981, 982, 983, 987, 990, 994, 995, 998, 1000, 1002, 1003, 1004, 1011, 1021, 1029, 1030, 1031, 1035, 1037, 1040, 1047, 1051, 1053, 1054, 1057, 1058, 1064, 1067, 1072, 1075, 1077, 1079, 1088, 1089, 1099, 1108, 1111, 1114, 1117, 1123, 1135, 1137, 1138, 1144, 1146, 1148, 1149, 1153, 1155, 1159, 1160, 1170, 1173, 1175, 1178, 1184, 1187, 1188, 1199, 1200, 1201, 1202, 1206, 1211, 1212, 1216, 1221, 1222, 1229, 1231, 1233, 1234, 1235, 1241, 1243, 1244, 1248, 1250, 1251, 1254, 1256, 1260, 1269, 1276, 1277, 1283, 1297, 1299, 1302, 1311, 1313, 1317, 1325, 1326, 1328, 1333, 1337, 1363, 1364, 1368, 1382, 1387, 1391, 1401, 1402, 1405, 1409, 1410, 1417, 1418, 1420, 1422, 1431, 1433, 1438, 1439, 1443, 1445, 1446, 1454, 1459, 1461, 1465, 1469, 1478, 1481, 1483, 1484, 1486, 1493, 1494, 1499, 1501, 1502, 1509, 1513, 1519, 1521, 1530, 1536, 1541, 1551, 1554, 1555, 1563, 1565, 1577, 1579, 1584, 1585, 1590, 1592, 1594, 1596, 1598, 1600, 1602, 1605, 1614, 1635, 1638, 1641, 1642, 1643, 1645, 1646, 1652, 1656, 1659, 1667, 1669, 1670, 1671, 1672, 1673, 1679, 1680, 1686, 1688, 1690, 1695, 1701, 1702, 1707, 1717, 1719, 1728, 1737, 1738, 1739, 1744, 1748, 1749, 1750, 1751, 1755, 1758, 1762, 1763, 1766, 1768, 1770, 1771, 1772, 1779, 1789, 1797, 1798, 1799, 1803, 1805, 1808, 1815, 1819, 1821, 1822, 1825, 1826, 1832, 1835, 1840, 1843, 1845, 1847, 1856, 1857, 1867, 1876, 1879, 1882, 1885, 1891, 1903, 1905, 1906, 1912, 1914, 1916, 1917, 1921, 1923, 1927, 1928, 1938, 1941, 1943, 1946, 1952, 1955, 1956, 1967, 1968, 1969, 1970, 1974, 1979, 1980, 1984, 1989, 1990, 1997, 1999, 2001, 2002, 2003, 2009, 2011, 2012, 2016, 2018, 2019, 2022, 2024, 2028, 2037, 2044, 2045, 2051, 2065, 2067, 2070, 2079, 2081, 2085, 2093, 2094, 2096, 2101, 2105, 2131, 2132, 2136, 2150, 2155, 2159, 2169, 2170, 2173, 2177, 2178, 2185, 2186, 2188, 2190, 2199, 2201, 2206, 2207, 2211, 2213, 2214, 2222, 2227, 2229, 2233, 2237, 2246, 2249, 2251, 2252, 2254, 2261, 2262, 2267, 2269, 2270, 2277, 2281, 2287, 2289, 2298, 2304, 2309, 2319, 2322, 2323, 2331, 2333, 2345, 2347, 2352, 2353, 2358, 2360, 2362, 2364, 2366, 2368, 2370, 2373, 2382, 2403, 2406, 2409, 2410, 2411, 2413, 2414, 2420, 2424, 2427, 2435, 2437, 2438, 2439, 2440, 2441, 2447, 2448, 2454, 2456, 2458, 2463, 2469, 2470, 2475, 2485, 2487, 2496, 2505, 2506, 2507, 2512, 2516, 2517, 2518, 2519, 2523, 2526, 2530, 2531, 2534, 2536, 2538, 2539, 2540, 2547, 2557, 2565, 2566, 2567, 2571, 2573, 2576, 2583, 2587, 2589, 2590, 2593, 2594, 2600, 2603, 2608, 2611, 2613, 2615, 2624, 2625, 2635, 2644, 2647, 2650, 2653, 2659, 2671, 2673, 2674, 2680, 2682, 2684, 2685, 2689, 2691, 2695, 2696, 2706, 2709, 2711, 2714, 2720, 2723, 2724, 2735, 2736, 2737, 2738, 2742, 2747, 2748, 2752, 2757, 2758, 2765, 2767, 2769, 2770, 2771, 2777, 2779, 2780, 2784, 2786, 2787, 2790, 2792, 2796, 2805, 2812, 2813, 2819, 2833, 2835, 2838, 2847, 2849, 2853, 2861, 2862, 2864, 2869, 2873, 2899, 2900, 2904, 2918, 2923, 2927, 2937, 2938, 2941, 2945, 2946, 2953, 2954, 2956, 2958, 2967, 2969, 2974, 2975, 2979, 2981, 2982, 2990, 2995, 2997, 3001, 3005, 3014, 3017, 3019, 3020, 3022, 3029, 3030, 3035, 3037, 3038, 3045, 3049, 3055, 3057, 3066]
WindowMSAPruner prune_out_channels idxs_repeated =  768
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
	get_all_groups -- Adding layer:  <class 'mmcv.cnn.bricks.wrappers.Linear'>
		prune_local()/  _check_pruning_ratio OK
Conv/Linear Output GroupNormImportance() / layer Type =  <class 'mmcv.cnn.bricks.wrappers.Linear'>
local_imp len =  3072 torch.Size([3072])
i =  5
<class 'mmcv.cnn.bricks.wrappers.Linear'> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501, 1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509, 1510, 1511, 1512, 1513, 1514, 1515, 1516, 1517, 1518, 1519, 1520, 1521, 1522, 1523, 1524, 1525, 1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533, 1534, 1535, 1536, 1537, 1538, 1539, 1540, 1541, 1542, 1543, 1544, 1545, 1546, 1547, 1548, 1549, 1550, 1551, 1552, 1553, 1554, 1555, 1556, 1557, 1558, 1559, 1560, 1561, 1562, 1563, 1564, 1565, 1566, 1567, 1568, 1569, 1570, 1571, 1572, 1573, 1574, 1575, 1576, 1577, 1578, 1579, 1580, 1581, 1582, 1583, 1584, 1585, 1586, 1587, 1588, 1589, 1590, 1591, 1592, 1593, 1594, 1595, 1596, 1597, 1598, 1599, 1600, 1601, 1602, 1603, 1604, 1605, 1606, 1607, 1608, 1609, 1610, 1611, 1612, 1613, 1614, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623, 1624, 1625, 1626, 1627, 1628, 1629, 1630, 1631, 1632, 1633, 1634, 1635, 1636, 1637, 1638, 1639, 1640, 1641, 1642, 1643, 1644, 1645, 1646, 1647, 1648, 1649, 1650, 1651, 1652, 1653, 1654, 1655, 1656, 1657, 1658, 1659, 1660, 1661, 1662, 1663, 1664, 1665, 1666, 1667, 1668, 1669, 1670, 1671, 1672, 1673, 1674, 1675, 1676, 1677, 1678, 1679, 1680, 1681, 1682, 1683, 1684, 1685, 1686, 1687, 1688, 1689, 1690, 1691, 1692, 1693, 1694, 1695, 1696, 1697, 1698, 1699, 1700, 1701, 1702, 1703, 1704, 1705, 1706, 1707, 1708, 1709, 1710, 1711, 1712, 1713, 1714, 1715, 1716, 1717, 1718, 1719, 1720, 1721, 1722, 1723, 1724, 1725, 1726, 1727, 1728, 1729, 1730, 1731, 1732, 1733, 1734, 1735, 1736, 1737, 1738, 1739, 1740, 1741, 1742, 1743, 1744, 1745, 1746, 1747, 1748, 1749, 1750, 1751, 1752, 1753, 1754, 1755, 1756, 1757, 1758, 1759, 1760, 1761, 1762, 1763, 1764, 1765, 1766, 1767, 1768, 1769, 1770, 1771, 1772, 1773, 1774, 1775, 1776, 1777, 1778, 1779, 1780, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789, 1790, 1791, 1792, 1793, 1794, 1795, 1796, 1797, 1798, 1799, 1800, 1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1809, 1810, 1811, 1812, 1813, 1814, 1815, 1816, 1817, 1818, 1819, 1820, 1821, 1822, 1823, 1824, 1825, 1826, 1827, 1828, 1829, 1830, 1831, 1832, 1833, 1834, 1835, 1836, 1837, 1838, 1839, 1840, 1841, 1842, 1843, 1844, 1845, 1846, 1847, 1848, 1849, 1850, 1851, 1852, 1853, 1854, 1855, 1856, 1857, 1858, 1859, 1860, 1861, 1862, 1863, 1864, 1865, 1866, 1867, 1868, 1869, 1870, 1871, 1872, 1873, 1874, 1875, 1876, 1877, 1878, 1879, 1880, 1881, 1882, 1883, 1884, 1885, 1886, 1887, 1888, 1889, 1890, 1891, 1892, 1893, 1894, 1895, 1896, 1897, 1898, 1899, 1900, 1901, 1902, 1903, 1904, 1905, 1906, 1907, 1908, 1909, 1910, 1911, 1912, 1913, 1914, 1915, 1916, 1917, 1918, 1919, 1920, 1921, 1922, 1923, 1924, 1925, 1926, 1927, 1928, 1929, 1930, 1931, 1932, 1933, 1934, 1935, 1936, 1937, 1938, 1939, 1940, 1941, 1942, 1943, 1944, 1945, 1946, 1947, 1948, 1949, 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031, 2032, 2033, 2034, 2035, 2036, 2037, 2038, 2039, 2040, 2041, 2042, 2043, 2044, 2045, 2046, 2047, 2048, 2049, 2050, 2051, 2052, 2053, 2054, 2055, 2056, 2057, 2058, 2059, 2060, 2061, 2062, 2063, 2064, 2065, 2066, 2067, 2068, 2069, 2070, 2071, 2072, 2073, 2074, 2075, 2076, 2077, 2078, 2079, 2080, 2081, 2082, 2083, 2084, 2085, 2086, 2087, 2088, 2089, 2090, 2091, 2092, 2093, 2094, 2095, 2096, 2097, 2098, 2099, 2100, 2101, 2102, 2103, 2104, 2105, 2106, 2107, 2108, 2109, 2110, 2111, 2112, 2113, 2114, 2115, 2116, 2117, 2118, 2119, 2120, 2121, 2122, 2123, 2124, 2125, 2126, 2127, 2128, 2129, 2130, 2131, 2132, 2133, 2134, 2135, 2136, 2137, 2138, 2139, 2140, 2141, 2142, 2143, 2144, 2145, 2146, 2147, 2148, 2149, 2150, 2151, 2152, 2153, 2154, 2155, 2156, 2157, 2158, 2159, 2160, 2161, 2162, 2163, 2164, 2165, 2166, 2167, 2168, 2169, 2170, 2171, 2172, 2173, 2174, 2175, 2176, 2177, 2178, 2179, 2180, 2181, 2182, 2183, 2184, 2185, 2186, 2187, 2188, 2189, 2190, 2191, 2192, 2193, 2194, 2195, 2196, 2197, 2198, 2199, 2200, 2201, 2202, 2203, 2204, 2205, 2206, 2207, 2208, 2209, 2210, 2211, 2212, 2213, 2214, 2215, 2216, 2217, 2218, 2219, 2220, 2221, 2222, 2223, 2224, 2225, 2226, 2227, 2228, 2229, 2230, 2231, 2232, 2233, 2234, 2235, 2236, 2237, 2238, 2239, 2240, 2241, 2242, 2243, 2244, 2245, 2246, 2247, 2248, 2249, 2250, 2251, 2252, 2253, 2254, 2255, 2256, 2257, 2258, 2259, 2260, 2261, 2262, 2263, 2264, 2265, 2266, 2267, 2268, 2269, 2270, 2271, 2272, 2273, 2274, 2275, 2276, 2277, 2278, 2279, 2280, 2281, 2282, 2283, 2284, 2285, 2286, 2287, 2288, 2289, 2290, 2291, 2292, 2293, 2294, 2295, 2296, 2297, 2298, 2299, 2300, 2301, 2302, 2303, 2304, 2305, 2306, 2307, 2308, 2309, 2310, 2311, 2312, 2313, 2314, 2315, 2316, 2317, 2318, 2319, 2320, 2321, 2322, 2323, 2324, 2325, 2326, 2327, 2328, 2329, 2330, 2331, 2332, 2333, 2334, 2335, 2336, 2337, 2338, 2339, 2340, 2341, 2342, 2343, 2344, 2345, 2346, 2347, 2348, 2349, 2350, 2351, 2352, 2353, 2354, 2355, 2356, 2357, 2358, 2359, 2360, 2361, 2362, 2363, 2364, 2365, 2366, 2367, 2368, 2369, 2370, 2371, 2372, 2373, 2374, 2375, 2376, 2377, 2378, 2379, 2380, 2381, 2382, 2383, 2384, 2385, 2386, 2387, 2388, 2389, 2390, 2391, 2392, 2393, 2394, 2395, 2396, 2397, 2398, 2399, 2400, 2401, 2402, 2403, 2404, 2405, 2406, 2407, 2408, 2409, 2410, 2411, 2412, 2413, 2414, 2415, 2416, 2417, 2418, 2419, 2420, 2421, 2422, 2423, 2424, 2425, 2426, 2427, 2428, 2429, 2430, 2431, 2432, 2433, 2434, 2435, 2436, 2437, 2438, 2439, 2440, 2441, 2442, 2443, 2444, 2445, 2446, 2447, 2448, 2449, 2450, 2451, 2452, 2453, 2454, 2455, 2456, 2457, 2458, 2459, 2460, 2461, 2462, 2463, 2464, 2465, 2466, 2467, 2468, 2469, 2470, 2471, 2472, 2473, 2474, 2475, 2476, 2477, 2478, 2479, 2480, 2481, 2482, 2483, 2484, 2485, 2486, 2487, 2488, 2489, 2490, 2491, 2492, 2493, 2494, 2495, 2496, 2497, 2498, 2499, 2500, 2501, 2502, 2503, 2504, 2505, 2506, 2507, 2508, 2509, 2510, 2511, 2512, 2513, 2514, 2515, 2516, 2517, 2518, 2519, 2520, 2521, 2522, 2523, 2524, 2525, 2526, 2527, 2528, 2529, 2530, 2531, 2532, 2533, 2534, 2535, 2536, 2537, 2538, 2539, 2540, 2541, 2542, 2543, 2544, 2545, 2546, 2547, 2548, 2549, 2550, 2551, 2552, 2553, 2554, 2555, 2556, 2557, 2558, 2559, 2560, 2561, 2562, 2563, 2564, 2565, 2566, 2567, 2568, 2569, 2570, 2571, 2572, 2573, 2574, 2575, 2576, 2577, 2578, 2579, 2580, 2581, 2582, 2583, 2584, 2585, 2586, 2587, 2588, 2589, 2590, 2591, 2592, 2593, 2594, 2595, 2596, 2597, 2598, 2599, 2600, 2601, 2602, 2603, 2604, 2605, 2606, 2607, 2608, 2609, 2610, 2611, 2612, 2613, 2614, 2615, 2616, 2617, 2618, 2619, 2620, 2621, 2622, 2623, 2624, 2625, 2626, 2627, 2628, 2629, 2630, 2631, 2632, 2633, 2634, 2635, 2636, 2637, 2638, 2639, 2640, 2641, 2642, 2643, 2644, 2645, 2646, 2647, 2648, 2649, 2650, 2651, 2652, 2653, 2654, 2655, 2656, 2657, 2658, 2659, 2660, 2661, 2662, 2663, 2664, 2665, 2666, 2667, 2668, 2669, 2670, 2671, 2672, 2673, 2674, 2675, 2676, 2677, 2678, 2679, 2680, 2681, 2682, 2683, 2684, 2685, 2686, 2687, 2688, 2689, 2690, 2691, 2692, 2693, 2694, 2695, 2696, 2697, 2698, 2699, 2700, 2701, 2702, 2703, 2704, 2705, 2706, 2707, 2708, 2709, 2710, 2711, 2712, 2713, 2714, 2715, 2716, 2717, 2718, 2719, 2720, 2721, 2722, 2723, 2724, 2725, 2726, 2727, 2728, 2729, 2730, 2731, 2732, 2733, 2734, 2735, 2736, 2737, 2738, 2739, 2740, 2741, 2742, 2743, 2744, 2745, 2746, 2747, 2748, 2749, 2750, 2751, 2752, 2753, 2754, 2755, 2756, 2757, 2758, 2759, 2760, 2761, 2762, 2763, 2764, 2765, 2766, 2767, 2768, 2769, 2770, 2771, 2772, 2773, 2774, 2775, 2776, 2777, 2778, 2779, 2780, 2781, 2782, 2783, 2784, 2785, 2786, 2787, 2788, 2789, 2790, 2791, 2792, 2793, 2794, 2795, 2796, 2797, 2798, 2799, 2800, 2801, 2802, 2803, 2804, 2805, 2806, 2807, 2808, 2809, 2810, 2811, 2812, 2813, 2814, 2815, 2816, 2817, 2818, 2819, 2820, 2821, 2822, 2823, 2824, 2825, 2826, 2827, 2828, 2829, 2830, 2831, 2832, 2833, 2834, 2835, 2836, 2837, 2838, 2839, 2840, 2841, 2842, 2843, 2844, 2845, 2846, 2847, 2848, 2849, 2850, 2851, 2852, 2853, 2854, 2855, 2856, 2857, 2858, 2859, 2860, 2861, 2862, 2863, 2864, 2865, 2866, 2867, 2868, 2869, 2870, 2871, 2872, 2873, 2874, 2875, 2876, 2877, 2878, 2879, 2880, 2881, 2882, 2883, 2884, 2885, 2886, 2887, 2888, 2889, 2890, 2891, 2892, 2893, 2894, 2895, 2896, 2897, 2898, 2899, 2900, 2901, 2902, 2903, 2904, 2905, 2906, 2907, 2908, 2909, 2910, 2911, 2912, 2913, 2914, 2915, 2916, 2917, 2918, 2919, 2920, 2921, 2922, 2923, 2924, 2925, 2926, 2927, 2928, 2929, 2930, 2931, 2932, 2933, 2934, 2935, 2936, 2937, 2938, 2939, 2940, 2941, 2942, 2943, 2944, 2945, 2946, 2947, 2948, 2949, 2950, 2951, 2952, 2953, 2954, 2955, 2956, 2957, 2958, 2959, 2960, 2961, 2962, 2963, 2964, 2965, 2966, 2967, 2968, 2969, 2970, 2971, 2972, 2973, 2974, 2975, 2976, 2977, 2978, 2979, 2980, 2981, 2982, 2983, 2984, 2985, 2986, 2987, 2988, 2989, 2990, 2991, 2992, 2993, 2994, 2995, 2996, 2997, 2998, 2999, 3000, 3001, 3002, 3003, 3004, 3005, 3006, 3007, 3008, 3009, 3010, 3011, 3012, 3013, 3014, 3015, 3016, 3017, 3018, 3019, 3020, 3021, 3022, 3023, 3024, 3025, 3026, 3027, 3028, 3029, 3030, 3031, 3032, 3033, 3034, 3035, 3036, 3037, 3038, 3039, 3040, 3041, 3042, 3043, 3044, 3045, 3046, 3047, 3048, 3049, 3050, 3051, 3052, 3053, 3054, 3055, 3056, 3057, 3058, 3059, 3060, 3061, 3062, 3063, 3064, 3065, 3066, 3067, 3068, 3069, 3070, 3071] Linear(in_features=3072, out_features=576, bias=True)

idxs =  768
prunable_chs =  3072
pruner.step 
--------------------------------
          Pruning Group
--------------------------------
[0] prune_out_channels on backbone.stages.3.blocks.1.ffn.layers.0.0 (Linear(in_features=576, out_features=3072, bias=True)) => prune_out_channels on backbone.stages.3.blocks.1.ffn.layers.0.0 (Linear(in_features=576, out_features=3072, bias=True)), #idxs=768
[1] prune_out_channels on backbone.stages.3.blocks.1.ffn.layers.0.0 (Linear(in_features=576, out_features=3072, bias=True)) => prune_out_channels on _ElementWiseOp_666(GeluBackward0), #idxs=768
[2] prune_out_channels on _ElementWiseOp_666(GeluBackward0) => prune_out_channels on _Reshape_664(), #idxs=768
[3] prune_out_channels on _Reshape_664() => prune_out_channels on _ElementWiseOp_663(AddmmBackward0), #idxs=768
[4] prune_out_channels on _ElementWiseOp_663(AddmmBackward0) => prune_out_channels on _ElementWiseOp_665(TBackward0), #idxs=768
[5] prune_out_channels on _ElementWiseOp_663(AddmmBackward0) => prune_in_channels on backbone.stages.3.blocks.1.ffn.layers.1 (Linear(in_features=3072, out_features=576, bias=True)), #idxs=768
--------------------------------

<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
continue 4
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
	get_all_groups -- Adding layer:  <class 'mmcv.cnn.bricks.wrappers.Linear'>
		prune_local()/  _check_pruning_ratio OK
Conv/Linear Output GroupNormImportance() / layer Type =  <class 'mmcv.cnn.bricks.wrappers.Linear'>
local_imp len =  3072 torch.Size([3072])
i =  5
<class 'mmcv.cnn.bricks.wrappers.Linear'> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501, 1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509, 1510, 1511, 1512, 1513, 1514, 1515, 1516, 1517, 1518, 1519, 1520, 1521, 1522, 1523, 1524, 1525, 1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533, 1534, 1535, 1536, 1537, 1538, 1539, 1540, 1541, 1542, 1543, 1544, 1545, 1546, 1547, 1548, 1549, 1550, 1551, 1552, 1553, 1554, 1555, 1556, 1557, 1558, 1559, 1560, 1561, 1562, 1563, 1564, 1565, 1566, 1567, 1568, 1569, 1570, 1571, 1572, 1573, 1574, 1575, 1576, 1577, 1578, 1579, 1580, 1581, 1582, 1583, 1584, 1585, 1586, 1587, 1588, 1589, 1590, 1591, 1592, 1593, 1594, 1595, 1596, 1597, 1598, 1599, 1600, 1601, 1602, 1603, 1604, 1605, 1606, 1607, 1608, 1609, 1610, 1611, 1612, 1613, 1614, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623, 1624, 1625, 1626, 1627, 1628, 1629, 1630, 1631, 1632, 1633, 1634, 1635, 1636, 1637, 1638, 1639, 1640, 1641, 1642, 1643, 1644, 1645, 1646, 1647, 1648, 1649, 1650, 1651, 1652, 1653, 1654, 1655, 1656, 1657, 1658, 1659, 1660, 1661, 1662, 1663, 1664, 1665, 1666, 1667, 1668, 1669, 1670, 1671, 1672, 1673, 1674, 1675, 1676, 1677, 1678, 1679, 1680, 1681, 1682, 1683, 1684, 1685, 1686, 1687, 1688, 1689, 1690, 1691, 1692, 1693, 1694, 1695, 1696, 1697, 1698, 1699, 1700, 1701, 1702, 1703, 1704, 1705, 1706, 1707, 1708, 1709, 1710, 1711, 1712, 1713, 1714, 1715, 1716, 1717, 1718, 1719, 1720, 1721, 1722, 1723, 1724, 1725, 1726, 1727, 1728, 1729, 1730, 1731, 1732, 1733, 1734, 1735, 1736, 1737, 1738, 1739, 1740, 1741, 1742, 1743, 1744, 1745, 1746, 1747, 1748, 1749, 1750, 1751, 1752, 1753, 1754, 1755, 1756, 1757, 1758, 1759, 1760, 1761, 1762, 1763, 1764, 1765, 1766, 1767, 1768, 1769, 1770, 1771, 1772, 1773, 1774, 1775, 1776, 1777, 1778, 1779, 1780, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789, 1790, 1791, 1792, 1793, 1794, 1795, 1796, 1797, 1798, 1799, 1800, 1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1809, 1810, 1811, 1812, 1813, 1814, 1815, 1816, 1817, 1818, 1819, 1820, 1821, 1822, 1823, 1824, 1825, 1826, 1827, 1828, 1829, 1830, 1831, 1832, 1833, 1834, 1835, 1836, 1837, 1838, 1839, 1840, 1841, 1842, 1843, 1844, 1845, 1846, 1847, 1848, 1849, 1850, 1851, 1852, 1853, 1854, 1855, 1856, 1857, 1858, 1859, 1860, 1861, 1862, 1863, 1864, 1865, 1866, 1867, 1868, 1869, 1870, 1871, 1872, 1873, 1874, 1875, 1876, 1877, 1878, 1879, 1880, 1881, 1882, 1883, 1884, 1885, 1886, 1887, 1888, 1889, 1890, 1891, 1892, 1893, 1894, 1895, 1896, 1897, 1898, 1899, 1900, 1901, 1902, 1903, 1904, 1905, 1906, 1907, 1908, 1909, 1910, 1911, 1912, 1913, 1914, 1915, 1916, 1917, 1918, 1919, 1920, 1921, 1922, 1923, 1924, 1925, 1926, 1927, 1928, 1929, 1930, 1931, 1932, 1933, 1934, 1935, 1936, 1937, 1938, 1939, 1940, 1941, 1942, 1943, 1944, 1945, 1946, 1947, 1948, 1949, 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031, 2032, 2033, 2034, 2035, 2036, 2037, 2038, 2039, 2040, 2041, 2042, 2043, 2044, 2045, 2046, 2047, 2048, 2049, 2050, 2051, 2052, 2053, 2054, 2055, 2056, 2057, 2058, 2059, 2060, 2061, 2062, 2063, 2064, 2065, 2066, 2067, 2068, 2069, 2070, 2071, 2072, 2073, 2074, 2075, 2076, 2077, 2078, 2079, 2080, 2081, 2082, 2083, 2084, 2085, 2086, 2087, 2088, 2089, 2090, 2091, 2092, 2093, 2094, 2095, 2096, 2097, 2098, 2099, 2100, 2101, 2102, 2103, 2104, 2105, 2106, 2107, 2108, 2109, 2110, 2111, 2112, 2113, 2114, 2115, 2116, 2117, 2118, 2119, 2120, 2121, 2122, 2123, 2124, 2125, 2126, 2127, 2128, 2129, 2130, 2131, 2132, 2133, 2134, 2135, 2136, 2137, 2138, 2139, 2140, 2141, 2142, 2143, 2144, 2145, 2146, 2147, 2148, 2149, 2150, 2151, 2152, 2153, 2154, 2155, 2156, 2157, 2158, 2159, 2160, 2161, 2162, 2163, 2164, 2165, 2166, 2167, 2168, 2169, 2170, 2171, 2172, 2173, 2174, 2175, 2176, 2177, 2178, 2179, 2180, 2181, 2182, 2183, 2184, 2185, 2186, 2187, 2188, 2189, 2190, 2191, 2192, 2193, 2194, 2195, 2196, 2197, 2198, 2199, 2200, 2201, 2202, 2203, 2204, 2205, 2206, 2207, 2208, 2209, 2210, 2211, 2212, 2213, 2214, 2215, 2216, 2217, 2218, 2219, 2220, 2221, 2222, 2223, 2224, 2225, 2226, 2227, 2228, 2229, 2230, 2231, 2232, 2233, 2234, 2235, 2236, 2237, 2238, 2239, 2240, 2241, 2242, 2243, 2244, 2245, 2246, 2247, 2248, 2249, 2250, 2251, 2252, 2253, 2254, 2255, 2256, 2257, 2258, 2259, 2260, 2261, 2262, 2263, 2264, 2265, 2266, 2267, 2268, 2269, 2270, 2271, 2272, 2273, 2274, 2275, 2276, 2277, 2278, 2279, 2280, 2281, 2282, 2283, 2284, 2285, 2286, 2287, 2288, 2289, 2290, 2291, 2292, 2293, 2294, 2295, 2296, 2297, 2298, 2299, 2300, 2301, 2302, 2303, 2304, 2305, 2306, 2307, 2308, 2309, 2310, 2311, 2312, 2313, 2314, 2315, 2316, 2317, 2318, 2319, 2320, 2321, 2322, 2323, 2324, 2325, 2326, 2327, 2328, 2329, 2330, 2331, 2332, 2333, 2334, 2335, 2336, 2337, 2338, 2339, 2340, 2341, 2342, 2343, 2344, 2345, 2346, 2347, 2348, 2349, 2350, 2351, 2352, 2353, 2354, 2355, 2356, 2357, 2358, 2359, 2360, 2361, 2362, 2363, 2364, 2365, 2366, 2367, 2368, 2369, 2370, 2371, 2372, 2373, 2374, 2375, 2376, 2377, 2378, 2379, 2380, 2381, 2382, 2383, 2384, 2385, 2386, 2387, 2388, 2389, 2390, 2391, 2392, 2393, 2394, 2395, 2396, 2397, 2398, 2399, 2400, 2401, 2402, 2403, 2404, 2405, 2406, 2407, 2408, 2409, 2410, 2411, 2412, 2413, 2414, 2415, 2416, 2417, 2418, 2419, 2420, 2421, 2422, 2423, 2424, 2425, 2426, 2427, 2428, 2429, 2430, 2431, 2432, 2433, 2434, 2435, 2436, 2437, 2438, 2439, 2440, 2441, 2442, 2443, 2444, 2445, 2446, 2447, 2448, 2449, 2450, 2451, 2452, 2453, 2454, 2455, 2456, 2457, 2458, 2459, 2460, 2461, 2462, 2463, 2464, 2465, 2466, 2467, 2468, 2469, 2470, 2471, 2472, 2473, 2474, 2475, 2476, 2477, 2478, 2479, 2480, 2481, 2482, 2483, 2484, 2485, 2486, 2487, 2488, 2489, 2490, 2491, 2492, 2493, 2494, 2495, 2496, 2497, 2498, 2499, 2500, 2501, 2502, 2503, 2504, 2505, 2506, 2507, 2508, 2509, 2510, 2511, 2512, 2513, 2514, 2515, 2516, 2517, 2518, 2519, 2520, 2521, 2522, 2523, 2524, 2525, 2526, 2527, 2528, 2529, 2530, 2531, 2532, 2533, 2534, 2535, 2536, 2537, 2538, 2539, 2540, 2541, 2542, 2543, 2544, 2545, 2546, 2547, 2548, 2549, 2550, 2551, 2552, 2553, 2554, 2555, 2556, 2557, 2558, 2559, 2560, 2561, 2562, 2563, 2564, 2565, 2566, 2567, 2568, 2569, 2570, 2571, 2572, 2573, 2574, 2575, 2576, 2577, 2578, 2579, 2580, 2581, 2582, 2583, 2584, 2585, 2586, 2587, 2588, 2589, 2590, 2591, 2592, 2593, 2594, 2595, 2596, 2597, 2598, 2599, 2600, 2601, 2602, 2603, 2604, 2605, 2606, 2607, 2608, 2609, 2610, 2611, 2612, 2613, 2614, 2615, 2616, 2617, 2618, 2619, 2620, 2621, 2622, 2623, 2624, 2625, 2626, 2627, 2628, 2629, 2630, 2631, 2632, 2633, 2634, 2635, 2636, 2637, 2638, 2639, 2640, 2641, 2642, 2643, 2644, 2645, 2646, 2647, 2648, 2649, 2650, 2651, 2652, 2653, 2654, 2655, 2656, 2657, 2658, 2659, 2660, 2661, 2662, 2663, 2664, 2665, 2666, 2667, 2668, 2669, 2670, 2671, 2672, 2673, 2674, 2675, 2676, 2677, 2678, 2679, 2680, 2681, 2682, 2683, 2684, 2685, 2686, 2687, 2688, 2689, 2690, 2691, 2692, 2693, 2694, 2695, 2696, 2697, 2698, 2699, 2700, 2701, 2702, 2703, 2704, 2705, 2706, 2707, 2708, 2709, 2710, 2711, 2712, 2713, 2714, 2715, 2716, 2717, 2718, 2719, 2720, 2721, 2722, 2723, 2724, 2725, 2726, 2727, 2728, 2729, 2730, 2731, 2732, 2733, 2734, 2735, 2736, 2737, 2738, 2739, 2740, 2741, 2742, 2743, 2744, 2745, 2746, 2747, 2748, 2749, 2750, 2751, 2752, 2753, 2754, 2755, 2756, 2757, 2758, 2759, 2760, 2761, 2762, 2763, 2764, 2765, 2766, 2767, 2768, 2769, 2770, 2771, 2772, 2773, 2774, 2775, 2776, 2777, 2778, 2779, 2780, 2781, 2782, 2783, 2784, 2785, 2786, 2787, 2788, 2789, 2790, 2791, 2792, 2793, 2794, 2795, 2796, 2797, 2798, 2799, 2800, 2801, 2802, 2803, 2804, 2805, 2806, 2807, 2808, 2809, 2810, 2811, 2812, 2813, 2814, 2815, 2816, 2817, 2818, 2819, 2820, 2821, 2822, 2823, 2824, 2825, 2826, 2827, 2828, 2829, 2830, 2831, 2832, 2833, 2834, 2835, 2836, 2837, 2838, 2839, 2840, 2841, 2842, 2843, 2844, 2845, 2846, 2847, 2848, 2849, 2850, 2851, 2852, 2853, 2854, 2855, 2856, 2857, 2858, 2859, 2860, 2861, 2862, 2863, 2864, 2865, 2866, 2867, 2868, 2869, 2870, 2871, 2872, 2873, 2874, 2875, 2876, 2877, 2878, 2879, 2880, 2881, 2882, 2883, 2884, 2885, 2886, 2887, 2888, 2889, 2890, 2891, 2892, 2893, 2894, 2895, 2896, 2897, 2898, 2899, 2900, 2901, 2902, 2903, 2904, 2905, 2906, 2907, 2908, 2909, 2910, 2911, 2912, 2913, 2914, 2915, 2916, 2917, 2918, 2919, 2920, 2921, 2922, 2923, 2924, 2925, 2926, 2927, 2928, 2929, 2930, 2931, 2932, 2933, 2934, 2935, 2936, 2937, 2938, 2939, 2940, 2941, 2942, 2943, 2944, 2945, 2946, 2947, 2948, 2949, 2950, 2951, 2952, 2953, 2954, 2955, 2956, 2957, 2958, 2959, 2960, 2961, 2962, 2963, 2964, 2965, 2966, 2967, 2968, 2969, 2970, 2971, 2972, 2973, 2974, 2975, 2976, 2977, 2978, 2979, 2980, 2981, 2982, 2983, 2984, 2985, 2986, 2987, 2988, 2989, 2990, 2991, 2992, 2993, 2994, 2995, 2996, 2997, 2998, 2999, 3000, 3001, 3002, 3003, 3004, 3005, 3006, 3007, 3008, 3009, 3010, 3011, 3012, 3013, 3014, 3015, 3016, 3017, 3018, 3019, 3020, 3021, 3022, 3023, 3024, 3025, 3026, 3027, 3028, 3029, 3030, 3031, 3032, 3033, 3034, 3035, 3036, 3037, 3038, 3039, 3040, 3041, 3042, 3043, 3044, 3045, 3046, 3047, 3048, 3049, 3050, 3051, 3052, 3053, 3054, 3055, 3056, 3057, 3058, 3059, 3060, 3061, 3062, 3063, 3064, 3065, 3066, 3067, 3068, 3069, 3070, 3071] Linear(in_features=3072, out_features=576, bias=True)

idxs =  768
prunable_chs =  3072
pruner.step 
--------------------------------
          Pruning Group
--------------------------------
[0] prune_out_channels on backbone.stages.3.blocks.0.ffn.layers.0.0 (Linear(in_features=576, out_features=3072, bias=True)) => prune_out_channels on backbone.stages.3.blocks.0.ffn.layers.0.0 (Linear(in_features=576, out_features=3072, bias=True)), #idxs=768
[1] prune_out_channels on backbone.stages.3.blocks.0.ffn.layers.0.0 (Linear(in_features=576, out_features=3072, bias=True)) => prune_out_channels on _ElementWiseOp_676(GeluBackward0), #idxs=768
[2] prune_out_channels on _ElementWiseOp_676(GeluBackward0) => prune_out_channels on _Reshape_674(), #idxs=768
[3] prune_out_channels on _Reshape_674() => prune_out_channels on _ElementWiseOp_673(AddmmBackward0), #idxs=768
[4] prune_out_channels on _ElementWiseOp_673(AddmmBackward0) => prune_out_channels on _ElementWiseOp_675(TBackward0), #idxs=768
[5] prune_out_channels on _ElementWiseOp_673(AddmmBackward0) => prune_in_channels on backbone.stages.3.blocks.0.ffn.layers.1 (Linear(in_features=3072, out_features=576, bias=True)), #idxs=768
--------------------------------

<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
continue 4
<class 'torch_pruning.ops._ReshapeOp'> continue 2
PatchMergingPruner () get_out_channels/  576 <class 'mmdet.models.layers.transformer.utils.PatchMerging'>
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
WindowMSAPruner layer.embed_dims =  576
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
WindowMSAPruner layer.embed_dims =  576
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
continue 4
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ReshapeOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch_pruning.ops._ElementWiseOp'> continue 2
<class 'torch.nn.modules.normalization.GroupNorm'> continue 2
	get_all_groups -- Adding layer:  <class 'torch.nn.modules.conv.Conv2d'>
		prune_local()/  _check_pruning_ratio OK
Conv/Linear Output GroupNormImportance() / layer Type =  <class 'torch.nn.modules.conv.Conv2d'>
idxs =  64
prunable_chs =  256
idxs =  64
prunable_chs =  256
pruner.step 
--------------------------------
          Pruning Group
--------------------------------
[0] prune_out_channels on neck.extra_convs.0.conv (Conv2d(576, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))) => prune_out_channels on neck.extra_convs.0.conv (Conv2d(576, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))), #idxs=64
[1] prune_out_channels on neck.extra_convs.0.conv (Conv2d(576, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))) => prune_out_channels on neck.extra_convs.0.gn (GroupNorm(32, 256, eps=1e-05, affine=True)), #idxs=64
--------------------------------

Sequential(
  (backbone): SwinTransformer(
    (patch_embed): PatchEmbed(
      (adap_padding): AdaptivePadding()
      (projection): Conv2d(3, 72, kernel_size=(4, 4), stride=(4, 4))
      (norm): LayerNorm((72,), eps=1e-05, elementwise_affine=True)
    )
    (drop_after_pos): Dropout(p=0.0, inplace=False)
    (stages): ModuleList(
      (0): SwinBlockSequence(
        (blocks): ModuleList(
          (0-1): 2 x SwinBlock(
            (norm1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=72, out_features=192, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=72, out_features=72, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((72,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=72, out_features=288, bias=True)
                  (1): GELU(approximate='none')
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=288, out_features=72, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
              (gamma2): Identity()
            )
          )
        )
        (downsample): PatchMerging(
          (adap_padding): AdaptivePadding()
          (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
          (norm): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
          (reduction): Linear(in_features=288, out_features=144, bias=False)
        )
      )
      (1): SwinBlockSequence(
        (blocks): ModuleList(
          (0-1): 2 x SwinBlock(
            (norm1): LayerNorm((144,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=144, out_features=384, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=144, out_features=144, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((144,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=144, out_features=576, bias=True)
                  (1): GELU(approximate='none')
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=576, out_features=144, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
              (gamma2): Identity()
            )
          )
        )
        (downsample): PatchMerging(
          (adap_padding): AdaptivePadding()
          (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
          (norm): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
          (reduction): Linear(in_features=576, out_features=288, bias=False)
        )
      )
      (2): SwinBlockSequence(
        (blocks): ModuleList(
          (0-5): 6 x SwinBlock(
            (norm1): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=288, out_features=768, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=288, out_features=288, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=288, out_features=1152, bias=True)
                  (1): GELU(approximate='none')
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=1152, out_features=288, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
              (gamma2): Identity()
            )
          )
        )
        (downsample): PatchMerging(
          (adap_padding): AdaptivePadding()
          (sampler): Unfold(kernel_size=(2, 2), dilation=(1, 1), padding=(0, 0), stride=(2, 2))
          (norm): LayerNorm((1152,), eps=1e-05, elementwise_affine=True)
          (reduction): Linear(in_features=1152, out_features=576, bias=False)
        )
      )
      (3): SwinBlockSequence(
        (blocks): ModuleList(
          (0-1): 2 x SwinBlock(
            (norm1): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftWindowMSA(
              (w_msa): WindowMSA(
                (qkv): Linear(in_features=576, out_features=1536, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=576, out_features=576, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop): DropPath()
            )
            (norm2): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
            (ffn): FFN(
              (layers): Sequential(
                (0): Sequential(
                  (0): Linear(in_features=576, out_features=2304, bias=True)
                  (1): GELU(approximate='none')
                  (2): Dropout(p=0.0, inplace=False)
                )
                (1): Linear(in_features=2304, out_features=576, bias=True)
                (2): Dropout(p=0.0, inplace=False)
              )
              (dropout_layer): DropPath()
              (gamma2): Identity()
            )
          )
        )
      )
    )
    (norm1): LayerNorm((144,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((288,), eps=1e-05, elementwise_affine=True)
    (norm3): LayerNorm((576,), eps=1e-05, elementwise_affine=True)
  )
  (neck): ChannelMapper(
    (convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(144, 192, kernel_size=(1, 1), stride=(1, 1))
        (gn): GroupNorm(32, 192, eps=1e-05, affine=True)
      )
      (1): ConvModule(
        (conv): Conv2d(288, 192, kernel_size=(1, 1), stride=(1, 1))
        (gn): GroupNorm(32, 192, eps=1e-05, affine=True)
      )
      (2): ConvModule(
        (conv): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1))
        (gn): GroupNorm(32, 192, eps=1e-05, affine=True)
      )
    )
    (extra_convs): ModuleList(
      (0): ConvModule(
        (conv): Conv2d(576, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (gn): GroupNorm(32, 192, eps=1e-05, affine=True)
      )
    )
  )
  init_cfg={'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}
)
- prev m.qkv.out_features =  192
- prev m.qkv.in_features =  72
 - prev m.num_heads =  3
 - prev m.embed_dims =  96
head_embed_dims =  32
0.1767766952966369 96
 - after m.embed_dims =  192

- prev m.qkv.out_features =  192
- prev m.qkv.in_features =  72
 - prev m.num_heads =  3
 - prev m.embed_dims =  96
head_embed_dims =  32
0.1767766952966369 96
 - after m.embed_dims =  192

- prev m.qkv.out_features =  384
- prev m.qkv.in_features =  144
 - prev m.num_heads =  6
 - prev m.embed_dims =  192
head_embed_dims =  32
0.1767766952966369 192
 - after m.embed_dims =  384

- prev m.qkv.out_features =  384
- prev m.qkv.in_features =  144
 - prev m.num_heads =  6
 - prev m.embed_dims =  192
head_embed_dims =  32
0.1767766952966369 192
 - after m.embed_dims =  384

- prev m.qkv.out_features =  768
- prev m.qkv.in_features =  288
 - prev m.num_heads =  12
 - prev m.embed_dims =  384
head_embed_dims =  32
0.1767766952966369 384
 - after m.embed_dims =  768

- prev m.qkv.out_features =  768
- prev m.qkv.in_features =  288
 - prev m.num_heads =  12
 - prev m.embed_dims =  384
head_embed_dims =  32
0.1767766952966369 384
 - after m.embed_dims =  768

- prev m.qkv.out_features =  768
- prev m.qkv.in_features =  288
 - prev m.num_heads =  12
 - prev m.embed_dims =  384
head_embed_dims =  32
0.1767766952966369 384
 - after m.embed_dims =  768

- prev m.qkv.out_features =  768
- prev m.qkv.in_features =  288
 - prev m.num_heads =  12
 - prev m.embed_dims =  384
head_embed_dims =  32
0.1767766952966369 384
 - after m.embed_dims =  768

- prev m.qkv.out_features =  768
- prev m.qkv.in_features =  288
 - prev m.num_heads =  12
 - prev m.embed_dims =  384
head_embed_dims =  32
0.1767766952966369 384
 - after m.embed_dims =  768

- prev m.qkv.out_features =  768
- prev m.qkv.in_features =  288
 - prev m.num_heads =  12
 - prev m.embed_dims =  384
head_embed_dims =  32
0.1767766952966369 384
 - after m.embed_dims =  768

- prev m.qkv.out_features =  1536
- prev m.qkv.in_features =  576
 - prev m.num_heads =  24
 - prev m.embed_dims =  768
head_embed_dims =  32
0.1767766952966369 768
 - after m.embed_dims =  1536

- prev m.qkv.out_features =  1536
- prev m.qkv.in_features =  576
 - prev m.num_heads =  24
 - prev m.embed_dims =  768
head_embed_dims =  32
0.1767766952966369 768
 - after m.embed_dims =  1536

0.1767766952966369 192
0.1767766952966369 192
0.1767766952966369 384
0.1767766952966369 384
0.1767766952966369 768
0.1767766952966369 768
0.1767766952966369 768
0.1767766952966369 768
0.1767766952966369 768
0.1767766952966369 768
0.1767766952966369 1536
0.1767766952966369 1536
-----NEW SwinTransformer FORWARD PASS 



		SwinTransformer input shape:  torch.Size([1, 3, 800, 1333])


------------SwinBlockSequence -  torch.Size([1, 66800, 72])
with_cp =  False
x.requires_grad =  True
ShiftWindowMSA = query view: B, L, C=  1 66800 72
	 hw_shape =  (200, 334)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([1392, 49, 72])
B=1392, N=49, C=72
the qkv mod: 72 192 Linear(in_features=72, out_features=192, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([1392, 49, 216])
NEW C= 72
qkv.shape =  torch.Size([1392, 49, 216])
qkv.flatten.shape =  torch.Size([14732928])
self.embed_dims =  192
+++++ reshaping ....  1392 49 3 self.num_heads= 3 24
1392 49 3 3 24  =  14732928
self.num_heads =  3
C =  72
qkv after reshape  torch.Size([1392, 49, 3, 3, 24])
qkv after permute  torch.Size([3, 1392, 3, 49, 24])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f1094964670>
--proj input shape =  torch.Size([1392, 49, 72])
--proj output shape =  torch.Size([1392, 49, 72])
------------WindowMSA - x_shape Output torch.Size([1392, 49, 72]) <UnsafeViewBackward0 object at 0x7f10949647f0>
attn_windows.shape before merge =  torch.Size([1392, 49, 72])
x.shape before view =  torch.Size([1, 200, 334, 72])
ShiftWindowMSA = before view B, H * W, C =  1 66800 72
------------ShiftWindowMSA -  <ViewBackward0 object at 0x7f10949647f0>
------------SwinBlock -  <AddBackward0 object at 0x7f1094964c70>
		block output:  torch.Size([1, 66800, 72])
------------SwinBlockSequence -  torch.Size([1, 66800, 72])
with_cp =  False
x.requires_grad =  True
ShiftWindowMSA = query view: B, L, C=  1 66800 72
	 hw_shape =  (200, 334)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([1392, 49, 72])
B=1392, N=49, C=72
the qkv mod: 72 192 Linear(in_features=72, out_features=192, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([1392, 49, 216])
NEW C= 72
qkv.shape =  torch.Size([1392, 49, 216])
qkv.flatten.shape =  torch.Size([14732928])
self.embed_dims =  192
+++++ reshaping ....  1392 49 3 self.num_heads= 3 24
1392 49 3 3 24  =  14732928
self.num_heads =  3
C =  72
qkv after reshape  torch.Size([1392, 49, 3, 3, 24])
qkv after permute  torch.Size([3, 1392, 3, 49, 24])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f1094964c70>
--proj input shape =  torch.Size([1392, 49, 72])
--proj output shape =  torch.Size([1392, 49, 72])
------------WindowMSA - x_shape Output torch.Size([1392, 49, 72]) <UnsafeViewBackward0 object at 0x7f1094964c70>
attn_windows.shape before merge =  torch.Size([1392, 49, 72])
x.shape before view =  torch.Size([1, 200, 334, 72])
ShiftWindowMSA = before view B, H * W, C =  1 66800 72
------------ShiftWindowMSA -  <ViewBackward0 object at 0x7f1094964c70>
------------SwinBlock -  <AddBackward0 object at 0x7f1094964c70>
		block output:  torch.Size([1, 66800, 72])
------------SwinBlockSequence/downsample -  torch.Size([1, 16700, 144]) <UnsafeViewBackward0 object at 0x7f1094964dc0>
down_hw_shape =  (100, 167)
x.shape =  torch.Size([1, 16700, 144])
hw_shape =  (100, 167)
stage out.shape =  torch.Size([1, 66800, 72])
out_hw_shape =  (200, 334)
stage =  0  / self.out_indices =  (1, 2, 3)
self.num_features =  [72, 144, 288, 576]
------------SwinBlockSequence -  torch.Size([1, 16700, 144])
with_cp =  False
x.requires_grad =  True
ShiftWindowMSA = query view: B, L, C=  1 16700 144
	 hw_shape =  (100, 167)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([360, 49, 144])
B=360, N=49, C=144
the qkv mod: 144 384 Linear(in_features=144, out_features=384, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([360, 49, 432])
NEW C= 144
qkv.shape =  torch.Size([360, 49, 432])
qkv.flatten.shape =  torch.Size([7620480])
self.embed_dims =  384
+++++ reshaping ....  360 49 3 self.num_heads= 6 24
360 49 3 6 24  =  7620480
self.num_heads =  6
C =  144
qkv after reshape  torch.Size([360, 49, 3, 6, 24])
qkv after permute  torch.Size([3, 360, 6, 49, 24])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f1094964c70>
--proj input shape =  torch.Size([360, 49, 144])
--proj output shape =  torch.Size([360, 49, 144])
------------WindowMSA - x_shape Output torch.Size([360, 49, 144]) <UnsafeViewBackward0 object at 0x7f1094964c70>
attn_windows.shape before merge =  torch.Size([360, 49, 144])
x.shape before view =  torch.Size([1, 100, 167, 144])
ShiftWindowMSA = before view B, H * W, C =  1 16700 144
------------ShiftWindowMSA -  <ViewBackward0 object at 0x7f1094964c70>
------------SwinBlock -  <AddBackward0 object at 0x7f1094964c70>
		block output:  torch.Size([1, 16700, 144])
------------SwinBlockSequence -  torch.Size([1, 16700, 144])
with_cp =  False
x.requires_grad =  True
ShiftWindowMSA = query view: B, L, C=  1 16700 144
	 hw_shape =  (100, 167)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([360, 49, 144])
B=360, N=49, C=144
the qkv mod: 144 384 Linear(in_features=144, out_features=384, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([360, 49, 432])
NEW C= 144
qkv.shape =  torch.Size([360, 49, 432])
qkv.flatten.shape =  torch.Size([7620480])
self.embed_dims =  384
+++++ reshaping ....  360 49 3 self.num_heads= 6 24
360 49 3 6 24  =  7620480
self.num_heads =  6
C =  144
qkv after reshape  torch.Size([360, 49, 3, 6, 24])
qkv after permute  torch.Size([3, 360, 6, 49, 24])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f1094964c70>
--proj input shape =  torch.Size([360, 49, 144])
--proj output shape =  torch.Size([360, 49, 144])
------------WindowMSA - x_shape Output torch.Size([360, 49, 144]) <UnsafeViewBackward0 object at 0x7f1094964c70>
attn_windows.shape before merge =  torch.Size([360, 49, 144])
x.shape before view =  torch.Size([1, 100, 167, 144])
ShiftWindowMSA = before view B, H * W, C =  1 16700 144
------------ShiftWindowMSA -  <ViewBackward0 object at 0x7f1094964c70>
------------SwinBlock -  <AddBackward0 object at 0x7f1094964c70>
		block output:  torch.Size([1, 16700, 144])
------------SwinBlockSequence/downsample -  torch.Size([1, 4200, 288]) <UnsafeViewBackward0 object at 0x7f1094964dc0>
down_hw_shape =  (50, 84)
x.shape =  torch.Size([1, 4200, 288])
hw_shape =  (50, 84)
stage out.shape =  torch.Size([1, 16700, 144])
out_hw_shape =  (100, 167)
stage =  1  / self.out_indices =  (1, 2, 3)
self.num_features =  [72, 144, 288, 576]
stage:  1  --- norm_layer out.shape =  torch.Size([1, 16700, 144])
torch.Size([1, 144, 100, 167])
------------SwinBlockSequence -  torch.Size([1, 4200, 288])
with_cp =  False
x.requires_grad =  True
ShiftWindowMSA = query view: B, L, C=  1 4200 288
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 288])
B=96, N=49, C=288
the qkv mod: 288 768 Linear(in_features=288, out_features=768, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 864])
NEW C= 288
qkv.shape =  torch.Size([96, 49, 864])
qkv.flatten.shape =  torch.Size([4064256])
self.embed_dims =  768
+++++ reshaping ....  96 49 3 self.num_heads= 12 24
96 49 3 12 24  =  4064256
self.num_heads =  12
C =  288
qkv after reshape  torch.Size([96, 49, 3, 12, 24])
qkv after permute  torch.Size([3, 96, 12, 49, 24])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f1094964c70>
--proj input shape =  torch.Size([96, 49, 288])
--proj output shape =  torch.Size([96, 49, 288])
------------WindowMSA - x_shape Output torch.Size([96, 49, 288]) <UnsafeViewBackward0 object at 0x7f1094964c70>
attn_windows.shape before merge =  torch.Size([96, 49, 288])
x.shape before view =  torch.Size([1, 50, 84, 288])
ShiftWindowMSA = before view B, H * W, C =  1 4200 288
------------ShiftWindowMSA -  <ViewBackward0 object at 0x7f1094964c70>
------------SwinBlock -  <AddBackward0 object at 0x7f1094964c70>
		block output:  torch.Size([1, 4200, 288])
------------SwinBlockSequence -  torch.Size([1, 4200, 288])
with_cp =  False
x.requires_grad =  True
ShiftWindowMSA = query view: B, L, C=  1 4200 288
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 288])
B=96, N=49, C=288
the qkv mod: 288 768 Linear(in_features=288, out_features=768, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 864])
NEW C= 288
qkv.shape =  torch.Size([96, 49, 864])
qkv.flatten.shape =  torch.Size([4064256])
self.embed_dims =  768
+++++ reshaping ....  96 49 3 self.num_heads= 12 24
96 49 3 12 24  =  4064256
self.num_heads =  12
C =  288
qkv after reshape  torch.Size([96, 49, 3, 12, 24])
qkv after permute  torch.Size([3, 96, 12, 49, 24])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f1094964c70>
--proj input shape =  torch.Size([96, 49, 288])
--proj output shape =  torch.Size([96, 49, 288])
------------WindowMSA - x_shape Output torch.Size([96, 49, 288]) <UnsafeViewBackward0 object at 0x7f1094964c70>
attn_windows.shape before merge =  torch.Size([96, 49, 288])
x.shape before view =  torch.Size([1, 50, 84, 288])
ShiftWindowMSA = before view B, H * W, C =  1 4200 288
------------ShiftWindowMSA -  <ViewBackward0 object at 0x7f1094964c70>
------------SwinBlock -  <AddBackward0 object at 0x7f1094964c70>
		block output:  torch.Size([1, 4200, 288])
------------SwinBlockSequence -  torch.Size([1, 4200, 288])
with_cp =  False
x.requires_grad =  True
ShiftWindowMSA = query view: B, L, C=  1 4200 288
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 288])
B=96, N=49, C=288
the qkv mod: 288 768 Linear(in_features=288, out_features=768, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 864])
NEW C= 288
qkv.shape =  torch.Size([96, 49, 864])
qkv.flatten.shape =  torch.Size([4064256])
self.embed_dims =  768
+++++ reshaping ....  96 49 3 self.num_heads= 12 24
96 49 3 12 24  =  4064256
self.num_heads =  12
C =  288
qkv after reshape  torch.Size([96, 49, 3, 12, 24])
qkv after permute  torch.Size([3, 96, 12, 49, 24])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f1094964c70>
--proj input shape =  torch.Size([96, 49, 288])
--proj output shape =  torch.Size([96, 49, 288])
------------WindowMSA - x_shape Output torch.Size([96, 49, 288]) <UnsafeViewBackward0 object at 0x7f1094964c70>
attn_windows.shape before merge =  torch.Size([96, 49, 288])
x.shape before view =  torch.Size([1, 50, 84, 288])
ShiftWindowMSA = before view B, H * W, C =  1 4200 288
------------ShiftWindowMSA -  <ViewBackward0 object at 0x7f1094964c70>
------------SwinBlock -  <AddBackward0 object at 0x7f1094964c70>
		block output:  torch.Size([1, 4200, 288])
------------SwinBlockSequence -  torch.Size([1, 4200, 288])
with_cp =  False
x.requires_grad =  True
ShiftWindowMSA = query view: B, L, C=  1 4200 288
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 288])
B=96, N=49, C=288
the qkv mod: 288 768 Linear(in_features=288, out_features=768, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 864])
NEW C= 288
qkv.shape =  torch.Size([96, 49, 864])
qkv.flatten.shape =  torch.Size([4064256])
self.embed_dims =  768
+++++ reshaping ....  96 49 3 self.num_heads= 12 24
96 49 3 12 24  =  4064256
self.num_heads =  12
C =  288
qkv after reshape  torch.Size([96, 49, 3, 12, 24])
qkv after permute  torch.Size([3, 96, 12, 49, 24])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f1094964c70>
--proj input shape =  torch.Size([96, 49, 288])
--proj output shape =  torch.Size([96, 49, 288])
------------WindowMSA - x_shape Output torch.Size([96, 49, 288]) <UnsafeViewBackward0 object at 0x7f1094964c70>
attn_windows.shape before merge =  torch.Size([96, 49, 288])
x.shape before view =  torch.Size([1, 50, 84, 288])
ShiftWindowMSA = before view B, H * W, C =  1 4200 288
------------ShiftWindowMSA -  <ViewBackward0 object at 0x7f1094964c70>
------------SwinBlock -  <AddBackward0 object at 0x7f1094964c70>
		block output:  torch.Size([1, 4200, 288])
------------SwinBlockSequence -  torch.Size([1, 4200, 288])
with_cp =  False
x.requires_grad =  True
ShiftWindowMSA = query view: B, L, C=  1 4200 288
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 288])
B=96, N=49, C=288
the qkv mod: 288 768 Linear(in_features=288, out_features=768, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 864])
NEW C= 288
qkv.shape =  torch.Size([96, 49, 864])
qkv.flatten.shape =  torch.Size([4064256])
self.embed_dims =  768
+++++ reshaping ....  96 49 3 self.num_heads= 12 24
96 49 3 12 24  =  4064256
self.num_heads =  12
C =  288
qkv after reshape  torch.Size([96, 49, 3, 12, 24])
qkv after permute  torch.Size([3, 96, 12, 49, 24])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f1094964c70>
--proj input shape =  torch.Size([96, 49, 288])
--proj output shape =  torch.Size([96, 49, 288])
------------WindowMSA - x_shape Output torch.Size([96, 49, 288]) <UnsafeViewBackward0 object at 0x7f1094964c70>
attn_windows.shape before merge =  torch.Size([96, 49, 288])
x.shape before view =  torch.Size([1, 50, 84, 288])
ShiftWindowMSA = before view B, H * W, C =  1 4200 288
------------ShiftWindowMSA -  <ViewBackward0 object at 0x7f1094964c70>
------------SwinBlock -  <AddBackward0 object at 0x7f1094964c70>
		block output:  torch.Size([1, 4200, 288])
------------SwinBlockSequence -  torch.Size([1, 4200, 288])
with_cp =  False
x.requires_grad =  True
ShiftWindowMSA = query view: B, L, C=  1 4200 288
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 288])
B=96, N=49, C=288
the qkv mod: 288 768 Linear(in_features=288, out_features=768, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 864])
NEW C= 288
qkv.shape =  torch.Size([96, 49, 864])
qkv.flatten.shape =  torch.Size([4064256])
self.embed_dims =  768
+++++ reshaping ....  96 49 3 self.num_heads= 12 24
96 49 3 12 24  =  4064256
self.num_heads =  12
C =  288
qkv after reshape  torch.Size([96, 49, 3, 12, 24])
qkv after permute  torch.Size([3, 96, 12, 49, 24])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f1094964af0>
--proj input shape =  torch.Size([96, 49, 288])
--proj output shape =  torch.Size([96, 49, 288])
------------WindowMSA - x_shape Output torch.Size([96, 49, 288]) <UnsafeViewBackward0 object at 0x7f1094964af0>
attn_windows.shape before merge =  torch.Size([96, 49, 288])
x.shape before view =  torch.Size([1, 50, 84, 288])
ShiftWindowMSA = before view B, H * W, C =  1 4200 288
------------ShiftWindowMSA -  <ViewBackward0 object at 0x7f1094964af0>
------------SwinBlock -  <AddBackward0 object at 0x7f1094964af0>
		block output:  torch.Size([1, 4200, 288])
------------SwinBlockSequence/downsample -  torch.Size([1, 1050, 576]) <UnsafeViewBackward0 object at 0x7f1094964dc0>
down_hw_shape =  (25, 42)
x.shape =  torch.Size([1, 1050, 576])
hw_shape =  (25, 42)
stage out.shape =  torch.Size([1, 4200, 288])
out_hw_shape =  (50, 84)
stage =  2  / self.out_indices =  (1, 2, 3)
self.num_features =  [72, 144, 288, 576]
stage:  2  --- norm_layer out.shape =  torch.Size([1, 4200, 288])
torch.Size([1, 288, 50, 84])
------------SwinBlockSequence -  torch.Size([1, 1050, 576])
with_cp =  False
x.requires_grad =  True
ShiftWindowMSA = query view: B, L, C=  1 1050 576
	 hw_shape =  (25, 42)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([24, 49, 576])
B=24, N=49, C=576
the qkv mod: 576 1536 Linear(in_features=576, out_features=1536, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([24, 49, 1728])
NEW C= 576
qkv.shape =  torch.Size([24, 49, 1728])
qkv.flatten.shape =  torch.Size([2032128])
self.embed_dims =  1536
+++++ reshaping ....  24 49 3 self.num_heads= 24 24
24 49 3 24 24  =  2032128
self.num_heads =  24
C =  576
qkv after reshape  torch.Size([24, 49, 3, 24, 24])
qkv after permute  torch.Size([3, 24, 24, 49, 24])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f1094964af0>
--proj input shape =  torch.Size([24, 49, 576])
--proj output shape =  torch.Size([24, 49, 576])
------------WindowMSA - x_shape Output torch.Size([24, 49, 576]) <UnsafeViewBackward0 object at 0x7f1094964af0>
attn_windows.shape before merge =  torch.Size([24, 49, 576])
x.shape before view =  torch.Size([1, 25, 42, 576])
ShiftWindowMSA = before view B, H * W, C =  1 1050 576
------------ShiftWindowMSA -  <ViewBackward0 object at 0x7f1094964af0>
------------SwinBlock -  <AddBackward0 object at 0x7f1094964af0>
		block output:  torch.Size([1, 1050, 576])
------------SwinBlockSequence -  torch.Size([1, 1050, 576])
with_cp =  False
x.requires_grad =  True
ShiftWindowMSA = query view: B, L, C=  1 1050 576
	 hw_shape =  (25, 42)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([24, 49, 576])
B=24, N=49, C=576
the qkv mod: 576 1536 Linear(in_features=576, out_features=1536, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([24, 49, 1728])
NEW C= 576
qkv.shape =  torch.Size([24, 49, 1728])
qkv.flatten.shape =  torch.Size([2032128])
self.embed_dims =  1536
+++++ reshaping ....  24 49 3 self.num_heads= 24 24
24 49 3 24 24  =  2032128
self.num_heads =  24
C =  576
qkv after reshape  torch.Size([24, 49, 3, 24, 24])
qkv after permute  torch.Size([3, 24, 24, 49, 24])
		qkv.grad_fn >>>>>>  <PermuteBackward0 object at 0x7f1094964af0>
--proj input shape =  torch.Size([24, 49, 576])
--proj output shape =  torch.Size([24, 49, 576])
------------WindowMSA - x_shape Output torch.Size([24, 49, 576]) <UnsafeViewBackward0 object at 0x7f1094964af0>
attn_windows.shape before merge =  torch.Size([24, 49, 576])
x.shape before view =  torch.Size([1, 25, 42, 576])
ShiftWindowMSA = before view B, H * W, C =  1 1050 576
------------ShiftWindowMSA -  <ViewBackward0 object at 0x7f1094964af0>
------------SwinBlock -  <AddBackward0 object at 0x7f1094964af0>
		block output:  torch.Size([1, 1050, 576])
x.shape =  torch.Size([1, 1050, 576])
hw_shape =  (25, 42)
stage out.shape =  torch.Size([1, 1050, 576])
out_hw_shape =  (25, 42)
stage =  3  / self.out_indices =  (1, 2, 3)
self.num_features =  [72, 144, 288, 576]
stage:  3  --- norm_layer out.shape =  torch.Size([1, 1050, 576])
torch.Size([1, 576, 25, 42])
------------SwinTransformer -  [<CloneBackward0 object at 0x7f1094964b50>, <CloneBackward0 object at 0x7f1094964af0>, <CloneBackward0 object at 0x7f1094964c70>]



[torch.Size([1, 144, 100, 167]), torch.Size([1, 288, 50, 84]), torch.Size([1, 576, 25, 42])]
Pruned forward_time =  0.5332927703857422
Pruned fps =  1.875142614959281
>>>>>>>>>>>>>>>>_freeze_stages =  -1
-----NEW SwinTransformer FORWARD PASS 



		SwinTransformer input shape:  torch.Size([1, 3, 800, 1333])


------------SwinBlockSequence -  torch.Size([1, 66800, 72])
with_cp =  False
x.requires_grad =  False
ShiftWindowMSA = query view: B, L, C=  1 66800 72
	 hw_shape =  (200, 334)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([1392, 49, 72])
B=1392, N=49, C=72
the qkv mod: 72 192 Linear(in_features=72, out_features=192, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([1392, 49, 216])
NEW C= 72
qkv.shape =  torch.Size([1392, 49, 216])
qkv.flatten.shape =  torch.Size([14732928])
self.embed_dims =  192
+++++ reshaping ....  1392 49 3 self.num_heads= 3 24
1392 49 3 3 24  =  14732928
self.num_heads =  3
C =  72
qkv after reshape  torch.Size([1392, 49, 3, 3, 24])
qkv after permute  torch.Size([3, 1392, 3, 49, 24])
		qkv.grad_fn >>>>>>  None
--proj input shape =  torch.Size([1392, 49, 72])
--proj output shape =  torch.Size([1392, 49, 72])
------------WindowMSA - x_shape Output torch.Size([1392, 49, 72]) None
attn_windows.shape before merge =  torch.Size([1392, 49, 72])
x.shape before view =  torch.Size([1, 200, 334, 72])
ShiftWindowMSA = before view B, H * W, C =  1 66800 72
------------ShiftWindowMSA -  None
------------SwinBlock -  None
		block output:  torch.Size([1, 66800, 72])
------------SwinBlockSequence -  torch.Size([1, 66800, 72])
with_cp =  False
x.requires_grad =  False
ShiftWindowMSA = query view: B, L, C=  1 66800 72
	 hw_shape =  (200, 334)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([1392, 49, 72])
B=1392, N=49, C=72
the qkv mod: 72 192 Linear(in_features=72, out_features=192, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([1392, 49, 216])
NEW C= 72
qkv.shape =  torch.Size([1392, 49, 216])
qkv.flatten.shape =  torch.Size([14732928])
self.embed_dims =  192
+++++ reshaping ....  1392 49 3 self.num_heads= 3 24
1392 49 3 3 24  =  14732928
self.num_heads =  3
C =  72
qkv after reshape  torch.Size([1392, 49, 3, 3, 24])
qkv after permute  torch.Size([3, 1392, 3, 49, 24])
		qkv.grad_fn >>>>>>  None
--proj input shape =  torch.Size([1392, 49, 72])
--proj output shape =  torch.Size([1392, 49, 72])
------------WindowMSA - x_shape Output torch.Size([1392, 49, 72]) None
attn_windows.shape before merge =  torch.Size([1392, 49, 72])
x.shape before view =  torch.Size([1, 200, 334, 72])
ShiftWindowMSA = before view B, H * W, C =  1 66800 72
------------ShiftWindowMSA -  None
------------SwinBlock -  None
		block output:  torch.Size([1, 66800, 72])
------------SwinBlockSequence/downsample -  torch.Size([1, 16700, 144]) None
down_hw_shape =  (100, 167)
x.shape =  torch.Size([1, 16700, 144])
hw_shape =  (100, 167)
stage out.shape =  torch.Size([1, 66800, 72])
out_hw_shape =  (200, 334)
stage =  0  / self.out_indices =  (1, 2, 3)
self.num_features =  [72, 144, 288, 576]
------------SwinBlockSequence -  torch.Size([1, 16700, 144])
with_cp =  False
x.requires_grad =  False
ShiftWindowMSA = query view: B, L, C=  1 16700 144
	 hw_shape =  (100, 167)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([360, 49, 144])
B=360, N=49, C=144
the qkv mod: 144 384 Linear(in_features=144, out_features=384, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([360, 49, 432])
NEW C= 144
qkv.shape =  torch.Size([360, 49, 432])
qkv.flatten.shape =  torch.Size([7620480])
self.embed_dims =  384
+++++ reshaping ....  360 49 3 self.num_heads= 6 24
360 49 3 6 24  =  7620480
self.num_heads =  6
C =  144
qkv after reshape  torch.Size([360, 49, 3, 6, 24])
qkv after permute  torch.Size([3, 360, 6, 49, 24])
		qkv.grad_fn >>>>>>  None
--proj input shape =  torch.Size([360, 49, 144])
--proj output shape =  torch.Size([360, 49, 144])
------------WindowMSA - x_shape Output torch.Size([360, 49, 144]) None
attn_windows.shape before merge =  torch.Size([360, 49, 144])
x.shape before view =  torch.Size([1, 100, 167, 144])
ShiftWindowMSA = before view B, H * W, C =  1 16700 144
------------ShiftWindowMSA -  None
------------SwinBlock -  None
		block output:  torch.Size([1, 16700, 144])
------------SwinBlockSequence -  torch.Size([1, 16700, 144])
with_cp =  False
x.requires_grad =  False
ShiftWindowMSA = query view: B, L, C=  1 16700 144
	 hw_shape =  (100, 167)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([360, 49, 144])
B=360, N=49, C=144
the qkv mod: 144 384 Linear(in_features=144, out_features=384, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([360, 49, 432])
NEW C= 144
qkv.shape =  torch.Size([360, 49, 432])
qkv.flatten.shape =  torch.Size([7620480])
self.embed_dims =  384
+++++ reshaping ....  360 49 3 self.num_heads= 6 24
360 49 3 6 24  =  7620480
self.num_heads =  6
C =  144
qkv after reshape  torch.Size([360, 49, 3, 6, 24])
qkv after permute  torch.Size([3, 360, 6, 49, 24])
		qkv.grad_fn >>>>>>  None
--proj input shape =  torch.Size([360, 49, 144])
--proj output shape =  torch.Size([360, 49, 144])
------------WindowMSA - x_shape Output torch.Size([360, 49, 144]) None
attn_windows.shape before merge =  torch.Size([360, 49, 144])
x.shape before view =  torch.Size([1, 100, 167, 144])
ShiftWindowMSA = before view B, H * W, C =  1 16700 144
------------ShiftWindowMSA -  None
------------SwinBlock -  None
		block output:  torch.Size([1, 16700, 144])
------------SwinBlockSequence/downsample -  torch.Size([1, 4200, 288]) None
down_hw_shape =  (50, 84)
x.shape =  torch.Size([1, 4200, 288])
hw_shape =  (50, 84)
stage out.shape =  torch.Size([1, 16700, 144])
out_hw_shape =  (100, 167)
stage =  1  / self.out_indices =  (1, 2, 3)
self.num_features =  [72, 144, 288, 576]
stage:  1  --- norm_layer out.shape =  torch.Size([1, 16700, 144])
torch.Size([1, 144, 100, 167])
------------SwinBlockSequence -  torch.Size([1, 4200, 288])
with_cp =  False
x.requires_grad =  False
ShiftWindowMSA = query view: B, L, C=  1 4200 288
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 288])
B=96, N=49, C=288
the qkv mod: 288 768 Linear(in_features=288, out_features=768, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 864])
NEW C= 288
qkv.shape =  torch.Size([96, 49, 864])
qkv.flatten.shape =  torch.Size([4064256])
self.embed_dims =  768
+++++ reshaping ....  96 49 3 self.num_heads= 12 24
96 49 3 12 24  =  4064256
self.num_heads =  12
C =  288
qkv after reshape  torch.Size([96, 49, 3, 12, 24])
qkv after permute  torch.Size([3, 96, 12, 49, 24])
		qkv.grad_fn >>>>>>  None
--proj input shape =  torch.Size([96, 49, 288])
--proj output shape =  torch.Size([96, 49, 288])
------------WindowMSA - x_shape Output torch.Size([96, 49, 288]) None
attn_windows.shape before merge =  torch.Size([96, 49, 288])
x.shape before view =  torch.Size([1, 50, 84, 288])
ShiftWindowMSA = before view B, H * W, C =  1 4200 288
------------ShiftWindowMSA -  None
------------SwinBlock -  None
		block output:  torch.Size([1, 4200, 288])
------------SwinBlockSequence -  torch.Size([1, 4200, 288])
with_cp =  False
x.requires_grad =  False
ShiftWindowMSA = query view: B, L, C=  1 4200 288
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 288])
B=96, N=49, C=288
the qkv mod: 288 768 Linear(in_features=288, out_features=768, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 864])
NEW C= 288
qkv.shape =  torch.Size([96, 49, 864])
qkv.flatten.shape =  torch.Size([4064256])
self.embed_dims =  768
+++++ reshaping ....  96 49 3 self.num_heads= 12 24
96 49 3 12 24  =  4064256
self.num_heads =  12
C =  288
qkv after reshape  torch.Size([96, 49, 3, 12, 24])
qkv after permute  torch.Size([3, 96, 12, 49, 24])
		qkv.grad_fn >>>>>>  None
--proj input shape =  torch.Size([96, 49, 288])
--proj output shape =  torch.Size([96, 49, 288])
------------WindowMSA - x_shape Output torch.Size([96, 49, 288]) None
attn_windows.shape before merge =  torch.Size([96, 49, 288])
x.shape before view =  torch.Size([1, 50, 84, 288])
ShiftWindowMSA = before view B, H * W, C =  1 4200 288
------------ShiftWindowMSA -  None
------------SwinBlock -  None
		block output:  torch.Size([1, 4200, 288])
------------SwinBlockSequence -  torch.Size([1, 4200, 288])
with_cp =  False
x.requires_grad =  False
ShiftWindowMSA = query view: B, L, C=  1 4200 288
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 288])
B=96, N=49, C=288
the qkv mod: 288 768 Linear(in_features=288, out_features=768, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 864])
NEW C= 288
qkv.shape =  torch.Size([96, 49, 864])
qkv.flatten.shape =  torch.Size([4064256])
self.embed_dims =  768
+++++ reshaping ....  96 49 3 self.num_heads= 12 24
96 49 3 12 24  =  4064256
self.num_heads =  12
C =  288
qkv after reshape  torch.Size([96, 49, 3, 12, 24])
qkv after permute  torch.Size([3, 96, 12, 49, 24])
		qkv.grad_fn >>>>>>  None
--proj input shape =  torch.Size([96, 49, 288])
--proj output shape =  torch.Size([96, 49, 288])
------------WindowMSA - x_shape Output torch.Size([96, 49, 288]) None
attn_windows.shape before merge =  torch.Size([96, 49, 288])
x.shape before view =  torch.Size([1, 50, 84, 288])
ShiftWindowMSA = before view B, H * W, C =  1 4200 288
------------ShiftWindowMSA -  None
------------SwinBlock -  None
		block output:  torch.Size([1, 4200, 288])
------------SwinBlockSequence -  torch.Size([1, 4200, 288])
with_cp =  False
x.requires_grad =  False
ShiftWindowMSA = query view: B, L, C=  1 4200 288
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 288])
B=96, N=49, C=288
the qkv mod: 288 768 Linear(in_features=288, out_features=768, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 864])
NEW C= 288
qkv.shape =  torch.Size([96, 49, 864])
qkv.flatten.shape =  torch.Size([4064256])
self.embed_dims =  768
+++++ reshaping ....  96 49 3 self.num_heads= 12 24
96 49 3 12 24  =  4064256
self.num_heads =  12
C =  288
qkv after reshape  torch.Size([96, 49, 3, 12, 24])
qkv after permute  torch.Size([3, 96, 12, 49, 24])
		qkv.grad_fn >>>>>>  None
--proj input shape =  torch.Size([96, 49, 288])
--proj output shape =  torch.Size([96, 49, 288])
------------WindowMSA - x_shape Output torch.Size([96, 49, 288]) None
attn_windows.shape before merge =  torch.Size([96, 49, 288])
x.shape before view =  torch.Size([1, 50, 84, 288])
ShiftWindowMSA = before view B, H * W, C =  1 4200 288
------------ShiftWindowMSA -  None
------------SwinBlock -  None
		block output:  torch.Size([1, 4200, 288])
------------SwinBlockSequence -  torch.Size([1, 4200, 288])
with_cp =  False
x.requires_grad =  False
ShiftWindowMSA = query view: B, L, C=  1 4200 288
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 288])
B=96, N=49, C=288
the qkv mod: 288 768 Linear(in_features=288, out_features=768, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 864])
NEW C= 288
qkv.shape =  torch.Size([96, 49, 864])
qkv.flatten.shape =  torch.Size([4064256])
self.embed_dims =  768
+++++ reshaping ....  96 49 3 self.num_heads= 12 24
96 49 3 12 24  =  4064256
self.num_heads =  12
C =  288
qkv after reshape  torch.Size([96, 49, 3, 12, 24])
qkv after permute  torch.Size([3, 96, 12, 49, 24])
		qkv.grad_fn >>>>>>  None
--proj input shape =  torch.Size([96, 49, 288])
--proj output shape =  torch.Size([96, 49, 288])
------------WindowMSA - x_shape Output torch.Size([96, 49, 288]) None
attn_windows.shape before merge =  torch.Size([96, 49, 288])
x.shape before view =  torch.Size([1, 50, 84, 288])
ShiftWindowMSA = before view B, H * W, C =  1 4200 288
------------ShiftWindowMSA -  None
------------SwinBlock -  None
		block output:  torch.Size([1, 4200, 288])
------------SwinBlockSequence -  torch.Size([1, 4200, 288])
with_cp =  False
x.requires_grad =  False
ShiftWindowMSA = query view: B, L, C=  1 4200 288
	 hw_shape =  (50, 84)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([96, 49, 288])
B=96, N=49, C=288
the qkv mod: 288 768 Linear(in_features=288, out_features=768, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([96, 49, 864])
NEW C= 288
qkv.shape =  torch.Size([96, 49, 864])
qkv.flatten.shape =  torch.Size([4064256])
self.embed_dims =  768
+++++ reshaping ....  96 49 3 self.num_heads= 12 24
96 49 3 12 24  =  4064256
self.num_heads =  12
C =  288
qkv after reshape  torch.Size([96, 49, 3, 12, 24])
qkv after permute  torch.Size([3, 96, 12, 49, 24])
		qkv.grad_fn >>>>>>  None
--proj input shape =  torch.Size([96, 49, 288])
--proj output shape =  torch.Size([96, 49, 288])
------------WindowMSA - x_shape Output torch.Size([96, 49, 288]) None
attn_windows.shape before merge =  torch.Size([96, 49, 288])
x.shape before view =  torch.Size([1, 50, 84, 288])
ShiftWindowMSA = before view B, H * W, C =  1 4200 288
------------ShiftWindowMSA -  None
------------SwinBlock -  None
		block output:  torch.Size([1, 4200, 288])
------------SwinBlockSequence/downsample -  torch.Size([1, 1050, 576]) None
down_hw_shape =  (25, 42)
x.shape =  torch.Size([1, 1050, 576])
hw_shape =  (25, 42)
stage out.shape =  torch.Size([1, 4200, 288])
out_hw_shape =  (50, 84)
stage =  2  / self.out_indices =  (1, 2, 3)
self.num_features =  [72, 144, 288, 576]
stage:  2  --- norm_layer out.shape =  torch.Size([1, 4200, 288])
torch.Size([1, 288, 50, 84])
------------SwinBlockSequence -  torch.Size([1, 1050, 576])
with_cp =  False
x.requires_grad =  False
ShiftWindowMSA = query view: B, L, C=  1 1050 576
	 hw_shape =  (25, 42)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([24, 49, 576])
B=24, N=49, C=576
the qkv mod: 576 1536 Linear(in_features=576, out_features=1536, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([24, 49, 1728])
NEW C= 576
qkv.shape =  torch.Size([24, 49, 1728])
qkv.flatten.shape =  torch.Size([2032128])
self.embed_dims =  1536
+++++ reshaping ....  24 49 3 self.num_heads= 24 24
24 49 3 24 24  =  2032128
self.num_heads =  24
C =  576
qkv after reshape  torch.Size([24, 49, 3, 24, 24])
qkv after permute  torch.Size([3, 24, 24, 49, 24])
		qkv.grad_fn >>>>>>  None
--proj input shape =  torch.Size([24, 49, 576])
--proj output shape =  torch.Size([24, 49, 576])
------------WindowMSA - x_shape Output torch.Size([24, 49, 576]) None
attn_windows.shape before merge =  torch.Size([24, 49, 576])
x.shape before view =  torch.Size([1, 25, 42, 576])
ShiftWindowMSA = before view B, H * W, C =  1 1050 576
------------ShiftWindowMSA -  None
------------SwinBlock -  None
		block output:  torch.Size([1, 1050, 576])
------------SwinBlockSequence -  torch.Size([1, 1050, 576])
with_cp =  False
x.requires_grad =  False
ShiftWindowMSA = query view: B, L, C=  1 1050 576
	 hw_shape =  (25, 42)
	 self.window_size =  7
------------WindowMSA - x_shape Input torch.Size([24, 49, 576])
B=24, N=49, C=576
the qkv mod: 576 1536 Linear(in_features=576, out_features=1536, bias=True)
 	 debug out shape only for qkv linear:  torch.Size([24, 49, 1728])
NEW C= 576
qkv.shape =  torch.Size([24, 49, 1728])
qkv.flatten.shape =  torch.Size([2032128])
self.embed_dims =  1536
+++++ reshaping ....  24 49 3 self.num_heads= 24 24
24 49 3 24 24  =  2032128
self.num_heads =  24
C =  576
qkv after reshape  torch.Size([24, 49, 3, 24, 24])
qkv after permute  torch.Size([3, 24, 24, 49, 24])
		qkv.grad_fn >>>>>>  None
--proj input shape =  torch.Size([24, 49, 576])
--proj output shape =  torch.Size([24, 49, 576])
------------WindowMSA - x_shape Output torch.Size([24, 49, 576]) None
attn_windows.shape before merge =  torch.Size([24, 49, 576])
x.shape before view =  torch.Size([1, 25, 42, 576])
ShiftWindowMSA = before view B, H * W, C =  1 1050 576
------------ShiftWindowMSA -  None
------------SwinBlock -  None
		block output:  torch.Size([1, 1050, 576])
x.shape =  torch.Size([1, 1050, 576])
hw_shape =  (25, 42)
stage out.shape =  torch.Size([1, 1050, 576])
out_hw_shape =  (25, 42)
stage =  3  / self.out_indices =  (1, 2, 3)
self.num_features =  [72, 144, 288, 576]
stage:  3  --- norm_layer out.shape =  torch.Size([1, 1050, 576])
torch.Size([1, 576, 25, 42])
------------SwinTransformer -  [None, None, None]



[torch.Size([1, 144, 100, 167]), torch.Size([1, 288, 50, 84]), torch.Size([1, 576, 25, 42])]
Base MACs: 54.894878 G, Pruned MACs: 30.995017 G
Base Params: 29.637114 M, Pruned Params: 16.694850 M
